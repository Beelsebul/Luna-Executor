{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.0,
  "eval_steps": 500,
  "global_step": 1911000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015698587127158557,
      "grad_norm": 6.659707546234131,
      "learning_rate": 4.999901883830455e-05,
      "loss": 1.9381,
      "step": 100
    },
    {
      "epoch": 0.0031397174254317113,
      "grad_norm": 6.4614338874816895,
      "learning_rate": 4.9998037676609104e-05,
      "loss": 1.6319,
      "step": 200
    },
    {
      "epoch": 0.004709576138147566,
      "grad_norm": 5.07819128036499,
      "learning_rate": 4.999705651491366e-05,
      "loss": 1.4807,
      "step": 300
    },
    {
      "epoch": 0.006279434850863423,
      "grad_norm": 6.320569038391113,
      "learning_rate": 4.999607535321821e-05,
      "loss": 1.4656,
      "step": 400
    },
    {
      "epoch": 0.007849293563579277,
      "grad_norm": 6.3309149742126465,
      "learning_rate": 4.999509419152277e-05,
      "loss": 1.3774,
      "step": 500
    },
    {
      "epoch": 0.009419152276295133,
      "grad_norm": 7.334023952484131,
      "learning_rate": 4.9994113029827315e-05,
      "loss": 1.3172,
      "step": 600
    },
    {
      "epoch": 0.01098901098901099,
      "grad_norm": 6.471298694610596,
      "learning_rate": 4.999313186813187e-05,
      "loss": 1.3362,
      "step": 700
    },
    {
      "epoch": 0.012558869701726845,
      "grad_norm": 5.702239990234375,
      "learning_rate": 4.9992150706436424e-05,
      "loss": 1.2417,
      "step": 800
    },
    {
      "epoch": 0.0141287284144427,
      "grad_norm": 5.828788757324219,
      "learning_rate": 4.9991169544740975e-05,
      "loss": 1.1856,
      "step": 900
    },
    {
      "epoch": 0.015698587127158554,
      "grad_norm": 5.186532974243164,
      "learning_rate": 4.9990188383045526e-05,
      "loss": 1.2562,
      "step": 1000
    },
    {
      "epoch": 0.01726844583987441,
      "grad_norm": 6.159421443939209,
      "learning_rate": 4.9989207221350084e-05,
      "loss": 1.2175,
      "step": 1100
    },
    {
      "epoch": 0.018838304552590265,
      "grad_norm": 6.110106468200684,
      "learning_rate": 4.9988226059654635e-05,
      "loss": 1.2268,
      "step": 1200
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 5.403290748596191,
      "learning_rate": 4.9987244897959185e-05,
      "loss": 1.1708,
      "step": 1300
    },
    {
      "epoch": 0.02197802197802198,
      "grad_norm": 4.6892194747924805,
      "learning_rate": 4.9986263736263736e-05,
      "loss": 1.1739,
      "step": 1400
    },
    {
      "epoch": 0.023547880690737835,
      "grad_norm": 4.931613922119141,
      "learning_rate": 4.9985282574568294e-05,
      "loss": 1.1056,
      "step": 1500
    },
    {
      "epoch": 0.02511773940345369,
      "grad_norm": 5.904727935791016,
      "learning_rate": 4.998430141287284e-05,
      "loss": 1.1675,
      "step": 1600
    },
    {
      "epoch": 0.026687598116169546,
      "grad_norm": 5.232728958129883,
      "learning_rate": 4.9983320251177396e-05,
      "loss": 1.1422,
      "step": 1700
    },
    {
      "epoch": 0.0282574568288854,
      "grad_norm": 6.030940532684326,
      "learning_rate": 4.998233908948195e-05,
      "loss": 1.1935,
      "step": 1800
    },
    {
      "epoch": 0.029827315541601257,
      "grad_norm": 5.702332496643066,
      "learning_rate": 4.9981357927786505e-05,
      "loss": 1.1456,
      "step": 1900
    },
    {
      "epoch": 0.03139717425431711,
      "grad_norm": 4.545316696166992,
      "learning_rate": 4.9980376766091056e-05,
      "loss": 1.1447,
      "step": 2000
    },
    {
      "epoch": 0.03296703296703297,
      "grad_norm": 5.262295722961426,
      "learning_rate": 4.997939560439561e-05,
      "loss": 1.1266,
      "step": 2100
    },
    {
      "epoch": 0.03453689167974882,
      "grad_norm": 5.472929000854492,
      "learning_rate": 4.997841444270016e-05,
      "loss": 1.0835,
      "step": 2200
    },
    {
      "epoch": 0.03610675039246468,
      "grad_norm": 5.505349636077881,
      "learning_rate": 4.997743328100471e-05,
      "loss": 1.1042,
      "step": 2300
    },
    {
      "epoch": 0.03767660910518053,
      "grad_norm": 4.782943248748779,
      "learning_rate": 4.9976452119309267e-05,
      "loss": 1.1291,
      "step": 2400
    },
    {
      "epoch": 0.03924646781789639,
      "grad_norm": 5.045526027679443,
      "learning_rate": 4.997547095761382e-05,
      "loss": 1.0765,
      "step": 2500
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 6.91735315322876,
      "learning_rate": 4.9974489795918375e-05,
      "loss": 1.0742,
      "step": 2600
    },
    {
      "epoch": 0.0423861852433281,
      "grad_norm": 4.927852630615234,
      "learning_rate": 4.997350863422292e-05,
      "loss": 1.1132,
      "step": 2700
    },
    {
      "epoch": 0.04395604395604396,
      "grad_norm": 5.663850784301758,
      "learning_rate": 4.997252747252748e-05,
      "loss": 1.1057,
      "step": 2800
    },
    {
      "epoch": 0.04552590266875981,
      "grad_norm": 6.098921298980713,
      "learning_rate": 4.997154631083203e-05,
      "loss": 1.1578,
      "step": 2900
    },
    {
      "epoch": 0.04709576138147567,
      "grad_norm": 5.214883804321289,
      "learning_rate": 4.997056514913658e-05,
      "loss": 1.06,
      "step": 3000
    },
    {
      "epoch": 0.04866562009419152,
      "grad_norm": 4.702010631561279,
      "learning_rate": 4.996958398744113e-05,
      "loss": 1.0542,
      "step": 3100
    },
    {
      "epoch": 0.05023547880690738,
      "grad_norm": 3.4948205947875977,
      "learning_rate": 4.996860282574569e-05,
      "loss": 1.0913,
      "step": 3200
    },
    {
      "epoch": 0.05180533751962323,
      "grad_norm": 5.717295169830322,
      "learning_rate": 4.996762166405024e-05,
      "loss": 1.1105,
      "step": 3300
    },
    {
      "epoch": 0.05337519623233909,
      "grad_norm": 4.8234429359436035,
      "learning_rate": 4.996664050235479e-05,
      "loss": 1.0403,
      "step": 3400
    },
    {
      "epoch": 0.054945054945054944,
      "grad_norm": 5.3488688468933105,
      "learning_rate": 4.996565934065934e-05,
      "loss": 1.0385,
      "step": 3500
    },
    {
      "epoch": 0.0565149136577708,
      "grad_norm": 4.841887950897217,
      "learning_rate": 4.99646781789639e-05,
      "loss": 1.0418,
      "step": 3600
    },
    {
      "epoch": 0.058084772370486655,
      "grad_norm": 4.999855041503906,
      "learning_rate": 4.996369701726844e-05,
      "loss": 1.0841,
      "step": 3700
    },
    {
      "epoch": 0.059654631083202514,
      "grad_norm": 5.906175136566162,
      "learning_rate": 4.9962715855573e-05,
      "loss": 1.0242,
      "step": 3800
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 3.356689214706421,
      "learning_rate": 4.996173469387755e-05,
      "loss": 1.0333,
      "step": 3900
    },
    {
      "epoch": 0.06279434850863422,
      "grad_norm": 4.593007564544678,
      "learning_rate": 4.996075353218211e-05,
      "loss": 1.0238,
      "step": 4000
    },
    {
      "epoch": 0.06436420722135008,
      "grad_norm": 4.568149566650391,
      "learning_rate": 4.995977237048666e-05,
      "loss": 1.0762,
      "step": 4100
    },
    {
      "epoch": 0.06593406593406594,
      "grad_norm": 5.709305763244629,
      "learning_rate": 4.995879120879121e-05,
      "loss": 1.0057,
      "step": 4200
    },
    {
      "epoch": 0.06750392464678179,
      "grad_norm": 6.071099281311035,
      "learning_rate": 4.995781004709576e-05,
      "loss": 0.9806,
      "step": 4300
    },
    {
      "epoch": 0.06907378335949764,
      "grad_norm": 5.7528862953186035,
      "learning_rate": 4.995682888540031e-05,
      "loss": 1.0852,
      "step": 4400
    },
    {
      "epoch": 0.0706436420722135,
      "grad_norm": 5.44267463684082,
      "learning_rate": 4.995584772370487e-05,
      "loss": 1.0103,
      "step": 4500
    },
    {
      "epoch": 0.07221350078492936,
      "grad_norm": 6.142986297607422,
      "learning_rate": 4.995486656200942e-05,
      "loss": 1.0668,
      "step": 4600
    },
    {
      "epoch": 0.07378335949764521,
      "grad_norm": 6.12429141998291,
      "learning_rate": 4.995388540031398e-05,
      "loss": 0.9911,
      "step": 4700
    },
    {
      "epoch": 0.07535321821036106,
      "grad_norm": 4.579387187957764,
      "learning_rate": 4.9952904238618524e-05,
      "loss": 1.0569,
      "step": 4800
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 4.637796878814697,
      "learning_rate": 4.995192307692308e-05,
      "loss": 1.0476,
      "step": 4900
    },
    {
      "epoch": 0.07849293563579278,
      "grad_norm": 5.085638999938965,
      "learning_rate": 4.995094191522763e-05,
      "loss": 1.0266,
      "step": 5000
    },
    {
      "epoch": 0.08006279434850863,
      "grad_norm": 4.370875358581543,
      "learning_rate": 4.9949960753532184e-05,
      "loss": 1.008,
      "step": 5100
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 5.739015102386475,
      "learning_rate": 4.9948979591836735e-05,
      "loss": 1.0262,
      "step": 5200
    },
    {
      "epoch": 0.08320251177394035,
      "grad_norm": 5.2862725257873535,
      "learning_rate": 4.994799843014129e-05,
      "loss": 1.028,
      "step": 5300
    },
    {
      "epoch": 0.0847723704866562,
      "grad_norm": 5.274377822875977,
      "learning_rate": 4.9947017268445843e-05,
      "loss": 1.0119,
      "step": 5400
    },
    {
      "epoch": 0.08634222919937205,
      "grad_norm": 6.166141510009766,
      "learning_rate": 4.9946036106750394e-05,
      "loss": 1.0437,
      "step": 5500
    },
    {
      "epoch": 0.08791208791208792,
      "grad_norm": 4.3717241287231445,
      "learning_rate": 4.9945054945054945e-05,
      "loss": 1.0207,
      "step": 5600
    },
    {
      "epoch": 0.08948194662480377,
      "grad_norm": 5.4477314949035645,
      "learning_rate": 4.99440737833595e-05,
      "loss": 1.0515,
      "step": 5700
    },
    {
      "epoch": 0.09105180533751962,
      "grad_norm": 4.570425987243652,
      "learning_rate": 4.994309262166405e-05,
      "loss": 0.9744,
      "step": 5800
    },
    {
      "epoch": 0.09262166405023547,
      "grad_norm": 4.4673075675964355,
      "learning_rate": 4.9942111459968605e-05,
      "loss": 1.0611,
      "step": 5900
    },
    {
      "epoch": 0.09419152276295134,
      "grad_norm": 5.835231304168701,
      "learning_rate": 4.9941130298273156e-05,
      "loss": 1.0347,
      "step": 6000
    },
    {
      "epoch": 0.09576138147566719,
      "grad_norm": 5.0285210609436035,
      "learning_rate": 4.9940149136577714e-05,
      "loss": 0.9548,
      "step": 6100
    },
    {
      "epoch": 0.09733124018838304,
      "grad_norm": 5.09715461730957,
      "learning_rate": 4.9939167974882265e-05,
      "loss": 0.999,
      "step": 6200
    },
    {
      "epoch": 0.0989010989010989,
      "grad_norm": 3.70473051071167,
      "learning_rate": 4.9938186813186816e-05,
      "loss": 0.9927,
      "step": 6300
    },
    {
      "epoch": 0.10047095761381476,
      "grad_norm": 4.671661853790283,
      "learning_rate": 4.993720565149137e-05,
      "loss": 0.9879,
      "step": 6400
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 4.0113959312438965,
      "learning_rate": 4.993622448979592e-05,
      "loss": 0.951,
      "step": 6500
    },
    {
      "epoch": 0.10361067503924647,
      "grad_norm": 5.585811614990234,
      "learning_rate": 4.9935243328100476e-05,
      "loss": 0.9578,
      "step": 6600
    },
    {
      "epoch": 0.10518053375196232,
      "grad_norm": 5.055874824523926,
      "learning_rate": 4.9934262166405027e-05,
      "loss": 0.9759,
      "step": 6700
    },
    {
      "epoch": 0.10675039246467818,
      "grad_norm": 5.564075469970703,
      "learning_rate": 4.9933281004709584e-05,
      "loss": 1.0407,
      "step": 6800
    },
    {
      "epoch": 0.10832025117739404,
      "grad_norm": 6.267728328704834,
      "learning_rate": 4.993229984301413e-05,
      "loss": 0.9686,
      "step": 6900
    },
    {
      "epoch": 0.10989010989010989,
      "grad_norm": 4.540794849395752,
      "learning_rate": 4.9931318681318686e-05,
      "loss": 0.9805,
      "step": 7000
    },
    {
      "epoch": 0.11145996860282574,
      "grad_norm": 4.743518352508545,
      "learning_rate": 4.993033751962324e-05,
      "loss": 0.9225,
      "step": 7100
    },
    {
      "epoch": 0.1130298273155416,
      "grad_norm": 6.597959041595459,
      "learning_rate": 4.992935635792779e-05,
      "loss": 0.9447,
      "step": 7200
    },
    {
      "epoch": 0.11459968602825746,
      "grad_norm": 5.192346572875977,
      "learning_rate": 4.992837519623234e-05,
      "loss": 0.9867,
      "step": 7300
    },
    {
      "epoch": 0.11616954474097331,
      "grad_norm": 4.73753547668457,
      "learning_rate": 4.99273940345369e-05,
      "loss": 0.9568,
      "step": 7400
    },
    {
      "epoch": 0.11773940345368916,
      "grad_norm": 4.9867777824401855,
      "learning_rate": 4.992641287284145e-05,
      "loss": 0.9803,
      "step": 7500
    },
    {
      "epoch": 0.11930926216640503,
      "grad_norm": 4.228918075561523,
      "learning_rate": 4.9925431711146e-05,
      "loss": 0.9967,
      "step": 7600
    },
    {
      "epoch": 0.12087912087912088,
      "grad_norm": 4.723293304443359,
      "learning_rate": 4.992445054945055e-05,
      "loss": 1.0411,
      "step": 7700
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 4.168940544128418,
      "learning_rate": 4.992346938775511e-05,
      "loss": 0.9997,
      "step": 7800
    },
    {
      "epoch": 0.12401883830455258,
      "grad_norm": 3.8881618976593018,
      "learning_rate": 4.992248822605965e-05,
      "loss": 1.0093,
      "step": 7900
    },
    {
      "epoch": 0.12558869701726844,
      "grad_norm": 4.23741340637207,
      "learning_rate": 4.992150706436421e-05,
      "loss": 0.9958,
      "step": 8000
    },
    {
      "epoch": 0.1271585557299843,
      "grad_norm": 4.415288925170898,
      "learning_rate": 4.992052590266876e-05,
      "loss": 0.9785,
      "step": 8100
    },
    {
      "epoch": 0.12872841444270017,
      "grad_norm": 5.500281810760498,
      "learning_rate": 4.991954474097332e-05,
      "loss": 0.9796,
      "step": 8200
    },
    {
      "epoch": 0.130298273155416,
      "grad_norm": 4.616672515869141,
      "learning_rate": 4.991856357927787e-05,
      "loss": 0.9766,
      "step": 8300
    },
    {
      "epoch": 0.13186813186813187,
      "grad_norm": 4.883312702178955,
      "learning_rate": 4.991758241758242e-05,
      "loss": 0.9655,
      "step": 8400
    },
    {
      "epoch": 0.13343799058084774,
      "grad_norm": 4.402161121368408,
      "learning_rate": 4.991660125588697e-05,
      "loss": 0.948,
      "step": 8500
    },
    {
      "epoch": 0.13500784929356358,
      "grad_norm": 4.585582733154297,
      "learning_rate": 4.991562009419152e-05,
      "loss": 0.9749,
      "step": 8600
    },
    {
      "epoch": 0.13657770800627944,
      "grad_norm": 5.074254512786865,
      "learning_rate": 4.991463893249608e-05,
      "loss": 0.9577,
      "step": 8700
    },
    {
      "epoch": 0.13814756671899528,
      "grad_norm": 4.685644149780273,
      "learning_rate": 4.991365777080063e-05,
      "loss": 0.9308,
      "step": 8800
    },
    {
      "epoch": 0.13971742543171115,
      "grad_norm": 5.164050579071045,
      "learning_rate": 4.991267660910519e-05,
      "loss": 0.9557,
      "step": 8900
    },
    {
      "epoch": 0.141287284144427,
      "grad_norm": 4.969560146331787,
      "learning_rate": 4.991169544740973e-05,
      "loss": 0.9281,
      "step": 9000
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 3.916684627532959,
      "learning_rate": 4.991071428571429e-05,
      "loss": 0.952,
      "step": 9100
    },
    {
      "epoch": 0.14442700156985872,
      "grad_norm": 5.325727462768555,
      "learning_rate": 4.990973312401884e-05,
      "loss": 1.0091,
      "step": 9200
    },
    {
      "epoch": 0.14599686028257458,
      "grad_norm": 4.465225696563721,
      "learning_rate": 4.990875196232339e-05,
      "loss": 1.0047,
      "step": 9300
    },
    {
      "epoch": 0.14756671899529042,
      "grad_norm": 5.328545570373535,
      "learning_rate": 4.9907770800627944e-05,
      "loss": 0.9548,
      "step": 9400
    },
    {
      "epoch": 0.14913657770800628,
      "grad_norm": 3.948073625564575,
      "learning_rate": 4.99067896389325e-05,
      "loss": 1.015,
      "step": 9500
    },
    {
      "epoch": 0.15070643642072212,
      "grad_norm": 4.98942756652832,
      "learning_rate": 4.990580847723705e-05,
      "loss": 0.9217,
      "step": 9600
    },
    {
      "epoch": 0.152276295133438,
      "grad_norm": 5.661880970001221,
      "learning_rate": 4.9904827315541603e-05,
      "loss": 0.9852,
      "step": 9700
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 3.887681722640991,
      "learning_rate": 4.9903846153846154e-05,
      "loss": 0.9743,
      "step": 9800
    },
    {
      "epoch": 0.1554160125588697,
      "grad_norm": 4.744401931762695,
      "learning_rate": 4.990286499215071e-05,
      "loss": 1.0173,
      "step": 9900
    },
    {
      "epoch": 0.15698587127158556,
      "grad_norm": 2.733383893966675,
      "learning_rate": 4.9901883830455256e-05,
      "loss": 0.9295,
      "step": 10000
    },
    {
      "epoch": 0.15855572998430142,
      "grad_norm": 4.4868083000183105,
      "learning_rate": 4.9900902668759814e-05,
      "loss": 0.9321,
      "step": 10100
    },
    {
      "epoch": 0.16012558869701726,
      "grad_norm": 4.484259128570557,
      "learning_rate": 4.9899921507064365e-05,
      "loss": 0.9287,
      "step": 10200
    },
    {
      "epoch": 0.16169544740973313,
      "grad_norm": 3.9875502586364746,
      "learning_rate": 4.989894034536892e-05,
      "loss": 0.9143,
      "step": 10300
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 4.977200031280518,
      "learning_rate": 4.9897959183673474e-05,
      "loss": 0.9312,
      "step": 10400
    },
    {
      "epoch": 0.16483516483516483,
      "grad_norm": 6.375011444091797,
      "learning_rate": 4.9896978021978025e-05,
      "loss": 0.8746,
      "step": 10500
    },
    {
      "epoch": 0.1664050235478807,
      "grad_norm": 5.114082336425781,
      "learning_rate": 4.9895996860282576e-05,
      "loss": 0.974,
      "step": 10600
    },
    {
      "epoch": 0.16797488226059654,
      "grad_norm": 3.5661051273345947,
      "learning_rate": 4.989501569858713e-05,
      "loss": 0.9111,
      "step": 10700
    },
    {
      "epoch": 0.1695447409733124,
      "grad_norm": 4.258633136749268,
      "learning_rate": 4.9894034536891685e-05,
      "loss": 0.8987,
      "step": 10800
    },
    {
      "epoch": 0.17111459968602827,
      "grad_norm": 5.159017562866211,
      "learning_rate": 4.9893053375196236e-05,
      "loss": 0.9102,
      "step": 10900
    },
    {
      "epoch": 0.1726844583987441,
      "grad_norm": 2.838740110397339,
      "learning_rate": 4.989207221350079e-05,
      "loss": 0.9552,
      "step": 11000
    },
    {
      "epoch": 0.17425431711145997,
      "grad_norm": 3.9449124336242676,
      "learning_rate": 4.989109105180534e-05,
      "loss": 0.8766,
      "step": 11100
    },
    {
      "epoch": 0.17582417582417584,
      "grad_norm": 4.341366291046143,
      "learning_rate": 4.9890109890109895e-05,
      "loss": 0.9091,
      "step": 11200
    },
    {
      "epoch": 0.17739403453689168,
      "grad_norm": 4.573445796966553,
      "learning_rate": 4.9889128728414446e-05,
      "loss": 0.9536,
      "step": 11300
    },
    {
      "epoch": 0.17896389324960754,
      "grad_norm": 4.362887859344482,
      "learning_rate": 4.9888147566719e-05,
      "loss": 0.9913,
      "step": 11400
    },
    {
      "epoch": 0.18053375196232338,
      "grad_norm": 4.592761516571045,
      "learning_rate": 4.988716640502355e-05,
      "loss": 0.9698,
      "step": 11500
    },
    {
      "epoch": 0.18210361067503925,
      "grad_norm": 5.4487152099609375,
      "learning_rate": 4.9886185243328106e-05,
      "loss": 0.966,
      "step": 11600
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 4.504117012023926,
      "learning_rate": 4.988520408163265e-05,
      "loss": 0.9007,
      "step": 11700
    },
    {
      "epoch": 0.18524332810047095,
      "grad_norm": 4.917527675628662,
      "learning_rate": 4.988422291993721e-05,
      "loss": 0.9413,
      "step": 11800
    },
    {
      "epoch": 0.18681318681318682,
      "grad_norm": 5.293863773345947,
      "learning_rate": 4.988324175824176e-05,
      "loss": 0.9268,
      "step": 11900
    },
    {
      "epoch": 0.18838304552590268,
      "grad_norm": 5.429148197174072,
      "learning_rate": 4.988226059654632e-05,
      "loss": 0.952,
      "step": 12000
    },
    {
      "epoch": 0.18995290423861852,
      "grad_norm": 5.2504754066467285,
      "learning_rate": 4.988127943485086e-05,
      "loss": 0.8827,
      "step": 12100
    },
    {
      "epoch": 0.19152276295133439,
      "grad_norm": 5.409339904785156,
      "learning_rate": 4.988029827315542e-05,
      "loss": 0.9488,
      "step": 12200
    },
    {
      "epoch": 0.19309262166405022,
      "grad_norm": 4.620992660522461,
      "learning_rate": 4.987931711145997e-05,
      "loss": 0.9021,
      "step": 12300
    },
    {
      "epoch": 0.1946624803767661,
      "grad_norm": 4.018003940582275,
      "learning_rate": 4.987833594976452e-05,
      "loss": 0.9456,
      "step": 12400
    },
    {
      "epoch": 0.19623233908948196,
      "grad_norm": 4.632608890533447,
      "learning_rate": 4.987735478806908e-05,
      "loss": 1.0144,
      "step": 12500
    },
    {
      "epoch": 0.1978021978021978,
      "grad_norm": 3.662468910217285,
      "learning_rate": 4.987637362637363e-05,
      "loss": 0.9134,
      "step": 12600
    },
    {
      "epoch": 0.19937205651491366,
      "grad_norm": 4.084578990936279,
      "learning_rate": 4.987539246467818e-05,
      "loss": 0.9598,
      "step": 12700
    },
    {
      "epoch": 0.20094191522762953,
      "grad_norm": 5.414217472076416,
      "learning_rate": 4.987441130298273e-05,
      "loss": 0.8866,
      "step": 12800
    },
    {
      "epoch": 0.20251177394034536,
      "grad_norm": 5.19182014465332,
      "learning_rate": 4.987343014128729e-05,
      "loss": 0.93,
      "step": 12900
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 4.727225303649902,
      "learning_rate": 4.987244897959184e-05,
      "loss": 0.9146,
      "step": 13000
    },
    {
      "epoch": 0.20565149136577707,
      "grad_norm": 4.905632019042969,
      "learning_rate": 4.987146781789639e-05,
      "loss": 0.9295,
      "step": 13100
    },
    {
      "epoch": 0.20722135007849293,
      "grad_norm": 5.132832050323486,
      "learning_rate": 4.987048665620094e-05,
      "loss": 0.9422,
      "step": 13200
    },
    {
      "epoch": 0.2087912087912088,
      "grad_norm": 3.657778263092041,
      "learning_rate": 4.98695054945055e-05,
      "loss": 0.9411,
      "step": 13300
    },
    {
      "epoch": 0.21036106750392464,
      "grad_norm": 4.934663772583008,
      "learning_rate": 4.986852433281005e-05,
      "loss": 0.8891,
      "step": 13400
    },
    {
      "epoch": 0.2119309262166405,
      "grad_norm": 5.589333534240723,
      "learning_rate": 4.98675431711146e-05,
      "loss": 0.9149,
      "step": 13500
    },
    {
      "epoch": 0.21350078492935637,
      "grad_norm": 4.431686878204346,
      "learning_rate": 4.986656200941915e-05,
      "loss": 0.8921,
      "step": 13600
    },
    {
      "epoch": 0.2150706436420722,
      "grad_norm": 5.177498817443848,
      "learning_rate": 4.986558084772371e-05,
      "loss": 0.9632,
      "step": 13700
    },
    {
      "epoch": 0.21664050235478807,
      "grad_norm": 3.815922260284424,
      "learning_rate": 4.9864599686028255e-05,
      "loss": 0.8683,
      "step": 13800
    },
    {
      "epoch": 0.21821036106750394,
      "grad_norm": 5.332522869110107,
      "learning_rate": 4.986361852433281e-05,
      "loss": 0.9204,
      "step": 13900
    },
    {
      "epoch": 0.21978021978021978,
      "grad_norm": 4.408043384552002,
      "learning_rate": 4.9862637362637363e-05,
      "loss": 0.8719,
      "step": 14000
    },
    {
      "epoch": 0.22135007849293564,
      "grad_norm": 4.757442474365234,
      "learning_rate": 4.986165620094192e-05,
      "loss": 0.9579,
      "step": 14100
    },
    {
      "epoch": 0.22291993720565148,
      "grad_norm": 4.7385454177856445,
      "learning_rate": 4.9860675039246465e-05,
      "loss": 0.8844,
      "step": 14200
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 5.729391574859619,
      "learning_rate": 4.985969387755102e-05,
      "loss": 0.9076,
      "step": 14300
    },
    {
      "epoch": 0.2260596546310832,
      "grad_norm": 5.627930164337158,
      "learning_rate": 4.9858712715855574e-05,
      "loss": 0.9659,
      "step": 14400
    },
    {
      "epoch": 0.22762951334379905,
      "grad_norm": 4.926706790924072,
      "learning_rate": 4.9857731554160125e-05,
      "loss": 0.8789,
      "step": 14500
    },
    {
      "epoch": 0.22919937205651492,
      "grad_norm": 4.708712577819824,
      "learning_rate": 4.985675039246468e-05,
      "loss": 0.932,
      "step": 14600
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 4.544669151306152,
      "learning_rate": 4.9855769230769234e-05,
      "loss": 0.9065,
      "step": 14700
    },
    {
      "epoch": 0.23233908948194662,
      "grad_norm": 4.104889869689941,
      "learning_rate": 4.9854788069073785e-05,
      "loss": 0.836,
      "step": 14800
    },
    {
      "epoch": 0.23390894819466249,
      "grad_norm": 5.149326324462891,
      "learning_rate": 4.9853806907378336e-05,
      "loss": 0.9297,
      "step": 14900
    },
    {
      "epoch": 0.23547880690737832,
      "grad_norm": 5.566789150238037,
      "learning_rate": 4.9852825745682894e-05,
      "loss": 0.922,
      "step": 15000
    },
    {
      "epoch": 0.2370486656200942,
      "grad_norm": 5.695842266082764,
      "learning_rate": 4.9851844583987445e-05,
      "loss": 0.8697,
      "step": 15100
    },
    {
      "epoch": 0.23861852433281006,
      "grad_norm": 4.891533851623535,
      "learning_rate": 4.9850863422291996e-05,
      "loss": 0.8823,
      "step": 15200
    },
    {
      "epoch": 0.2401883830455259,
      "grad_norm": 4.352617263793945,
      "learning_rate": 4.9849882260596546e-05,
      "loss": 0.9237,
      "step": 15300
    },
    {
      "epoch": 0.24175824175824176,
      "grad_norm": 4.296303749084473,
      "learning_rate": 4.9848901098901104e-05,
      "loss": 0.8457,
      "step": 15400
    },
    {
      "epoch": 0.24332810047095763,
      "grad_norm": 5.71793794631958,
      "learning_rate": 4.9847919937205655e-05,
      "loss": 0.9147,
      "step": 15500
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 4.307261943817139,
      "learning_rate": 4.9846938775510206e-05,
      "loss": 0.9239,
      "step": 15600
    },
    {
      "epoch": 0.24646781789638933,
      "grad_norm": 4.448241710662842,
      "learning_rate": 4.984595761381476e-05,
      "loss": 0.9211,
      "step": 15700
    },
    {
      "epoch": 0.24803767660910517,
      "grad_norm": 3.9853618144989014,
      "learning_rate": 4.9844976452119315e-05,
      "loss": 0.8466,
      "step": 15800
    },
    {
      "epoch": 0.24960753532182103,
      "grad_norm": 5.055346965789795,
      "learning_rate": 4.984399529042386e-05,
      "loss": 0.8896,
      "step": 15900
    },
    {
      "epoch": 0.25117739403453687,
      "grad_norm": 3.9759793281555176,
      "learning_rate": 4.984301412872842e-05,
      "loss": 0.8462,
      "step": 16000
    },
    {
      "epoch": 0.25274725274725274,
      "grad_norm": 5.231631755828857,
      "learning_rate": 4.984203296703297e-05,
      "loss": 0.9114,
      "step": 16100
    },
    {
      "epoch": 0.2543171114599686,
      "grad_norm": 4.687506675720215,
      "learning_rate": 4.9841051805337526e-05,
      "loss": 0.8988,
      "step": 16200
    },
    {
      "epoch": 0.25588697017268447,
      "grad_norm": 4.284743309020996,
      "learning_rate": 4.984007064364207e-05,
      "loss": 0.8755,
      "step": 16300
    },
    {
      "epoch": 0.25745682888540034,
      "grad_norm": 4.995612144470215,
      "learning_rate": 4.983908948194663e-05,
      "loss": 0.8886,
      "step": 16400
    },
    {
      "epoch": 0.25902668759811615,
      "grad_norm": 4.649316787719727,
      "learning_rate": 4.983810832025118e-05,
      "loss": 0.9014,
      "step": 16500
    },
    {
      "epoch": 0.260596546310832,
      "grad_norm": 4.561774730682373,
      "learning_rate": 4.983712715855573e-05,
      "loss": 0.9278,
      "step": 16600
    },
    {
      "epoch": 0.2621664050235479,
      "grad_norm": 4.501826763153076,
      "learning_rate": 4.983614599686029e-05,
      "loss": 0.8854,
      "step": 16700
    },
    {
      "epoch": 0.26373626373626374,
      "grad_norm": 5.7751665115356445,
      "learning_rate": 4.983516483516484e-05,
      "loss": 0.9376,
      "step": 16800
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 4.421820640563965,
      "learning_rate": 4.983418367346939e-05,
      "loss": 0.9228,
      "step": 16900
    },
    {
      "epoch": 0.2668759811616955,
      "grad_norm": 4.034546852111816,
      "learning_rate": 4.983320251177394e-05,
      "loss": 0.8934,
      "step": 17000
    },
    {
      "epoch": 0.2684458398744113,
      "grad_norm": 4.549082279205322,
      "learning_rate": 4.98322213500785e-05,
      "loss": 0.9072,
      "step": 17100
    },
    {
      "epoch": 0.27001569858712715,
      "grad_norm": 5.580557823181152,
      "learning_rate": 4.983124018838305e-05,
      "loss": 0.8756,
      "step": 17200
    },
    {
      "epoch": 0.271585557299843,
      "grad_norm": 4.75112247467041,
      "learning_rate": 4.98302590266876e-05,
      "loss": 0.8914,
      "step": 17300
    },
    {
      "epoch": 0.2731554160125589,
      "grad_norm": 4.95440673828125,
      "learning_rate": 4.982927786499215e-05,
      "loss": 0.8631,
      "step": 17400
    },
    {
      "epoch": 0.27472527472527475,
      "grad_norm": 5.567151069641113,
      "learning_rate": 4.982829670329671e-05,
      "loss": 0.8929,
      "step": 17500
    },
    {
      "epoch": 0.27629513343799056,
      "grad_norm": 4.201920509338379,
      "learning_rate": 4.982731554160126e-05,
      "loss": 0.8451,
      "step": 17600
    },
    {
      "epoch": 0.2778649921507064,
      "grad_norm": 5.08156156539917,
      "learning_rate": 4.982633437990581e-05,
      "loss": 0.9151,
      "step": 17700
    },
    {
      "epoch": 0.2794348508634223,
      "grad_norm": 4.701020240783691,
      "learning_rate": 4.982535321821036e-05,
      "loss": 0.8881,
      "step": 17800
    },
    {
      "epoch": 0.28100470957613816,
      "grad_norm": 4.981975078582764,
      "learning_rate": 4.982437205651492e-05,
      "loss": 0.8756,
      "step": 17900
    },
    {
      "epoch": 0.282574568288854,
      "grad_norm": 3.8580031394958496,
      "learning_rate": 4.9823390894819464e-05,
      "loss": 0.9089,
      "step": 18000
    },
    {
      "epoch": 0.28414442700156983,
      "grad_norm": 4.006866931915283,
      "learning_rate": 4.982240973312402e-05,
      "loss": 0.9401,
      "step": 18100
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 6.169759750366211,
      "learning_rate": 4.982142857142857e-05,
      "loss": 0.8939,
      "step": 18200
    },
    {
      "epoch": 0.28728414442700156,
      "grad_norm": 4.262808799743652,
      "learning_rate": 4.982044740973313e-05,
      "loss": 0.9019,
      "step": 18300
    },
    {
      "epoch": 0.28885400313971743,
      "grad_norm": 4.413633823394775,
      "learning_rate": 4.9819466248037674e-05,
      "loss": 0.9004,
      "step": 18400
    },
    {
      "epoch": 0.2904238618524333,
      "grad_norm": 5.382455348968506,
      "learning_rate": 4.981848508634223e-05,
      "loss": 0.8449,
      "step": 18500
    },
    {
      "epoch": 0.29199372056514916,
      "grad_norm": 4.257999897003174,
      "learning_rate": 4.981750392464678e-05,
      "loss": 0.9412,
      "step": 18600
    },
    {
      "epoch": 0.29356357927786497,
      "grad_norm": 3.6487550735473633,
      "learning_rate": 4.9816522762951334e-05,
      "loss": 0.9062,
      "step": 18700
    },
    {
      "epoch": 0.29513343799058084,
      "grad_norm": 4.198863506317139,
      "learning_rate": 4.9815541601255885e-05,
      "loss": 0.8717,
      "step": 18800
    },
    {
      "epoch": 0.2967032967032967,
      "grad_norm": 4.865671634674072,
      "learning_rate": 4.981456043956044e-05,
      "loss": 0.8965,
      "step": 18900
    },
    {
      "epoch": 0.29827315541601257,
      "grad_norm": 3.3522424697875977,
      "learning_rate": 4.9813579277864994e-05,
      "loss": 0.8882,
      "step": 19000
    },
    {
      "epoch": 0.29984301412872844,
      "grad_norm": 3.1848604679107666,
      "learning_rate": 4.9812598116169545e-05,
      "loss": 0.8826,
      "step": 19100
    },
    {
      "epoch": 0.30141287284144425,
      "grad_norm": 3.997774362564087,
      "learning_rate": 4.98116169544741e-05,
      "loss": 0.8821,
      "step": 19200
    },
    {
      "epoch": 0.3029827315541601,
      "grad_norm": 4.336977958679199,
      "learning_rate": 4.9810635792778654e-05,
      "loss": 0.899,
      "step": 19300
    },
    {
      "epoch": 0.304552590266876,
      "grad_norm": 3.7035305500030518,
      "learning_rate": 4.9809654631083204e-05,
      "loss": 0.8821,
      "step": 19400
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 3.776954174041748,
      "learning_rate": 4.9808673469387755e-05,
      "loss": 0.8477,
      "step": 19500
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 4.446315765380859,
      "learning_rate": 4.980769230769231e-05,
      "loss": 0.8809,
      "step": 19600
    },
    {
      "epoch": 0.3092621664050236,
      "grad_norm": 5.308003902435303,
      "learning_rate": 4.9806711145996864e-05,
      "loss": 0.9148,
      "step": 19700
    },
    {
      "epoch": 0.3108320251177394,
      "grad_norm": 4.9696245193481445,
      "learning_rate": 4.9805729984301415e-05,
      "loss": 0.9073,
      "step": 19800
    },
    {
      "epoch": 0.31240188383045525,
      "grad_norm": 4.489208221435547,
      "learning_rate": 4.9804748822605966e-05,
      "loss": 0.885,
      "step": 19900
    },
    {
      "epoch": 0.3139717425431711,
      "grad_norm": 3.7777445316314697,
      "learning_rate": 4.9803767660910524e-05,
      "loss": 0.9076,
      "step": 20000
    },
    {
      "epoch": 0.315541601255887,
      "grad_norm": 4.739073753356934,
      "learning_rate": 4.980278649921507e-05,
      "loss": 0.8849,
      "step": 20100
    },
    {
      "epoch": 0.31711145996860285,
      "grad_norm": 4.347486972808838,
      "learning_rate": 4.9801805337519626e-05,
      "loss": 0.8504,
      "step": 20200
    },
    {
      "epoch": 0.31868131868131866,
      "grad_norm": 4.766229152679443,
      "learning_rate": 4.980082417582418e-05,
      "loss": 0.9313,
      "step": 20300
    },
    {
      "epoch": 0.3202511773940345,
      "grad_norm": 5.2859673500061035,
      "learning_rate": 4.9799843014128735e-05,
      "loss": 0.8224,
      "step": 20400
    },
    {
      "epoch": 0.3218210361067504,
      "grad_norm": 4.786915302276611,
      "learning_rate": 4.979886185243328e-05,
      "loss": 0.8766,
      "step": 20500
    },
    {
      "epoch": 0.32339089481946626,
      "grad_norm": 3.7541604042053223,
      "learning_rate": 4.9797880690737837e-05,
      "loss": 0.8792,
      "step": 20600
    },
    {
      "epoch": 0.3249607535321821,
      "grad_norm": 4.7948079109191895,
      "learning_rate": 4.979689952904239e-05,
      "loss": 0.8687,
      "step": 20700
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 4.725884914398193,
      "learning_rate": 4.979591836734694e-05,
      "loss": 0.8733,
      "step": 20800
    },
    {
      "epoch": 0.3281004709576138,
      "grad_norm": 3.5970046520233154,
      "learning_rate": 4.979493720565149e-05,
      "loss": 0.899,
      "step": 20900
    },
    {
      "epoch": 0.32967032967032966,
      "grad_norm": 4.405435085296631,
      "learning_rate": 4.979395604395605e-05,
      "loss": 0.9027,
      "step": 21000
    },
    {
      "epoch": 0.33124018838304553,
      "grad_norm": 5.438555717468262,
      "learning_rate": 4.97929748822606e-05,
      "loss": 0.8813,
      "step": 21100
    },
    {
      "epoch": 0.3328100470957614,
      "grad_norm": 4.609579563140869,
      "learning_rate": 4.979199372056515e-05,
      "loss": 0.8889,
      "step": 21200
    },
    {
      "epoch": 0.33437990580847726,
      "grad_norm": 3.9554481506347656,
      "learning_rate": 4.979101255886971e-05,
      "loss": 0.8307,
      "step": 21300
    },
    {
      "epoch": 0.3359497645211931,
      "grad_norm": 4.20311164855957,
      "learning_rate": 4.979003139717426e-05,
      "loss": 0.893,
      "step": 21400
    },
    {
      "epoch": 0.33751962323390894,
      "grad_norm": 5.151994228363037,
      "learning_rate": 4.978905023547881e-05,
      "loss": 0.8519,
      "step": 21500
    },
    {
      "epoch": 0.3390894819466248,
      "grad_norm": 4.830306529998779,
      "learning_rate": 4.978806907378336e-05,
      "loss": 0.876,
      "step": 21600
    },
    {
      "epoch": 0.34065934065934067,
      "grad_norm": 5.205686569213867,
      "learning_rate": 4.978708791208792e-05,
      "loss": 0.8637,
      "step": 21700
    },
    {
      "epoch": 0.34222919937205654,
      "grad_norm": 4.7429938316345215,
      "learning_rate": 4.978610675039247e-05,
      "loss": 0.9128,
      "step": 21800
    },
    {
      "epoch": 0.34379905808477235,
      "grad_norm": 3.843940258026123,
      "learning_rate": 4.978512558869702e-05,
      "loss": 0.8878,
      "step": 21900
    },
    {
      "epoch": 0.3453689167974882,
      "grad_norm": 4.536914825439453,
      "learning_rate": 4.978414442700157e-05,
      "loss": 0.8822,
      "step": 22000
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 4.25846004486084,
      "learning_rate": 4.978316326530613e-05,
      "loss": 0.8859,
      "step": 22100
    },
    {
      "epoch": 0.34850863422291994,
      "grad_norm": 4.7789626121521,
      "learning_rate": 4.978218210361067e-05,
      "loss": 0.8686,
      "step": 22200
    },
    {
      "epoch": 0.3500784929356358,
      "grad_norm": 3.7835333347320557,
      "learning_rate": 4.978120094191523e-05,
      "loss": 0.8519,
      "step": 22300
    },
    {
      "epoch": 0.3516483516483517,
      "grad_norm": 4.215607643127441,
      "learning_rate": 4.978021978021978e-05,
      "loss": 0.8518,
      "step": 22400
    },
    {
      "epoch": 0.3532182103610675,
      "grad_norm": 3.9436442852020264,
      "learning_rate": 4.977923861852434e-05,
      "loss": 0.877,
      "step": 22500
    },
    {
      "epoch": 0.35478806907378335,
      "grad_norm": 5.369701862335205,
      "learning_rate": 4.977825745682888e-05,
      "loss": 0.8507,
      "step": 22600
    },
    {
      "epoch": 0.3563579277864992,
      "grad_norm": 4.215433597564697,
      "learning_rate": 4.977727629513344e-05,
      "loss": 0.8611,
      "step": 22700
    },
    {
      "epoch": 0.3579277864992151,
      "grad_norm": 4.808346271514893,
      "learning_rate": 4.977629513343799e-05,
      "loss": 0.8738,
      "step": 22800
    },
    {
      "epoch": 0.35949764521193095,
      "grad_norm": 4.72641134262085,
      "learning_rate": 4.977531397174254e-05,
      "loss": 0.8289,
      "step": 22900
    },
    {
      "epoch": 0.36106750392464676,
      "grad_norm": 4.751847267150879,
      "learning_rate": 4.9774332810047094e-05,
      "loss": 0.9285,
      "step": 23000
    },
    {
      "epoch": 0.3626373626373626,
      "grad_norm": 4.674197673797607,
      "learning_rate": 4.977335164835165e-05,
      "loss": 0.8142,
      "step": 23100
    },
    {
      "epoch": 0.3642072213500785,
      "grad_norm": 4.415899753570557,
      "learning_rate": 4.97723704866562e-05,
      "loss": 0.8477,
      "step": 23200
    },
    {
      "epoch": 0.36577708006279436,
      "grad_norm": 4.818538665771484,
      "learning_rate": 4.9771389324960754e-05,
      "loss": 0.8902,
      "step": 23300
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 5.125466823577881,
      "learning_rate": 4.977040816326531e-05,
      "loss": 0.8688,
      "step": 23400
    },
    {
      "epoch": 0.36891679748822603,
      "grad_norm": 3.999032974243164,
      "learning_rate": 4.976942700156986e-05,
      "loss": 0.8254,
      "step": 23500
    },
    {
      "epoch": 0.3704866562009419,
      "grad_norm": 4.531442165374756,
      "learning_rate": 4.9768445839874413e-05,
      "loss": 0.849,
      "step": 23600
    },
    {
      "epoch": 0.37205651491365777,
      "grad_norm": 4.886351108551025,
      "learning_rate": 4.9767464678178964e-05,
      "loss": 0.8821,
      "step": 23700
    },
    {
      "epoch": 0.37362637362637363,
      "grad_norm": 3.9916083812713623,
      "learning_rate": 4.976648351648352e-05,
      "loss": 0.8474,
      "step": 23800
    },
    {
      "epoch": 0.3751962323390895,
      "grad_norm": 4.660285472869873,
      "learning_rate": 4.976550235478807e-05,
      "loss": 0.8698,
      "step": 23900
    },
    {
      "epoch": 0.37676609105180536,
      "grad_norm": 4.247408866882324,
      "learning_rate": 4.9764521193092624e-05,
      "loss": 0.8381,
      "step": 24000
    },
    {
      "epoch": 0.3783359497645212,
      "grad_norm": 4.678214073181152,
      "learning_rate": 4.9763540031397175e-05,
      "loss": 0.8602,
      "step": 24100
    },
    {
      "epoch": 0.37990580847723704,
      "grad_norm": 3.258051872253418,
      "learning_rate": 4.976255886970173e-05,
      "loss": 0.8481,
      "step": 24200
    },
    {
      "epoch": 0.3814756671899529,
      "grad_norm": 4.727330684661865,
      "learning_rate": 4.976157770800628e-05,
      "loss": 0.8259,
      "step": 24300
    },
    {
      "epoch": 0.38304552590266877,
      "grad_norm": 4.7012529373168945,
      "learning_rate": 4.9760596546310835e-05,
      "loss": 0.8852,
      "step": 24400
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 4.26056432723999,
      "learning_rate": 4.9759615384615386e-05,
      "loss": 0.8115,
      "step": 24500
    },
    {
      "epoch": 0.38618524332810045,
      "grad_norm": 3.991880178451538,
      "learning_rate": 4.9758634222919944e-05,
      "loss": 0.8429,
      "step": 24600
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 4.178058624267578,
      "learning_rate": 4.975765306122449e-05,
      "loss": 0.8629,
      "step": 24700
    },
    {
      "epoch": 0.3893249607535322,
      "grad_norm": 3.929433584213257,
      "learning_rate": 4.9756671899529046e-05,
      "loss": 0.8345,
      "step": 24800
    },
    {
      "epoch": 0.39089481946624804,
      "grad_norm": 5.845942497253418,
      "learning_rate": 4.9755690737833597e-05,
      "loss": 0.8883,
      "step": 24900
    },
    {
      "epoch": 0.3924646781789639,
      "grad_norm": 4.635286331176758,
      "learning_rate": 4.975470957613815e-05,
      "loss": 0.8622,
      "step": 25000
    },
    {
      "epoch": 0.3940345368916798,
      "grad_norm": 3.863929510116577,
      "learning_rate": 4.97537284144427e-05,
      "loss": 0.8392,
      "step": 25100
    },
    {
      "epoch": 0.3956043956043956,
      "grad_norm": 4.990351676940918,
      "learning_rate": 4.9752747252747256e-05,
      "loss": 0.838,
      "step": 25200
    },
    {
      "epoch": 0.39717425431711145,
      "grad_norm": 5.487148761749268,
      "learning_rate": 4.975176609105181e-05,
      "loss": 0.8349,
      "step": 25300
    },
    {
      "epoch": 0.3987441130298273,
      "grad_norm": 5.012301921844482,
      "learning_rate": 4.975078492935636e-05,
      "loss": 0.8827,
      "step": 25400
    },
    {
      "epoch": 0.4003139717425432,
      "grad_norm": 5.095057964324951,
      "learning_rate": 4.9749803767660916e-05,
      "loss": 0.8476,
      "step": 25500
    },
    {
      "epoch": 0.40188383045525905,
      "grad_norm": 5.6058549880981445,
      "learning_rate": 4.974882260596547e-05,
      "loss": 0.8281,
      "step": 25600
    },
    {
      "epoch": 0.40345368916797486,
      "grad_norm": 3.7595551013946533,
      "learning_rate": 4.974784144427002e-05,
      "loss": 0.8866,
      "step": 25700
    },
    {
      "epoch": 0.4050235478806907,
      "grad_norm": 5.175686359405518,
      "learning_rate": 4.974686028257457e-05,
      "loss": 0.8489,
      "step": 25800
    },
    {
      "epoch": 0.4065934065934066,
      "grad_norm": 3.9783456325531006,
      "learning_rate": 4.974587912087913e-05,
      "loss": 0.9,
      "step": 25900
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 4.882892608642578,
      "learning_rate": 4.974489795918368e-05,
      "loss": 0.8425,
      "step": 26000
    },
    {
      "epoch": 0.4097331240188383,
      "grad_norm": 5.452770709991455,
      "learning_rate": 4.974391679748823e-05,
      "loss": 0.8098,
      "step": 26100
    },
    {
      "epoch": 0.41130298273155413,
      "grad_norm": 3.7227566242218018,
      "learning_rate": 4.974293563579278e-05,
      "loss": 0.9092,
      "step": 26200
    },
    {
      "epoch": 0.41287284144427,
      "grad_norm": 4.522994518280029,
      "learning_rate": 4.974195447409734e-05,
      "loss": 0.8604,
      "step": 26300
    },
    {
      "epoch": 0.41444270015698587,
      "grad_norm": 4.556796550750732,
      "learning_rate": 4.974097331240188e-05,
      "loss": 0.8478,
      "step": 26400
    },
    {
      "epoch": 0.41601255886970173,
      "grad_norm": 4.041748046875,
      "learning_rate": 4.973999215070644e-05,
      "loss": 0.8399,
      "step": 26500
    },
    {
      "epoch": 0.4175824175824176,
      "grad_norm": 3.9944002628326416,
      "learning_rate": 4.973901098901099e-05,
      "loss": 0.8874,
      "step": 26600
    },
    {
      "epoch": 0.41915227629513346,
      "grad_norm": 4.075374603271484,
      "learning_rate": 4.973802982731555e-05,
      "loss": 0.8771,
      "step": 26700
    },
    {
      "epoch": 0.4207221350078493,
      "grad_norm": 3.2981221675872803,
      "learning_rate": 4.973704866562009e-05,
      "loss": 0.809,
      "step": 26800
    },
    {
      "epoch": 0.42229199372056514,
      "grad_norm": 4.569493770599365,
      "learning_rate": 4.973606750392465e-05,
      "loss": 0.8493,
      "step": 26900
    },
    {
      "epoch": 0.423861852433281,
      "grad_norm": 4.869147777557373,
      "learning_rate": 4.97350863422292e-05,
      "loss": 0.8585,
      "step": 27000
    },
    {
      "epoch": 0.42543171114599687,
      "grad_norm": 4.422493934631348,
      "learning_rate": 4.973410518053375e-05,
      "loss": 0.8352,
      "step": 27100
    },
    {
      "epoch": 0.42700156985871274,
      "grad_norm": 4.23952579498291,
      "learning_rate": 4.97331240188383e-05,
      "loss": 0.8289,
      "step": 27200
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 4.951995849609375,
      "learning_rate": 4.973214285714286e-05,
      "loss": 0.8327,
      "step": 27300
    },
    {
      "epoch": 0.4301412872841444,
      "grad_norm": 4.685556411743164,
      "learning_rate": 4.973116169544741e-05,
      "loss": 0.9182,
      "step": 27400
    },
    {
      "epoch": 0.4317111459968603,
      "grad_norm": 5.1496148109436035,
      "learning_rate": 4.973018053375196e-05,
      "loss": 0.7978,
      "step": 27500
    },
    {
      "epoch": 0.43328100470957615,
      "grad_norm": 4.4139404296875,
      "learning_rate": 4.972919937205652e-05,
      "loss": 0.8895,
      "step": 27600
    },
    {
      "epoch": 0.434850863422292,
      "grad_norm": 4.447625160217285,
      "learning_rate": 4.972821821036107e-05,
      "loss": 0.8068,
      "step": 27700
    },
    {
      "epoch": 0.4364207221350079,
      "grad_norm": 4.858394145965576,
      "learning_rate": 4.972723704866562e-05,
      "loss": 0.8756,
      "step": 27800
    },
    {
      "epoch": 0.4379905808477237,
      "grad_norm": 4.273231506347656,
      "learning_rate": 4.9726255886970173e-05,
      "loss": 0.8855,
      "step": 27900
    },
    {
      "epoch": 0.43956043956043955,
      "grad_norm": 4.212184906005859,
      "learning_rate": 4.972527472527473e-05,
      "loss": 0.8766,
      "step": 28000
    },
    {
      "epoch": 0.4411302982731554,
      "grad_norm": 5.398836135864258,
      "learning_rate": 4.972429356357928e-05,
      "loss": 0.8084,
      "step": 28100
    },
    {
      "epoch": 0.4427001569858713,
      "grad_norm": 4.637157440185547,
      "learning_rate": 4.972331240188383e-05,
      "loss": 0.8858,
      "step": 28200
    },
    {
      "epoch": 0.44427001569858715,
      "grad_norm": 4.888664245605469,
      "learning_rate": 4.9722331240188384e-05,
      "loss": 0.8501,
      "step": 28300
    },
    {
      "epoch": 0.44583987441130296,
      "grad_norm": 4.172804832458496,
      "learning_rate": 4.972135007849294e-05,
      "loss": 0.8534,
      "step": 28400
    },
    {
      "epoch": 0.4474097331240188,
      "grad_norm": 4.276927947998047,
      "learning_rate": 4.9720368916797486e-05,
      "loss": 0.8338,
      "step": 28500
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 3.9716134071350098,
      "learning_rate": 4.9719387755102044e-05,
      "loss": 0.8563,
      "step": 28600
    },
    {
      "epoch": 0.45054945054945056,
      "grad_norm": 4.013527870178223,
      "learning_rate": 4.9718406593406595e-05,
      "loss": 0.8649,
      "step": 28700
    },
    {
      "epoch": 0.4521193092621664,
      "grad_norm": 3.9604105949401855,
      "learning_rate": 4.971742543171115e-05,
      "loss": 0.8663,
      "step": 28800
    },
    {
      "epoch": 0.45368916797488223,
      "grad_norm": 4.846940994262695,
      "learning_rate": 4.97164442700157e-05,
      "loss": 0.8213,
      "step": 28900
    },
    {
      "epoch": 0.4552590266875981,
      "grad_norm": 5.195522308349609,
      "learning_rate": 4.9715463108320255e-05,
      "loss": 0.826,
      "step": 29000
    },
    {
      "epoch": 0.45682888540031397,
      "grad_norm": 4.528500080108643,
      "learning_rate": 4.9714481946624806e-05,
      "loss": 0.8372,
      "step": 29100
    },
    {
      "epoch": 0.45839874411302983,
      "grad_norm": 4.436499118804932,
      "learning_rate": 4.9713500784929357e-05,
      "loss": 0.8355,
      "step": 29200
    },
    {
      "epoch": 0.4599686028257457,
      "grad_norm": 4.880756855010986,
      "learning_rate": 4.971251962323391e-05,
      "loss": 0.8411,
      "step": 29300
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 4.9406280517578125,
      "learning_rate": 4.9711538461538465e-05,
      "loss": 0.8142,
      "step": 29400
    },
    {
      "epoch": 0.4631083202511774,
      "grad_norm": 4.360330581665039,
      "learning_rate": 4.9710557299843016e-05,
      "loss": 0.8769,
      "step": 29500
    },
    {
      "epoch": 0.46467817896389324,
      "grad_norm": 4.251989841461182,
      "learning_rate": 4.970957613814757e-05,
      "loss": 0.8419,
      "step": 29600
    },
    {
      "epoch": 0.4662480376766091,
      "grad_norm": 4.737494468688965,
      "learning_rate": 4.9708594976452125e-05,
      "loss": 0.9146,
      "step": 29700
    },
    {
      "epoch": 0.46781789638932497,
      "grad_norm": 4.517764091491699,
      "learning_rate": 4.9707613814756676e-05,
      "loss": 0.8508,
      "step": 29800
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 3.6390717029571533,
      "learning_rate": 4.970663265306123e-05,
      "loss": 0.8389,
      "step": 29900
    },
    {
      "epoch": 0.47095761381475665,
      "grad_norm": 3.2756404876708984,
      "learning_rate": 4.970565149136578e-05,
      "loss": 0.8251,
      "step": 30000
    },
    {
      "epoch": 0.4725274725274725,
      "grad_norm": 4.5375213623046875,
      "learning_rate": 4.9704670329670336e-05,
      "loss": 0.8799,
      "step": 30100
    },
    {
      "epoch": 0.4740973312401884,
      "grad_norm": 4.665286540985107,
      "learning_rate": 4.970368916797489e-05,
      "loss": 0.8462,
      "step": 30200
    },
    {
      "epoch": 0.47566718995290425,
      "grad_norm": 5.115809917449951,
      "learning_rate": 4.970270800627944e-05,
      "loss": 0.8823,
      "step": 30300
    },
    {
      "epoch": 0.4772370486656201,
      "grad_norm": 4.510003566741943,
      "learning_rate": 4.970172684458399e-05,
      "loss": 0.838,
      "step": 30400
    },
    {
      "epoch": 0.478806907378336,
      "grad_norm": 5.092260360717773,
      "learning_rate": 4.9700745682888546e-05,
      "loss": 0.8375,
      "step": 30500
    },
    {
      "epoch": 0.4803767660910518,
      "grad_norm": 4.313407897949219,
      "learning_rate": 4.969976452119309e-05,
      "loss": 0.8351,
      "step": 30600
    },
    {
      "epoch": 0.48194662480376765,
      "grad_norm": 4.999098777770996,
      "learning_rate": 4.969878335949765e-05,
      "loss": 0.8357,
      "step": 30700
    },
    {
      "epoch": 0.4835164835164835,
      "grad_norm": 4.552789688110352,
      "learning_rate": 4.96978021978022e-05,
      "loss": 0.8153,
      "step": 30800
    },
    {
      "epoch": 0.4850863422291994,
      "grad_norm": 3.8199357986450195,
      "learning_rate": 4.969682103610676e-05,
      "loss": 0.8184,
      "step": 30900
    },
    {
      "epoch": 0.48665620094191525,
      "grad_norm": 4.259797096252441,
      "learning_rate": 4.96958398744113e-05,
      "loss": 0.8795,
      "step": 31000
    },
    {
      "epoch": 0.48822605965463106,
      "grad_norm": 4.07320499420166,
      "learning_rate": 4.969485871271586e-05,
      "loss": 0.8788,
      "step": 31100
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 4.832005023956299,
      "learning_rate": 4.969387755102041e-05,
      "loss": 0.8441,
      "step": 31200
    },
    {
      "epoch": 0.4913657770800628,
      "grad_norm": 4.208081245422363,
      "learning_rate": 4.969289638932496e-05,
      "loss": 0.8654,
      "step": 31300
    },
    {
      "epoch": 0.49293563579277866,
      "grad_norm": 3.82315731048584,
      "learning_rate": 4.969191522762951e-05,
      "loss": 0.828,
      "step": 31400
    },
    {
      "epoch": 0.4945054945054945,
      "grad_norm": 4.046810150146484,
      "learning_rate": 4.969093406593407e-05,
      "loss": 0.7997,
      "step": 31500
    },
    {
      "epoch": 0.49607535321821034,
      "grad_norm": 5.303223609924316,
      "learning_rate": 4.968995290423862e-05,
      "loss": 0.8527,
      "step": 31600
    },
    {
      "epoch": 0.4976452119309262,
      "grad_norm": 4.757104873657227,
      "learning_rate": 4.968897174254317e-05,
      "loss": 0.8306,
      "step": 31700
    },
    {
      "epoch": 0.49921507064364207,
      "grad_norm": 4.661187171936035,
      "learning_rate": 4.968799058084773e-05,
      "loss": 0.8156,
      "step": 31800
    },
    {
      "epoch": 0.5007849293563579,
      "grad_norm": 4.314532279968262,
      "learning_rate": 4.968700941915228e-05,
      "loss": 0.8433,
      "step": 31900
    },
    {
      "epoch": 0.5023547880690737,
      "grad_norm": 3.9675285816192627,
      "learning_rate": 4.968602825745683e-05,
      "loss": 0.8839,
      "step": 32000
    },
    {
      "epoch": 0.5039246467817896,
      "grad_norm": 5.246296405792236,
      "learning_rate": 4.968504709576138e-05,
      "loss": 0.8302,
      "step": 32100
    },
    {
      "epoch": 0.5054945054945055,
      "grad_norm": 4.5932440757751465,
      "learning_rate": 4.968406593406594e-05,
      "loss": 0.8711,
      "step": 32200
    },
    {
      "epoch": 0.5070643642072213,
      "grad_norm": 4.194288730621338,
      "learning_rate": 4.968308477237049e-05,
      "loss": 0.851,
      "step": 32300
    },
    {
      "epoch": 0.5086342229199372,
      "grad_norm": 4.338903903961182,
      "learning_rate": 4.968210361067504e-05,
      "loss": 0.7998,
      "step": 32400
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 5.136180877685547,
      "learning_rate": 4.968112244897959e-05,
      "loss": 0.8298,
      "step": 32500
    },
    {
      "epoch": 0.5117739403453689,
      "grad_norm": 5.825897216796875,
      "learning_rate": 4.968014128728415e-05,
      "loss": 0.8599,
      "step": 32600
    },
    {
      "epoch": 0.5133437990580848,
      "grad_norm": 5.152547359466553,
      "learning_rate": 4.9679160125588695e-05,
      "loss": 0.844,
      "step": 32700
    },
    {
      "epoch": 0.5149136577708007,
      "grad_norm": 4.597870349884033,
      "learning_rate": 4.967817896389325e-05,
      "loss": 0.8528,
      "step": 32800
    },
    {
      "epoch": 0.5164835164835165,
      "grad_norm": 4.083693981170654,
      "learning_rate": 4.9677197802197804e-05,
      "loss": 0.8487,
      "step": 32900
    },
    {
      "epoch": 0.5180533751962323,
      "grad_norm": 4.2721686363220215,
      "learning_rate": 4.967621664050236e-05,
      "loss": 0.8839,
      "step": 33000
    },
    {
      "epoch": 0.5196232339089482,
      "grad_norm": 4.0708537101745605,
      "learning_rate": 4.9675235478806906e-05,
      "loss": 0.8443,
      "step": 33100
    },
    {
      "epoch": 0.521193092621664,
      "grad_norm": 4.487679481506348,
      "learning_rate": 4.9674254317111464e-05,
      "loss": 0.8488,
      "step": 33200
    },
    {
      "epoch": 0.5227629513343799,
      "grad_norm": 3.655649185180664,
      "learning_rate": 4.9673273155416015e-05,
      "loss": 0.8314,
      "step": 33300
    },
    {
      "epoch": 0.5243328100470958,
      "grad_norm": 4.163240909576416,
      "learning_rate": 4.9672291993720566e-05,
      "loss": 0.8727,
      "step": 33400
    },
    {
      "epoch": 0.5259026687598116,
      "grad_norm": 4.246090412139893,
      "learning_rate": 4.9671310832025116e-05,
      "loss": 0.8707,
      "step": 33500
    },
    {
      "epoch": 0.5274725274725275,
      "grad_norm": 4.623417377471924,
      "learning_rate": 4.9670329670329674e-05,
      "loss": 0.8605,
      "step": 33600
    },
    {
      "epoch": 0.5290423861852434,
      "grad_norm": 4.777970790863037,
      "learning_rate": 4.9669348508634225e-05,
      "loss": 0.814,
      "step": 33700
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 4.589809894561768,
      "learning_rate": 4.9668367346938776e-05,
      "loss": 0.8047,
      "step": 33800
    },
    {
      "epoch": 0.5321821036106751,
      "grad_norm": 5.813843727111816,
      "learning_rate": 4.9667386185243334e-05,
      "loss": 0.8101,
      "step": 33900
    },
    {
      "epoch": 0.533751962323391,
      "grad_norm": 4.922992706298828,
      "learning_rate": 4.9666405023547885e-05,
      "loss": 0.8205,
      "step": 34000
    },
    {
      "epoch": 0.5353218210361067,
      "grad_norm": 3.6372506618499756,
      "learning_rate": 4.9665423861852436e-05,
      "loss": 0.7866,
      "step": 34100
    },
    {
      "epoch": 0.5368916797488226,
      "grad_norm": 4.754805088043213,
      "learning_rate": 4.966444270015699e-05,
      "loss": 0.8199,
      "step": 34200
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 4.970324516296387,
      "learning_rate": 4.9663461538461545e-05,
      "loss": 0.815,
      "step": 34300
    },
    {
      "epoch": 0.5400313971742543,
      "grad_norm": 4.21127462387085,
      "learning_rate": 4.966248037676609e-05,
      "loss": 0.7779,
      "step": 34400
    },
    {
      "epoch": 0.5416012558869702,
      "grad_norm": 4.554904937744141,
      "learning_rate": 4.966149921507065e-05,
      "loss": 0.8214,
      "step": 34500
    },
    {
      "epoch": 0.543171114599686,
      "grad_norm": 3.5540943145751953,
      "learning_rate": 4.96605180533752e-05,
      "loss": 0.8733,
      "step": 34600
    },
    {
      "epoch": 0.5447409733124019,
      "grad_norm": 4.382591724395752,
      "learning_rate": 4.9659536891679755e-05,
      "loss": 0.7941,
      "step": 34700
    },
    {
      "epoch": 0.5463108320251178,
      "grad_norm": 5.359100818634033,
      "learning_rate": 4.96585557299843e-05,
      "loss": 0.7992,
      "step": 34800
    },
    {
      "epoch": 0.5478806907378336,
      "grad_norm": 5.20298957824707,
      "learning_rate": 4.965757456828886e-05,
      "loss": 0.814,
      "step": 34900
    },
    {
      "epoch": 0.5494505494505495,
      "grad_norm": 5.4648542404174805,
      "learning_rate": 4.965659340659341e-05,
      "loss": 0.814,
      "step": 35000
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 5.01630163192749,
      "learning_rate": 4.965561224489796e-05,
      "loss": 0.8685,
      "step": 35100
    },
    {
      "epoch": 0.5525902668759811,
      "grad_norm": 4.647847652435303,
      "learning_rate": 4.965463108320251e-05,
      "loss": 0.8649,
      "step": 35200
    },
    {
      "epoch": 0.554160125588697,
      "grad_norm": 5.016851425170898,
      "learning_rate": 4.965364992150707e-05,
      "loss": 0.8833,
      "step": 35300
    },
    {
      "epoch": 0.5557299843014128,
      "grad_norm": 3.7312633991241455,
      "learning_rate": 4.965266875981162e-05,
      "loss": 0.8301,
      "step": 35400
    },
    {
      "epoch": 0.5572998430141287,
      "grad_norm": 5.070457935333252,
      "learning_rate": 4.965168759811617e-05,
      "loss": 0.842,
      "step": 35500
    },
    {
      "epoch": 0.5588697017268446,
      "grad_norm": 3.3875246047973633,
      "learning_rate": 4.965070643642072e-05,
      "loss": 0.8526,
      "step": 35600
    },
    {
      "epoch": 0.5604395604395604,
      "grad_norm": 4.121779441833496,
      "learning_rate": 4.964972527472528e-05,
      "loss": 0.8141,
      "step": 35700
    },
    {
      "epoch": 0.5620094191522763,
      "grad_norm": 5.672417163848877,
      "learning_rate": 4.964874411302983e-05,
      "loss": 0.8647,
      "step": 35800
    },
    {
      "epoch": 0.5635792778649922,
      "grad_norm": 5.058023452758789,
      "learning_rate": 4.964776295133438e-05,
      "loss": 0.8115,
      "step": 35900
    },
    {
      "epoch": 0.565149136577708,
      "grad_norm": 4.090623378753662,
      "learning_rate": 4.964678178963894e-05,
      "loss": 0.8141,
      "step": 36000
    },
    {
      "epoch": 0.5667189952904239,
      "grad_norm": 4.162033557891846,
      "learning_rate": 4.964580062794349e-05,
      "loss": 0.8407,
      "step": 36100
    },
    {
      "epoch": 0.5682888540031397,
      "grad_norm": 4.386438846588135,
      "learning_rate": 4.964481946624804e-05,
      "loss": 0.7621,
      "step": 36200
    },
    {
      "epoch": 0.5698587127158555,
      "grad_norm": 5.598862648010254,
      "learning_rate": 4.964383830455259e-05,
      "loss": 0.8461,
      "step": 36300
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 4.293498992919922,
      "learning_rate": 4.964285714285715e-05,
      "loss": 0.8158,
      "step": 36400
    },
    {
      "epoch": 0.5729984301412873,
      "grad_norm": 5.179502964019775,
      "learning_rate": 4.964187598116169e-05,
      "loss": 0.7944,
      "step": 36500
    },
    {
      "epoch": 0.5745682888540031,
      "grad_norm": 5.081101894378662,
      "learning_rate": 4.964089481946625e-05,
      "loss": 0.812,
      "step": 36600
    },
    {
      "epoch": 0.576138147566719,
      "grad_norm": 4.162936687469482,
      "learning_rate": 4.96399136577708e-05,
      "loss": 0.8513,
      "step": 36700
    },
    {
      "epoch": 0.5777080062794349,
      "grad_norm": 4.518476486206055,
      "learning_rate": 4.963893249607536e-05,
      "loss": 0.8319,
      "step": 36800
    },
    {
      "epoch": 0.5792778649921507,
      "grad_norm": 4.620619297027588,
      "learning_rate": 4.9637951334379904e-05,
      "loss": 0.8174,
      "step": 36900
    },
    {
      "epoch": 0.5808477237048666,
      "grad_norm": 4.944860458374023,
      "learning_rate": 4.963697017268446e-05,
      "loss": 0.8304,
      "step": 37000
    },
    {
      "epoch": 0.5824175824175825,
      "grad_norm": 5.311035633087158,
      "learning_rate": 4.963598901098901e-05,
      "loss": 0.8002,
      "step": 37100
    },
    {
      "epoch": 0.5839874411302983,
      "grad_norm": 3.031092643737793,
      "learning_rate": 4.9635007849293564e-05,
      "loss": 0.7981,
      "step": 37200
    },
    {
      "epoch": 0.5855572998430141,
      "grad_norm": 4.561835765838623,
      "learning_rate": 4.9634026687598115e-05,
      "loss": 0.8199,
      "step": 37300
    },
    {
      "epoch": 0.5871271585557299,
      "grad_norm": 5.077752590179443,
      "learning_rate": 4.963304552590267e-05,
      "loss": 0.8566,
      "step": 37400
    },
    {
      "epoch": 0.5886970172684458,
      "grad_norm": 4.487312316894531,
      "learning_rate": 4.9632064364207224e-05,
      "loss": 0.843,
      "step": 37500
    },
    {
      "epoch": 0.5902668759811617,
      "grad_norm": 5.228646278381348,
      "learning_rate": 4.9631083202511774e-05,
      "loss": 0.8713,
      "step": 37600
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 4.024026393890381,
      "learning_rate": 4.9630102040816325e-05,
      "loss": 0.8337,
      "step": 37700
    },
    {
      "epoch": 0.5934065934065934,
      "grad_norm": 3.358938217163086,
      "learning_rate": 4.962912087912088e-05,
      "loss": 0.7748,
      "step": 37800
    },
    {
      "epoch": 0.5949764521193093,
      "grad_norm": 4.524643421173096,
      "learning_rate": 4.9628139717425434e-05,
      "loss": 0.8667,
      "step": 37900
    },
    {
      "epoch": 0.5965463108320251,
      "grad_norm": 4.950138568878174,
      "learning_rate": 4.9627158555729985e-05,
      "loss": 0.7894,
      "step": 38000
    },
    {
      "epoch": 0.598116169544741,
      "grad_norm": 3.9200289249420166,
      "learning_rate": 4.962617739403454e-05,
      "loss": 0.8162,
      "step": 38100
    },
    {
      "epoch": 0.5996860282574569,
      "grad_norm": 4.307744979858398,
      "learning_rate": 4.9625196232339094e-05,
      "loss": 0.8217,
      "step": 38200
    },
    {
      "epoch": 0.6012558869701727,
      "grad_norm": 4.435638427734375,
      "learning_rate": 4.9624215070643645e-05,
      "loss": 0.8476,
      "step": 38300
    },
    {
      "epoch": 0.6028257456828885,
      "grad_norm": 4.018533229827881,
      "learning_rate": 4.9623233908948196e-05,
      "loss": 0.8121,
      "step": 38400
    },
    {
      "epoch": 0.6043956043956044,
      "grad_norm": 5.029332160949707,
      "learning_rate": 4.9622252747252754e-05,
      "loss": 0.8214,
      "step": 38500
    },
    {
      "epoch": 0.6059654631083202,
      "grad_norm": 3.5009119510650635,
      "learning_rate": 4.96212715855573e-05,
      "loss": 0.8166,
      "step": 38600
    },
    {
      "epoch": 0.6075353218210361,
      "grad_norm": 5.744623184204102,
      "learning_rate": 4.9620290423861856e-05,
      "loss": 0.8259,
      "step": 38700
    },
    {
      "epoch": 0.609105180533752,
      "grad_norm": 5.421357154846191,
      "learning_rate": 4.9619309262166407e-05,
      "loss": 0.8646,
      "step": 38800
    },
    {
      "epoch": 0.6106750392464678,
      "grad_norm": 4.245697021484375,
      "learning_rate": 4.9618328100470964e-05,
      "loss": 0.7851,
      "step": 38900
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 4.575499057769775,
      "learning_rate": 4.961734693877551e-05,
      "loss": 0.7848,
      "step": 39000
    },
    {
      "epoch": 0.6138147566718996,
      "grad_norm": 3.751803159713745,
      "learning_rate": 4.9616365777080066e-05,
      "loss": 0.7976,
      "step": 39100
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 4.5400872230529785,
      "learning_rate": 4.961538461538462e-05,
      "loss": 0.7846,
      "step": 39200
    },
    {
      "epoch": 0.6169544740973313,
      "grad_norm": 4.120179653167725,
      "learning_rate": 4.961440345368917e-05,
      "loss": 0.7807,
      "step": 39300
    },
    {
      "epoch": 0.6185243328100472,
      "grad_norm": 3.5969455242156982,
      "learning_rate": 4.961342229199372e-05,
      "loss": 0.818,
      "step": 39400
    },
    {
      "epoch": 0.6200941915227629,
      "grad_norm": 3.901196241378784,
      "learning_rate": 4.961244113029828e-05,
      "loss": 0.8487,
      "step": 39500
    },
    {
      "epoch": 0.6216640502354788,
      "grad_norm": 2.93708872795105,
      "learning_rate": 4.961145996860283e-05,
      "loss": 0.8372,
      "step": 39600
    },
    {
      "epoch": 0.6232339089481946,
      "grad_norm": 3.949522018432617,
      "learning_rate": 4.961047880690738e-05,
      "loss": 0.7988,
      "step": 39700
    },
    {
      "epoch": 0.6248037676609105,
      "grad_norm": 4.949484825134277,
      "learning_rate": 4.960949764521193e-05,
      "loss": 0.8117,
      "step": 39800
    },
    {
      "epoch": 0.6263736263736264,
      "grad_norm": 4.781815052032471,
      "learning_rate": 4.960851648351649e-05,
      "loss": 0.822,
      "step": 39900
    },
    {
      "epoch": 0.6279434850863422,
      "grad_norm": 3.361798048019409,
      "learning_rate": 4.960753532182104e-05,
      "loss": 0.8034,
      "step": 40000
    },
    {
      "epoch": 0.6295133437990581,
      "grad_norm": 4.359850883483887,
      "learning_rate": 4.960655416012559e-05,
      "loss": 0.7836,
      "step": 40100
    },
    {
      "epoch": 0.631083202511774,
      "grad_norm": 4.516506195068359,
      "learning_rate": 4.960557299843015e-05,
      "loss": 0.868,
      "step": 40200
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 3.539855480194092,
      "learning_rate": 4.96045918367347e-05,
      "loss": 0.8483,
      "step": 40300
    },
    {
      "epoch": 0.6342229199372057,
      "grad_norm": 5.321126461029053,
      "learning_rate": 4.960361067503925e-05,
      "loss": 0.84,
      "step": 40400
    },
    {
      "epoch": 0.6357927786499215,
      "grad_norm": 5.189310550689697,
      "learning_rate": 4.96026295133438e-05,
      "loss": 0.8078,
      "step": 40500
    },
    {
      "epoch": 0.6373626373626373,
      "grad_norm": 3.439690351486206,
      "learning_rate": 4.960164835164836e-05,
      "loss": 0.8187,
      "step": 40600
    },
    {
      "epoch": 0.6389324960753532,
      "grad_norm": 4.296807765960693,
      "learning_rate": 4.96006671899529e-05,
      "loss": 0.8323,
      "step": 40700
    },
    {
      "epoch": 0.640502354788069,
      "grad_norm": 4.093379020690918,
      "learning_rate": 4.959968602825746e-05,
      "loss": 0.7848,
      "step": 40800
    },
    {
      "epoch": 0.6420722135007849,
      "grad_norm": 3.459275960922241,
      "learning_rate": 4.959870486656201e-05,
      "loss": 0.7959,
      "step": 40900
    },
    {
      "epoch": 0.6436420722135008,
      "grad_norm": 4.98042106628418,
      "learning_rate": 4.959772370486657e-05,
      "loss": 0.8374,
      "step": 41000
    },
    {
      "epoch": 0.6452119309262166,
      "grad_norm": 4.80870246887207,
      "learning_rate": 4.959674254317111e-05,
      "loss": 0.8332,
      "step": 41100
    },
    {
      "epoch": 0.6467817896389325,
      "grad_norm": 4.477703094482422,
      "learning_rate": 4.959576138147567e-05,
      "loss": 0.8272,
      "step": 41200
    },
    {
      "epoch": 0.6483516483516484,
      "grad_norm": 3.8427345752716064,
      "learning_rate": 4.959478021978022e-05,
      "loss": 0.822,
      "step": 41300
    },
    {
      "epoch": 0.6499215070643642,
      "grad_norm": 4.456334590911865,
      "learning_rate": 4.959379905808477e-05,
      "loss": 0.8548,
      "step": 41400
    },
    {
      "epoch": 0.6514913657770801,
      "grad_norm": 4.913059711456299,
      "learning_rate": 4.9592817896389324e-05,
      "loss": 0.8254,
      "step": 41500
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 4.521251201629639,
      "learning_rate": 4.959183673469388e-05,
      "loss": 0.8125,
      "step": 41600
    },
    {
      "epoch": 0.6546310832025117,
      "grad_norm": 4.1708879470825195,
      "learning_rate": 4.959085557299843e-05,
      "loss": 0.8006,
      "step": 41700
    },
    {
      "epoch": 0.6562009419152276,
      "grad_norm": 4.145576000213623,
      "learning_rate": 4.9589874411302983e-05,
      "loss": 0.8193,
      "step": 41800
    },
    {
      "epoch": 0.6577708006279435,
      "grad_norm": 4.786271095275879,
      "learning_rate": 4.9588893249607534e-05,
      "loss": 0.8135,
      "step": 41900
    },
    {
      "epoch": 0.6593406593406593,
      "grad_norm": 4.494139671325684,
      "learning_rate": 4.958791208791209e-05,
      "loss": 0.8265,
      "step": 42000
    },
    {
      "epoch": 0.6609105180533752,
      "grad_norm": 4.308200836181641,
      "learning_rate": 4.958693092621664e-05,
      "loss": 0.7893,
      "step": 42100
    },
    {
      "epoch": 0.6624803767660911,
      "grad_norm": 5.376277446746826,
      "learning_rate": 4.9585949764521194e-05,
      "loss": 0.7966,
      "step": 42200
    },
    {
      "epoch": 0.6640502354788069,
      "grad_norm": 3.929201126098633,
      "learning_rate": 4.958496860282575e-05,
      "loss": 0.8112,
      "step": 42300
    },
    {
      "epoch": 0.6656200941915228,
      "grad_norm": 4.213441848754883,
      "learning_rate": 4.95839874411303e-05,
      "loss": 0.8163,
      "step": 42400
    },
    {
      "epoch": 0.6671899529042387,
      "grad_norm": 4.674266338348389,
      "learning_rate": 4.9583006279434854e-05,
      "loss": 0.7799,
      "step": 42500
    },
    {
      "epoch": 0.6687598116169545,
      "grad_norm": 4.493956089019775,
      "learning_rate": 4.9582025117739405e-05,
      "loss": 0.8121,
      "step": 42600
    },
    {
      "epoch": 0.6703296703296703,
      "grad_norm": 4.126016616821289,
      "learning_rate": 4.958104395604396e-05,
      "loss": 0.7707,
      "step": 42700
    },
    {
      "epoch": 0.6718995290423861,
      "grad_norm": 4.450253486633301,
      "learning_rate": 4.958006279434851e-05,
      "loss": 0.7515,
      "step": 42800
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 4.440205097198486,
      "learning_rate": 4.9579081632653065e-05,
      "loss": 0.8745,
      "step": 42900
    },
    {
      "epoch": 0.6750392464678179,
      "grad_norm": 4.758547306060791,
      "learning_rate": 4.9578100470957616e-05,
      "loss": 0.8015,
      "step": 43000
    },
    {
      "epoch": 0.6766091051805337,
      "grad_norm": 4.656100273132324,
      "learning_rate": 4.957711930926217e-05,
      "loss": 0.7766,
      "step": 43100
    },
    {
      "epoch": 0.6781789638932496,
      "grad_norm": 4.071404933929443,
      "learning_rate": 4.957613814756672e-05,
      "loss": 0.8269,
      "step": 43200
    },
    {
      "epoch": 0.6797488226059655,
      "grad_norm": 3.8937442302703857,
      "learning_rate": 4.9575156985871275e-05,
      "loss": 0.8191,
      "step": 43300
    },
    {
      "epoch": 0.6813186813186813,
      "grad_norm": 4.244994640350342,
      "learning_rate": 4.9574175824175826e-05,
      "loss": 0.7833,
      "step": 43400
    },
    {
      "epoch": 0.6828885400313972,
      "grad_norm": 4.834120750427246,
      "learning_rate": 4.957319466248038e-05,
      "loss": 0.8064,
      "step": 43500
    },
    {
      "epoch": 0.6844583987441131,
      "grad_norm": 4.144522666931152,
      "learning_rate": 4.957221350078493e-05,
      "loss": 0.7915,
      "step": 43600
    },
    {
      "epoch": 0.6860282574568289,
      "grad_norm": 3.895869731903076,
      "learning_rate": 4.9571232339089486e-05,
      "loss": 0.8127,
      "step": 43700
    },
    {
      "epoch": 0.6875981161695447,
      "grad_norm": 4.47464656829834,
      "learning_rate": 4.957025117739404e-05,
      "loss": 0.8023,
      "step": 43800
    },
    {
      "epoch": 0.6891679748822606,
      "grad_norm": 4.930471897125244,
      "learning_rate": 4.956927001569859e-05,
      "loss": 0.7874,
      "step": 43900
    },
    {
      "epoch": 0.6907378335949764,
      "grad_norm": 4.672404766082764,
      "learning_rate": 4.956828885400314e-05,
      "loss": 0.7856,
      "step": 44000
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 4.356161117553711,
      "learning_rate": 4.95673076923077e-05,
      "loss": 0.8273,
      "step": 44100
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 4.0739359855651855,
      "learning_rate": 4.956632653061225e-05,
      "loss": 0.8137,
      "step": 44200
    },
    {
      "epoch": 0.695447409733124,
      "grad_norm": 2.1109395027160645,
      "learning_rate": 4.95653453689168e-05,
      "loss": 0.8099,
      "step": 44300
    },
    {
      "epoch": 0.6970172684458399,
      "grad_norm": 4.235645294189453,
      "learning_rate": 4.9564364207221356e-05,
      "loss": 0.8275,
      "step": 44400
    },
    {
      "epoch": 0.6985871271585558,
      "grad_norm": 5.277720928192139,
      "learning_rate": 4.956338304552591e-05,
      "loss": 0.8264,
      "step": 44500
    },
    {
      "epoch": 0.7001569858712716,
      "grad_norm": 3.993408203125,
      "learning_rate": 4.956240188383046e-05,
      "loss": 0.775,
      "step": 44600
    },
    {
      "epoch": 0.7017268445839875,
      "grad_norm": 4.822939872741699,
      "learning_rate": 4.956142072213501e-05,
      "loss": 0.7333,
      "step": 44700
    },
    {
      "epoch": 0.7032967032967034,
      "grad_norm": 5.109651565551758,
      "learning_rate": 4.956043956043957e-05,
      "loss": 0.807,
      "step": 44800
    },
    {
      "epoch": 0.7048665620094191,
      "grad_norm": 4.1397905349731445,
      "learning_rate": 4.955945839874411e-05,
      "loss": 0.7714,
      "step": 44900
    },
    {
      "epoch": 0.706436420722135,
      "grad_norm": 3.3680715560913086,
      "learning_rate": 4.955847723704867e-05,
      "loss": 0.8025,
      "step": 45000
    },
    {
      "epoch": 0.7080062794348508,
      "grad_norm": 3.9567527770996094,
      "learning_rate": 4.955749607535322e-05,
      "loss": 0.8215,
      "step": 45100
    },
    {
      "epoch": 0.7095761381475667,
      "grad_norm": 4.196969985961914,
      "learning_rate": 4.955651491365778e-05,
      "loss": 0.7799,
      "step": 45200
    },
    {
      "epoch": 0.7111459968602826,
      "grad_norm": 4.471611499786377,
      "learning_rate": 4.955553375196232e-05,
      "loss": 0.829,
      "step": 45300
    },
    {
      "epoch": 0.7127158555729984,
      "grad_norm": 3.9522032737731934,
      "learning_rate": 4.955455259026688e-05,
      "loss": 0.7936,
      "step": 45400
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 4.300734996795654,
      "learning_rate": 4.955357142857143e-05,
      "loss": 0.7712,
      "step": 45500
    },
    {
      "epoch": 0.7158555729984302,
      "grad_norm": 5.2931060791015625,
      "learning_rate": 4.955259026687598e-05,
      "loss": 0.8279,
      "step": 45600
    },
    {
      "epoch": 0.717425431711146,
      "grad_norm": 4.370993614196777,
      "learning_rate": 4.955160910518053e-05,
      "loss": 0.8486,
      "step": 45700
    },
    {
      "epoch": 0.7189952904238619,
      "grad_norm": 4.842863082885742,
      "learning_rate": 4.955062794348509e-05,
      "loss": 0.8389,
      "step": 45800
    },
    {
      "epoch": 0.7205651491365777,
      "grad_norm": 4.148212432861328,
      "learning_rate": 4.954964678178964e-05,
      "loss": 0.7821,
      "step": 45900
    },
    {
      "epoch": 0.7221350078492935,
      "grad_norm": 4.358235836029053,
      "learning_rate": 4.954866562009419e-05,
      "loss": 0.8045,
      "step": 46000
    },
    {
      "epoch": 0.7237048665620094,
      "grad_norm": 4.371609210968018,
      "learning_rate": 4.9547684458398743e-05,
      "loss": 0.8332,
      "step": 46100
    },
    {
      "epoch": 0.7252747252747253,
      "grad_norm": 5.273245334625244,
      "learning_rate": 4.95467032967033e-05,
      "loss": 0.766,
      "step": 46200
    },
    {
      "epoch": 0.7268445839874411,
      "grad_norm": 4.022218227386475,
      "learning_rate": 4.954572213500785e-05,
      "loss": 0.7717,
      "step": 46300
    },
    {
      "epoch": 0.728414442700157,
      "grad_norm": 4.915586471557617,
      "learning_rate": 4.95447409733124e-05,
      "loss": 0.7923,
      "step": 46400
    },
    {
      "epoch": 0.7299843014128728,
      "grad_norm": 4.439593315124512,
      "learning_rate": 4.954375981161696e-05,
      "loss": 0.7934,
      "step": 46500
    },
    {
      "epoch": 0.7315541601255887,
      "grad_norm": 4.911107540130615,
      "learning_rate": 4.954277864992151e-05,
      "loss": 0.7876,
      "step": 46600
    },
    {
      "epoch": 0.7331240188383046,
      "grad_norm": 4.947508335113525,
      "learning_rate": 4.954179748822606e-05,
      "loss": 0.7893,
      "step": 46700
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 4.673035144805908,
      "learning_rate": 4.9540816326530614e-05,
      "loss": 0.8009,
      "step": 46800
    },
    {
      "epoch": 0.7362637362637363,
      "grad_norm": 4.094566822052002,
      "learning_rate": 4.953983516483517e-05,
      "loss": 0.8034,
      "step": 46900
    },
    {
      "epoch": 0.7378335949764521,
      "grad_norm": 4.076046943664551,
      "learning_rate": 4.9538854003139716e-05,
      "loss": 0.8137,
      "step": 47000
    },
    {
      "epoch": 0.7394034536891679,
      "grad_norm": 5.077541351318359,
      "learning_rate": 4.9537872841444274e-05,
      "loss": 0.8119,
      "step": 47100
    },
    {
      "epoch": 0.7409733124018838,
      "grad_norm": 4.571070671081543,
      "learning_rate": 4.9536891679748825e-05,
      "loss": 0.8355,
      "step": 47200
    },
    {
      "epoch": 0.7425431711145997,
      "grad_norm": 3.5037777423858643,
      "learning_rate": 4.953591051805338e-05,
      "loss": 0.8378,
      "step": 47300
    },
    {
      "epoch": 0.7441130298273155,
      "grad_norm": 5.107666969299316,
      "learning_rate": 4.9534929356357927e-05,
      "loss": 0.8117,
      "step": 47400
    },
    {
      "epoch": 0.7456828885400314,
      "grad_norm": 4.243663311004639,
      "learning_rate": 4.9533948194662484e-05,
      "loss": 0.8038,
      "step": 47500
    },
    {
      "epoch": 0.7472527472527473,
      "grad_norm": 4.997926235198975,
      "learning_rate": 4.9532967032967035e-05,
      "loss": 0.8185,
      "step": 47600
    },
    {
      "epoch": 0.7488226059654631,
      "grad_norm": 3.7149832248687744,
      "learning_rate": 4.9531985871271586e-05,
      "loss": 0.8671,
      "step": 47700
    },
    {
      "epoch": 0.750392464678179,
      "grad_norm": 4.669812202453613,
      "learning_rate": 4.953100470957614e-05,
      "loss": 0.7762,
      "step": 47800
    },
    {
      "epoch": 0.7519623233908949,
      "grad_norm": 4.226022720336914,
      "learning_rate": 4.9530023547880695e-05,
      "loss": 0.8049,
      "step": 47900
    },
    {
      "epoch": 0.7535321821036107,
      "grad_norm": 4.629610538482666,
      "learning_rate": 4.9529042386185246e-05,
      "loss": 0.7928,
      "step": 48000
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 4.241724491119385,
      "learning_rate": 4.95280612244898e-05,
      "loss": 0.8252,
      "step": 48100
    },
    {
      "epoch": 0.7566718995290423,
      "grad_norm": 3.424590587615967,
      "learning_rate": 4.952708006279435e-05,
      "loss": 0.7769,
      "step": 48200
    },
    {
      "epoch": 0.7582417582417582,
      "grad_norm": 3.6467349529266357,
      "learning_rate": 4.9526098901098906e-05,
      "loss": 0.7843,
      "step": 48300
    },
    {
      "epoch": 0.7598116169544741,
      "grad_norm": 4.299863815307617,
      "learning_rate": 4.952511773940346e-05,
      "loss": 0.7767,
      "step": 48400
    },
    {
      "epoch": 0.7613814756671899,
      "grad_norm": 4.1070661544799805,
      "learning_rate": 4.952413657770801e-05,
      "loss": 0.8278,
      "step": 48500
    },
    {
      "epoch": 0.7629513343799058,
      "grad_norm": 4.720722675323486,
      "learning_rate": 4.9523155416012565e-05,
      "loss": 0.7506,
      "step": 48600
    },
    {
      "epoch": 0.7645211930926217,
      "grad_norm": 4.6446123123168945,
      "learning_rate": 4.9522174254317116e-05,
      "loss": 0.7004,
      "step": 48700
    },
    {
      "epoch": 0.7660910518053375,
      "grad_norm": 5.357848644256592,
      "learning_rate": 4.952119309262167e-05,
      "loss": 0.7723,
      "step": 48800
    },
    {
      "epoch": 0.7676609105180534,
      "grad_norm": 4.033183574676514,
      "learning_rate": 4.952021193092622e-05,
      "loss": 0.75,
      "step": 48900
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 5.214511871337891,
      "learning_rate": 4.9519230769230776e-05,
      "loss": 0.7769,
      "step": 49000
    },
    {
      "epoch": 0.7708006279434851,
      "grad_norm": 4.026225566864014,
      "learning_rate": 4.951824960753532e-05,
      "loss": 0.7904,
      "step": 49100
    },
    {
      "epoch": 0.7723704866562009,
      "grad_norm": 4.4892168045043945,
      "learning_rate": 4.951726844583988e-05,
      "loss": 0.8261,
      "step": 49200
    },
    {
      "epoch": 0.7739403453689168,
      "grad_norm": 4.645023345947266,
      "learning_rate": 4.951628728414443e-05,
      "loss": 0.8058,
      "step": 49300
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 2.8961074352264404,
      "learning_rate": 4.951530612244899e-05,
      "loss": 0.7559,
      "step": 49400
    },
    {
      "epoch": 0.7770800627943485,
      "grad_norm": 4.794370651245117,
      "learning_rate": 4.951432496075353e-05,
      "loss": 0.8189,
      "step": 49500
    },
    {
      "epoch": 0.7786499215070644,
      "grad_norm": 4.671910762786865,
      "learning_rate": 4.951334379905809e-05,
      "loss": 0.8214,
      "step": 49600
    },
    {
      "epoch": 0.7802197802197802,
      "grad_norm": 5.2236504554748535,
      "learning_rate": 4.951236263736264e-05,
      "loss": 0.7938,
      "step": 49700
    },
    {
      "epoch": 0.7817896389324961,
      "grad_norm": 5.531501770019531,
      "learning_rate": 4.951138147566719e-05,
      "loss": 0.8107,
      "step": 49800
    },
    {
      "epoch": 0.783359497645212,
      "grad_norm": 4.76591157913208,
      "learning_rate": 4.951040031397174e-05,
      "loss": 0.7694,
      "step": 49900
    },
    {
      "epoch": 0.7849293563579278,
      "grad_norm": 4.873288154602051,
      "learning_rate": 4.95094191522763e-05,
      "loss": 0.7827,
      "step": 50000
    },
    {
      "epoch": 0.7864992150706437,
      "grad_norm": 5.4819488525390625,
      "learning_rate": 4.950843799058085e-05,
      "loss": 0.797,
      "step": 50100
    },
    {
      "epoch": 0.7880690737833596,
      "grad_norm": 3.6167404651641846,
      "learning_rate": 4.95074568288854e-05,
      "loss": 0.7652,
      "step": 50200
    },
    {
      "epoch": 0.7896389324960753,
      "grad_norm": 4.158849239349365,
      "learning_rate": 4.950647566718995e-05,
      "loss": 0.7945,
      "step": 50300
    },
    {
      "epoch": 0.7912087912087912,
      "grad_norm": 4.741782188415527,
      "learning_rate": 4.950549450549451e-05,
      "loss": 0.7583,
      "step": 50400
    },
    {
      "epoch": 0.792778649921507,
      "grad_norm": 3.977769374847412,
      "learning_rate": 4.950451334379906e-05,
      "loss": 0.8326,
      "step": 50500
    },
    {
      "epoch": 0.7943485086342229,
      "grad_norm": 5.568187236785889,
      "learning_rate": 4.950353218210361e-05,
      "loss": 0.8325,
      "step": 50600
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 4.070608139038086,
      "learning_rate": 4.950255102040817e-05,
      "loss": 0.8095,
      "step": 50700
    },
    {
      "epoch": 0.7974882260596546,
      "grad_norm": 4.806958198547363,
      "learning_rate": 4.950156985871272e-05,
      "loss": 0.8134,
      "step": 50800
    },
    {
      "epoch": 0.7990580847723705,
      "grad_norm": 4.121597766876221,
      "learning_rate": 4.950058869701727e-05,
      "loss": 0.7683,
      "step": 50900
    },
    {
      "epoch": 0.8006279434850864,
      "grad_norm": 4.441647052764893,
      "learning_rate": 4.949960753532182e-05,
      "loss": 0.7578,
      "step": 51000
    },
    {
      "epoch": 0.8021978021978022,
      "grad_norm": 5.335739612579346,
      "learning_rate": 4.949862637362638e-05,
      "loss": 0.7598,
      "step": 51100
    },
    {
      "epoch": 0.8037676609105181,
      "grad_norm": 4.13727331161499,
      "learning_rate": 4.9497645211930925e-05,
      "loss": 0.7347,
      "step": 51200
    },
    {
      "epoch": 0.8053375196232339,
      "grad_norm": 4.671699047088623,
      "learning_rate": 4.949666405023548e-05,
      "loss": 0.847,
      "step": 51300
    },
    {
      "epoch": 0.8069073783359497,
      "grad_norm": 4.457029342651367,
      "learning_rate": 4.9495682888540034e-05,
      "loss": 0.7928,
      "step": 51400
    },
    {
      "epoch": 0.8084772370486656,
      "grad_norm": 4.218601703643799,
      "learning_rate": 4.949470172684459e-05,
      "loss": 0.779,
      "step": 51500
    },
    {
      "epoch": 0.8100470957613815,
      "grad_norm": 3.66839861869812,
      "learning_rate": 4.9493720565149135e-05,
      "loss": 0.7546,
      "step": 51600
    },
    {
      "epoch": 0.8116169544740973,
      "grad_norm": 4.08121919631958,
      "learning_rate": 4.949273940345369e-05,
      "loss": 0.7986,
      "step": 51700
    },
    {
      "epoch": 0.8131868131868132,
      "grad_norm": 4.505765438079834,
      "learning_rate": 4.9491758241758244e-05,
      "loss": 0.793,
      "step": 51800
    },
    {
      "epoch": 0.814756671899529,
      "grad_norm": 4.203800201416016,
      "learning_rate": 4.9490777080062795e-05,
      "loss": 0.7912,
      "step": 51900
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 5.390296936035156,
      "learning_rate": 4.9489795918367346e-05,
      "loss": 0.7601,
      "step": 52000
    },
    {
      "epoch": 0.8178963893249608,
      "grad_norm": 3.9066529273986816,
      "learning_rate": 4.9488814756671904e-05,
      "loss": 0.7815,
      "step": 52100
    },
    {
      "epoch": 0.8194662480376766,
      "grad_norm": 4.366098880767822,
      "learning_rate": 4.9487833594976455e-05,
      "loss": 0.8108,
      "step": 52200
    },
    {
      "epoch": 0.8210361067503925,
      "grad_norm": 4.602433204650879,
      "learning_rate": 4.9486852433281006e-05,
      "loss": 0.7757,
      "step": 52300
    },
    {
      "epoch": 0.8226059654631083,
      "grad_norm": 3.335156202316284,
      "learning_rate": 4.948587127158556e-05,
      "loss": 0.7878,
      "step": 52400
    },
    {
      "epoch": 0.8241758241758241,
      "grad_norm": 4.690138339996338,
      "learning_rate": 4.9484890109890115e-05,
      "loss": 0.7747,
      "step": 52500
    },
    {
      "epoch": 0.82574568288854,
      "grad_norm": 5.14065408706665,
      "learning_rate": 4.9483908948194666e-05,
      "loss": 0.7788,
      "step": 52600
    },
    {
      "epoch": 0.8273155416012559,
      "grad_norm": 4.856861114501953,
      "learning_rate": 4.948292778649922e-05,
      "loss": 0.7745,
      "step": 52700
    },
    {
      "epoch": 0.8288854003139717,
      "grad_norm": 4.157369613647461,
      "learning_rate": 4.9481946624803774e-05,
      "loss": 0.8036,
      "step": 52800
    },
    {
      "epoch": 0.8304552590266876,
      "grad_norm": 4.134673595428467,
      "learning_rate": 4.9480965463108325e-05,
      "loss": 0.7528,
      "step": 52900
    },
    {
      "epoch": 0.8320251177394035,
      "grad_norm": 4.87172794342041,
      "learning_rate": 4.9479984301412876e-05,
      "loss": 0.7944,
      "step": 53000
    },
    {
      "epoch": 0.8335949764521193,
      "grad_norm": 4.153871536254883,
      "learning_rate": 4.947900313971743e-05,
      "loss": 0.8089,
      "step": 53100
    },
    {
      "epoch": 0.8351648351648352,
      "grad_norm": 4.635761260986328,
      "learning_rate": 4.9478021978021985e-05,
      "loss": 0.8064,
      "step": 53200
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 3.9576518535614014,
      "learning_rate": 4.947704081632653e-05,
      "loss": 0.8609,
      "step": 53300
    },
    {
      "epoch": 0.8383045525902669,
      "grad_norm": 4.521429061889648,
      "learning_rate": 4.947605965463109e-05,
      "loss": 0.7973,
      "step": 53400
    },
    {
      "epoch": 0.8398744113029827,
      "grad_norm": 4.4791975021362305,
      "learning_rate": 4.947507849293564e-05,
      "loss": 0.7724,
      "step": 53500
    },
    {
      "epoch": 0.8414442700156985,
      "grad_norm": 4.351619243621826,
      "learning_rate": 4.9474097331240196e-05,
      "loss": 0.7904,
      "step": 53600
    },
    {
      "epoch": 0.8430141287284144,
      "grad_norm": 4.2489542961120605,
      "learning_rate": 4.947311616954474e-05,
      "loss": 0.8046,
      "step": 53700
    },
    {
      "epoch": 0.8445839874411303,
      "grad_norm": 4.700350761413574,
      "learning_rate": 4.94721350078493e-05,
      "loss": 0.7727,
      "step": 53800
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 4.180152416229248,
      "learning_rate": 4.947115384615385e-05,
      "loss": 0.7657,
      "step": 53900
    },
    {
      "epoch": 0.847723704866562,
      "grad_norm": 3.6030893325805664,
      "learning_rate": 4.94701726844584e-05,
      "loss": 0.7973,
      "step": 54000
    },
    {
      "epoch": 0.8492935635792779,
      "grad_norm": 3.358302593231201,
      "learning_rate": 4.946919152276295e-05,
      "loss": 0.7205,
      "step": 54100
    },
    {
      "epoch": 0.8508634222919937,
      "grad_norm": 4.1973981857299805,
      "learning_rate": 4.946821036106751e-05,
      "loss": 0.7839,
      "step": 54200
    },
    {
      "epoch": 0.8524332810047096,
      "grad_norm": 3.852498769760132,
      "learning_rate": 4.946722919937206e-05,
      "loss": 0.8525,
      "step": 54300
    },
    {
      "epoch": 0.8540031397174255,
      "grad_norm": 3.99365234375,
      "learning_rate": 4.946624803767661e-05,
      "loss": 0.8019,
      "step": 54400
    },
    {
      "epoch": 0.8555729984301413,
      "grad_norm": 4.403565883636475,
      "learning_rate": 4.946526687598116e-05,
      "loss": 0.8038,
      "step": 54500
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 4.237264156341553,
      "learning_rate": 4.946428571428572e-05,
      "loss": 0.7828,
      "step": 54600
    },
    {
      "epoch": 0.858712715855573,
      "grad_norm": 4.9246602058410645,
      "learning_rate": 4.946330455259027e-05,
      "loss": 0.7605,
      "step": 54700
    },
    {
      "epoch": 0.8602825745682888,
      "grad_norm": 4.106598854064941,
      "learning_rate": 4.946232339089482e-05,
      "loss": 0.777,
      "step": 54800
    },
    {
      "epoch": 0.8618524332810047,
      "grad_norm": 4.8271164894104,
      "learning_rate": 4.946134222919938e-05,
      "loss": 0.8079,
      "step": 54900
    },
    {
      "epoch": 0.8634222919937206,
      "grad_norm": 4.403468132019043,
      "learning_rate": 4.946036106750393e-05,
      "loss": 0.7984,
      "step": 55000
    },
    {
      "epoch": 0.8649921507064364,
      "grad_norm": 3.2624902725219727,
      "learning_rate": 4.945937990580848e-05,
      "loss": 0.803,
      "step": 55100
    },
    {
      "epoch": 0.8665620094191523,
      "grad_norm": 5.10495662689209,
      "learning_rate": 4.945839874411303e-05,
      "loss": 0.8237,
      "step": 55200
    },
    {
      "epoch": 0.8681318681318682,
      "grad_norm": 4.667323112487793,
      "learning_rate": 4.945741758241759e-05,
      "loss": 0.7933,
      "step": 55300
    },
    {
      "epoch": 0.869701726844584,
      "grad_norm": 4.930912017822266,
      "learning_rate": 4.9456436420722134e-05,
      "loss": 0.82,
      "step": 55400
    },
    {
      "epoch": 0.8712715855572999,
      "grad_norm": 4.054489612579346,
      "learning_rate": 4.945545525902669e-05,
      "loss": 0.8034,
      "step": 55500
    },
    {
      "epoch": 0.8728414442700158,
      "grad_norm": 3.563081979751587,
      "learning_rate": 4.945447409733124e-05,
      "loss": 0.7616,
      "step": 55600
    },
    {
      "epoch": 0.8744113029827315,
      "grad_norm": 3.8549752235412598,
      "learning_rate": 4.94534929356358e-05,
      "loss": 0.8263,
      "step": 55700
    },
    {
      "epoch": 0.8759811616954474,
      "grad_norm": 3.4894638061523438,
      "learning_rate": 4.9452511773940344e-05,
      "loss": 0.823,
      "step": 55800
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 4.764000415802002,
      "learning_rate": 4.94515306122449e-05,
      "loss": 0.7905,
      "step": 55900
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 4.222670078277588,
      "learning_rate": 4.945054945054945e-05,
      "loss": 0.7444,
      "step": 56000
    },
    {
      "epoch": 0.880690737833595,
      "grad_norm": 4.4973931312561035,
      "learning_rate": 4.9449568288854004e-05,
      "loss": 0.7677,
      "step": 56100
    },
    {
      "epoch": 0.8822605965463108,
      "grad_norm": 3.717649221420288,
      "learning_rate": 4.9448587127158555e-05,
      "loss": 0.7503,
      "step": 56200
    },
    {
      "epoch": 0.8838304552590267,
      "grad_norm": 3.1822550296783447,
      "learning_rate": 4.944760596546311e-05,
      "loss": 0.7966,
      "step": 56300
    },
    {
      "epoch": 0.8854003139717426,
      "grad_norm": 4.353942394256592,
      "learning_rate": 4.9446624803767664e-05,
      "loss": 0.7831,
      "step": 56400
    },
    {
      "epoch": 0.8869701726844584,
      "grad_norm": 5.910861015319824,
      "learning_rate": 4.9445643642072215e-05,
      "loss": 0.7666,
      "step": 56500
    },
    {
      "epoch": 0.8885400313971743,
      "grad_norm": 4.901097297668457,
      "learning_rate": 4.9444662480376766e-05,
      "loss": 0.8425,
      "step": 56600
    },
    {
      "epoch": 0.8901098901098901,
      "grad_norm": 4.522639751434326,
      "learning_rate": 4.9443681318681324e-05,
      "loss": 0.7548,
      "step": 56700
    },
    {
      "epoch": 0.8916797488226059,
      "grad_norm": 4.1565351486206055,
      "learning_rate": 4.9442700156985875e-05,
      "loss": 0.8211,
      "step": 56800
    },
    {
      "epoch": 0.8932496075353218,
      "grad_norm": 3.9302752017974854,
      "learning_rate": 4.9441718995290426e-05,
      "loss": 0.7799,
      "step": 56900
    },
    {
      "epoch": 0.8948194662480377,
      "grad_norm": 4.499650478363037,
      "learning_rate": 4.944073783359498e-05,
      "loss": 0.7907,
      "step": 57000
    },
    {
      "epoch": 0.8963893249607535,
      "grad_norm": 4.019600868225098,
      "learning_rate": 4.943975667189953e-05,
      "loss": 0.7824,
      "step": 57100
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 4.688340187072754,
      "learning_rate": 4.9438775510204085e-05,
      "loss": 0.7783,
      "step": 57200
    },
    {
      "epoch": 0.8995290423861853,
      "grad_norm": 3.7313051223754883,
      "learning_rate": 4.9437794348508636e-05,
      "loss": 0.8106,
      "step": 57300
    },
    {
      "epoch": 0.9010989010989011,
      "grad_norm": 4.158056259155273,
      "learning_rate": 4.9436813186813194e-05,
      "loss": 0.7987,
      "step": 57400
    },
    {
      "epoch": 0.902668759811617,
      "grad_norm": 4.418284893035889,
      "learning_rate": 4.943583202511774e-05,
      "loss": 0.7892,
      "step": 57500
    },
    {
      "epoch": 0.9042386185243328,
      "grad_norm": 4.428102970123291,
      "learning_rate": 4.9434850863422296e-05,
      "loss": 0.8078,
      "step": 57600
    },
    {
      "epoch": 0.9058084772370487,
      "grad_norm": 4.241779804229736,
      "learning_rate": 4.943386970172685e-05,
      "loss": 0.7727,
      "step": 57700
    },
    {
      "epoch": 0.9073783359497645,
      "grad_norm": 4.942924499511719,
      "learning_rate": 4.94328885400314e-05,
      "loss": 0.7971,
      "step": 57800
    },
    {
      "epoch": 0.9089481946624803,
      "grad_norm": 3.3066787719726562,
      "learning_rate": 4.943190737833595e-05,
      "loss": 0.8168,
      "step": 57900
    },
    {
      "epoch": 0.9105180533751962,
      "grad_norm": 3.420506238937378,
      "learning_rate": 4.943092621664051e-05,
      "loss": 0.8062,
      "step": 58000
    },
    {
      "epoch": 0.9120879120879121,
      "grad_norm": 4.296144008636475,
      "learning_rate": 4.942994505494506e-05,
      "loss": 0.7612,
      "step": 58100
    },
    {
      "epoch": 0.9136577708006279,
      "grad_norm": 5.026120662689209,
      "learning_rate": 4.942896389324961e-05,
      "loss": 0.8044,
      "step": 58200
    },
    {
      "epoch": 0.9152276295133438,
      "grad_norm": 5.829637050628662,
      "learning_rate": 4.942798273155416e-05,
      "loss": 0.7803,
      "step": 58300
    },
    {
      "epoch": 0.9167974882260597,
      "grad_norm": 4.7253828048706055,
      "learning_rate": 4.942700156985872e-05,
      "loss": 0.7989,
      "step": 58400
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 3.320732593536377,
      "learning_rate": 4.942602040816326e-05,
      "loss": 0.7889,
      "step": 58500
    },
    {
      "epoch": 0.9199372056514914,
      "grad_norm": 4.702397346496582,
      "learning_rate": 4.942503924646782e-05,
      "loss": 0.7598,
      "step": 58600
    },
    {
      "epoch": 0.9215070643642073,
      "grad_norm": 4.777096748352051,
      "learning_rate": 4.942405808477237e-05,
      "loss": 0.7753,
      "step": 58700
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 4.505520343780518,
      "learning_rate": 4.942307692307693e-05,
      "loss": 0.7853,
      "step": 58800
    },
    {
      "epoch": 0.9246467817896389,
      "grad_norm": 4.8396100997924805,
      "learning_rate": 4.942209576138148e-05,
      "loss": 0.8338,
      "step": 58900
    },
    {
      "epoch": 0.9262166405023547,
      "grad_norm": 4.810981273651123,
      "learning_rate": 4.942111459968603e-05,
      "loss": 0.7613,
      "step": 59000
    },
    {
      "epoch": 0.9277864992150706,
      "grad_norm": 5.50040340423584,
      "learning_rate": 4.942013343799059e-05,
      "loss": 0.7654,
      "step": 59100
    },
    {
      "epoch": 0.9293563579277865,
      "grad_norm": 4.58488130569458,
      "learning_rate": 4.941915227629513e-05,
      "loss": 0.7779,
      "step": 59200
    },
    {
      "epoch": 0.9309262166405023,
      "grad_norm": 4.215264797210693,
      "learning_rate": 4.941817111459969e-05,
      "loss": 0.8098,
      "step": 59300
    },
    {
      "epoch": 0.9324960753532182,
      "grad_norm": 4.275949001312256,
      "learning_rate": 4.941718995290424e-05,
      "loss": 0.7862,
      "step": 59400
    },
    {
      "epoch": 0.9340659340659341,
      "grad_norm": 5.319999694824219,
      "learning_rate": 4.94162087912088e-05,
      "loss": 0.7775,
      "step": 59500
    },
    {
      "epoch": 0.9356357927786499,
      "grad_norm": 4.717414379119873,
      "learning_rate": 4.941522762951334e-05,
      "loss": 0.8161,
      "step": 59600
    },
    {
      "epoch": 0.9372056514913658,
      "grad_norm": 3.8800721168518066,
      "learning_rate": 4.94142464678179e-05,
      "loss": 0.8283,
      "step": 59700
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 5.022037982940674,
      "learning_rate": 4.941326530612245e-05,
      "loss": 0.7762,
      "step": 59800
    },
    {
      "epoch": 0.9403453689167975,
      "grad_norm": 5.103584289550781,
      "learning_rate": 4.9412284144427e-05,
      "loss": 0.821,
      "step": 59900
    },
    {
      "epoch": 0.9419152276295133,
      "grad_norm": 4.27816104888916,
      "learning_rate": 4.9411302982731553e-05,
      "loss": 0.8195,
      "step": 60000
    },
    {
      "epoch": 0.9434850863422292,
      "grad_norm": 4.77872896194458,
      "learning_rate": 4.941032182103611e-05,
      "loss": 0.772,
      "step": 60100
    },
    {
      "epoch": 0.945054945054945,
      "grad_norm": 4.634819030761719,
      "learning_rate": 4.940934065934066e-05,
      "loss": 0.7698,
      "step": 60200
    },
    {
      "epoch": 0.9466248037676609,
      "grad_norm": 4.479843616485596,
      "learning_rate": 4.940835949764521e-05,
      "loss": 0.7999,
      "step": 60300
    },
    {
      "epoch": 0.9481946624803768,
      "grad_norm": 4.159773826599121,
      "learning_rate": 4.9407378335949764e-05,
      "loss": 0.8096,
      "step": 60400
    },
    {
      "epoch": 0.9497645211930926,
      "grad_norm": 3.958665132522583,
      "learning_rate": 4.940639717425432e-05,
      "loss": 0.7757,
      "step": 60500
    },
    {
      "epoch": 0.9513343799058085,
      "grad_norm": 4.86243200302124,
      "learning_rate": 4.9405416012558866e-05,
      "loss": 0.8037,
      "step": 60600
    },
    {
      "epoch": 0.9529042386185244,
      "grad_norm": 3.2407829761505127,
      "learning_rate": 4.9404434850863424e-05,
      "loss": 0.7823,
      "step": 60700
    },
    {
      "epoch": 0.9544740973312402,
      "grad_norm": 5.047921180725098,
      "learning_rate": 4.9403453689167975e-05,
      "loss": 0.8297,
      "step": 60800
    },
    {
      "epoch": 0.9560439560439561,
      "grad_norm": 4.508062362670898,
      "learning_rate": 4.940247252747253e-05,
      "loss": 0.7687,
      "step": 60900
    },
    {
      "epoch": 0.957613814756672,
      "grad_norm": 5.365232944488525,
      "learning_rate": 4.9401491365777084e-05,
      "loss": 0.7831,
      "step": 61000
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 4.590469837188721,
      "learning_rate": 4.9400510204081635e-05,
      "loss": 0.7506,
      "step": 61100
    },
    {
      "epoch": 0.9607535321821036,
      "grad_norm": 3.511080026626587,
      "learning_rate": 4.9399529042386186e-05,
      "loss": 0.7638,
      "step": 61200
    },
    {
      "epoch": 0.9623233908948194,
      "grad_norm": 4.74686861038208,
      "learning_rate": 4.9398547880690737e-05,
      "loss": 0.7635,
      "step": 61300
    },
    {
      "epoch": 0.9638932496075353,
      "grad_norm": 3.861149549484253,
      "learning_rate": 4.9397566718995294e-05,
      "loss": 0.7961,
      "step": 61400
    },
    {
      "epoch": 0.9654631083202512,
      "grad_norm": 3.756634473800659,
      "learning_rate": 4.9396585557299845e-05,
      "loss": 0.7729,
      "step": 61500
    },
    {
      "epoch": 0.967032967032967,
      "grad_norm": 4.768380641937256,
      "learning_rate": 4.93956043956044e-05,
      "loss": 0.7465,
      "step": 61600
    },
    {
      "epoch": 0.9686028257456829,
      "grad_norm": 4.977330207824707,
      "learning_rate": 4.939462323390895e-05,
      "loss": 0.7554,
      "step": 61700
    },
    {
      "epoch": 0.9701726844583988,
      "grad_norm": 3.89259934425354,
      "learning_rate": 4.9393642072213505e-05,
      "loss": 0.7631,
      "step": 61800
    },
    {
      "epoch": 0.9717425431711146,
      "grad_norm": 4.589770317077637,
      "learning_rate": 4.9392660910518056e-05,
      "loss": 0.7691,
      "step": 61900
    },
    {
      "epoch": 0.9733124018838305,
      "grad_norm": 4.950412750244141,
      "learning_rate": 4.939167974882261e-05,
      "loss": 0.7631,
      "step": 62000
    },
    {
      "epoch": 0.9748822605965463,
      "grad_norm": 4.264495849609375,
      "learning_rate": 4.939069858712716e-05,
      "loss": 0.7842,
      "step": 62100
    },
    {
      "epoch": 0.9764521193092621,
      "grad_norm": 4.878943920135498,
      "learning_rate": 4.9389717425431716e-05,
      "loss": 0.769,
      "step": 62200
    },
    {
      "epoch": 0.978021978021978,
      "grad_norm": 3.518317461013794,
      "learning_rate": 4.938873626373627e-05,
      "loss": 0.7641,
      "step": 62300
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 5.0724663734436035,
      "learning_rate": 4.938775510204082e-05,
      "loss": 0.742,
      "step": 62400
    },
    {
      "epoch": 0.9811616954474097,
      "grad_norm": 5.158936977386475,
      "learning_rate": 4.938677394034537e-05,
      "loss": 0.8099,
      "step": 62500
    },
    {
      "epoch": 0.9827315541601256,
      "grad_norm": 4.6417670249938965,
      "learning_rate": 4.9385792778649926e-05,
      "loss": 0.7827,
      "step": 62600
    },
    {
      "epoch": 0.9843014128728415,
      "grad_norm": 4.874378204345703,
      "learning_rate": 4.938481161695447e-05,
      "loss": 0.7628,
      "step": 62700
    },
    {
      "epoch": 0.9858712715855573,
      "grad_norm": 4.101387977600098,
      "learning_rate": 4.938383045525903e-05,
      "loss": 0.8006,
      "step": 62800
    },
    {
      "epoch": 0.9874411302982732,
      "grad_norm": 4.79415225982666,
      "learning_rate": 4.938284929356358e-05,
      "loss": 0.7946,
      "step": 62900
    },
    {
      "epoch": 0.989010989010989,
      "grad_norm": 4.505658149719238,
      "learning_rate": 4.938186813186814e-05,
      "loss": 0.7696,
      "step": 63000
    },
    {
      "epoch": 0.9905808477237049,
      "grad_norm": 4.9503889083862305,
      "learning_rate": 4.938088697017269e-05,
      "loss": 0.8052,
      "step": 63100
    },
    {
      "epoch": 0.9921507064364207,
      "grad_norm": 4.092396259307861,
      "learning_rate": 4.937990580847724e-05,
      "loss": 0.7747,
      "step": 63200
    },
    {
      "epoch": 0.9937205651491365,
      "grad_norm": 4.275240421295166,
      "learning_rate": 4.937892464678179e-05,
      "loss": 0.7819,
      "step": 63300
    },
    {
      "epoch": 0.9952904238618524,
      "grad_norm": 4.544571876525879,
      "learning_rate": 4.937794348508634e-05,
      "loss": 0.8009,
      "step": 63400
    },
    {
      "epoch": 0.9968602825745683,
      "grad_norm": 3.738800048828125,
      "learning_rate": 4.93769623233909e-05,
      "loss": 0.7328,
      "step": 63500
    },
    {
      "epoch": 0.9984301412872841,
      "grad_norm": 3.7675623893737793,
      "learning_rate": 4.937598116169545e-05,
      "loss": 0.7261,
      "step": 63600
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.1250529289245605,
      "learning_rate": 4.937500000000001e-05,
      "loss": 0.7575,
      "step": 63700
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.0849114656448364,
      "eval_runtime": 14.8135,
      "eval_samples_per_second": 226.348,
      "eval_steps_per_second": 226.348,
      "step": 63700
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6174501180648804,
      "eval_runtime": 280.2919,
      "eval_samples_per_second": 227.263,
      "eval_steps_per_second": 227.263,
      "step": 63700
    },
    {
      "epoch": 1.0015698587127158,
      "grad_norm": 4.735575199127197,
      "learning_rate": 4.937401883830455e-05,
      "loss": 0.7684,
      "step": 63800
    },
    {
      "epoch": 1.0031397174254317,
      "grad_norm": 5.10084867477417,
      "learning_rate": 4.937303767660911e-05,
      "loss": 0.7383,
      "step": 63900
    },
    {
      "epoch": 1.0047095761381475,
      "grad_norm": 5.410142421722412,
      "learning_rate": 4.937205651491366e-05,
      "loss": 0.743,
      "step": 64000
    },
    {
      "epoch": 1.0062794348508635,
      "grad_norm": 4.498143196105957,
      "learning_rate": 4.937107535321821e-05,
      "loss": 0.7887,
      "step": 64100
    },
    {
      "epoch": 1.0078492935635792,
      "grad_norm": 4.059206485748291,
      "learning_rate": 4.937009419152276e-05,
      "loss": 0.7782,
      "step": 64200
    },
    {
      "epoch": 1.0094191522762952,
      "grad_norm": 3.685535192489624,
      "learning_rate": 4.936911302982732e-05,
      "loss": 0.7757,
      "step": 64300
    },
    {
      "epoch": 1.010989010989011,
      "grad_norm": 4.831624984741211,
      "learning_rate": 4.936813186813187e-05,
      "loss": 0.8196,
      "step": 64400
    },
    {
      "epoch": 1.012558869701727,
      "grad_norm": 4.983075141906738,
      "learning_rate": 4.936715070643642e-05,
      "loss": 0.8081,
      "step": 64500
    },
    {
      "epoch": 1.0141287284144427,
      "grad_norm": 4.488121509552002,
      "learning_rate": 4.936616954474097e-05,
      "loss": 0.7811,
      "step": 64600
    },
    {
      "epoch": 1.0156985871271587,
      "grad_norm": 4.968857288360596,
      "learning_rate": 4.936518838304553e-05,
      "loss": 0.7916,
      "step": 64700
    },
    {
      "epoch": 1.0172684458398744,
      "grad_norm": 5.041505336761475,
      "learning_rate": 4.9364207221350075e-05,
      "loss": 0.7679,
      "step": 64800
    },
    {
      "epoch": 1.0188383045525902,
      "grad_norm": 4.59891414642334,
      "learning_rate": 4.936322605965463e-05,
      "loss": 0.7808,
      "step": 64900
    },
    {
      "epoch": 1.0204081632653061,
      "grad_norm": 4.516188621520996,
      "learning_rate": 4.9362244897959184e-05,
      "loss": 0.786,
      "step": 65000
    },
    {
      "epoch": 1.021978021978022,
      "grad_norm": 4.342172622680664,
      "learning_rate": 4.936126373626374e-05,
      "loss": 0.8072,
      "step": 65100
    },
    {
      "epoch": 1.0235478806907379,
      "grad_norm": 4.053976535797119,
      "learning_rate": 4.936028257456829e-05,
      "loss": 0.7411,
      "step": 65200
    },
    {
      "epoch": 1.0251177394034536,
      "grad_norm": 2.9384498596191406,
      "learning_rate": 4.9359301412872844e-05,
      "loss": 0.7737,
      "step": 65300
    },
    {
      "epoch": 1.0266875981161696,
      "grad_norm": 4.305695056915283,
      "learning_rate": 4.9358320251177395e-05,
      "loss": 0.7426,
      "step": 65400
    },
    {
      "epoch": 1.0282574568288854,
      "grad_norm": 4.207804203033447,
      "learning_rate": 4.9357339089481946e-05,
      "loss": 0.7389,
      "step": 65500
    },
    {
      "epoch": 1.0298273155416013,
      "grad_norm": 4.68092679977417,
      "learning_rate": 4.93563579277865e-05,
      "loss": 0.7739,
      "step": 65600
    },
    {
      "epoch": 1.031397174254317,
      "grad_norm": 3.606738328933716,
      "learning_rate": 4.9355376766091054e-05,
      "loss": 0.7793,
      "step": 65700
    },
    {
      "epoch": 1.032967032967033,
      "grad_norm": 4.4795002937316895,
      "learning_rate": 4.935439560439561e-05,
      "loss": 0.7823,
      "step": 65800
    },
    {
      "epoch": 1.0345368916797488,
      "grad_norm": 5.951931476593018,
      "learning_rate": 4.9353414442700156e-05,
      "loss": 0.7564,
      "step": 65900
    },
    {
      "epoch": 1.0361067503924646,
      "grad_norm": 4.110368728637695,
      "learning_rate": 4.9352433281004714e-05,
      "loss": 0.7547,
      "step": 66000
    },
    {
      "epoch": 1.0376766091051806,
      "grad_norm": 3.8160438537597656,
      "learning_rate": 4.9351452119309265e-05,
      "loss": 0.805,
      "step": 66100
    },
    {
      "epoch": 1.0392464678178963,
      "grad_norm": 4.310070514678955,
      "learning_rate": 4.9350470957613816e-05,
      "loss": 0.7705,
      "step": 66200
    },
    {
      "epoch": 1.0408163265306123,
      "grad_norm": 4.268405914306641,
      "learning_rate": 4.934948979591837e-05,
      "loss": 0.7402,
      "step": 66300
    },
    {
      "epoch": 1.042386185243328,
      "grad_norm": 3.6014232635498047,
      "learning_rate": 4.9348508634222925e-05,
      "loss": 0.76,
      "step": 66400
    },
    {
      "epoch": 1.043956043956044,
      "grad_norm": 3.0962562561035156,
      "learning_rate": 4.9347527472527476e-05,
      "loss": 0.7333,
      "step": 66500
    },
    {
      "epoch": 1.0455259026687598,
      "grad_norm": 4.29154109954834,
      "learning_rate": 4.934654631083203e-05,
      "loss": 0.7624,
      "step": 66600
    },
    {
      "epoch": 1.0470957613814758,
      "grad_norm": 4.773361682891846,
      "learning_rate": 4.934556514913658e-05,
      "loss": 0.7864,
      "step": 66700
    },
    {
      "epoch": 1.0486656200941915,
      "grad_norm": 4.320716857910156,
      "learning_rate": 4.9344583987441135e-05,
      "loss": 0.7501,
      "step": 66800
    },
    {
      "epoch": 1.0502354788069075,
      "grad_norm": 3.811213493347168,
      "learning_rate": 4.934360282574568e-05,
      "loss": 0.7482,
      "step": 66900
    },
    {
      "epoch": 1.0518053375196232,
      "grad_norm": 4.312719821929932,
      "learning_rate": 4.934262166405024e-05,
      "loss": 0.7766,
      "step": 67000
    },
    {
      "epoch": 1.053375196232339,
      "grad_norm": 4.188240051269531,
      "learning_rate": 4.934164050235479e-05,
      "loss": 0.7435,
      "step": 67100
    },
    {
      "epoch": 1.054945054945055,
      "grad_norm": 4.6111578941345215,
      "learning_rate": 4.9340659340659346e-05,
      "loss": 0.8119,
      "step": 67200
    },
    {
      "epoch": 1.0565149136577707,
      "grad_norm": 5.288651943206787,
      "learning_rate": 4.93396781789639e-05,
      "loss": 0.7739,
      "step": 67300
    },
    {
      "epoch": 1.0580847723704867,
      "grad_norm": 3.793752908706665,
      "learning_rate": 4.933869701726845e-05,
      "loss": 0.7642,
      "step": 67400
    },
    {
      "epoch": 1.0596546310832025,
      "grad_norm": 4.0095062255859375,
      "learning_rate": 4.9337715855573e-05,
      "loss": 0.7377,
      "step": 67500
    },
    {
      "epoch": 1.0612244897959184,
      "grad_norm": 6.023867607116699,
      "learning_rate": 4.933673469387755e-05,
      "loss": 0.7575,
      "step": 67600
    },
    {
      "epoch": 1.0627943485086342,
      "grad_norm": 5.041622638702393,
      "learning_rate": 4.933575353218211e-05,
      "loss": 0.7165,
      "step": 67700
    },
    {
      "epoch": 1.0643642072213502,
      "grad_norm": 4.919727802276611,
      "learning_rate": 4.933477237048666e-05,
      "loss": 0.7462,
      "step": 67800
    },
    {
      "epoch": 1.065934065934066,
      "grad_norm": 4.89773416519165,
      "learning_rate": 4.9333791208791217e-05,
      "loss": 0.7314,
      "step": 67900
    },
    {
      "epoch": 1.0675039246467817,
      "grad_norm": 5.5291290283203125,
      "learning_rate": 4.933281004709576e-05,
      "loss": 0.8505,
      "step": 68000
    },
    {
      "epoch": 1.0690737833594977,
      "grad_norm": 4.243226528167725,
      "learning_rate": 4.933182888540032e-05,
      "loss": 0.749,
      "step": 68100
    },
    {
      "epoch": 1.0706436420722134,
      "grad_norm": 4.328510761260986,
      "learning_rate": 4.933084772370487e-05,
      "loss": 0.8235,
      "step": 68200
    },
    {
      "epoch": 1.0722135007849294,
      "grad_norm": 3.4768388271331787,
      "learning_rate": 4.932986656200942e-05,
      "loss": 0.7981,
      "step": 68300
    },
    {
      "epoch": 1.0737833594976451,
      "grad_norm": 3.8654887676239014,
      "learning_rate": 4.932888540031397e-05,
      "loss": 0.7537,
      "step": 68400
    },
    {
      "epoch": 1.0753532182103611,
      "grad_norm": 4.518398284912109,
      "learning_rate": 4.932790423861853e-05,
      "loss": 0.7452,
      "step": 68500
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 3.4398064613342285,
      "learning_rate": 4.932692307692308e-05,
      "loss": 0.7516,
      "step": 68600
    },
    {
      "epoch": 1.0784929356357928,
      "grad_norm": 4.807605266571045,
      "learning_rate": 4.932594191522763e-05,
      "loss": 0.8114,
      "step": 68700
    },
    {
      "epoch": 1.0800627943485086,
      "grad_norm": 3.8993258476257324,
      "learning_rate": 4.932496075353218e-05,
      "loss": 0.8226,
      "step": 68800
    },
    {
      "epoch": 1.0816326530612246,
      "grad_norm": 3.788233995437622,
      "learning_rate": 4.932397959183674e-05,
      "loss": 0.7972,
      "step": 68900
    },
    {
      "epoch": 1.0832025117739403,
      "grad_norm": 3.390704870223999,
      "learning_rate": 4.9322998430141284e-05,
      "loss": 0.7188,
      "step": 69000
    },
    {
      "epoch": 1.084772370486656,
      "grad_norm": 4.252943515777588,
      "learning_rate": 4.932201726844584e-05,
      "loss": 0.7853,
      "step": 69100
    },
    {
      "epoch": 1.086342229199372,
      "grad_norm": 4.41823673248291,
      "learning_rate": 4.932103610675039e-05,
      "loss": 0.7575,
      "step": 69200
    },
    {
      "epoch": 1.0879120879120878,
      "grad_norm": 5.225766658782959,
      "learning_rate": 4.932005494505495e-05,
      "loss": 0.7404,
      "step": 69300
    },
    {
      "epoch": 1.0894819466248038,
      "grad_norm": 5.0644707679748535,
      "learning_rate": 4.93190737833595e-05,
      "loss": 0.77,
      "step": 69400
    },
    {
      "epoch": 1.0910518053375196,
      "grad_norm": 4.065659046173096,
      "learning_rate": 4.931809262166405e-05,
      "loss": 0.7621,
      "step": 69500
    },
    {
      "epoch": 1.0926216640502355,
      "grad_norm": 4.054235458374023,
      "learning_rate": 4.9317111459968604e-05,
      "loss": 0.76,
      "step": 69600
    },
    {
      "epoch": 1.0941915227629513,
      "grad_norm": 5.1040167808532715,
      "learning_rate": 4.9316130298273155e-05,
      "loss": 0.7409,
      "step": 69700
    },
    {
      "epoch": 1.0957613814756673,
      "grad_norm": 3.958892583847046,
      "learning_rate": 4.931514913657771e-05,
      "loss": 0.8131,
      "step": 69800
    },
    {
      "epoch": 1.097331240188383,
      "grad_norm": 3.2628085613250732,
      "learning_rate": 4.931416797488226e-05,
      "loss": 0.7419,
      "step": 69900
    },
    {
      "epoch": 1.098901098901099,
      "grad_norm": 3.4590272903442383,
      "learning_rate": 4.931318681318682e-05,
      "loss": 0.7305,
      "step": 70000
    },
    {
      "epoch": 1.1004709576138147,
      "grad_norm": 4.713141918182373,
      "learning_rate": 4.9312205651491365e-05,
      "loss": 0.7603,
      "step": 70100
    },
    {
      "epoch": 1.1020408163265305,
      "grad_norm": 4.7150492668151855,
      "learning_rate": 4.931122448979592e-05,
      "loss": 0.7576,
      "step": 70200
    },
    {
      "epoch": 1.1036106750392465,
      "grad_norm": 4.924499988555908,
      "learning_rate": 4.9310243328100474e-05,
      "loss": 0.7733,
      "step": 70300
    },
    {
      "epoch": 1.1051805337519622,
      "grad_norm": 4.781326770782471,
      "learning_rate": 4.9309262166405025e-05,
      "loss": 0.7481,
      "step": 70400
    },
    {
      "epoch": 1.1067503924646782,
      "grad_norm": 4.246909141540527,
      "learning_rate": 4.9308281004709576e-05,
      "loss": 0.7652,
      "step": 70500
    },
    {
      "epoch": 1.108320251177394,
      "grad_norm": 4.785752296447754,
      "learning_rate": 4.9307299843014134e-05,
      "loss": 0.7608,
      "step": 70600
    },
    {
      "epoch": 1.10989010989011,
      "grad_norm": 4.253579139709473,
      "learning_rate": 4.9306318681318685e-05,
      "loss": 0.7978,
      "step": 70700
    },
    {
      "epoch": 1.1114599686028257,
      "grad_norm": 4.0628557205200195,
      "learning_rate": 4.9305337519623236e-05,
      "loss": 0.7687,
      "step": 70800
    },
    {
      "epoch": 1.1130298273155417,
      "grad_norm": 4.115346431732178,
      "learning_rate": 4.930435635792779e-05,
      "loss": 0.7672,
      "step": 70900
    },
    {
      "epoch": 1.1145996860282574,
      "grad_norm": 3.3610916137695312,
      "learning_rate": 4.9303375196232344e-05,
      "loss": 0.7617,
      "step": 71000
    },
    {
      "epoch": 1.1161695447409734,
      "grad_norm": 4.448246955871582,
      "learning_rate": 4.930239403453689e-05,
      "loss": 0.712,
      "step": 71100
    },
    {
      "epoch": 1.1177394034536892,
      "grad_norm": 4.917850971221924,
      "learning_rate": 4.9301412872841446e-05,
      "loss": 0.7402,
      "step": 71200
    },
    {
      "epoch": 1.119309262166405,
      "grad_norm": 5.000461101531982,
      "learning_rate": 4.9300431711146e-05,
      "loss": 0.7846,
      "step": 71300
    },
    {
      "epoch": 1.120879120879121,
      "grad_norm": 4.254511833190918,
      "learning_rate": 4.9299450549450555e-05,
      "loss": 0.8019,
      "step": 71400
    },
    {
      "epoch": 1.1224489795918366,
      "grad_norm": 4.670138359069824,
      "learning_rate": 4.9298469387755106e-05,
      "loss": 0.7817,
      "step": 71500
    },
    {
      "epoch": 1.1240188383045526,
      "grad_norm": 4.252340316772461,
      "learning_rate": 4.929748822605966e-05,
      "loss": 0.8107,
      "step": 71600
    },
    {
      "epoch": 1.1255886970172684,
      "grad_norm": 4.739249229431152,
      "learning_rate": 4.929650706436421e-05,
      "loss": 0.7627,
      "step": 71700
    },
    {
      "epoch": 1.1271585557299844,
      "grad_norm": 3.2835488319396973,
      "learning_rate": 4.929552590266876e-05,
      "loss": 0.7586,
      "step": 71800
    },
    {
      "epoch": 1.1287284144427001,
      "grad_norm": 4.600184917449951,
      "learning_rate": 4.929454474097332e-05,
      "loss": 0.7501,
      "step": 71900
    },
    {
      "epoch": 1.130298273155416,
      "grad_norm": 3.744539976119995,
      "learning_rate": 4.929356357927787e-05,
      "loss": 0.7423,
      "step": 72000
    },
    {
      "epoch": 1.1318681318681318,
      "grad_norm": 4.760754108428955,
      "learning_rate": 4.9292582417582425e-05,
      "loss": 0.7997,
      "step": 72100
    },
    {
      "epoch": 1.1334379905808478,
      "grad_norm": 4.347365379333496,
      "learning_rate": 4.929160125588697e-05,
      "loss": 0.771,
      "step": 72200
    },
    {
      "epoch": 1.1350078492935636,
      "grad_norm": 4.141330718994141,
      "learning_rate": 4.929062009419153e-05,
      "loss": 0.7434,
      "step": 72300
    },
    {
      "epoch": 1.1365777080062793,
      "grad_norm": 4.2054219245910645,
      "learning_rate": 4.928963893249608e-05,
      "loss": 0.7595,
      "step": 72400
    },
    {
      "epoch": 1.1381475667189953,
      "grad_norm": 3.8328094482421875,
      "learning_rate": 4.928865777080063e-05,
      "loss": 0.7944,
      "step": 72500
    },
    {
      "epoch": 1.139717425431711,
      "grad_norm": 3.7347252368927,
      "learning_rate": 4.928767660910518e-05,
      "loss": 0.7626,
      "step": 72600
    },
    {
      "epoch": 1.141287284144427,
      "grad_norm": 4.485983848571777,
      "learning_rate": 4.928669544740974e-05,
      "loss": 0.808,
      "step": 72700
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 3.9514505863189697,
      "learning_rate": 4.928571428571429e-05,
      "loss": 0.7554,
      "step": 72800
    },
    {
      "epoch": 1.1444270015698588,
      "grad_norm": 3.8675410747528076,
      "learning_rate": 4.928473312401884e-05,
      "loss": 0.7615,
      "step": 72900
    },
    {
      "epoch": 1.1459968602825745,
      "grad_norm": 3.922215223312378,
      "learning_rate": 4.928375196232339e-05,
      "loss": 0.7819,
      "step": 73000
    },
    {
      "epoch": 1.1475667189952905,
      "grad_norm": 4.844797611236572,
      "learning_rate": 4.928277080062795e-05,
      "loss": 0.7326,
      "step": 73100
    },
    {
      "epoch": 1.1491365777080063,
      "grad_norm": 4.830899238586426,
      "learning_rate": 4.928178963893249e-05,
      "loss": 0.7179,
      "step": 73200
    },
    {
      "epoch": 1.1507064364207222,
      "grad_norm": 5.588432312011719,
      "learning_rate": 4.928080847723705e-05,
      "loss": 0.7558,
      "step": 73300
    },
    {
      "epoch": 1.152276295133438,
      "grad_norm": 4.542299270629883,
      "learning_rate": 4.92798273155416e-05,
      "loss": 0.7708,
      "step": 73400
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 3.761993646621704,
      "learning_rate": 4.927884615384616e-05,
      "loss": 0.7695,
      "step": 73500
    },
    {
      "epoch": 1.1554160125588697,
      "grad_norm": 4.378283977508545,
      "learning_rate": 4.927786499215071e-05,
      "loss": 0.7728,
      "step": 73600
    },
    {
      "epoch": 1.1569858712715855,
      "grad_norm": 3.736405611038208,
      "learning_rate": 4.927688383045526e-05,
      "loss": 0.7744,
      "step": 73700
    },
    {
      "epoch": 1.1585557299843015,
      "grad_norm": 3.738497495651245,
      "learning_rate": 4.927590266875981e-05,
      "loss": 0.7685,
      "step": 73800
    },
    {
      "epoch": 1.1601255886970172,
      "grad_norm": 4.773867130279541,
      "learning_rate": 4.9274921507064363e-05,
      "loss": 0.7463,
      "step": 73900
    },
    {
      "epoch": 1.1616954474097332,
      "grad_norm": 4.05750846862793,
      "learning_rate": 4.927394034536892e-05,
      "loss": 0.7809,
      "step": 74000
    },
    {
      "epoch": 1.163265306122449,
      "grad_norm": 4.350325107574463,
      "learning_rate": 4.927295918367347e-05,
      "loss": 0.7697,
      "step": 74100
    },
    {
      "epoch": 1.164835164835165,
      "grad_norm": 4.229294300079346,
      "learning_rate": 4.927197802197803e-05,
      "loss": 0.7232,
      "step": 74200
    },
    {
      "epoch": 1.1664050235478807,
      "grad_norm": 3.7898130416870117,
      "learning_rate": 4.9270996860282574e-05,
      "loss": 0.7519,
      "step": 74300
    },
    {
      "epoch": 1.1679748822605966,
      "grad_norm": 5.044505596160889,
      "learning_rate": 4.927001569858713e-05,
      "loss": 0.7066,
      "step": 74400
    },
    {
      "epoch": 1.1695447409733124,
      "grad_norm": 4.7236647605896,
      "learning_rate": 4.926903453689168e-05,
      "loss": 0.763,
      "step": 74500
    },
    {
      "epoch": 1.1711145996860282,
      "grad_norm": 4.515085220336914,
      "learning_rate": 4.9268053375196234e-05,
      "loss": 0.7622,
      "step": 74600
    },
    {
      "epoch": 1.1726844583987441,
      "grad_norm": 4.182746410369873,
      "learning_rate": 4.9267072213500785e-05,
      "loss": 0.7563,
      "step": 74700
    },
    {
      "epoch": 1.1742543171114599,
      "grad_norm": 4.507289409637451,
      "learning_rate": 4.926609105180534e-05,
      "loss": 0.7344,
      "step": 74800
    },
    {
      "epoch": 1.1758241758241759,
      "grad_norm": 3.416134834289551,
      "learning_rate": 4.9265109890109894e-05,
      "loss": 0.7931,
      "step": 74900
    },
    {
      "epoch": 1.1773940345368916,
      "grad_norm": 4.554042816162109,
      "learning_rate": 4.9264128728414445e-05,
      "loss": 0.7502,
      "step": 75000
    },
    {
      "epoch": 1.1789638932496076,
      "grad_norm": 4.207318305969238,
      "learning_rate": 4.9263147566718996e-05,
      "loss": 0.7865,
      "step": 75100
    },
    {
      "epoch": 1.1805337519623234,
      "grad_norm": 4.379655361175537,
      "learning_rate": 4.926216640502355e-05,
      "loss": 0.7458,
      "step": 75200
    },
    {
      "epoch": 1.1821036106750393,
      "grad_norm": 5.083130836486816,
      "learning_rate": 4.92611852433281e-05,
      "loss": 0.7758,
      "step": 75300
    },
    {
      "epoch": 1.183673469387755,
      "grad_norm": 4.174050331115723,
      "learning_rate": 4.9260204081632655e-05,
      "loss": 0.7735,
      "step": 75400
    },
    {
      "epoch": 1.185243328100471,
      "grad_norm": 4.466348171234131,
      "learning_rate": 4.9259222919937206e-05,
      "loss": 0.7498,
      "step": 75500
    },
    {
      "epoch": 1.1868131868131868,
      "grad_norm": 3.778017997741699,
      "learning_rate": 4.9258241758241764e-05,
      "loss": 0.7354,
      "step": 75600
    },
    {
      "epoch": 1.1883830455259026,
      "grad_norm": 3.678828239440918,
      "learning_rate": 4.9257260596546315e-05,
      "loss": 0.7398,
      "step": 75700
    },
    {
      "epoch": 1.1899529042386185,
      "grad_norm": 3.4315977096557617,
      "learning_rate": 4.9256279434850866e-05,
      "loss": 0.7614,
      "step": 75800
    },
    {
      "epoch": 1.1915227629513343,
      "grad_norm": 4.527956962585449,
      "learning_rate": 4.925529827315542e-05,
      "loss": 0.7824,
      "step": 75900
    },
    {
      "epoch": 1.1930926216640503,
      "grad_norm": 3.343449115753174,
      "learning_rate": 4.925431711145997e-05,
      "loss": 0.754,
      "step": 76000
    },
    {
      "epoch": 1.194662480376766,
      "grad_norm": 4.6009840965271,
      "learning_rate": 4.9253335949764526e-05,
      "loss": 0.7357,
      "step": 76100
    },
    {
      "epoch": 1.196232339089482,
      "grad_norm": 4.433352470397949,
      "learning_rate": 4.925235478806908e-05,
      "loss": 0.7644,
      "step": 76200
    },
    {
      "epoch": 1.1978021978021978,
      "grad_norm": 5.265154838562012,
      "learning_rate": 4.9251373626373634e-05,
      "loss": 0.7775,
      "step": 76300
    },
    {
      "epoch": 1.1993720565149137,
      "grad_norm": 4.484283447265625,
      "learning_rate": 4.925039246467818e-05,
      "loss": 0.7961,
      "step": 76400
    },
    {
      "epoch": 1.2009419152276295,
      "grad_norm": 4.174435138702393,
      "learning_rate": 4.9249411302982736e-05,
      "loss": 0.7946,
      "step": 76500
    },
    {
      "epoch": 1.2025117739403455,
      "grad_norm": 4.30825138092041,
      "learning_rate": 4.924843014128729e-05,
      "loss": 0.776,
      "step": 76600
    },
    {
      "epoch": 1.2040816326530612,
      "grad_norm": 3.5419061183929443,
      "learning_rate": 4.924744897959184e-05,
      "loss": 0.7512,
      "step": 76700
    },
    {
      "epoch": 1.205651491365777,
      "grad_norm": 4.659671783447266,
      "learning_rate": 4.924646781789639e-05,
      "loss": 0.7572,
      "step": 76800
    },
    {
      "epoch": 1.207221350078493,
      "grad_norm": 5.214090347290039,
      "learning_rate": 4.924548665620095e-05,
      "loss": 0.7479,
      "step": 76900
    },
    {
      "epoch": 1.2087912087912087,
      "grad_norm": 4.455277442932129,
      "learning_rate": 4.92445054945055e-05,
      "loss": 0.7503,
      "step": 77000
    },
    {
      "epoch": 1.2103610675039247,
      "grad_norm": 4.326798439025879,
      "learning_rate": 4.924352433281005e-05,
      "loss": 0.7628,
      "step": 77100
    },
    {
      "epoch": 1.2119309262166404,
      "grad_norm": 4.6865997314453125,
      "learning_rate": 4.92425431711146e-05,
      "loss": 0.7636,
      "step": 77200
    },
    {
      "epoch": 1.2135007849293564,
      "grad_norm": 4.510838508605957,
      "learning_rate": 4.924156200941916e-05,
      "loss": 0.7807,
      "step": 77300
    },
    {
      "epoch": 1.2150706436420722,
      "grad_norm": 4.178847789764404,
      "learning_rate": 4.92405808477237e-05,
      "loss": 0.7417,
      "step": 77400
    },
    {
      "epoch": 1.2166405023547882,
      "grad_norm": 3.9243884086608887,
      "learning_rate": 4.923959968602826e-05,
      "loss": 0.8151,
      "step": 77500
    },
    {
      "epoch": 1.218210361067504,
      "grad_norm": 3.8695285320281982,
      "learning_rate": 4.923861852433281e-05,
      "loss": 0.7774,
      "step": 77600
    },
    {
      "epoch": 1.2197802197802199,
      "grad_norm": 3.8613743782043457,
      "learning_rate": 4.923763736263737e-05,
      "loss": 0.7487,
      "step": 77700
    },
    {
      "epoch": 1.2213500784929356,
      "grad_norm": 4.893017292022705,
      "learning_rate": 4.923665620094192e-05,
      "loss": 0.7728,
      "step": 77800
    },
    {
      "epoch": 1.2229199372056514,
      "grad_norm": 4.628710746765137,
      "learning_rate": 4.923567503924647e-05,
      "loss": 0.7269,
      "step": 77900
    },
    {
      "epoch": 1.2244897959183674,
      "grad_norm": 3.9759554862976074,
      "learning_rate": 4.923469387755102e-05,
      "loss": 0.7335,
      "step": 78000
    },
    {
      "epoch": 1.2260596546310831,
      "grad_norm": 4.162051200866699,
      "learning_rate": 4.923371271585557e-05,
      "loss": 0.7611,
      "step": 78100
    },
    {
      "epoch": 1.227629513343799,
      "grad_norm": 3.1550369262695312,
      "learning_rate": 4.923273155416013e-05,
      "loss": 0.7509,
      "step": 78200
    },
    {
      "epoch": 1.2291993720565149,
      "grad_norm": 4.325747489929199,
      "learning_rate": 4.923175039246468e-05,
      "loss": 0.7548,
      "step": 78300
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 4.555430889129639,
      "learning_rate": 4.923076923076924e-05,
      "loss": 0.7671,
      "step": 78400
    },
    {
      "epoch": 1.2323390894819466,
      "grad_norm": 3.6396825313568115,
      "learning_rate": 4.922978806907378e-05,
      "loss": 0.7419,
      "step": 78500
    },
    {
      "epoch": 1.2339089481946626,
      "grad_norm": 4.309146881103516,
      "learning_rate": 4.922880690737834e-05,
      "loss": 0.7222,
      "step": 78600
    },
    {
      "epoch": 1.2354788069073783,
      "grad_norm": 4.630756855010986,
      "learning_rate": 4.922782574568289e-05,
      "loss": 0.7655,
      "step": 78700
    },
    {
      "epoch": 1.2370486656200943,
      "grad_norm": 4.492467880249023,
      "learning_rate": 4.922684458398744e-05,
      "loss": 0.7193,
      "step": 78800
    },
    {
      "epoch": 1.23861852433281,
      "grad_norm": 4.713857173919678,
      "learning_rate": 4.9225863422291994e-05,
      "loss": 0.774,
      "step": 78900
    },
    {
      "epoch": 1.2401883830455258,
      "grad_norm": 4.312688827514648,
      "learning_rate": 4.922488226059655e-05,
      "loss": 0.7385,
      "step": 79000
    },
    {
      "epoch": 1.2417582417582418,
      "grad_norm": 4.5998148918151855,
      "learning_rate": 4.92239010989011e-05,
      "loss": 0.764,
      "step": 79100
    },
    {
      "epoch": 1.2433281004709575,
      "grad_norm": 3.3202970027923584,
      "learning_rate": 4.9222919937205654e-05,
      "loss": 0.794,
      "step": 79200
    },
    {
      "epoch": 1.2448979591836735,
      "grad_norm": 4.351808071136475,
      "learning_rate": 4.9221938775510205e-05,
      "loss": 0.7774,
      "step": 79300
    },
    {
      "epoch": 1.2464678178963893,
      "grad_norm": 4.480055332183838,
      "learning_rate": 4.922095761381476e-05,
      "loss": 0.7564,
      "step": 79400
    },
    {
      "epoch": 1.2480376766091053,
      "grad_norm": 3.808682441711426,
      "learning_rate": 4.9219976452119307e-05,
      "loss": 0.7719,
      "step": 79500
    },
    {
      "epoch": 1.249607535321821,
      "grad_norm": 4.413092136383057,
      "learning_rate": 4.9218995290423864e-05,
      "loss": 0.7487,
      "step": 79600
    },
    {
      "epoch": 1.2511773940345368,
      "grad_norm": 3.805490255355835,
      "learning_rate": 4.9218014128728415e-05,
      "loss": 0.7454,
      "step": 79700
    },
    {
      "epoch": 1.2527472527472527,
      "grad_norm": 3.886428117752075,
      "learning_rate": 4.9217032967032966e-05,
      "loss": 0.7771,
      "step": 79800
    },
    {
      "epoch": 1.2543171114599687,
      "grad_norm": 3.833555221557617,
      "learning_rate": 4.9216051805337524e-05,
      "loss": 0.7309,
      "step": 79900
    },
    {
      "epoch": 1.2558869701726845,
      "grad_norm": 4.647435665130615,
      "learning_rate": 4.9215070643642075e-05,
      "loss": 0.7633,
      "step": 80000
    },
    {
      "epoch": 1.2574568288854002,
      "grad_norm": 4.678268909454346,
      "learning_rate": 4.9214089481946626e-05,
      "loss": 0.7374,
      "step": 80100
    },
    {
      "epoch": 1.2590266875981162,
      "grad_norm": 4.438427448272705,
      "learning_rate": 4.921310832025118e-05,
      "loss": 0.8024,
      "step": 80200
    },
    {
      "epoch": 1.260596546310832,
      "grad_norm": 4.053921222686768,
      "learning_rate": 4.9212127158555735e-05,
      "loss": 0.7777,
      "step": 80300
    },
    {
      "epoch": 1.262166405023548,
      "grad_norm": 4.349181175231934,
      "learning_rate": 4.9211145996860286e-05,
      "loss": 0.7839,
      "step": 80400
    },
    {
      "epoch": 1.2637362637362637,
      "grad_norm": 4.525758743286133,
      "learning_rate": 4.921016483516484e-05,
      "loss": 0.7964,
      "step": 80500
    },
    {
      "epoch": 1.2653061224489797,
      "grad_norm": 4.056788444519043,
      "learning_rate": 4.920918367346939e-05,
      "loss": 0.7124,
      "step": 80600
    },
    {
      "epoch": 1.2668759811616954,
      "grad_norm": 3.9436557292938232,
      "learning_rate": 4.9208202511773945e-05,
      "loss": 0.7802,
      "step": 80700
    },
    {
      "epoch": 1.2684458398744112,
      "grad_norm": 4.247996807098389,
      "learning_rate": 4.9207221350078496e-05,
      "loss": 0.7733,
      "step": 80800
    },
    {
      "epoch": 1.2700156985871272,
      "grad_norm": 3.72356915473938,
      "learning_rate": 4.920624018838305e-05,
      "loss": 0.7427,
      "step": 80900
    },
    {
      "epoch": 1.2715855572998431,
      "grad_norm": 2.8300180435180664,
      "learning_rate": 4.92052590266876e-05,
      "loss": 0.7648,
      "step": 81000
    },
    {
      "epoch": 1.2731554160125589,
      "grad_norm": 4.542043685913086,
      "learning_rate": 4.9204277864992156e-05,
      "loss": 0.7602,
      "step": 81100
    },
    {
      "epoch": 1.2747252747252746,
      "grad_norm": 4.328848361968994,
      "learning_rate": 4.92032967032967e-05,
      "loss": 0.7842,
      "step": 81200
    },
    {
      "epoch": 1.2762951334379906,
      "grad_norm": 5.081377029418945,
      "learning_rate": 4.920231554160126e-05,
      "loss": 0.7631,
      "step": 81300
    },
    {
      "epoch": 1.2778649921507064,
      "grad_norm": 4.185920238494873,
      "learning_rate": 4.920133437990581e-05,
      "loss": 0.7366,
      "step": 81400
    },
    {
      "epoch": 1.2794348508634223,
      "grad_norm": 5.538674354553223,
      "learning_rate": 4.920035321821037e-05,
      "loss": 0.8017,
      "step": 81500
    },
    {
      "epoch": 1.281004709576138,
      "grad_norm": 3.6200006008148193,
      "learning_rate": 4.919937205651491e-05,
      "loss": 0.735,
      "step": 81600
    },
    {
      "epoch": 1.282574568288854,
      "grad_norm": 4.684442520141602,
      "learning_rate": 4.919839089481947e-05,
      "loss": 0.7551,
      "step": 81700
    },
    {
      "epoch": 1.2841444270015698,
      "grad_norm": 4.98200798034668,
      "learning_rate": 4.919740973312402e-05,
      "loss": 0.7704,
      "step": 81800
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 5.24729585647583,
      "learning_rate": 4.919642857142857e-05,
      "loss": 0.7311,
      "step": 81900
    },
    {
      "epoch": 1.2872841444270016,
      "grad_norm": 3.587444305419922,
      "learning_rate": 4.919544740973313e-05,
      "loss": 0.7254,
      "step": 82000
    },
    {
      "epoch": 1.2888540031397175,
      "grad_norm": 3.7278337478637695,
      "learning_rate": 4.919446624803768e-05,
      "loss": 0.7831,
      "step": 82100
    },
    {
      "epoch": 1.2904238618524333,
      "grad_norm": 4.4115800857543945,
      "learning_rate": 4.919348508634223e-05,
      "loss": 0.7087,
      "step": 82200
    },
    {
      "epoch": 1.291993720565149,
      "grad_norm": 3.808784246444702,
      "learning_rate": 4.919250392464678e-05,
      "loss": 0.7479,
      "step": 82300
    },
    {
      "epoch": 1.293563579277865,
      "grad_norm": 4.047694683074951,
      "learning_rate": 4.919152276295134e-05,
      "loss": 0.7965,
      "step": 82400
    },
    {
      "epoch": 1.2951334379905808,
      "grad_norm": 4.937098503112793,
      "learning_rate": 4.919054160125589e-05,
      "loss": 0.722,
      "step": 82500
    },
    {
      "epoch": 1.2967032967032968,
      "grad_norm": 3.8027007579803467,
      "learning_rate": 4.918956043956044e-05,
      "loss": 0.7128,
      "step": 82600
    },
    {
      "epoch": 1.2982731554160125,
      "grad_norm": 4.357639312744141,
      "learning_rate": 4.918857927786499e-05,
      "loss": 0.7555,
      "step": 82700
    },
    {
      "epoch": 1.2998430141287285,
      "grad_norm": 4.858974456787109,
      "learning_rate": 4.918759811616955e-05,
      "loss": 0.7607,
      "step": 82800
    },
    {
      "epoch": 1.3014128728414442,
      "grad_norm": 4.849387168884277,
      "learning_rate": 4.91866169544741e-05,
      "loss": 0.7567,
      "step": 82900
    },
    {
      "epoch": 1.30298273155416,
      "grad_norm": 3.864598274230957,
      "learning_rate": 4.918563579277865e-05,
      "loss": 0.7337,
      "step": 83000
    },
    {
      "epoch": 1.304552590266876,
      "grad_norm": 3.933180570602417,
      "learning_rate": 4.91846546310832e-05,
      "loss": 0.7973,
      "step": 83100
    },
    {
      "epoch": 1.306122448979592,
      "grad_norm": 3.9738657474517822,
      "learning_rate": 4.918367346938776e-05,
      "loss": 0.8037,
      "step": 83200
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 4.717374324798584,
      "learning_rate": 4.9182692307692305e-05,
      "loss": 0.7932,
      "step": 83300
    },
    {
      "epoch": 1.3092621664050235,
      "grad_norm": 4.461583137512207,
      "learning_rate": 4.918171114599686e-05,
      "loss": 0.7855,
      "step": 83400
    },
    {
      "epoch": 1.3108320251177394,
      "grad_norm": 4.331399917602539,
      "learning_rate": 4.9180729984301414e-05,
      "loss": 0.7551,
      "step": 83500
    },
    {
      "epoch": 1.3124018838304552,
      "grad_norm": 4.750341415405273,
      "learning_rate": 4.917974882260597e-05,
      "loss": 0.78,
      "step": 83600
    },
    {
      "epoch": 1.3139717425431712,
      "grad_norm": 4.397961139678955,
      "learning_rate": 4.9178767660910516e-05,
      "loss": 0.771,
      "step": 83700
    },
    {
      "epoch": 1.315541601255887,
      "grad_norm": 4.385669231414795,
      "learning_rate": 4.917778649921507e-05,
      "loss": 0.7534,
      "step": 83800
    },
    {
      "epoch": 1.317111459968603,
      "grad_norm": 4.750272274017334,
      "learning_rate": 4.9176805337519624e-05,
      "loss": 0.7762,
      "step": 83900
    },
    {
      "epoch": 1.3186813186813187,
      "grad_norm": 4.085953235626221,
      "learning_rate": 4.9175824175824175e-05,
      "loss": 0.7229,
      "step": 84000
    },
    {
      "epoch": 1.3202511773940344,
      "grad_norm": 4.1732916831970215,
      "learning_rate": 4.917484301412873e-05,
      "loss": 0.7585,
      "step": 84100
    },
    {
      "epoch": 1.3218210361067504,
      "grad_norm": 4.51860237121582,
      "learning_rate": 4.9173861852433284e-05,
      "loss": 0.7454,
      "step": 84200
    },
    {
      "epoch": 1.3233908948194664,
      "grad_norm": 5.276190757751465,
      "learning_rate": 4.9172880690737835e-05,
      "loss": 0.7342,
      "step": 84300
    },
    {
      "epoch": 1.3249607535321821,
      "grad_norm": 4.620984077453613,
      "learning_rate": 4.9171899529042386e-05,
      "loss": 0.7412,
      "step": 84400
    },
    {
      "epoch": 1.3265306122448979,
      "grad_norm": 3.99985933303833,
      "learning_rate": 4.9170918367346944e-05,
      "loss": 0.7644,
      "step": 84500
    },
    {
      "epoch": 1.3281004709576139,
      "grad_norm": 4.926188945770264,
      "learning_rate": 4.9169937205651495e-05,
      "loss": 0.7341,
      "step": 84600
    },
    {
      "epoch": 1.3296703296703296,
      "grad_norm": 4.073534965515137,
      "learning_rate": 4.9168956043956046e-05,
      "loss": 0.7493,
      "step": 84700
    },
    {
      "epoch": 1.3312401883830456,
      "grad_norm": 4.787813663482666,
      "learning_rate": 4.91679748822606e-05,
      "loss": 0.7386,
      "step": 84800
    },
    {
      "epoch": 1.3328100470957613,
      "grad_norm": 3.871919870376587,
      "learning_rate": 4.9166993720565154e-05,
      "loss": 0.6941,
      "step": 84900
    },
    {
      "epoch": 1.3343799058084773,
      "grad_norm": 6.538022041320801,
      "learning_rate": 4.9166012558869705e-05,
      "loss": 0.7886,
      "step": 85000
    },
    {
      "epoch": 1.335949764521193,
      "grad_norm": 4.0036234855651855,
      "learning_rate": 4.9165031397174256e-05,
      "loss": 0.7544,
      "step": 85100
    },
    {
      "epoch": 1.3375196232339088,
      "grad_norm": 5.498431205749512,
      "learning_rate": 4.916405023547881e-05,
      "loss": 0.73,
      "step": 85200
    },
    {
      "epoch": 1.3390894819466248,
      "grad_norm": 4.951104164123535,
      "learning_rate": 4.9163069073783365e-05,
      "loss": 0.7859,
      "step": 85300
    },
    {
      "epoch": 1.3406593406593408,
      "grad_norm": 3.8102145195007324,
      "learning_rate": 4.916208791208791e-05,
      "loss": 0.7356,
      "step": 85400
    },
    {
      "epoch": 1.3422291993720565,
      "grad_norm": 5.068419933319092,
      "learning_rate": 4.916110675039247e-05,
      "loss": 0.7204,
      "step": 85500
    },
    {
      "epoch": 1.3437990580847723,
      "grad_norm": 5.229875087738037,
      "learning_rate": 4.916012558869702e-05,
      "loss": 0.7694,
      "step": 85600
    },
    {
      "epoch": 1.3453689167974883,
      "grad_norm": 3.1962132453918457,
      "learning_rate": 4.9159144427001576e-05,
      "loss": 0.7431,
      "step": 85700
    },
    {
      "epoch": 1.346938775510204,
      "grad_norm": 4.244580268859863,
      "learning_rate": 4.915816326530612e-05,
      "loss": 0.7549,
      "step": 85800
    },
    {
      "epoch": 1.34850863422292,
      "grad_norm": 4.156358242034912,
      "learning_rate": 4.915718210361068e-05,
      "loss": 0.7347,
      "step": 85900
    },
    {
      "epoch": 1.3500784929356358,
      "grad_norm": 5.537713527679443,
      "learning_rate": 4.915620094191523e-05,
      "loss": 0.7763,
      "step": 86000
    },
    {
      "epoch": 1.3516483516483517,
      "grad_norm": 4.181643009185791,
      "learning_rate": 4.915521978021978e-05,
      "loss": 0.7276,
      "step": 86100
    },
    {
      "epoch": 1.3532182103610675,
      "grad_norm": 3.248134136199951,
      "learning_rate": 4.915423861852434e-05,
      "loss": 0.771,
      "step": 86200
    },
    {
      "epoch": 1.3547880690737832,
      "grad_norm": 3.6310031414031982,
      "learning_rate": 4.915325745682889e-05,
      "loss": 0.7039,
      "step": 86300
    },
    {
      "epoch": 1.3563579277864992,
      "grad_norm": 4.884064674377441,
      "learning_rate": 4.915227629513344e-05,
      "loss": 0.7628,
      "step": 86400
    },
    {
      "epoch": 1.3579277864992152,
      "grad_norm": 3.5595436096191406,
      "learning_rate": 4.915129513343799e-05,
      "loss": 0.7485,
      "step": 86500
    },
    {
      "epoch": 1.359497645211931,
      "grad_norm": 3.982675075531006,
      "learning_rate": 4.915031397174255e-05,
      "loss": 0.7576,
      "step": 86600
    },
    {
      "epoch": 1.3610675039246467,
      "grad_norm": 4.6952433586120605,
      "learning_rate": 4.91493328100471e-05,
      "loss": 0.7863,
      "step": 86700
    },
    {
      "epoch": 1.3626373626373627,
      "grad_norm": 3.2875804901123047,
      "learning_rate": 4.914835164835165e-05,
      "loss": 0.721,
      "step": 86800
    },
    {
      "epoch": 1.3642072213500784,
      "grad_norm": 3.9342997074127197,
      "learning_rate": 4.91473704866562e-05,
      "loss": 0.7263,
      "step": 86900
    },
    {
      "epoch": 1.3657770800627944,
      "grad_norm": 5.08966588973999,
      "learning_rate": 4.914638932496076e-05,
      "loss": 0.7519,
      "step": 87000
    },
    {
      "epoch": 1.3673469387755102,
      "grad_norm": 3.9374608993530273,
      "learning_rate": 4.914540816326531e-05,
      "loss": 0.7435,
      "step": 87100
    },
    {
      "epoch": 1.3689167974882261,
      "grad_norm": 4.761080741882324,
      "learning_rate": 4.914442700156986e-05,
      "loss": 0.7753,
      "step": 87200
    },
    {
      "epoch": 1.370486656200942,
      "grad_norm": 3.1290974617004395,
      "learning_rate": 4.914344583987441e-05,
      "loss": 0.7891,
      "step": 87300
    },
    {
      "epoch": 1.3720565149136577,
      "grad_norm": 5.081421375274658,
      "learning_rate": 4.914246467817897e-05,
      "loss": 0.7715,
      "step": 87400
    },
    {
      "epoch": 1.3736263736263736,
      "grad_norm": 4.71203088760376,
      "learning_rate": 4.9141483516483514e-05,
      "loss": 0.7603,
      "step": 87500
    },
    {
      "epoch": 1.3751962323390896,
      "grad_norm": 4.577914237976074,
      "learning_rate": 4.914050235478807e-05,
      "loss": 0.7332,
      "step": 87600
    },
    {
      "epoch": 1.3767660910518054,
      "grad_norm": 3.9747090339660645,
      "learning_rate": 4.913952119309262e-05,
      "loss": 0.7474,
      "step": 87700
    },
    {
      "epoch": 1.3783359497645211,
      "grad_norm": 4.452672004699707,
      "learning_rate": 4.913854003139718e-05,
      "loss": 0.7621,
      "step": 87800
    },
    {
      "epoch": 1.379905808477237,
      "grad_norm": 4.59859561920166,
      "learning_rate": 4.9137558869701724e-05,
      "loss": 0.7406,
      "step": 87900
    },
    {
      "epoch": 1.3814756671899528,
      "grad_norm": 3.5600624084472656,
      "learning_rate": 4.913657770800628e-05,
      "loss": 0.7545,
      "step": 88000
    },
    {
      "epoch": 1.3830455259026688,
      "grad_norm": 4.550363063812256,
      "learning_rate": 4.913559654631083e-05,
      "loss": 0.7669,
      "step": 88100
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 3.9835803508758545,
      "learning_rate": 4.9134615384615384e-05,
      "loss": 0.7386,
      "step": 88200
    },
    {
      "epoch": 1.3861852433281006,
      "grad_norm": 4.698947429656982,
      "learning_rate": 4.913363422291994e-05,
      "loss": 0.7217,
      "step": 88300
    },
    {
      "epoch": 1.3877551020408163,
      "grad_norm": 3.57379150390625,
      "learning_rate": 4.913265306122449e-05,
      "loss": 0.7824,
      "step": 88400
    },
    {
      "epoch": 1.389324960753532,
      "grad_norm": 5.338812351226807,
      "learning_rate": 4.9131671899529044e-05,
      "loss": 0.7546,
      "step": 88500
    },
    {
      "epoch": 1.390894819466248,
      "grad_norm": 3.3336341381073,
      "learning_rate": 4.9130690737833595e-05,
      "loss": 0.7333,
      "step": 88600
    },
    {
      "epoch": 1.392464678178964,
      "grad_norm": 3.3685789108276367,
      "learning_rate": 4.912970957613815e-05,
      "loss": 0.7249,
      "step": 88700
    },
    {
      "epoch": 1.3940345368916798,
      "grad_norm": 3.517313003540039,
      "learning_rate": 4.9128728414442704e-05,
      "loss": 0.7818,
      "step": 88800
    },
    {
      "epoch": 1.3956043956043955,
      "grad_norm": 4.162750720977783,
      "learning_rate": 4.9127747252747255e-05,
      "loss": 0.7921,
      "step": 88900
    },
    {
      "epoch": 1.3971742543171115,
      "grad_norm": 3.630221366882324,
      "learning_rate": 4.9126766091051806e-05,
      "loss": 0.7827,
      "step": 89000
    },
    {
      "epoch": 1.3987441130298273,
      "grad_norm": 4.161312580108643,
      "learning_rate": 4.912578492935636e-05,
      "loss": 0.6899,
      "step": 89100
    },
    {
      "epoch": 1.4003139717425432,
      "grad_norm": 5.424911022186279,
      "learning_rate": 4.9124803767660914e-05,
      "loss": 0.7822,
      "step": 89200
    },
    {
      "epoch": 1.401883830455259,
      "grad_norm": 4.691526412963867,
      "learning_rate": 4.9123822605965465e-05,
      "loss": 0.7423,
      "step": 89300
    },
    {
      "epoch": 1.403453689167975,
      "grad_norm": 4.422622203826904,
      "learning_rate": 4.9122841444270016e-05,
      "loss": 0.7311,
      "step": 89400
    },
    {
      "epoch": 1.4050235478806907,
      "grad_norm": 4.242785453796387,
      "learning_rate": 4.9121860282574574e-05,
      "loss": 0.7231,
      "step": 89500
    },
    {
      "epoch": 1.4065934065934065,
      "grad_norm": 4.885825157165527,
      "learning_rate": 4.912087912087912e-05,
      "loss": 0.7364,
      "step": 89600
    },
    {
      "epoch": 1.4081632653061225,
      "grad_norm": 4.564454555511475,
      "learning_rate": 4.9119897959183676e-05,
      "loss": 0.7147,
      "step": 89700
    },
    {
      "epoch": 1.4097331240188384,
      "grad_norm": 4.632638931274414,
      "learning_rate": 4.911891679748823e-05,
      "loss": 0.7576,
      "step": 89800
    },
    {
      "epoch": 1.4113029827315542,
      "grad_norm": 3.935163974761963,
      "learning_rate": 4.9117935635792785e-05,
      "loss": 0.7843,
      "step": 89900
    },
    {
      "epoch": 1.41287284144427,
      "grad_norm": 4.383311748504639,
      "learning_rate": 4.911695447409733e-05,
      "loss": 0.7651,
      "step": 90000
    },
    {
      "epoch": 1.414442700156986,
      "grad_norm": 4.8329362869262695,
      "learning_rate": 4.911597331240189e-05,
      "loss": 0.7384,
      "step": 90100
    },
    {
      "epoch": 1.4160125588697017,
      "grad_norm": 4.082204341888428,
      "learning_rate": 4.911499215070644e-05,
      "loss": 0.7797,
      "step": 90200
    },
    {
      "epoch": 1.4175824175824177,
      "grad_norm": 4.335079193115234,
      "learning_rate": 4.911401098901099e-05,
      "loss": 0.7784,
      "step": 90300
    },
    {
      "epoch": 1.4191522762951334,
      "grad_norm": 4.383034706115723,
      "learning_rate": 4.9113029827315546e-05,
      "loss": 0.7406,
      "step": 90400
    },
    {
      "epoch": 1.4207221350078494,
      "grad_norm": 3.828953742980957,
      "learning_rate": 4.91120486656201e-05,
      "loss": 0.7818,
      "step": 90500
    },
    {
      "epoch": 1.4222919937205651,
      "grad_norm": 4.833615779876709,
      "learning_rate": 4.911106750392465e-05,
      "loss": 0.7461,
      "step": 90600
    },
    {
      "epoch": 1.423861852433281,
      "grad_norm": 3.2666144371032715,
      "learning_rate": 4.91100863422292e-05,
      "loss": 0.758,
      "step": 90700
    },
    {
      "epoch": 1.4254317111459969,
      "grad_norm": 4.679104804992676,
      "learning_rate": 4.910910518053376e-05,
      "loss": 0.7386,
      "step": 90800
    },
    {
      "epoch": 1.4270015698587128,
      "grad_norm": 4.387648582458496,
      "learning_rate": 4.910812401883831e-05,
      "loss": 0.7758,
      "step": 90900
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 4.515703201293945,
      "learning_rate": 4.910714285714286e-05,
      "loss": 0.7754,
      "step": 91000
    },
    {
      "epoch": 1.4301412872841444,
      "grad_norm": 4.9265666007995605,
      "learning_rate": 4.910616169544741e-05,
      "loss": 0.7584,
      "step": 91100
    },
    {
      "epoch": 1.4317111459968603,
      "grad_norm": 4.155123710632324,
      "learning_rate": 4.910518053375197e-05,
      "loss": 0.7415,
      "step": 91200
    },
    {
      "epoch": 1.433281004709576,
      "grad_norm": 4.1411638259887695,
      "learning_rate": 4.910419937205652e-05,
      "loss": 0.7195,
      "step": 91300
    },
    {
      "epoch": 1.434850863422292,
      "grad_norm": 3.938575267791748,
      "learning_rate": 4.910321821036107e-05,
      "loss": 0.7706,
      "step": 91400
    },
    {
      "epoch": 1.4364207221350078,
      "grad_norm": 2.8679251670837402,
      "learning_rate": 4.910223704866562e-05,
      "loss": 0.7486,
      "step": 91500
    },
    {
      "epoch": 1.4379905808477238,
      "grad_norm": 5.082307815551758,
      "learning_rate": 4.910125588697018e-05,
      "loss": 0.7429,
      "step": 91600
    },
    {
      "epoch": 1.4395604395604396,
      "grad_norm": 4.670349597930908,
      "learning_rate": 4.910027472527472e-05,
      "loss": 0.7851,
      "step": 91700
    },
    {
      "epoch": 1.4411302982731553,
      "grad_norm": 4.749091148376465,
      "learning_rate": 4.909929356357928e-05,
      "loss": 0.7987,
      "step": 91800
    },
    {
      "epoch": 1.4427001569858713,
      "grad_norm": 4.109521865844727,
      "learning_rate": 4.909831240188383e-05,
      "loss": 0.7451,
      "step": 91900
    },
    {
      "epoch": 1.4442700156985873,
      "grad_norm": 3.9905664920806885,
      "learning_rate": 4.909733124018839e-05,
      "loss": 0.6991,
      "step": 92000
    },
    {
      "epoch": 1.445839874411303,
      "grad_norm": 4.018106937408447,
      "learning_rate": 4.9096350078492933e-05,
      "loss": 0.7748,
      "step": 92100
    },
    {
      "epoch": 1.4474097331240188,
      "grad_norm": 3.3935327529907227,
      "learning_rate": 4.909536891679749e-05,
      "loss": 0.7603,
      "step": 92200
    },
    {
      "epoch": 1.4489795918367347,
      "grad_norm": 4.519028186798096,
      "learning_rate": 4.909438775510204e-05,
      "loss": 0.7746,
      "step": 92300
    },
    {
      "epoch": 1.4505494505494505,
      "grad_norm": 3.995676279067993,
      "learning_rate": 4.909340659340659e-05,
      "loss": 0.7563,
      "step": 92400
    },
    {
      "epoch": 1.4521193092621665,
      "grad_norm": 3.9571385383605957,
      "learning_rate": 4.909242543171115e-05,
      "loss": 0.7268,
      "step": 92500
    },
    {
      "epoch": 1.4536891679748822,
      "grad_norm": 3.664682149887085,
      "learning_rate": 4.90914442700157e-05,
      "loss": 0.7377,
      "step": 92600
    },
    {
      "epoch": 1.4552590266875982,
      "grad_norm": 4.744308948516846,
      "learning_rate": 4.909046310832025e-05,
      "loss": 0.7569,
      "step": 92700
    },
    {
      "epoch": 1.456828885400314,
      "grad_norm": 4.402767658233643,
      "learning_rate": 4.9089481946624804e-05,
      "loss": 0.7231,
      "step": 92800
    },
    {
      "epoch": 1.4583987441130297,
      "grad_norm": 4.517458438873291,
      "learning_rate": 4.908850078492936e-05,
      "loss": 0.7582,
      "step": 92900
    },
    {
      "epoch": 1.4599686028257457,
      "grad_norm": 3.1073105335235596,
      "learning_rate": 4.908751962323391e-05,
      "loss": 0.7245,
      "step": 93000
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 3.8838632106781006,
      "learning_rate": 4.9086538461538464e-05,
      "loss": 0.7561,
      "step": 93100
    },
    {
      "epoch": 1.4631083202511774,
      "grad_norm": 4.940913677215576,
      "learning_rate": 4.9085557299843015e-05,
      "loss": 0.772,
      "step": 93200
    },
    {
      "epoch": 1.4646781789638932,
      "grad_norm": 3.208104133605957,
      "learning_rate": 4.908457613814757e-05,
      "loss": 0.7544,
      "step": 93300
    },
    {
      "epoch": 1.4662480376766092,
      "grad_norm": 3.661371946334839,
      "learning_rate": 4.908359497645212e-05,
      "loss": 0.7287,
      "step": 93400
    },
    {
      "epoch": 1.467817896389325,
      "grad_norm": 3.922619104385376,
      "learning_rate": 4.9082613814756674e-05,
      "loss": 0.7168,
      "step": 93500
    },
    {
      "epoch": 1.469387755102041,
      "grad_norm": 4.566423416137695,
      "learning_rate": 4.9081632653061225e-05,
      "loss": 0.756,
      "step": 93600
    },
    {
      "epoch": 1.4709576138147566,
      "grad_norm": 4.9696125984191895,
      "learning_rate": 4.908065149136578e-05,
      "loss": 0.7577,
      "step": 93700
    },
    {
      "epoch": 1.4725274725274726,
      "grad_norm": 4.058633804321289,
      "learning_rate": 4.907967032967033e-05,
      "loss": 0.7477,
      "step": 93800
    },
    {
      "epoch": 1.4740973312401884,
      "grad_norm": 3.7493391036987305,
      "learning_rate": 4.9078689167974885e-05,
      "loss": 0.7225,
      "step": 93900
    },
    {
      "epoch": 1.4756671899529041,
      "grad_norm": 4.879592418670654,
      "learning_rate": 4.9077708006279436e-05,
      "loss": 0.7378,
      "step": 94000
    },
    {
      "epoch": 1.4772370486656201,
      "grad_norm": 3.816091775894165,
      "learning_rate": 4.9076726844583994e-05,
      "loss": 0.7271,
      "step": 94100
    },
    {
      "epoch": 1.478806907378336,
      "grad_norm": 4.415065288543701,
      "learning_rate": 4.907574568288854e-05,
      "loss": 0.7155,
      "step": 94200
    },
    {
      "epoch": 1.4803767660910518,
      "grad_norm": 3.5826196670532227,
      "learning_rate": 4.9074764521193096e-05,
      "loss": 0.7576,
      "step": 94300
    },
    {
      "epoch": 1.4819466248037676,
      "grad_norm": 3.806749105453491,
      "learning_rate": 4.907378335949765e-05,
      "loss": 0.7341,
      "step": 94400
    },
    {
      "epoch": 1.4835164835164836,
      "grad_norm": 3.3721072673797607,
      "learning_rate": 4.90728021978022e-05,
      "loss": 0.7298,
      "step": 94500
    },
    {
      "epoch": 1.4850863422291993,
      "grad_norm": 4.116828441619873,
      "learning_rate": 4.9071821036106755e-05,
      "loss": 0.7569,
      "step": 94600
    },
    {
      "epoch": 1.4866562009419153,
      "grad_norm": 5.105883598327637,
      "learning_rate": 4.9070839874411306e-05,
      "loss": 0.749,
      "step": 94700
    },
    {
      "epoch": 1.488226059654631,
      "grad_norm": 4.682467937469482,
      "learning_rate": 4.906985871271586e-05,
      "loss": 0.7461,
      "step": 94800
    },
    {
      "epoch": 1.489795918367347,
      "grad_norm": 3.1366701126098633,
      "learning_rate": 4.906887755102041e-05,
      "loss": 0.7291,
      "step": 94900
    },
    {
      "epoch": 1.4913657770800628,
      "grad_norm": 4.1312479972839355,
      "learning_rate": 4.9067896389324966e-05,
      "loss": 0.7339,
      "step": 95000
    },
    {
      "epoch": 1.4929356357927785,
      "grad_norm": 4.349081993103027,
      "learning_rate": 4.906691522762952e-05,
      "loss": 0.7623,
      "step": 95100
    },
    {
      "epoch": 1.4945054945054945,
      "grad_norm": 3.190756320953369,
      "learning_rate": 4.906593406593407e-05,
      "loss": 0.743,
      "step": 95200
    },
    {
      "epoch": 1.4960753532182103,
      "grad_norm": 3.2969138622283936,
      "learning_rate": 4.906495290423862e-05,
      "loss": 0.7374,
      "step": 95300
    },
    {
      "epoch": 1.4976452119309263,
      "grad_norm": 4.105679988861084,
      "learning_rate": 4.906397174254318e-05,
      "loss": 0.7154,
      "step": 95400
    },
    {
      "epoch": 1.499215070643642,
      "grad_norm": 3.5319979190826416,
      "learning_rate": 4.906299058084773e-05,
      "loss": 0.7425,
      "step": 95500
    },
    {
      "epoch": 1.500784929356358,
      "grad_norm": 3.9094362258911133,
      "learning_rate": 4.906200941915228e-05,
      "loss": 0.7758,
      "step": 95600
    },
    {
      "epoch": 1.5023547880690737,
      "grad_norm": 2.9128715991973877,
      "learning_rate": 4.906102825745683e-05,
      "loss": 0.7947,
      "step": 95700
    },
    {
      "epoch": 1.5039246467817895,
      "grad_norm": 3.642402172088623,
      "learning_rate": 4.906004709576139e-05,
      "loss": 0.703,
      "step": 95800
    },
    {
      "epoch": 1.5054945054945055,
      "grad_norm": 3.1922507286071777,
      "learning_rate": 4.905906593406593e-05,
      "loss": 0.7083,
      "step": 95900
    },
    {
      "epoch": 1.5070643642072215,
      "grad_norm": 3.9578311443328857,
      "learning_rate": 4.905808477237049e-05,
      "loss": 0.698,
      "step": 96000
    },
    {
      "epoch": 1.5086342229199372,
      "grad_norm": 4.6916913986206055,
      "learning_rate": 4.905710361067504e-05,
      "loss": 0.798,
      "step": 96100
    },
    {
      "epoch": 1.510204081632653,
      "grad_norm": 3.7722880840301514,
      "learning_rate": 4.90561224489796e-05,
      "loss": 0.7263,
      "step": 96200
    },
    {
      "epoch": 1.511773940345369,
      "grad_norm": 4.2974748611450195,
      "learning_rate": 4.905514128728414e-05,
      "loss": 0.7822,
      "step": 96300
    },
    {
      "epoch": 1.513343799058085,
      "grad_norm": 4.895412921905518,
      "learning_rate": 4.90541601255887e-05,
      "loss": 0.7876,
      "step": 96400
    },
    {
      "epoch": 1.5149136577708007,
      "grad_norm": 4.006764888763428,
      "learning_rate": 4.905317896389325e-05,
      "loss": 0.7447,
      "step": 96500
    },
    {
      "epoch": 1.5164835164835164,
      "grad_norm": 4.430293083190918,
      "learning_rate": 4.90521978021978e-05,
      "loss": 0.7373,
      "step": 96600
    },
    {
      "epoch": 1.5180533751962324,
      "grad_norm": 4.034417629241943,
      "learning_rate": 4.905121664050235e-05,
      "loss": 0.7388,
      "step": 96700
    },
    {
      "epoch": 1.5196232339089482,
      "grad_norm": 4.811624526977539,
      "learning_rate": 4.905023547880691e-05,
      "loss": 0.7824,
      "step": 96800
    },
    {
      "epoch": 1.521193092621664,
      "grad_norm": 4.373205184936523,
      "learning_rate": 4.904925431711146e-05,
      "loss": 0.7641,
      "step": 96900
    },
    {
      "epoch": 1.5227629513343799,
      "grad_norm": 3.9551987648010254,
      "learning_rate": 4.904827315541601e-05,
      "loss": 0.7378,
      "step": 97000
    },
    {
      "epoch": 1.5243328100470959,
      "grad_norm": 3.351914405822754,
      "learning_rate": 4.904729199372057e-05,
      "loss": 0.7681,
      "step": 97100
    },
    {
      "epoch": 1.5259026687598116,
      "grad_norm": 3.789283514022827,
      "learning_rate": 4.904631083202512e-05,
      "loss": 0.7377,
      "step": 97200
    },
    {
      "epoch": 1.5274725274725274,
      "grad_norm": 4.3244853019714355,
      "learning_rate": 4.904532967032967e-05,
      "loss": 0.7447,
      "step": 97300
    },
    {
      "epoch": 1.5290423861852434,
      "grad_norm": 1.927196979522705,
      "learning_rate": 4.9044348508634224e-05,
      "loss": 0.7167,
      "step": 97400
    },
    {
      "epoch": 1.5306122448979593,
      "grad_norm": 4.5181050300598145,
      "learning_rate": 4.904336734693878e-05,
      "loss": 0.7744,
      "step": 97500
    },
    {
      "epoch": 1.532182103610675,
      "grad_norm": 4.635211944580078,
      "learning_rate": 4.904238618524333e-05,
      "loss": 0.7365,
      "step": 97600
    },
    {
      "epoch": 1.5337519623233908,
      "grad_norm": 4.412332534790039,
      "learning_rate": 4.904140502354788e-05,
      "loss": 0.7706,
      "step": 97700
    },
    {
      "epoch": 1.5353218210361068,
      "grad_norm": 4.114267826080322,
      "learning_rate": 4.9040423861852434e-05,
      "loss": 0.7636,
      "step": 97800
    },
    {
      "epoch": 1.5368916797488226,
      "grad_norm": 5.386493682861328,
      "learning_rate": 4.903944270015699e-05,
      "loss": 0.7477,
      "step": 97900
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 4.257207870483398,
      "learning_rate": 4.9038461538461536e-05,
      "loss": 0.7432,
      "step": 98000
    },
    {
      "epoch": 1.5400313971742543,
      "grad_norm": 4.549527168273926,
      "learning_rate": 4.9037480376766094e-05,
      "loss": 0.7939,
      "step": 98100
    },
    {
      "epoch": 1.5416012558869703,
      "grad_norm": 3.979297161102295,
      "learning_rate": 4.9036499215070645e-05,
      "loss": 0.704,
      "step": 98200
    },
    {
      "epoch": 1.543171114599686,
      "grad_norm": 4.345700263977051,
      "learning_rate": 4.90355180533752e-05,
      "loss": 0.7408,
      "step": 98300
    },
    {
      "epoch": 1.5447409733124018,
      "grad_norm": 4.523331642150879,
      "learning_rate": 4.903453689167975e-05,
      "loss": 0.732,
      "step": 98400
    },
    {
      "epoch": 1.5463108320251178,
      "grad_norm": 3.7150957584381104,
      "learning_rate": 4.9033555729984305e-05,
      "loss": 0.7334,
      "step": 98500
    },
    {
      "epoch": 1.5478806907378337,
      "grad_norm": 3.753464698791504,
      "learning_rate": 4.9032574568288856e-05,
      "loss": 0.7128,
      "step": 98600
    },
    {
      "epoch": 1.5494505494505495,
      "grad_norm": 4.5967912673950195,
      "learning_rate": 4.903159340659341e-05,
      "loss": 0.7015,
      "step": 98700
    },
    {
      "epoch": 1.5510204081632653,
      "grad_norm": 4.561300754547119,
      "learning_rate": 4.903061224489796e-05,
      "loss": 0.7368,
      "step": 98800
    },
    {
      "epoch": 1.5525902668759812,
      "grad_norm": 4.789823055267334,
      "learning_rate": 4.9029631083202515e-05,
      "loss": 0.7182,
      "step": 98900
    },
    {
      "epoch": 1.554160125588697,
      "grad_norm": 3.0798685550689697,
      "learning_rate": 4.9028649921507066e-05,
      "loss": 0.7615,
      "step": 99000
    },
    {
      "epoch": 1.5557299843014127,
      "grad_norm": 4.970143795013428,
      "learning_rate": 4.902766875981162e-05,
      "loss": 0.7518,
      "step": 99100
    },
    {
      "epoch": 1.5572998430141287,
      "grad_norm": 4.174806594848633,
      "learning_rate": 4.9026687598116175e-05,
      "loss": 0.7429,
      "step": 99200
    },
    {
      "epoch": 1.5588697017268447,
      "grad_norm": 3.8004684448242188,
      "learning_rate": 4.9025706436420726e-05,
      "loss": 0.7775,
      "step": 99300
    },
    {
      "epoch": 1.5604395604395604,
      "grad_norm": 4.074008464813232,
      "learning_rate": 4.902472527472528e-05,
      "loss": 0.7758,
      "step": 99400
    },
    {
      "epoch": 1.5620094191522762,
      "grad_norm": 4.3148369789123535,
      "learning_rate": 4.902374411302983e-05,
      "loss": 0.7234,
      "step": 99500
    },
    {
      "epoch": 1.5635792778649922,
      "grad_norm": 4.3228302001953125,
      "learning_rate": 4.9022762951334386e-05,
      "loss": 0.7915,
      "step": 99600
    },
    {
      "epoch": 1.5651491365777082,
      "grad_norm": 4.465612411499023,
      "learning_rate": 4.902178178963894e-05,
      "loss": 0.7268,
      "step": 99700
    },
    {
      "epoch": 1.566718995290424,
      "grad_norm": 4.1957831382751465,
      "learning_rate": 4.902080062794349e-05,
      "loss": 0.7302,
      "step": 99800
    },
    {
      "epoch": 1.5682888540031397,
      "grad_norm": 4.283447742462158,
      "learning_rate": 4.901981946624804e-05,
      "loss": 0.7143,
      "step": 99900
    },
    {
      "epoch": 1.5698587127158556,
      "grad_norm": 3.544281482696533,
      "learning_rate": 4.9018838304552597e-05,
      "loss": 0.7255,
      "step": 100000
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 4.672060012817383,
      "learning_rate": 4.901785714285714e-05,
      "loss": 0.7215,
      "step": 100100
    },
    {
      "epoch": 1.5729984301412872,
      "grad_norm": 4.492674350738525,
      "learning_rate": 4.90168759811617e-05,
      "loss": 0.8008,
      "step": 100200
    },
    {
      "epoch": 1.5745682888540031,
      "grad_norm": 4.125282287597656,
      "learning_rate": 4.901589481946625e-05,
      "loss": 0.7362,
      "step": 100300
    },
    {
      "epoch": 1.576138147566719,
      "grad_norm": 4.0159101486206055,
      "learning_rate": 4.901491365777081e-05,
      "loss": 0.7392,
      "step": 100400
    },
    {
      "epoch": 1.5777080062794349,
      "grad_norm": 4.722817420959473,
      "learning_rate": 4.901393249607535e-05,
      "loss": 0.7129,
      "step": 100500
    },
    {
      "epoch": 1.5792778649921506,
      "grad_norm": 4.596367835998535,
      "learning_rate": 4.901295133437991e-05,
      "loss": 0.7626,
      "step": 100600
    },
    {
      "epoch": 1.5808477237048666,
      "grad_norm": 4.359861373901367,
      "learning_rate": 4.901197017268446e-05,
      "loss": 0.7503,
      "step": 100700
    },
    {
      "epoch": 1.5824175824175826,
      "grad_norm": 3.51080584526062,
      "learning_rate": 4.901098901098901e-05,
      "loss": 0.7399,
      "step": 100800
    },
    {
      "epoch": 1.5839874411302983,
      "grad_norm": 4.364923477172852,
      "learning_rate": 4.901000784929356e-05,
      "loss": 0.7458,
      "step": 100900
    },
    {
      "epoch": 1.585557299843014,
      "grad_norm": 2.4231462478637695,
      "learning_rate": 4.900902668759812e-05,
      "loss": 0.7282,
      "step": 101000
    },
    {
      "epoch": 1.58712715855573,
      "grad_norm": 5.580516338348389,
      "learning_rate": 4.900804552590267e-05,
      "loss": 0.7112,
      "step": 101100
    },
    {
      "epoch": 1.5886970172684458,
      "grad_norm": 4.8214802742004395,
      "learning_rate": 4.900706436420722e-05,
      "loss": 0.7138,
      "step": 101200
    },
    {
      "epoch": 1.5902668759811616,
      "grad_norm": 4.262359619140625,
      "learning_rate": 4.900608320251178e-05,
      "loss": 0.7496,
      "step": 101300
    },
    {
      "epoch": 1.5918367346938775,
      "grad_norm": 4.298004150390625,
      "learning_rate": 4.900510204081633e-05,
      "loss": 0.7734,
      "step": 101400
    },
    {
      "epoch": 1.5934065934065935,
      "grad_norm": 3.1699061393737793,
      "learning_rate": 4.900412087912088e-05,
      "loss": 0.7283,
      "step": 101500
    },
    {
      "epoch": 1.5949764521193093,
      "grad_norm": 4.558064937591553,
      "learning_rate": 4.900313971742543e-05,
      "loss": 0.768,
      "step": 101600
    },
    {
      "epoch": 1.596546310832025,
      "grad_norm": 3.495560646057129,
      "learning_rate": 4.900215855572999e-05,
      "loss": 0.7548,
      "step": 101700
    },
    {
      "epoch": 1.598116169544741,
      "grad_norm": 4.160162925720215,
      "learning_rate": 4.900117739403454e-05,
      "loss": 0.7543,
      "step": 101800
    },
    {
      "epoch": 1.599686028257457,
      "grad_norm": 3.8510425090789795,
      "learning_rate": 4.900019623233909e-05,
      "loss": 0.7458,
      "step": 101900
    },
    {
      "epoch": 1.6012558869701727,
      "grad_norm": 4.586343288421631,
      "learning_rate": 4.899921507064364e-05,
      "loss": 0.7686,
      "step": 102000
    },
    {
      "epoch": 1.6028257456828885,
      "grad_norm": 4.232314586639404,
      "learning_rate": 4.89982339089482e-05,
      "loss": 0.7517,
      "step": 102100
    },
    {
      "epoch": 1.6043956043956045,
      "grad_norm": 4.888771057128906,
      "learning_rate": 4.8997252747252745e-05,
      "loss": 0.7698,
      "step": 102200
    },
    {
      "epoch": 1.6059654631083202,
      "grad_norm": 4.226758003234863,
      "learning_rate": 4.89962715855573e-05,
      "loss": 0.7519,
      "step": 102300
    },
    {
      "epoch": 1.607535321821036,
      "grad_norm": 3.9484431743621826,
      "learning_rate": 4.8995290423861854e-05,
      "loss": 0.7614,
      "step": 102400
    },
    {
      "epoch": 1.609105180533752,
      "grad_norm": 3.717705726623535,
      "learning_rate": 4.8994309262166405e-05,
      "loss": 0.7215,
      "step": 102500
    },
    {
      "epoch": 1.610675039246468,
      "grad_norm": 4.08009147644043,
      "learning_rate": 4.8993328100470956e-05,
      "loss": 0.7451,
      "step": 102600
    },
    {
      "epoch": 1.6122448979591837,
      "grad_norm": 4.161942481994629,
      "learning_rate": 4.8992346938775514e-05,
      "loss": 0.7618,
      "step": 102700
    },
    {
      "epoch": 1.6138147566718994,
      "grad_norm": 4.306634902954102,
      "learning_rate": 4.8991365777080065e-05,
      "loss": 0.72,
      "step": 102800
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 4.586840629577637,
      "learning_rate": 4.8990384615384616e-05,
      "loss": 0.6905,
      "step": 102900
    },
    {
      "epoch": 1.6169544740973314,
      "grad_norm": 3.6396539211273193,
      "learning_rate": 4.898940345368917e-05,
      "loss": 0.7644,
      "step": 103000
    },
    {
      "epoch": 1.6185243328100472,
      "grad_norm": 4.2462592124938965,
      "learning_rate": 4.8988422291993724e-05,
      "loss": 0.7294,
      "step": 103100
    },
    {
      "epoch": 1.620094191522763,
      "grad_norm": 4.277878761291504,
      "learning_rate": 4.8987441130298275e-05,
      "loss": 0.6862,
      "step": 103200
    },
    {
      "epoch": 1.6216640502354789,
      "grad_norm": 4.203491687774658,
      "learning_rate": 4.8986459968602826e-05,
      "loss": 0.7072,
      "step": 103300
    },
    {
      "epoch": 1.6232339089481946,
      "grad_norm": 3.1142170429229736,
      "learning_rate": 4.8985478806907384e-05,
      "loss": 0.7543,
      "step": 103400
    },
    {
      "epoch": 1.6248037676609104,
      "grad_norm": 4.261941432952881,
      "learning_rate": 4.8984497645211935e-05,
      "loss": 0.7374,
      "step": 103500
    },
    {
      "epoch": 1.6263736263736264,
      "grad_norm": 3.892000675201416,
      "learning_rate": 4.8983516483516486e-05,
      "loss": 0.7458,
      "step": 103600
    },
    {
      "epoch": 1.6279434850863423,
      "grad_norm": 4.4653520584106445,
      "learning_rate": 4.898253532182104e-05,
      "loss": 0.7438,
      "step": 103700
    },
    {
      "epoch": 1.629513343799058,
      "grad_norm": 4.831782817840576,
      "learning_rate": 4.8981554160125595e-05,
      "loss": 0.7861,
      "step": 103800
    },
    {
      "epoch": 1.6310832025117739,
      "grad_norm": 3.5905299186706543,
      "learning_rate": 4.898057299843014e-05,
      "loss": 0.7674,
      "step": 103900
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 5.109299182891846,
      "learning_rate": 4.89795918367347e-05,
      "loss": 0.7715,
      "step": 104000
    },
    {
      "epoch": 1.6342229199372058,
      "grad_norm": 4.795836448669434,
      "learning_rate": 4.897861067503925e-05,
      "loss": 0.7235,
      "step": 104100
    },
    {
      "epoch": 1.6357927786499213,
      "grad_norm": 3.9185609817504883,
      "learning_rate": 4.8977629513343806e-05,
      "loss": 0.7368,
      "step": 104200
    },
    {
      "epoch": 1.6373626373626373,
      "grad_norm": 3.474360466003418,
      "learning_rate": 4.897664835164835e-05,
      "loss": 0.7165,
      "step": 104300
    },
    {
      "epoch": 1.6389324960753533,
      "grad_norm": 4.58082914352417,
      "learning_rate": 4.897566718995291e-05,
      "loss": 0.7275,
      "step": 104400
    },
    {
      "epoch": 1.640502354788069,
      "grad_norm": 4.820155620574951,
      "learning_rate": 4.897468602825746e-05,
      "loss": 0.7258,
      "step": 104500
    },
    {
      "epoch": 1.6420722135007848,
      "grad_norm": 3.1622085571289062,
      "learning_rate": 4.897370486656201e-05,
      "loss": 0.6909,
      "step": 104600
    },
    {
      "epoch": 1.6436420722135008,
      "grad_norm": 4.048205375671387,
      "learning_rate": 4.897272370486656e-05,
      "loss": 0.716,
      "step": 104700
    },
    {
      "epoch": 1.6452119309262168,
      "grad_norm": 3.542924404144287,
      "learning_rate": 4.897174254317112e-05,
      "loss": 0.7472,
      "step": 104800
    },
    {
      "epoch": 1.6467817896389325,
      "grad_norm": 4.053045272827148,
      "learning_rate": 4.897076138147567e-05,
      "loss": 0.7315,
      "step": 104900
    },
    {
      "epoch": 1.6483516483516483,
      "grad_norm": 4.468730926513672,
      "learning_rate": 4.896978021978022e-05,
      "loss": 0.7768,
      "step": 105000
    },
    {
      "epoch": 1.6499215070643642,
      "grad_norm": 3.283123016357422,
      "learning_rate": 4.896879905808477e-05,
      "loss": 0.7243,
      "step": 105100
    },
    {
      "epoch": 1.6514913657770802,
      "grad_norm": 4.0453691482543945,
      "learning_rate": 4.896781789638933e-05,
      "loss": 0.7187,
      "step": 105200
    },
    {
      "epoch": 1.6530612244897958,
      "grad_norm": 4.711455821990967,
      "learning_rate": 4.896683673469388e-05,
      "loss": 0.7631,
      "step": 105300
    },
    {
      "epoch": 1.6546310832025117,
      "grad_norm": 3.758070230484009,
      "learning_rate": 4.896585557299843e-05,
      "loss": 0.7608,
      "step": 105400
    },
    {
      "epoch": 1.6562009419152277,
      "grad_norm": 3.440945863723755,
      "learning_rate": 4.896487441130299e-05,
      "loss": 0.7462,
      "step": 105500
    },
    {
      "epoch": 1.6577708006279435,
      "grad_norm": 4.529268741607666,
      "learning_rate": 4.896389324960754e-05,
      "loss": 0.7804,
      "step": 105600
    },
    {
      "epoch": 1.6593406593406592,
      "grad_norm": 2.9590439796447754,
      "learning_rate": 4.896291208791209e-05,
      "loss": 0.7778,
      "step": 105700
    },
    {
      "epoch": 1.6609105180533752,
      "grad_norm": 3.3937759399414062,
      "learning_rate": 4.896193092621664e-05,
      "loss": 0.7013,
      "step": 105800
    },
    {
      "epoch": 1.6624803767660912,
      "grad_norm": 4.4515814781188965,
      "learning_rate": 4.89609497645212e-05,
      "loss": 0.7717,
      "step": 105900
    },
    {
      "epoch": 1.664050235478807,
      "grad_norm": 4.422486782073975,
      "learning_rate": 4.8959968602825744e-05,
      "loss": 0.7453,
      "step": 106000
    },
    {
      "epoch": 1.6656200941915227,
      "grad_norm": 3.641953945159912,
      "learning_rate": 4.89589874411303e-05,
      "loss": 0.7916,
      "step": 106100
    },
    {
      "epoch": 1.6671899529042387,
      "grad_norm": 5.515211582183838,
      "learning_rate": 4.895800627943485e-05,
      "loss": 0.7926,
      "step": 106200
    },
    {
      "epoch": 1.6687598116169546,
      "grad_norm": 4.559413909912109,
      "learning_rate": 4.895702511773941e-05,
      "loss": 0.7264,
      "step": 106300
    },
    {
      "epoch": 1.6703296703296702,
      "grad_norm": 5.041187286376953,
      "learning_rate": 4.8956043956043954e-05,
      "loss": 0.758,
      "step": 106400
    },
    {
      "epoch": 1.6718995290423861,
      "grad_norm": 3.5864200592041016,
      "learning_rate": 4.895506279434851e-05,
      "loss": 0.7525,
      "step": 106500
    },
    {
      "epoch": 1.6734693877551021,
      "grad_norm": 3.9477853775024414,
      "learning_rate": 4.895408163265306e-05,
      "loss": 0.7006,
      "step": 106600
    },
    {
      "epoch": 1.6750392464678179,
      "grad_norm": 4.068239688873291,
      "learning_rate": 4.8953100470957614e-05,
      "loss": 0.7387,
      "step": 106700
    },
    {
      "epoch": 1.6766091051805336,
      "grad_norm": 5.531103134155273,
      "learning_rate": 4.8952119309262165e-05,
      "loss": 0.726,
      "step": 106800
    },
    {
      "epoch": 1.6781789638932496,
      "grad_norm": 3.3881354331970215,
      "learning_rate": 4.895113814756672e-05,
      "loss": 0.7774,
      "step": 106900
    },
    {
      "epoch": 1.6797488226059656,
      "grad_norm": 3.22574782371521,
      "learning_rate": 4.8950156985871274e-05,
      "loss": 0.755,
      "step": 107000
    },
    {
      "epoch": 1.6813186813186813,
      "grad_norm": 4.342994213104248,
      "learning_rate": 4.8949175824175825e-05,
      "loss": 0.7339,
      "step": 107100
    },
    {
      "epoch": 1.682888540031397,
      "grad_norm": 4.368803977966309,
      "learning_rate": 4.8948194662480376e-05,
      "loss": 0.678,
      "step": 107200
    },
    {
      "epoch": 1.684458398744113,
      "grad_norm": 4.200274467468262,
      "learning_rate": 4.894721350078493e-05,
      "loss": 0.7335,
      "step": 107300
    },
    {
      "epoch": 1.686028257456829,
      "grad_norm": 4.575896263122559,
      "learning_rate": 4.8946232339089484e-05,
      "loss": 0.7428,
      "step": 107400
    },
    {
      "epoch": 1.6875981161695446,
      "grad_norm": 4.487062931060791,
      "learning_rate": 4.8945251177394035e-05,
      "loss": 0.7241,
      "step": 107500
    },
    {
      "epoch": 1.6891679748822606,
      "grad_norm": 3.92676043510437,
      "learning_rate": 4.894427001569859e-05,
      "loss": 0.6948,
      "step": 107600
    },
    {
      "epoch": 1.6907378335949765,
      "grad_norm": 4.35791015625,
      "learning_rate": 4.8943288854003144e-05,
      "loss": 0.7155,
      "step": 107700
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 4.686146259307861,
      "learning_rate": 4.8942307692307695e-05,
      "loss": 0.738,
      "step": 107800
    },
    {
      "epoch": 1.693877551020408,
      "grad_norm": 3.6447484493255615,
      "learning_rate": 4.8941326530612246e-05,
      "loss": 0.7114,
      "step": 107900
    },
    {
      "epoch": 1.695447409733124,
      "grad_norm": 4.188714504241943,
      "learning_rate": 4.8940345368916804e-05,
      "loss": 0.7352,
      "step": 108000
    },
    {
      "epoch": 1.69701726844584,
      "grad_norm": 3.7857043743133545,
      "learning_rate": 4.893936420722135e-05,
      "loss": 0.7424,
      "step": 108100
    },
    {
      "epoch": 1.6985871271585558,
      "grad_norm": 4.6335554122924805,
      "learning_rate": 4.8938383045525906e-05,
      "loss": 0.7181,
      "step": 108200
    },
    {
      "epoch": 1.7001569858712715,
      "grad_norm": 4.502342224121094,
      "learning_rate": 4.893740188383046e-05,
      "loss": 0.7543,
      "step": 108300
    },
    {
      "epoch": 1.7017268445839875,
      "grad_norm": 4.380910873413086,
      "learning_rate": 4.8936420722135014e-05,
      "loss": 0.7351,
      "step": 108400
    },
    {
      "epoch": 1.7032967032967035,
      "grad_norm": 3.9884772300720215,
      "learning_rate": 4.893543956043956e-05,
      "loss": 0.7009,
      "step": 108500
    },
    {
      "epoch": 1.704866562009419,
      "grad_norm": 4.936764240264893,
      "learning_rate": 4.8934458398744116e-05,
      "loss": 0.7108,
      "step": 108600
    },
    {
      "epoch": 1.706436420722135,
      "grad_norm": 4.604593753814697,
      "learning_rate": 4.893347723704867e-05,
      "loss": 0.7216,
      "step": 108700
    },
    {
      "epoch": 1.708006279434851,
      "grad_norm": 4.6564412117004395,
      "learning_rate": 4.893249607535322e-05,
      "loss": 0.7288,
      "step": 108800
    },
    {
      "epoch": 1.7095761381475667,
      "grad_norm": 4.116952419281006,
      "learning_rate": 4.893151491365777e-05,
      "loss": 0.7311,
      "step": 108900
    },
    {
      "epoch": 1.7111459968602825,
      "grad_norm": 4.064249038696289,
      "learning_rate": 4.893053375196233e-05,
      "loss": 0.744,
      "step": 109000
    },
    {
      "epoch": 1.7127158555729984,
      "grad_norm": 4.244164943695068,
      "learning_rate": 4.892955259026688e-05,
      "loss": 0.7518,
      "step": 109100
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 3.3097808361053467,
      "learning_rate": 4.892857142857143e-05,
      "loss": 0.7227,
      "step": 109200
    },
    {
      "epoch": 1.7158555729984302,
      "grad_norm": 4.041328430175781,
      "learning_rate": 4.892759026687598e-05,
      "loss": 0.7336,
      "step": 109300
    },
    {
      "epoch": 1.717425431711146,
      "grad_norm": 4.4679107666015625,
      "learning_rate": 4.892660910518054e-05,
      "loss": 0.7334,
      "step": 109400
    },
    {
      "epoch": 1.718995290423862,
      "grad_norm": 4.083629608154297,
      "learning_rate": 4.892562794348509e-05,
      "loss": 0.7267,
      "step": 109500
    },
    {
      "epoch": 1.7205651491365777,
      "grad_norm": 4.686398983001709,
      "learning_rate": 4.892464678178964e-05,
      "loss": 0.7559,
      "step": 109600
    },
    {
      "epoch": 1.7221350078492934,
      "grad_norm": 4.404486656188965,
      "learning_rate": 4.89236656200942e-05,
      "loss": 0.7125,
      "step": 109700
    },
    {
      "epoch": 1.7237048665620094,
      "grad_norm": 2.865426778793335,
      "learning_rate": 4.892268445839875e-05,
      "loss": 0.7269,
      "step": 109800
    },
    {
      "epoch": 1.7252747252747254,
      "grad_norm": 5.177528381347656,
      "learning_rate": 4.89217032967033e-05,
      "loss": 0.7734,
      "step": 109900
    },
    {
      "epoch": 1.7268445839874411,
      "grad_norm": 3.8308396339416504,
      "learning_rate": 4.892072213500785e-05,
      "loss": 0.7497,
      "step": 110000
    },
    {
      "epoch": 1.7284144427001569,
      "grad_norm": 4.666362285614014,
      "learning_rate": 4.891974097331241e-05,
      "loss": 0.7466,
      "step": 110100
    },
    {
      "epoch": 1.7299843014128728,
      "grad_norm": 3.224001884460449,
      "learning_rate": 4.891875981161695e-05,
      "loss": 0.7293,
      "step": 110200
    },
    {
      "epoch": 1.7315541601255888,
      "grad_norm": 5.1202311515808105,
      "learning_rate": 4.891777864992151e-05,
      "loss": 0.7039,
      "step": 110300
    },
    {
      "epoch": 1.7331240188383046,
      "grad_norm": 4.908113479614258,
      "learning_rate": 4.891679748822606e-05,
      "loss": 0.738,
      "step": 110400
    },
    {
      "epoch": 1.7346938775510203,
      "grad_norm": 4.646077632904053,
      "learning_rate": 4.891581632653062e-05,
      "loss": 0.7453,
      "step": 110500
    },
    {
      "epoch": 1.7362637362637363,
      "grad_norm": 4.8105292320251465,
      "learning_rate": 4.891483516483516e-05,
      "loss": 0.7269,
      "step": 110600
    },
    {
      "epoch": 1.737833594976452,
      "grad_norm": 4.652907371520996,
      "learning_rate": 4.891385400313972e-05,
      "loss": 0.7513,
      "step": 110700
    },
    {
      "epoch": 1.7394034536891678,
      "grad_norm": 4.340034008026123,
      "learning_rate": 4.891287284144427e-05,
      "loss": 0.7102,
      "step": 110800
    },
    {
      "epoch": 1.7409733124018838,
      "grad_norm": 4.846996307373047,
      "learning_rate": 4.891189167974882e-05,
      "loss": 0.7538,
      "step": 110900
    },
    {
      "epoch": 1.7425431711145998,
      "grad_norm": 3.636648416519165,
      "learning_rate": 4.8910910518053374e-05,
      "loss": 0.716,
      "step": 111000
    },
    {
      "epoch": 1.7441130298273155,
      "grad_norm": 5.0030717849731445,
      "learning_rate": 4.890992935635793e-05,
      "loss": 0.741,
      "step": 111100
    },
    {
      "epoch": 1.7456828885400313,
      "grad_norm": 4.780132293701172,
      "learning_rate": 4.890894819466248e-05,
      "loss": 0.7398,
      "step": 111200
    },
    {
      "epoch": 1.7472527472527473,
      "grad_norm": 4.2477874755859375,
      "learning_rate": 4.8907967032967034e-05,
      "loss": 0.7422,
      "step": 111300
    },
    {
      "epoch": 1.7488226059654632,
      "grad_norm": 4.65086030960083,
      "learning_rate": 4.8906985871271585e-05,
      "loss": 0.7634,
      "step": 111400
    },
    {
      "epoch": 1.750392464678179,
      "grad_norm": 4.7884626388549805,
      "learning_rate": 4.890600470957614e-05,
      "loss": 0.7394,
      "step": 111500
    },
    {
      "epoch": 1.7519623233908947,
      "grad_norm": 2.624330759048462,
      "learning_rate": 4.890502354788069e-05,
      "loss": 0.6893,
      "step": 111600
    },
    {
      "epoch": 1.7535321821036107,
      "grad_norm": 4.781460285186768,
      "learning_rate": 4.8904042386185244e-05,
      "loss": 0.7723,
      "step": 111700
    },
    {
      "epoch": 1.7551020408163265,
      "grad_norm": 4.113710880279541,
      "learning_rate": 4.89030612244898e-05,
      "loss": 0.7543,
      "step": 111800
    },
    {
      "epoch": 1.7566718995290422,
      "grad_norm": 3.584176540374756,
      "learning_rate": 4.890208006279435e-05,
      "loss": 0.7376,
      "step": 111900
    },
    {
      "epoch": 1.7582417582417582,
      "grad_norm": 4.019650459289551,
      "learning_rate": 4.8901098901098904e-05,
      "loss": 0.7318,
      "step": 112000
    },
    {
      "epoch": 1.7598116169544742,
      "grad_norm": 3.794076919555664,
      "learning_rate": 4.8900117739403455e-05,
      "loss": 0.6987,
      "step": 112100
    },
    {
      "epoch": 1.76138147566719,
      "grad_norm": 4.197059631347656,
      "learning_rate": 4.889913657770801e-05,
      "loss": 0.7152,
      "step": 112200
    },
    {
      "epoch": 1.7629513343799057,
      "grad_norm": 4.245429515838623,
      "learning_rate": 4.889815541601256e-05,
      "loss": 0.7239,
      "step": 112300
    },
    {
      "epoch": 1.7645211930926217,
      "grad_norm": 3.682360887527466,
      "learning_rate": 4.8897174254317115e-05,
      "loss": 0.742,
      "step": 112400
    },
    {
      "epoch": 1.7660910518053377,
      "grad_norm": 4.774621486663818,
      "learning_rate": 4.8896193092621666e-05,
      "loss": 0.7491,
      "step": 112500
    },
    {
      "epoch": 1.7676609105180534,
      "grad_norm": 4.543732166290283,
      "learning_rate": 4.8895211930926223e-05,
      "loss": 0.7639,
      "step": 112600
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 4.27077054977417,
      "learning_rate": 4.889423076923077e-05,
      "loss": 0.7522,
      "step": 112700
    },
    {
      "epoch": 1.7708006279434851,
      "grad_norm": 4.277409553527832,
      "learning_rate": 4.8893249607535325e-05,
      "loss": 0.7226,
      "step": 112800
    },
    {
      "epoch": 1.772370486656201,
      "grad_norm": 4.128832817077637,
      "learning_rate": 4.8892268445839876e-05,
      "loss": 0.794,
      "step": 112900
    },
    {
      "epoch": 1.7739403453689166,
      "grad_norm": 4.209840297698975,
      "learning_rate": 4.889128728414443e-05,
      "loss": 0.7367,
      "step": 113000
    },
    {
      "epoch": 1.7755102040816326,
      "grad_norm": 4.390556812286377,
      "learning_rate": 4.889030612244898e-05,
      "loss": 0.7369,
      "step": 113100
    },
    {
      "epoch": 1.7770800627943486,
      "grad_norm": 4.131532192230225,
      "learning_rate": 4.8889324960753536e-05,
      "loss": 0.7394,
      "step": 113200
    },
    {
      "epoch": 1.7786499215070644,
      "grad_norm": 3.9428822994232178,
      "learning_rate": 4.888834379905809e-05,
      "loss": 0.7527,
      "step": 113300
    },
    {
      "epoch": 1.7802197802197801,
      "grad_norm": 4.714062213897705,
      "learning_rate": 4.888736263736264e-05,
      "loss": 0.7689,
      "step": 113400
    },
    {
      "epoch": 1.781789638932496,
      "grad_norm": 4.371356010437012,
      "learning_rate": 4.888638147566719e-05,
      "loss": 0.7156,
      "step": 113500
    },
    {
      "epoch": 1.783359497645212,
      "grad_norm": 3.5214200019836426,
      "learning_rate": 4.888540031397175e-05,
      "loss": 0.7215,
      "step": 113600
    },
    {
      "epoch": 1.7849293563579278,
      "grad_norm": 3.6353812217712402,
      "learning_rate": 4.88844191522763e-05,
      "loss": 0.7103,
      "step": 113700
    },
    {
      "epoch": 1.7864992150706436,
      "grad_norm": 4.385167598724365,
      "learning_rate": 4.888343799058085e-05,
      "loss": 0.7439,
      "step": 113800
    },
    {
      "epoch": 1.7880690737833596,
      "grad_norm": 4.216983318328857,
      "learning_rate": 4.8882456828885407e-05,
      "loss": 0.7483,
      "step": 113900
    },
    {
      "epoch": 1.7896389324960753,
      "grad_norm": 4.3810133934021,
      "learning_rate": 4.888147566718996e-05,
      "loss": 0.7536,
      "step": 114000
    },
    {
      "epoch": 1.791208791208791,
      "grad_norm": 3.304018974304199,
      "learning_rate": 4.888049450549451e-05,
      "loss": 0.7297,
      "step": 114100
    },
    {
      "epoch": 1.792778649921507,
      "grad_norm": 4.63045597076416,
      "learning_rate": 4.887951334379906e-05,
      "loss": 0.7762,
      "step": 114200
    },
    {
      "epoch": 1.794348508634223,
      "grad_norm": 3.951047658920288,
      "learning_rate": 4.887853218210362e-05,
      "loss": 0.7025,
      "step": 114300
    },
    {
      "epoch": 1.7959183673469388,
      "grad_norm": 4.076790809631348,
      "learning_rate": 4.887755102040816e-05,
      "loss": 0.7378,
      "step": 114400
    },
    {
      "epoch": 1.7974882260596545,
      "grad_norm": 4.4807305335998535,
      "learning_rate": 4.887656985871272e-05,
      "loss": 0.7645,
      "step": 114500
    },
    {
      "epoch": 1.7990580847723705,
      "grad_norm": 2.6670212745666504,
      "learning_rate": 4.887558869701727e-05,
      "loss": 0.7453,
      "step": 114600
    },
    {
      "epoch": 1.8006279434850865,
      "grad_norm": 4.564827919006348,
      "learning_rate": 4.887460753532183e-05,
      "loss": 0.7624,
      "step": 114700
    },
    {
      "epoch": 1.8021978021978022,
      "grad_norm": 4.166738986968994,
      "learning_rate": 4.887362637362637e-05,
      "loss": 0.7431,
      "step": 114800
    },
    {
      "epoch": 1.803767660910518,
      "grad_norm": 3.5382139682769775,
      "learning_rate": 4.887264521193093e-05,
      "loss": 0.7167,
      "step": 114900
    },
    {
      "epoch": 1.805337519623234,
      "grad_norm": 4.684324741363525,
      "learning_rate": 4.887166405023548e-05,
      "loss": 0.7722,
      "step": 115000
    },
    {
      "epoch": 1.8069073783359497,
      "grad_norm": 4.270512580871582,
      "learning_rate": 4.887068288854003e-05,
      "loss": 0.7033,
      "step": 115100
    },
    {
      "epoch": 1.8084772370486655,
      "grad_norm": 3.9011852741241455,
      "learning_rate": 4.886970172684458e-05,
      "loss": 0.7201,
      "step": 115200
    },
    {
      "epoch": 1.8100470957613815,
      "grad_norm": 4.900398254394531,
      "learning_rate": 4.886872056514914e-05,
      "loss": 0.7368,
      "step": 115300
    },
    {
      "epoch": 1.8116169544740974,
      "grad_norm": 2.6841273307800293,
      "learning_rate": 4.886773940345369e-05,
      "loss": 0.7424,
      "step": 115400
    },
    {
      "epoch": 1.8131868131868132,
      "grad_norm": 4.101842880249023,
      "learning_rate": 4.886675824175824e-05,
      "loss": 0.7493,
      "step": 115500
    },
    {
      "epoch": 1.814756671899529,
      "grad_norm": 3.7755322456359863,
      "learning_rate": 4.8865777080062794e-05,
      "loss": 0.7501,
      "step": 115600
    },
    {
      "epoch": 1.816326530612245,
      "grad_norm": 4.586812973022461,
      "learning_rate": 4.886479591836735e-05,
      "loss": 0.7179,
      "step": 115700
    },
    {
      "epoch": 1.817896389324961,
      "grad_norm": 4.250763893127441,
      "learning_rate": 4.88638147566719e-05,
      "loss": 0.6956,
      "step": 115800
    },
    {
      "epoch": 1.8194662480376766,
      "grad_norm": 4.047979354858398,
      "learning_rate": 4.886283359497645e-05,
      "loss": 0.7144,
      "step": 115900
    },
    {
      "epoch": 1.8210361067503924,
      "grad_norm": 4.263500690460205,
      "learning_rate": 4.886185243328101e-05,
      "loss": 0.7151,
      "step": 116000
    },
    {
      "epoch": 1.8226059654631084,
      "grad_norm": 2.9747350215911865,
      "learning_rate": 4.886087127158556e-05,
      "loss": 0.6962,
      "step": 116100
    },
    {
      "epoch": 1.8241758241758241,
      "grad_norm": 2.8854575157165527,
      "learning_rate": 4.885989010989011e-05,
      "loss": 0.6726,
      "step": 116200
    },
    {
      "epoch": 1.82574568288854,
      "grad_norm": 3.789193868637085,
      "learning_rate": 4.8858908948194664e-05,
      "loss": 0.7242,
      "step": 116300
    },
    {
      "epoch": 1.8273155416012559,
      "grad_norm": 2.845167636871338,
      "learning_rate": 4.885792778649922e-05,
      "loss": 0.739,
      "step": 116400
    },
    {
      "epoch": 1.8288854003139718,
      "grad_norm": 3.604088068008423,
      "learning_rate": 4.8856946624803766e-05,
      "loss": 0.7043,
      "step": 116500
    },
    {
      "epoch": 1.8304552590266876,
      "grad_norm": 3.1008999347686768,
      "learning_rate": 4.8855965463108324e-05,
      "loss": 0.7159,
      "step": 116600
    },
    {
      "epoch": 1.8320251177394034,
      "grad_norm": 4.119625091552734,
      "learning_rate": 4.8854984301412875e-05,
      "loss": 0.8038,
      "step": 116700
    },
    {
      "epoch": 1.8335949764521193,
      "grad_norm": 3.8171703815460205,
      "learning_rate": 4.885400313971743e-05,
      "loss": 0.7182,
      "step": 116800
    },
    {
      "epoch": 1.8351648351648353,
      "grad_norm": 4.0498857498168945,
      "learning_rate": 4.885302197802198e-05,
      "loss": 0.7112,
      "step": 116900
    },
    {
      "epoch": 1.836734693877551,
      "grad_norm": 3.7505829334259033,
      "learning_rate": 4.8852040816326534e-05,
      "loss": 0.7562,
      "step": 117000
    },
    {
      "epoch": 1.8383045525902668,
      "grad_norm": 3.6187829971313477,
      "learning_rate": 4.8851059654631085e-05,
      "loss": 0.727,
      "step": 117100
    },
    {
      "epoch": 1.8398744113029828,
      "grad_norm": 3.8150103092193604,
      "learning_rate": 4.8850078492935636e-05,
      "loss": 0.7322,
      "step": 117200
    },
    {
      "epoch": 1.8414442700156985,
      "grad_norm": 5.222250938415527,
      "learning_rate": 4.884909733124019e-05,
      "loss": 0.7393,
      "step": 117300
    },
    {
      "epoch": 1.8430141287284143,
      "grad_norm": 3.6328394412994385,
      "learning_rate": 4.8848116169544745e-05,
      "loss": 0.7136,
      "step": 117400
    },
    {
      "epoch": 1.8445839874411303,
      "grad_norm": 4.773694038391113,
      "learning_rate": 4.8847135007849296e-05,
      "loss": 0.7214,
      "step": 117500
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 3.522176742553711,
      "learning_rate": 4.884615384615385e-05,
      "loss": 0.7301,
      "step": 117600
    },
    {
      "epoch": 1.847723704866562,
      "grad_norm": 3.3853397369384766,
      "learning_rate": 4.88451726844584e-05,
      "loss": 0.7361,
      "step": 117700
    },
    {
      "epoch": 1.8492935635792778,
      "grad_norm": 4.3494553565979,
      "learning_rate": 4.8844191522762956e-05,
      "loss": 0.7157,
      "step": 117800
    },
    {
      "epoch": 1.8508634222919937,
      "grad_norm": 3.962679624557495,
      "learning_rate": 4.884321036106751e-05,
      "loss": 0.7524,
      "step": 117900
    },
    {
      "epoch": 1.8524332810047097,
      "grad_norm": 4.7350358963012695,
      "learning_rate": 4.884222919937206e-05,
      "loss": 0.7303,
      "step": 118000
    },
    {
      "epoch": 1.8540031397174255,
      "grad_norm": 4.31149435043335,
      "learning_rate": 4.8841248037676616e-05,
      "loss": 0.7485,
      "step": 118100
    },
    {
      "epoch": 1.8555729984301412,
      "grad_norm": 3.5668201446533203,
      "learning_rate": 4.8840266875981167e-05,
      "loss": 0.7295,
      "step": 118200
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 3.6904518604278564,
      "learning_rate": 4.883928571428572e-05,
      "loss": 0.7167,
      "step": 118300
    },
    {
      "epoch": 1.858712715855573,
      "grad_norm": 3.878983497619629,
      "learning_rate": 4.883830455259027e-05,
      "loss": 0.735,
      "step": 118400
    },
    {
      "epoch": 1.8602825745682887,
      "grad_norm": 4.868516445159912,
      "learning_rate": 4.8837323390894826e-05,
      "loss": 0.7168,
      "step": 118500
    },
    {
      "epoch": 1.8618524332810047,
      "grad_norm": 4.303530216217041,
      "learning_rate": 4.883634222919937e-05,
      "loss": 0.73,
      "step": 118600
    },
    {
      "epoch": 1.8634222919937207,
      "grad_norm": 4.613081455230713,
      "learning_rate": 4.883536106750393e-05,
      "loss": 0.7371,
      "step": 118700
    },
    {
      "epoch": 1.8649921507064364,
      "grad_norm": 3.2400174140930176,
      "learning_rate": 4.883437990580848e-05,
      "loss": 0.7341,
      "step": 118800
    },
    {
      "epoch": 1.8665620094191522,
      "grad_norm": 4.700238227844238,
      "learning_rate": 4.883339874411304e-05,
      "loss": 0.716,
      "step": 118900
    },
    {
      "epoch": 1.8681318681318682,
      "grad_norm": 4.015198707580566,
      "learning_rate": 4.883241758241758e-05,
      "loss": 0.6993,
      "step": 119000
    },
    {
      "epoch": 1.8697017268445841,
      "grad_norm": 3.745785713195801,
      "learning_rate": 4.883143642072214e-05,
      "loss": 0.6852,
      "step": 119100
    },
    {
      "epoch": 1.8712715855572999,
      "grad_norm": 4.893641471862793,
      "learning_rate": 4.883045525902669e-05,
      "loss": 0.7401,
      "step": 119200
    },
    {
      "epoch": 1.8728414442700156,
      "grad_norm": 4.363086700439453,
      "learning_rate": 4.882947409733124e-05,
      "loss": 0.757,
      "step": 119300
    },
    {
      "epoch": 1.8744113029827316,
      "grad_norm": 4.307236194610596,
      "learning_rate": 4.882849293563579e-05,
      "loss": 0.7609,
      "step": 119400
    },
    {
      "epoch": 1.8759811616954474,
      "grad_norm": 4.720778942108154,
      "learning_rate": 4.882751177394035e-05,
      "loss": 0.7203,
      "step": 119500
    },
    {
      "epoch": 1.8775510204081631,
      "grad_norm": 4.490189075469971,
      "learning_rate": 4.88265306122449e-05,
      "loss": 0.7512,
      "step": 119600
    },
    {
      "epoch": 1.879120879120879,
      "grad_norm": 4.523027420043945,
      "learning_rate": 4.882554945054945e-05,
      "loss": 0.7801,
      "step": 119700
    },
    {
      "epoch": 1.880690737833595,
      "grad_norm": 4.544380187988281,
      "learning_rate": 4.8824568288854e-05,
      "loss": 0.7116,
      "step": 119800
    },
    {
      "epoch": 1.8822605965463108,
      "grad_norm": 4.278217792510986,
      "learning_rate": 4.882358712715856e-05,
      "loss": 0.7425,
      "step": 119900
    },
    {
      "epoch": 1.8838304552590266,
      "grad_norm": 3.665233850479126,
      "learning_rate": 4.882260596546311e-05,
      "loss": 0.7079,
      "step": 120000
    },
    {
      "epoch": 1.8854003139717426,
      "grad_norm": 4.2946319580078125,
      "learning_rate": 4.882162480376766e-05,
      "loss": 0.7372,
      "step": 120100
    },
    {
      "epoch": 1.8869701726844585,
      "grad_norm": 3.6681406497955322,
      "learning_rate": 4.882064364207222e-05,
      "loss": 0.691,
      "step": 120200
    },
    {
      "epoch": 1.8885400313971743,
      "grad_norm": 3.4942073822021484,
      "learning_rate": 4.881966248037677e-05,
      "loss": 0.7147,
      "step": 120300
    },
    {
      "epoch": 1.89010989010989,
      "grad_norm": 5.04554557800293,
      "learning_rate": 4.881868131868132e-05,
      "loss": 0.7138,
      "step": 120400
    },
    {
      "epoch": 1.891679748822606,
      "grad_norm": 4.005674839019775,
      "learning_rate": 4.881770015698587e-05,
      "loss": 0.6989,
      "step": 120500
    },
    {
      "epoch": 1.8932496075353218,
      "grad_norm": 4.5356950759887695,
      "learning_rate": 4.881671899529043e-05,
      "loss": 0.7168,
      "step": 120600
    },
    {
      "epoch": 1.8948194662480375,
      "grad_norm": 3.926809310913086,
      "learning_rate": 4.8815737833594975e-05,
      "loss": 0.7264,
      "step": 120700
    },
    {
      "epoch": 1.8963893249607535,
      "grad_norm": 4.0492143630981445,
      "learning_rate": 4.881475667189953e-05,
      "loss": 0.735,
      "step": 120800
    },
    {
      "epoch": 1.8979591836734695,
      "grad_norm": 4.574363708496094,
      "learning_rate": 4.8813775510204084e-05,
      "loss": 0.7171,
      "step": 120900
    },
    {
      "epoch": 1.8995290423861853,
      "grad_norm": 3.770958185195923,
      "learning_rate": 4.881279434850864e-05,
      "loss": 0.8037,
      "step": 121000
    },
    {
      "epoch": 1.901098901098901,
      "grad_norm": 4.472909927368164,
      "learning_rate": 4.8811813186813186e-05,
      "loss": 0.7643,
      "step": 121100
    },
    {
      "epoch": 1.902668759811617,
      "grad_norm": 3.1532602310180664,
      "learning_rate": 4.8810832025117743e-05,
      "loss": 0.8015,
      "step": 121200
    },
    {
      "epoch": 1.904238618524333,
      "grad_norm": 3.5716805458068848,
      "learning_rate": 4.8809850863422294e-05,
      "loss": 0.7308,
      "step": 121300
    },
    {
      "epoch": 1.9058084772370487,
      "grad_norm": 4.514975547790527,
      "learning_rate": 4.8808869701726845e-05,
      "loss": 0.7148,
      "step": 121400
    },
    {
      "epoch": 1.9073783359497645,
      "grad_norm": 4.9843974113464355,
      "learning_rate": 4.8807888540031396e-05,
      "loss": 0.7466,
      "step": 121500
    },
    {
      "epoch": 1.9089481946624804,
      "grad_norm": 3.390139579772949,
      "learning_rate": 4.8806907378335954e-05,
      "loss": 0.7035,
      "step": 121600
    },
    {
      "epoch": 1.9105180533751962,
      "grad_norm": 4.90725040435791,
      "learning_rate": 4.8805926216640505e-05,
      "loss": 0.7426,
      "step": 121700
    },
    {
      "epoch": 1.912087912087912,
      "grad_norm": 4.041497230529785,
      "learning_rate": 4.8804945054945056e-05,
      "loss": 0.6692,
      "step": 121800
    },
    {
      "epoch": 1.913657770800628,
      "grad_norm": 4.016250133514404,
      "learning_rate": 4.880396389324961e-05,
      "loss": 0.7517,
      "step": 121900
    },
    {
      "epoch": 1.915227629513344,
      "grad_norm": 4.859304428100586,
      "learning_rate": 4.8802982731554165e-05,
      "loss": 0.7247,
      "step": 122000
    },
    {
      "epoch": 1.9167974882260597,
      "grad_norm": 3.672137975692749,
      "learning_rate": 4.8802001569858716e-05,
      "loss": 0.7361,
      "step": 122100
    },
    {
      "epoch": 1.9183673469387754,
      "grad_norm": 3.736726760864258,
      "learning_rate": 4.880102040816327e-05,
      "loss": 0.7115,
      "step": 122200
    },
    {
      "epoch": 1.9199372056514914,
      "grad_norm": 4.236260414123535,
      "learning_rate": 4.8800039246467825e-05,
      "loss": 0.7442,
      "step": 122300
    },
    {
      "epoch": 1.9215070643642074,
      "grad_norm": 4.604803085327148,
      "learning_rate": 4.8799058084772376e-05,
      "loss": 0.7415,
      "step": 122400
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 3.6283302307128906,
      "learning_rate": 4.8798076923076926e-05,
      "loss": 0.732,
      "step": 122500
    },
    {
      "epoch": 1.9246467817896389,
      "grad_norm": 5.054747581481934,
      "learning_rate": 4.879709576138148e-05,
      "loss": 0.6672,
      "step": 122600
    },
    {
      "epoch": 1.9262166405023549,
      "grad_norm": 4.092116832733154,
      "learning_rate": 4.8796114599686035e-05,
      "loss": 0.7,
      "step": 122700
    },
    {
      "epoch": 1.9277864992150706,
      "grad_norm": 4.005279541015625,
      "learning_rate": 4.879513343799058e-05,
      "loss": 0.6901,
      "step": 122800
    },
    {
      "epoch": 1.9293563579277864,
      "grad_norm": 3.0696678161621094,
      "learning_rate": 4.879415227629514e-05,
      "loss": 0.7495,
      "step": 122900
    },
    {
      "epoch": 1.9309262166405023,
      "grad_norm": 3.9196860790252686,
      "learning_rate": 4.879317111459969e-05,
      "loss": 0.7304,
      "step": 123000
    },
    {
      "epoch": 1.9324960753532183,
      "grad_norm": 3.4908688068389893,
      "learning_rate": 4.8792189952904246e-05,
      "loss": 0.7084,
      "step": 123100
    },
    {
      "epoch": 1.934065934065934,
      "grad_norm": 4.178983688354492,
      "learning_rate": 4.879120879120879e-05,
      "loss": 0.7281,
      "step": 123200
    },
    {
      "epoch": 1.9356357927786498,
      "grad_norm": 4.1669816970825195,
      "learning_rate": 4.879022762951335e-05,
      "loss": 0.7437,
      "step": 123300
    },
    {
      "epoch": 1.9372056514913658,
      "grad_norm": 4.8267340660095215,
      "learning_rate": 4.87892464678179e-05,
      "loss": 0.7219,
      "step": 123400
    },
    {
      "epoch": 1.9387755102040818,
      "grad_norm": 3.906815528869629,
      "learning_rate": 4.878826530612245e-05,
      "loss": 0.6972,
      "step": 123500
    },
    {
      "epoch": 1.9403453689167975,
      "grad_norm": 4.39310884475708,
      "learning_rate": 4.8787284144427e-05,
      "loss": 0.7433,
      "step": 123600
    },
    {
      "epoch": 1.9419152276295133,
      "grad_norm": 3.1473727226257324,
      "learning_rate": 4.878630298273156e-05,
      "loss": 0.7493,
      "step": 123700
    },
    {
      "epoch": 1.9434850863422293,
      "grad_norm": 4.543034076690674,
      "learning_rate": 4.878532182103611e-05,
      "loss": 0.7286,
      "step": 123800
    },
    {
      "epoch": 1.945054945054945,
      "grad_norm": 4.500302791595459,
      "learning_rate": 4.878434065934066e-05,
      "loss": 0.7157,
      "step": 123900
    },
    {
      "epoch": 1.9466248037676608,
      "grad_norm": 3.9700911045074463,
      "learning_rate": 4.878335949764521e-05,
      "loss": 0.7647,
      "step": 124000
    },
    {
      "epoch": 1.9481946624803768,
      "grad_norm": 3.199429988861084,
      "learning_rate": 4.878237833594977e-05,
      "loss": 0.7584,
      "step": 124100
    },
    {
      "epoch": 1.9497645211930927,
      "grad_norm": 4.394949436187744,
      "learning_rate": 4.878139717425432e-05,
      "loss": 0.7338,
      "step": 124200
    },
    {
      "epoch": 1.9513343799058085,
      "grad_norm": 4.4921650886535645,
      "learning_rate": 4.878041601255887e-05,
      "loss": 0.6778,
      "step": 124300
    },
    {
      "epoch": 1.9529042386185242,
      "grad_norm": 4.248411655426025,
      "learning_rate": 4.877943485086343e-05,
      "loss": 0.7353,
      "step": 124400
    },
    {
      "epoch": 1.9544740973312402,
      "grad_norm": 3.491746664047241,
      "learning_rate": 4.877845368916798e-05,
      "loss": 0.7125,
      "step": 124500
    },
    {
      "epoch": 1.9560439560439562,
      "grad_norm": 3.7850332260131836,
      "learning_rate": 4.877747252747253e-05,
      "loss": 0.7426,
      "step": 124600
    },
    {
      "epoch": 1.957613814756672,
      "grad_norm": 4.411780834197998,
      "learning_rate": 4.877649136577708e-05,
      "loss": 0.7086,
      "step": 124700
    },
    {
      "epoch": 1.9591836734693877,
      "grad_norm": 3.0148510932922363,
      "learning_rate": 4.877551020408164e-05,
      "loss": 0.7148,
      "step": 124800
    },
    {
      "epoch": 1.9607535321821037,
      "grad_norm": 5.0430521965026855,
      "learning_rate": 4.8774529042386184e-05,
      "loss": 0.7391,
      "step": 124900
    },
    {
      "epoch": 1.9623233908948194,
      "grad_norm": 5.423569679260254,
      "learning_rate": 4.877354788069074e-05,
      "loss": 0.7437,
      "step": 125000
    },
    {
      "epoch": 1.9638932496075352,
      "grad_norm": 3.3332715034484863,
      "learning_rate": 4.877256671899529e-05,
      "loss": 0.7177,
      "step": 125100
    },
    {
      "epoch": 1.9654631083202512,
      "grad_norm": 4.183953285217285,
      "learning_rate": 4.8771585557299844e-05,
      "loss": 0.7122,
      "step": 125200
    },
    {
      "epoch": 1.9670329670329672,
      "grad_norm": 3.930504083633423,
      "learning_rate": 4.8770604395604395e-05,
      "loss": 0.6939,
      "step": 125300
    },
    {
      "epoch": 1.968602825745683,
      "grad_norm": 4.936831474304199,
      "learning_rate": 4.876962323390895e-05,
      "loss": 0.7109,
      "step": 125400
    },
    {
      "epoch": 1.9701726844583987,
      "grad_norm": 4.231422424316406,
      "learning_rate": 4.87686420722135e-05,
      "loss": 0.7142,
      "step": 125500
    },
    {
      "epoch": 1.9717425431711146,
      "grad_norm": 3.522296667098999,
      "learning_rate": 4.8767660910518054e-05,
      "loss": 0.7629,
      "step": 125600
    },
    {
      "epoch": 1.9733124018838306,
      "grad_norm": 4.692615032196045,
      "learning_rate": 4.8766679748822605e-05,
      "loss": 0.7289,
      "step": 125700
    },
    {
      "epoch": 1.9748822605965461,
      "grad_norm": 3.2110631465911865,
      "learning_rate": 4.876569858712716e-05,
      "loss": 0.7375,
      "step": 125800
    },
    {
      "epoch": 1.9764521193092621,
      "grad_norm": 3.7684319019317627,
      "learning_rate": 4.8764717425431714e-05,
      "loss": 0.7046,
      "step": 125900
    },
    {
      "epoch": 1.978021978021978,
      "grad_norm": 4.141819953918457,
      "learning_rate": 4.8763736263736265e-05,
      "loss": 0.6816,
      "step": 126000
    },
    {
      "epoch": 1.9795918367346939,
      "grad_norm": 4.182517051696777,
      "learning_rate": 4.8762755102040816e-05,
      "loss": 0.724,
      "step": 126100
    },
    {
      "epoch": 1.9811616954474096,
      "grad_norm": 4.3962836265563965,
      "learning_rate": 4.8761773940345374e-05,
      "loss": 0.6699,
      "step": 126200
    },
    {
      "epoch": 1.9827315541601256,
      "grad_norm": 3.578831672668457,
      "learning_rate": 4.8760792778649925e-05,
      "loss": 0.707,
      "step": 126300
    },
    {
      "epoch": 1.9843014128728416,
      "grad_norm": 4.505619049072266,
      "learning_rate": 4.8759811616954476e-05,
      "loss": 0.7411,
      "step": 126400
    },
    {
      "epoch": 1.9858712715855573,
      "grad_norm": 4.860474109649658,
      "learning_rate": 4.8758830455259034e-05,
      "loss": 0.7079,
      "step": 126500
    },
    {
      "epoch": 1.987441130298273,
      "grad_norm": 4.2994561195373535,
      "learning_rate": 4.875784929356358e-05,
      "loss": 0.7332,
      "step": 126600
    },
    {
      "epoch": 1.989010989010989,
      "grad_norm": 3.9309020042419434,
      "learning_rate": 4.8756868131868135e-05,
      "loss": 0.7139,
      "step": 126700
    },
    {
      "epoch": 1.990580847723705,
      "grad_norm": 3.5454111099243164,
      "learning_rate": 4.8755886970172686e-05,
      "loss": 0.7271,
      "step": 126800
    },
    {
      "epoch": 1.9921507064364206,
      "grad_norm": 4.4471845626831055,
      "learning_rate": 4.8754905808477244e-05,
      "loss": 0.7691,
      "step": 126900
    },
    {
      "epoch": 1.9937205651491365,
      "grad_norm": 4.673951625823975,
      "learning_rate": 4.875392464678179e-05,
      "loss": 0.7001,
      "step": 127000
    },
    {
      "epoch": 1.9952904238618525,
      "grad_norm": 2.7576942443847656,
      "learning_rate": 4.8752943485086346e-05,
      "loss": 0.7251,
      "step": 127100
    },
    {
      "epoch": 1.9968602825745683,
      "grad_norm": 4.8528900146484375,
      "learning_rate": 4.87519623233909e-05,
      "loss": 0.7586,
      "step": 127200
    },
    {
      "epoch": 1.998430141287284,
      "grad_norm": 3.4685580730438232,
      "learning_rate": 4.875098116169545e-05,
      "loss": 0.7112,
      "step": 127300
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.7686567306518555,
      "learning_rate": 4.875e-05,
      "loss": 0.7367,
      "step": 127400
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.0522571802139282,
      "eval_runtime": 14.6746,
      "eval_samples_per_second": 228.49,
      "eval_steps_per_second": 228.49,
      "step": 127400
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.5729185342788696,
      "eval_runtime": 182.392,
      "eval_samples_per_second": 349.248,
      "eval_steps_per_second": 349.248,
      "step": 127400
    },
    {
      "epoch": 2.001569858712716,
      "grad_norm": 3.6045608520507812,
      "learning_rate": 4.874901883830456e-05,
      "loss": 0.7213,
      "step": 127500
    },
    {
      "epoch": 2.0031397174254315,
      "grad_norm": 4.650818347930908,
      "learning_rate": 4.874803767660911e-05,
      "loss": 0.6855,
      "step": 127600
    },
    {
      "epoch": 2.0047095761381475,
      "grad_norm": 3.861605167388916,
      "learning_rate": 4.874705651491366e-05,
      "loss": 0.6846,
      "step": 127700
    },
    {
      "epoch": 2.0062794348508635,
      "grad_norm": 4.415987014770508,
      "learning_rate": 4.874607535321821e-05,
      "loss": 0.6978,
      "step": 127800
    },
    {
      "epoch": 2.0078492935635794,
      "grad_norm": 4.814986228942871,
      "learning_rate": 4.874509419152277e-05,
      "loss": 0.7366,
      "step": 127900
    },
    {
      "epoch": 2.009419152276295,
      "grad_norm": 4.663508415222168,
      "learning_rate": 4.874411302982732e-05,
      "loss": 0.7261,
      "step": 128000
    },
    {
      "epoch": 2.010989010989011,
      "grad_norm": 4.7788262367248535,
      "learning_rate": 4.874313186813187e-05,
      "loss": 0.6935,
      "step": 128100
    },
    {
      "epoch": 2.012558869701727,
      "grad_norm": 4.6081953048706055,
      "learning_rate": 4.874215070643642e-05,
      "loss": 0.7305,
      "step": 128200
    },
    {
      "epoch": 2.014128728414443,
      "grad_norm": 4.019172668457031,
      "learning_rate": 4.874116954474098e-05,
      "loss": 0.6997,
      "step": 128300
    },
    {
      "epoch": 2.0156985871271584,
      "grad_norm": 5.161468029022217,
      "learning_rate": 4.874018838304553e-05,
      "loss": 0.687,
      "step": 128400
    },
    {
      "epoch": 2.0172684458398744,
      "grad_norm": 4.3979573249816895,
      "learning_rate": 4.873920722135008e-05,
      "loss": 0.7219,
      "step": 128500
    },
    {
      "epoch": 2.0188383045525904,
      "grad_norm": 4.3778767585754395,
      "learning_rate": 4.873822605965464e-05,
      "loss": 0.7106,
      "step": 128600
    },
    {
      "epoch": 2.020408163265306,
      "grad_norm": 4.978126525878906,
      "learning_rate": 4.873724489795918e-05,
      "loss": 0.7281,
      "step": 128700
    },
    {
      "epoch": 2.021978021978022,
      "grad_norm": 3.267946243286133,
      "learning_rate": 4.873626373626374e-05,
      "loss": 0.7285,
      "step": 128800
    },
    {
      "epoch": 2.023547880690738,
      "grad_norm": 4.438923358917236,
      "learning_rate": 4.873528257456829e-05,
      "loss": 0.7232,
      "step": 128900
    },
    {
      "epoch": 2.025117739403454,
      "grad_norm": 3.6592540740966797,
      "learning_rate": 4.873430141287285e-05,
      "loss": 0.715,
      "step": 129000
    },
    {
      "epoch": 2.0266875981161694,
      "grad_norm": 5.048126697540283,
      "learning_rate": 4.873332025117739e-05,
      "loss": 0.6898,
      "step": 129100
    },
    {
      "epoch": 2.0282574568288854,
      "grad_norm": 4.545914649963379,
      "learning_rate": 4.873233908948195e-05,
      "loss": 0.6952,
      "step": 129200
    },
    {
      "epoch": 2.0298273155416013,
      "grad_norm": 4.046875,
      "learning_rate": 4.87313579277865e-05,
      "loss": 0.7635,
      "step": 129300
    },
    {
      "epoch": 2.0313971742543173,
      "grad_norm": 4.355501174926758,
      "learning_rate": 4.873037676609105e-05,
      "loss": 0.729,
      "step": 129400
    },
    {
      "epoch": 2.032967032967033,
      "grad_norm": 3.8731212615966797,
      "learning_rate": 4.8729395604395604e-05,
      "loss": 0.7464,
      "step": 129500
    },
    {
      "epoch": 2.034536891679749,
      "grad_norm": 4.222245693206787,
      "learning_rate": 4.872841444270016e-05,
      "loss": 0.7223,
      "step": 129600
    },
    {
      "epoch": 2.036106750392465,
      "grad_norm": 4.757095813751221,
      "learning_rate": 4.872743328100471e-05,
      "loss": 0.7353,
      "step": 129700
    },
    {
      "epoch": 2.0376766091051803,
      "grad_norm": 4.139702796936035,
      "learning_rate": 4.872645211930926e-05,
      "loss": 0.7257,
      "step": 129800
    },
    {
      "epoch": 2.0392464678178963,
      "grad_norm": 3.8962368965148926,
      "learning_rate": 4.8725470957613814e-05,
      "loss": 0.7295,
      "step": 129900
    },
    {
      "epoch": 2.0408163265306123,
      "grad_norm": 4.291989326477051,
      "learning_rate": 4.872448979591837e-05,
      "loss": 0.7331,
      "step": 130000
    },
    {
      "epoch": 2.0423861852433283,
      "grad_norm": 4.220066070556641,
      "learning_rate": 4.872350863422292e-05,
      "loss": 0.68,
      "step": 130100
    },
    {
      "epoch": 2.043956043956044,
      "grad_norm": 4.535017490386963,
      "learning_rate": 4.8722527472527474e-05,
      "loss": 0.7521,
      "step": 130200
    },
    {
      "epoch": 2.0455259026687598,
      "grad_norm": 4.603078365325928,
      "learning_rate": 4.8721546310832025e-05,
      "loss": 0.6978,
      "step": 130300
    },
    {
      "epoch": 2.0470957613814758,
      "grad_norm": 4.214568614959717,
      "learning_rate": 4.872056514913658e-05,
      "loss": 0.7494,
      "step": 130400
    },
    {
      "epoch": 2.0486656200941917,
      "grad_norm": 3.949662923812866,
      "learning_rate": 4.8719583987441134e-05,
      "loss": 0.7389,
      "step": 130500
    },
    {
      "epoch": 2.0502354788069073,
      "grad_norm": 4.214412212371826,
      "learning_rate": 4.8718602825745685e-05,
      "loss": 0.7096,
      "step": 130600
    },
    {
      "epoch": 2.0518053375196232,
      "grad_norm": 3.223839044570923,
      "learning_rate": 4.871762166405024e-05,
      "loss": 0.7469,
      "step": 130700
    },
    {
      "epoch": 2.053375196232339,
      "grad_norm": 3.9398014545440674,
      "learning_rate": 4.871664050235479e-05,
      "loss": 0.7154,
      "step": 130800
    },
    {
      "epoch": 2.0549450549450547,
      "grad_norm": 15.670774459838867,
      "learning_rate": 4.8715659340659344e-05,
      "loss": 0.702,
      "step": 130900
    },
    {
      "epoch": 2.0565149136577707,
      "grad_norm": 4.142209053039551,
      "learning_rate": 4.8714678178963895e-05,
      "loss": 0.7215,
      "step": 131000
    },
    {
      "epoch": 2.0580847723704867,
      "grad_norm": 3.7949910163879395,
      "learning_rate": 4.871369701726845e-05,
      "loss": 0.6896,
      "step": 131100
    },
    {
      "epoch": 2.0596546310832027,
      "grad_norm": 4.44201135635376,
      "learning_rate": 4.8712715855573e-05,
      "loss": 0.7234,
      "step": 131200
    },
    {
      "epoch": 2.061224489795918,
      "grad_norm": 4.066244125366211,
      "learning_rate": 4.8711734693877555e-05,
      "loss": 0.6901,
      "step": 131300
    },
    {
      "epoch": 2.062794348508634,
      "grad_norm": 3.825787305831909,
      "learning_rate": 4.8710753532182106e-05,
      "loss": 0.7106,
      "step": 131400
    },
    {
      "epoch": 2.06436420722135,
      "grad_norm": 4.2800469398498535,
      "learning_rate": 4.870977237048666e-05,
      "loss": 0.7493,
      "step": 131500
    },
    {
      "epoch": 2.065934065934066,
      "grad_norm": 4.092620372772217,
      "learning_rate": 4.870879120879121e-05,
      "loss": 0.7461,
      "step": 131600
    },
    {
      "epoch": 2.0675039246467817,
      "grad_norm": 3.7553534507751465,
      "learning_rate": 4.8707810047095766e-05,
      "loss": 0.6971,
      "step": 131700
    },
    {
      "epoch": 2.0690737833594977,
      "grad_norm": 3.825509548187256,
      "learning_rate": 4.870682888540032e-05,
      "loss": 0.7295,
      "step": 131800
    },
    {
      "epoch": 2.0706436420722136,
      "grad_norm": 3.7315635681152344,
      "learning_rate": 4.870584772370487e-05,
      "loss": 0.7192,
      "step": 131900
    },
    {
      "epoch": 2.072213500784929,
      "grad_norm": 4.447840690612793,
      "learning_rate": 4.870486656200942e-05,
      "loss": 0.7163,
      "step": 132000
    },
    {
      "epoch": 2.073783359497645,
      "grad_norm": 2.92120623588562,
      "learning_rate": 4.8703885400313977e-05,
      "loss": 0.7368,
      "step": 132100
    },
    {
      "epoch": 2.075353218210361,
      "grad_norm": 4.308827877044678,
      "learning_rate": 4.870290423861852e-05,
      "loss": 0.701,
      "step": 132200
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 3.3487319946289062,
      "learning_rate": 4.870192307692308e-05,
      "loss": 0.7239,
      "step": 132300
    },
    {
      "epoch": 2.0784929356357926,
      "grad_norm": 3.619743585586548,
      "learning_rate": 4.870094191522763e-05,
      "loss": 0.7472,
      "step": 132400
    },
    {
      "epoch": 2.0800627943485086,
      "grad_norm": 4.298393726348877,
      "learning_rate": 4.869996075353219e-05,
      "loss": 0.7245,
      "step": 132500
    },
    {
      "epoch": 2.0816326530612246,
      "grad_norm": 4.581872463226318,
      "learning_rate": 4.869897959183674e-05,
      "loss": 0.7625,
      "step": 132600
    },
    {
      "epoch": 2.0832025117739406,
      "grad_norm": 4.093611240386963,
      "learning_rate": 4.869799843014129e-05,
      "loss": 0.7525,
      "step": 132700
    },
    {
      "epoch": 2.084772370486656,
      "grad_norm": 3.7484169006347656,
      "learning_rate": 4.869701726844585e-05,
      "loss": 0.717,
      "step": 132800
    },
    {
      "epoch": 2.086342229199372,
      "grad_norm": 3.8339712619781494,
      "learning_rate": 4.869603610675039e-05,
      "loss": 0.6967,
      "step": 132900
    },
    {
      "epoch": 2.087912087912088,
      "grad_norm": 4.867641448974609,
      "learning_rate": 4.869505494505495e-05,
      "loss": 0.7177,
      "step": 133000
    },
    {
      "epoch": 2.0894819466248036,
      "grad_norm": 3.61651349067688,
      "learning_rate": 4.86940737833595e-05,
      "loss": 0.7215,
      "step": 133100
    },
    {
      "epoch": 2.0910518053375196,
      "grad_norm": 3.258619546890259,
      "learning_rate": 4.869309262166406e-05,
      "loss": 0.6596,
      "step": 133200
    },
    {
      "epoch": 2.0926216640502355,
      "grad_norm": 3.7995920181274414,
      "learning_rate": 4.86921114599686e-05,
      "loss": 0.7139,
      "step": 133300
    },
    {
      "epoch": 2.0941915227629515,
      "grad_norm": 4.691001892089844,
      "learning_rate": 4.869113029827316e-05,
      "loss": 0.7155,
      "step": 133400
    },
    {
      "epoch": 2.095761381475667,
      "grad_norm": 4.417195796966553,
      "learning_rate": 4.869014913657771e-05,
      "loss": 0.6956,
      "step": 133500
    },
    {
      "epoch": 2.097331240188383,
      "grad_norm": 3.0445103645324707,
      "learning_rate": 4.868916797488226e-05,
      "loss": 0.7236,
      "step": 133600
    },
    {
      "epoch": 2.098901098901099,
      "grad_norm": 4.868675708770752,
      "learning_rate": 4.868818681318681e-05,
      "loss": 0.7133,
      "step": 133700
    },
    {
      "epoch": 2.100470957613815,
      "grad_norm": 4.529696941375732,
      "learning_rate": 4.868720565149137e-05,
      "loss": 0.6738,
      "step": 133800
    },
    {
      "epoch": 2.1020408163265305,
      "grad_norm": 4.813287258148193,
      "learning_rate": 4.868622448979592e-05,
      "loss": 0.7159,
      "step": 133900
    },
    {
      "epoch": 2.1036106750392465,
      "grad_norm": 4.289118766784668,
      "learning_rate": 4.868524332810047e-05,
      "loss": 0.7421,
      "step": 134000
    },
    {
      "epoch": 2.1051805337519625,
      "grad_norm": 4.976364612579346,
      "learning_rate": 4.868426216640502e-05,
      "loss": 0.7391,
      "step": 134100
    },
    {
      "epoch": 2.106750392464678,
      "grad_norm": 4.413183212280273,
      "learning_rate": 4.868328100470958e-05,
      "loss": 0.7157,
      "step": 134200
    },
    {
      "epoch": 2.108320251177394,
      "grad_norm": 4.350271701812744,
      "learning_rate": 4.8682299843014125e-05,
      "loss": 0.6817,
      "step": 134300
    },
    {
      "epoch": 2.10989010989011,
      "grad_norm": 4.4399285316467285,
      "learning_rate": 4.868131868131868e-05,
      "loss": 0.7488,
      "step": 134400
    },
    {
      "epoch": 2.111459968602826,
      "grad_norm": 4.202695369720459,
      "learning_rate": 4.8680337519623234e-05,
      "loss": 0.767,
      "step": 134500
    },
    {
      "epoch": 2.1130298273155415,
      "grad_norm": 3.7920782566070557,
      "learning_rate": 4.867935635792779e-05,
      "loss": 0.7451,
      "step": 134600
    },
    {
      "epoch": 2.1145996860282574,
      "grad_norm": 3.900094747543335,
      "learning_rate": 4.867837519623234e-05,
      "loss": 0.7445,
      "step": 134700
    },
    {
      "epoch": 2.1161695447409734,
      "grad_norm": 4.55816650390625,
      "learning_rate": 4.8677394034536894e-05,
      "loss": 0.7111,
      "step": 134800
    },
    {
      "epoch": 2.1177394034536894,
      "grad_norm": 3.5670900344848633,
      "learning_rate": 4.867641287284145e-05,
      "loss": 0.6938,
      "step": 134900
    },
    {
      "epoch": 2.119309262166405,
      "grad_norm": 4.12021017074585,
      "learning_rate": 4.8675431711145996e-05,
      "loss": 0.6865,
      "step": 135000
    },
    {
      "epoch": 2.120879120879121,
      "grad_norm": 3.521867036819458,
      "learning_rate": 4.8674450549450553e-05,
      "loss": 0.6862,
      "step": 135100
    },
    {
      "epoch": 2.122448979591837,
      "grad_norm": 3.743014335632324,
      "learning_rate": 4.8673469387755104e-05,
      "loss": 0.7349,
      "step": 135200
    },
    {
      "epoch": 2.1240188383045524,
      "grad_norm": 3.9066734313964844,
      "learning_rate": 4.867248822605966e-05,
      "loss": 0.6782,
      "step": 135300
    },
    {
      "epoch": 2.1255886970172684,
      "grad_norm": 3.953221321105957,
      "learning_rate": 4.8671507064364206e-05,
      "loss": 0.7337,
      "step": 135400
    },
    {
      "epoch": 2.1271585557299844,
      "grad_norm": 3.991132974624634,
      "learning_rate": 4.8670525902668764e-05,
      "loss": 0.7278,
      "step": 135500
    },
    {
      "epoch": 2.1287284144427003,
      "grad_norm": 5.267293453216553,
      "learning_rate": 4.8669544740973315e-05,
      "loss": 0.7233,
      "step": 135600
    },
    {
      "epoch": 2.130298273155416,
      "grad_norm": 4.973804950714111,
      "learning_rate": 4.8668563579277866e-05,
      "loss": 0.689,
      "step": 135700
    },
    {
      "epoch": 2.131868131868132,
      "grad_norm": 3.834956645965576,
      "learning_rate": 4.866758241758242e-05,
      "loss": 0.7402,
      "step": 135800
    },
    {
      "epoch": 2.133437990580848,
      "grad_norm": 4.216073989868164,
      "learning_rate": 4.8666601255886975e-05,
      "loss": 0.6589,
      "step": 135900
    },
    {
      "epoch": 2.1350078492935634,
      "grad_norm": 4.578843593597412,
      "learning_rate": 4.8665620094191526e-05,
      "loss": 0.7165,
      "step": 136000
    },
    {
      "epoch": 2.1365777080062793,
      "grad_norm": 4.796851634979248,
      "learning_rate": 4.866463893249608e-05,
      "loss": 0.691,
      "step": 136100
    },
    {
      "epoch": 2.1381475667189953,
      "grad_norm": 4.054928302764893,
      "learning_rate": 4.866365777080063e-05,
      "loss": 0.7498,
      "step": 136200
    },
    {
      "epoch": 2.1397174254317113,
      "grad_norm": 4.022515773773193,
      "learning_rate": 4.8662676609105186e-05,
      "loss": 0.6794,
      "step": 136300
    },
    {
      "epoch": 2.141287284144427,
      "grad_norm": 2.1515111923217773,
      "learning_rate": 4.866169544740973e-05,
      "loss": 0.7106,
      "step": 136400
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 3.9572412967681885,
      "learning_rate": 4.866071428571429e-05,
      "loss": 0.7031,
      "step": 136500
    },
    {
      "epoch": 2.1444270015698588,
      "grad_norm": 4.1131911277771,
      "learning_rate": 4.865973312401884e-05,
      "loss": 0.7296,
      "step": 136600
    },
    {
      "epoch": 2.1459968602825747,
      "grad_norm": 4.970435619354248,
      "learning_rate": 4.8658751962323396e-05,
      "loss": 0.7542,
      "step": 136700
    },
    {
      "epoch": 2.1475667189952903,
      "grad_norm": 4.441712379455566,
      "learning_rate": 4.865777080062795e-05,
      "loss": 0.751,
      "step": 136800
    },
    {
      "epoch": 2.1491365777080063,
      "grad_norm": 4.4130778312683105,
      "learning_rate": 4.86567896389325e-05,
      "loss": 0.7003,
      "step": 136900
    },
    {
      "epoch": 2.1507064364207222,
      "grad_norm": 4.211536407470703,
      "learning_rate": 4.8655808477237056e-05,
      "loss": 0.7125,
      "step": 137000
    },
    {
      "epoch": 2.152276295133438,
      "grad_norm": 3.5026729106903076,
      "learning_rate": 4.86548273155416e-05,
      "loss": 0.703,
      "step": 137100
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 3.4827120304107666,
      "learning_rate": 4.865384615384616e-05,
      "loss": 0.705,
      "step": 137200
    },
    {
      "epoch": 2.1554160125588697,
      "grad_norm": 3.9123401641845703,
      "learning_rate": 4.865286499215071e-05,
      "loss": 0.6906,
      "step": 137300
    },
    {
      "epoch": 2.1569858712715857,
      "grad_norm": 4.196873664855957,
      "learning_rate": 4.865188383045527e-05,
      "loss": 0.7203,
      "step": 137400
    },
    {
      "epoch": 2.1585557299843012,
      "grad_norm": 4.450971603393555,
      "learning_rate": 4.865090266875981e-05,
      "loss": 0.7001,
      "step": 137500
    },
    {
      "epoch": 2.160125588697017,
      "grad_norm": 4.190698623657227,
      "learning_rate": 4.864992150706437e-05,
      "loss": 0.6794,
      "step": 137600
    },
    {
      "epoch": 2.161695447409733,
      "grad_norm": 3.9671969413757324,
      "learning_rate": 4.864894034536892e-05,
      "loss": 0.7567,
      "step": 137700
    },
    {
      "epoch": 2.163265306122449,
      "grad_norm": 3.486301898956299,
      "learning_rate": 4.864795918367347e-05,
      "loss": 0.7246,
      "step": 137800
    },
    {
      "epoch": 2.1648351648351647,
      "grad_norm": 3.8813652992248535,
      "learning_rate": 4.864697802197802e-05,
      "loss": 0.7,
      "step": 137900
    },
    {
      "epoch": 2.1664050235478807,
      "grad_norm": 4.131479740142822,
      "learning_rate": 4.864599686028258e-05,
      "loss": 0.714,
      "step": 138000
    },
    {
      "epoch": 2.1679748822605966,
      "grad_norm": 3.500854253768921,
      "learning_rate": 4.864501569858713e-05,
      "loss": 0.7161,
      "step": 138100
    },
    {
      "epoch": 2.169544740973312,
      "grad_norm": 4.557347774505615,
      "learning_rate": 4.864403453689168e-05,
      "loss": 0.7524,
      "step": 138200
    },
    {
      "epoch": 2.171114599686028,
      "grad_norm": 4.051050662994385,
      "learning_rate": 4.864305337519623e-05,
      "loss": 0.7006,
      "step": 138300
    },
    {
      "epoch": 2.172684458398744,
      "grad_norm": 3.7614903450012207,
      "learning_rate": 4.864207221350079e-05,
      "loss": 0.7549,
      "step": 138400
    },
    {
      "epoch": 2.17425431711146,
      "grad_norm": 3.2153871059417725,
      "learning_rate": 4.8641091051805334e-05,
      "loss": 0.7026,
      "step": 138500
    },
    {
      "epoch": 2.1758241758241756,
      "grad_norm": 4.322448253631592,
      "learning_rate": 4.864010989010989e-05,
      "loss": 0.7253,
      "step": 138600
    },
    {
      "epoch": 2.1773940345368916,
      "grad_norm": 4.199275970458984,
      "learning_rate": 4.863912872841444e-05,
      "loss": 0.6803,
      "step": 138700
    },
    {
      "epoch": 2.1789638932496076,
      "grad_norm": 4.589212417602539,
      "learning_rate": 4.8638147566719e-05,
      "loss": 0.7236,
      "step": 138800
    },
    {
      "epoch": 2.1805337519623236,
      "grad_norm": 2.9052975177764893,
      "learning_rate": 4.863716640502355e-05,
      "loss": 0.7151,
      "step": 138900
    },
    {
      "epoch": 2.182103610675039,
      "grad_norm": 4.20217752456665,
      "learning_rate": 4.86361852433281e-05,
      "loss": 0.7141,
      "step": 139000
    },
    {
      "epoch": 2.183673469387755,
      "grad_norm": 4.490422248840332,
      "learning_rate": 4.863520408163266e-05,
      "loss": 0.6742,
      "step": 139100
    },
    {
      "epoch": 2.185243328100471,
      "grad_norm": 5.152436256408691,
      "learning_rate": 4.8634222919937205e-05,
      "loss": 0.7318,
      "step": 139200
    },
    {
      "epoch": 2.186813186813187,
      "grad_norm": 4.0577192306518555,
      "learning_rate": 4.863324175824176e-05,
      "loss": 0.6929,
      "step": 139300
    },
    {
      "epoch": 2.1883830455259026,
      "grad_norm": 3.9972217082977295,
      "learning_rate": 4.8632260596546313e-05,
      "loss": 0.7222,
      "step": 139400
    },
    {
      "epoch": 2.1899529042386185,
      "grad_norm": 3.2949092388153076,
      "learning_rate": 4.863127943485087e-05,
      "loss": 0.7084,
      "step": 139500
    },
    {
      "epoch": 2.1915227629513345,
      "grad_norm": 3.1939547061920166,
      "learning_rate": 4.8630298273155415e-05,
      "loss": 0.7112,
      "step": 139600
    },
    {
      "epoch": 2.19309262166405,
      "grad_norm": 3.9206461906433105,
      "learning_rate": 4.862931711145997e-05,
      "loss": 0.6961,
      "step": 139700
    },
    {
      "epoch": 2.194662480376766,
      "grad_norm": 4.684448719024658,
      "learning_rate": 4.8628335949764524e-05,
      "loss": 0.742,
      "step": 139800
    },
    {
      "epoch": 2.196232339089482,
      "grad_norm": 4.375234603881836,
      "learning_rate": 4.8627354788069075e-05,
      "loss": 0.7181,
      "step": 139900
    },
    {
      "epoch": 2.197802197802198,
      "grad_norm": 4.018752098083496,
      "learning_rate": 4.8626373626373626e-05,
      "loss": 0.7021,
      "step": 140000
    },
    {
      "epoch": 2.1993720565149135,
      "grad_norm": 4.525646686553955,
      "learning_rate": 4.8625392464678184e-05,
      "loss": 0.7348,
      "step": 140100
    },
    {
      "epoch": 2.2009419152276295,
      "grad_norm": 4.631534576416016,
      "learning_rate": 4.8624411302982735e-05,
      "loss": 0.688,
      "step": 140200
    },
    {
      "epoch": 2.2025117739403455,
      "grad_norm": 3.6773593425750732,
      "learning_rate": 4.8623430141287286e-05,
      "loss": 0.6831,
      "step": 140300
    },
    {
      "epoch": 2.204081632653061,
      "grad_norm": 4.248289585113525,
      "learning_rate": 4.862244897959184e-05,
      "loss": 0.7278,
      "step": 140400
    },
    {
      "epoch": 2.205651491365777,
      "grad_norm": 4.628339767456055,
      "learning_rate": 4.8621467817896395e-05,
      "loss": 0.676,
      "step": 140500
    },
    {
      "epoch": 2.207221350078493,
      "grad_norm": 4.454334259033203,
      "learning_rate": 4.862048665620094e-05,
      "loss": 0.6884,
      "step": 140600
    },
    {
      "epoch": 2.208791208791209,
      "grad_norm": 5.1812896728515625,
      "learning_rate": 4.8619505494505496e-05,
      "loss": 0.7361,
      "step": 140700
    },
    {
      "epoch": 2.2103610675039245,
      "grad_norm": 3.6113550662994385,
      "learning_rate": 4.861852433281005e-05,
      "loss": 0.6934,
      "step": 140800
    },
    {
      "epoch": 2.2119309262166404,
      "grad_norm": 3.884805202484131,
      "learning_rate": 4.8617543171114605e-05,
      "loss": 0.7219,
      "step": 140900
    },
    {
      "epoch": 2.2135007849293564,
      "grad_norm": 4.212229251861572,
      "learning_rate": 4.8616562009419156e-05,
      "loss": 0.7352,
      "step": 141000
    },
    {
      "epoch": 2.2150706436420724,
      "grad_norm": 4.497398376464844,
      "learning_rate": 4.861558084772371e-05,
      "loss": 0.703,
      "step": 141100
    },
    {
      "epoch": 2.216640502354788,
      "grad_norm": 3.583372116088867,
      "learning_rate": 4.861459968602826e-05,
      "loss": 0.7097,
      "step": 141200
    },
    {
      "epoch": 2.218210361067504,
      "grad_norm": 4.46348237991333,
      "learning_rate": 4.861361852433281e-05,
      "loss": 0.7085,
      "step": 141300
    },
    {
      "epoch": 2.21978021978022,
      "grad_norm": 4.823422431945801,
      "learning_rate": 4.861263736263737e-05,
      "loss": 0.7323,
      "step": 141400
    },
    {
      "epoch": 2.221350078492936,
      "grad_norm": 4.671446323394775,
      "learning_rate": 4.861165620094192e-05,
      "loss": 0.719,
      "step": 141500
    },
    {
      "epoch": 2.2229199372056514,
      "grad_norm": 4.277563571929932,
      "learning_rate": 4.8610675039246476e-05,
      "loss": 0.6891,
      "step": 141600
    },
    {
      "epoch": 2.2244897959183674,
      "grad_norm": 4.8380656242370605,
      "learning_rate": 4.860969387755102e-05,
      "loss": 0.7231,
      "step": 141700
    },
    {
      "epoch": 2.2260596546310834,
      "grad_norm": 4.855588912963867,
      "learning_rate": 4.860871271585558e-05,
      "loss": 0.7656,
      "step": 141800
    },
    {
      "epoch": 2.227629513343799,
      "grad_norm": 4.369980812072754,
      "learning_rate": 4.860773155416013e-05,
      "loss": 0.7374,
      "step": 141900
    },
    {
      "epoch": 2.229199372056515,
      "grad_norm": 4.353010654449463,
      "learning_rate": 4.860675039246468e-05,
      "loss": 0.6886,
      "step": 142000
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 3.65934157371521,
      "learning_rate": 4.860576923076923e-05,
      "loss": 0.6903,
      "step": 142100
    },
    {
      "epoch": 2.232339089481947,
      "grad_norm": 4.1961541175842285,
      "learning_rate": 4.860478806907379e-05,
      "loss": 0.6928,
      "step": 142200
    },
    {
      "epoch": 2.2339089481946623,
      "grad_norm": 4.0096564292907715,
      "learning_rate": 4.860380690737834e-05,
      "loss": 0.7667,
      "step": 142300
    },
    {
      "epoch": 2.2354788069073783,
      "grad_norm": 3.0172572135925293,
      "learning_rate": 4.860282574568289e-05,
      "loss": 0.7317,
      "step": 142400
    },
    {
      "epoch": 2.2370486656200943,
      "grad_norm": 5.16865873336792,
      "learning_rate": 4.860184458398744e-05,
      "loss": 0.6735,
      "step": 142500
    },
    {
      "epoch": 2.23861852433281,
      "grad_norm": 4.08328914642334,
      "learning_rate": 4.8600863422292e-05,
      "loss": 0.752,
      "step": 142600
    },
    {
      "epoch": 2.240188383045526,
      "grad_norm": 4.977783203125,
      "learning_rate": 4.859988226059654e-05,
      "loss": 0.7652,
      "step": 142700
    },
    {
      "epoch": 2.241758241758242,
      "grad_norm": 3.321547031402588,
      "learning_rate": 4.85989010989011e-05,
      "loss": 0.7223,
      "step": 142800
    },
    {
      "epoch": 2.2433281004709578,
      "grad_norm": 3.8749144077301025,
      "learning_rate": 4.859791993720565e-05,
      "loss": 0.7167,
      "step": 142900
    },
    {
      "epoch": 2.2448979591836733,
      "grad_norm": 4.446248531341553,
      "learning_rate": 4.859693877551021e-05,
      "loss": 0.7249,
      "step": 143000
    },
    {
      "epoch": 2.2464678178963893,
      "grad_norm": 4.260748863220215,
      "learning_rate": 4.859595761381476e-05,
      "loss": 0.7008,
      "step": 143100
    },
    {
      "epoch": 2.2480376766091053,
      "grad_norm": 3.477736473083496,
      "learning_rate": 4.859497645211931e-05,
      "loss": 0.7278,
      "step": 143200
    },
    {
      "epoch": 2.2496075353218212,
      "grad_norm": 4.533027172088623,
      "learning_rate": 4.859399529042386e-05,
      "loss": 0.6718,
      "step": 143300
    },
    {
      "epoch": 2.2511773940345368,
      "grad_norm": 4.468142032623291,
      "learning_rate": 4.8593014128728414e-05,
      "loss": 0.7057,
      "step": 143400
    },
    {
      "epoch": 2.2527472527472527,
      "grad_norm": 3.782181978225708,
      "learning_rate": 4.859203296703297e-05,
      "loss": 0.7051,
      "step": 143500
    },
    {
      "epoch": 2.2543171114599687,
      "grad_norm": 4.31187629699707,
      "learning_rate": 4.859105180533752e-05,
      "loss": 0.7624,
      "step": 143600
    },
    {
      "epoch": 2.2558869701726847,
      "grad_norm": 4.120409965515137,
      "learning_rate": 4.859007064364208e-05,
      "loss": 0.7282,
      "step": 143700
    },
    {
      "epoch": 2.2574568288854002,
      "grad_norm": 3.1537411212921143,
      "learning_rate": 4.8589089481946624e-05,
      "loss": 0.6999,
      "step": 143800
    },
    {
      "epoch": 2.259026687598116,
      "grad_norm": 3.4244515895843506,
      "learning_rate": 4.858810832025118e-05,
      "loss": 0.7371,
      "step": 143900
    },
    {
      "epoch": 2.260596546310832,
      "grad_norm": 4.223016262054443,
      "learning_rate": 4.858712715855573e-05,
      "loss": 0.7168,
      "step": 144000
    },
    {
      "epoch": 2.2621664050235477,
      "grad_norm": 4.984370231628418,
      "learning_rate": 4.8586145996860284e-05,
      "loss": 0.7469,
      "step": 144100
    },
    {
      "epoch": 2.2637362637362637,
      "grad_norm": 2.1397576332092285,
      "learning_rate": 4.8585164835164835e-05,
      "loss": 0.7176,
      "step": 144200
    },
    {
      "epoch": 2.2653061224489797,
      "grad_norm": 5.04909086227417,
      "learning_rate": 4.858418367346939e-05,
      "loss": 0.7391,
      "step": 144300
    },
    {
      "epoch": 2.2668759811616956,
      "grad_norm": 4.219911575317383,
      "learning_rate": 4.8583202511773944e-05,
      "loss": 0.7257,
      "step": 144400
    },
    {
      "epoch": 2.268445839874411,
      "grad_norm": 4.288543224334717,
      "learning_rate": 4.8582221350078495e-05,
      "loss": 0.7315,
      "step": 144500
    },
    {
      "epoch": 2.270015698587127,
      "grad_norm": 4.086916446685791,
      "learning_rate": 4.8581240188383046e-05,
      "loss": 0.6997,
      "step": 144600
    },
    {
      "epoch": 2.271585557299843,
      "grad_norm": 3.7630300521850586,
      "learning_rate": 4.8580259026687604e-05,
      "loss": 0.7121,
      "step": 144700
    },
    {
      "epoch": 2.2731554160125587,
      "grad_norm": 3.92858624458313,
      "learning_rate": 4.857927786499215e-05,
      "loss": 0.7316,
      "step": 144800
    },
    {
      "epoch": 2.2747252747252746,
      "grad_norm": 3.551123857498169,
      "learning_rate": 4.8578296703296705e-05,
      "loss": 0.7623,
      "step": 144900
    },
    {
      "epoch": 2.2762951334379906,
      "grad_norm": 4.295485019683838,
      "learning_rate": 4.8577315541601256e-05,
      "loss": 0.6674,
      "step": 145000
    },
    {
      "epoch": 2.2778649921507066,
      "grad_norm": 4.694091796875,
      "learning_rate": 4.8576334379905814e-05,
      "loss": 0.708,
      "step": 145100
    },
    {
      "epoch": 2.279434850863422,
      "grad_norm": 3.442652940750122,
      "learning_rate": 4.8575353218210365e-05,
      "loss": 0.6988,
      "step": 145200
    },
    {
      "epoch": 2.281004709576138,
      "grad_norm": 4.192695140838623,
      "learning_rate": 4.8574372056514916e-05,
      "loss": 0.7428,
      "step": 145300
    },
    {
      "epoch": 2.282574568288854,
      "grad_norm": 5.879350662231445,
      "learning_rate": 4.857339089481947e-05,
      "loss": 0.721,
      "step": 145400
    },
    {
      "epoch": 2.2841444270015696,
      "grad_norm": 4.753203868865967,
      "learning_rate": 4.857240973312402e-05,
      "loss": 0.6841,
      "step": 145500
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 4.186070442199707,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 0.7242,
      "step": 145600
    },
    {
      "epoch": 2.2872841444270016,
      "grad_norm": 4.367593288421631,
      "learning_rate": 4.857044740973313e-05,
      "loss": 0.7541,
      "step": 145700
    },
    {
      "epoch": 2.2888540031397175,
      "grad_norm": 4.300141334533691,
      "learning_rate": 4.8569466248037685e-05,
      "loss": 0.6895,
      "step": 145800
    },
    {
      "epoch": 2.2904238618524335,
      "grad_norm": 4.363309860229492,
      "learning_rate": 4.856848508634223e-05,
      "loss": 0.6906,
      "step": 145900
    },
    {
      "epoch": 2.291993720565149,
      "grad_norm": 2.9977991580963135,
      "learning_rate": 4.8567503924646787e-05,
      "loss": 0.6908,
      "step": 146000
    },
    {
      "epoch": 2.293563579277865,
      "grad_norm": 3.9850752353668213,
      "learning_rate": 4.856652276295134e-05,
      "loss": 0.7401,
      "step": 146100
    },
    {
      "epoch": 2.295133437990581,
      "grad_norm": 4.084120273590088,
      "learning_rate": 4.856554160125589e-05,
      "loss": 0.7026,
      "step": 146200
    },
    {
      "epoch": 2.2967032967032965,
      "grad_norm": 2.1083006858825684,
      "learning_rate": 4.856456043956044e-05,
      "loss": 0.7054,
      "step": 146300
    },
    {
      "epoch": 2.2982731554160125,
      "grad_norm": 4.956758499145508,
      "learning_rate": 4.8563579277865e-05,
      "loss": 0.7011,
      "step": 146400
    },
    {
      "epoch": 2.2998430141287285,
      "grad_norm": 5.347259044647217,
      "learning_rate": 4.856259811616955e-05,
      "loss": 0.6993,
      "step": 146500
    },
    {
      "epoch": 2.3014128728414445,
      "grad_norm": 5.020639896392822,
      "learning_rate": 4.85616169544741e-05,
      "loss": 0.7301,
      "step": 146600
    },
    {
      "epoch": 2.30298273155416,
      "grad_norm": 3.799797773361206,
      "learning_rate": 4.856063579277865e-05,
      "loss": 0.7203,
      "step": 146700
    },
    {
      "epoch": 2.304552590266876,
      "grad_norm": 4.236428737640381,
      "learning_rate": 4.855965463108321e-05,
      "loss": 0.7425,
      "step": 146800
    },
    {
      "epoch": 2.306122448979592,
      "grad_norm": 3.4473519325256348,
      "learning_rate": 4.855867346938775e-05,
      "loss": 0.7418,
      "step": 146900
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 4.785095691680908,
      "learning_rate": 4.855769230769231e-05,
      "loss": 0.7273,
      "step": 147000
    },
    {
      "epoch": 2.3092621664050235,
      "grad_norm": 3.0891659259796143,
      "learning_rate": 4.855671114599686e-05,
      "loss": 0.7174,
      "step": 147100
    },
    {
      "epoch": 2.3108320251177394,
      "grad_norm": 5.065426349639893,
      "learning_rate": 4.855572998430142e-05,
      "loss": 0.743,
      "step": 147200
    },
    {
      "epoch": 2.3124018838304554,
      "grad_norm": 4.0544962882995605,
      "learning_rate": 4.855474882260597e-05,
      "loss": 0.7461,
      "step": 147300
    },
    {
      "epoch": 2.313971742543171,
      "grad_norm": 2.83767032623291,
      "learning_rate": 4.855376766091052e-05,
      "loss": 0.6878,
      "step": 147400
    },
    {
      "epoch": 2.315541601255887,
      "grad_norm": 4.820251941680908,
      "learning_rate": 4.855278649921507e-05,
      "loss": 0.7042,
      "step": 147500
    },
    {
      "epoch": 2.317111459968603,
      "grad_norm": 4.12872838973999,
      "learning_rate": 4.855180533751962e-05,
      "loss": 0.749,
      "step": 147600
    },
    {
      "epoch": 2.3186813186813184,
      "grad_norm": 3.0605194568634033,
      "learning_rate": 4.855082417582418e-05,
      "loss": 0.7022,
      "step": 147700
    },
    {
      "epoch": 2.3202511773940344,
      "grad_norm": 5.260761737823486,
      "learning_rate": 4.854984301412873e-05,
      "loss": 0.7391,
      "step": 147800
    },
    {
      "epoch": 2.3218210361067504,
      "grad_norm": 4.744481563568115,
      "learning_rate": 4.854886185243328e-05,
      "loss": 0.7002,
      "step": 147900
    },
    {
      "epoch": 2.3233908948194664,
      "grad_norm": 4.544833660125732,
      "learning_rate": 4.854788069073783e-05,
      "loss": 0.7139,
      "step": 148000
    },
    {
      "epoch": 2.3249607535321823,
      "grad_norm": 3.9290878772735596,
      "learning_rate": 4.854689952904239e-05,
      "loss": 0.7473,
      "step": 148100
    },
    {
      "epoch": 2.326530612244898,
      "grad_norm": 4.107222557067871,
      "learning_rate": 4.854591836734694e-05,
      "loss": 0.7205,
      "step": 148200
    },
    {
      "epoch": 2.328100470957614,
      "grad_norm": 4.13818883895874,
      "learning_rate": 4.854493720565149e-05,
      "loss": 0.7025,
      "step": 148300
    },
    {
      "epoch": 2.32967032967033,
      "grad_norm": 4.008937835693359,
      "learning_rate": 4.8543956043956044e-05,
      "loss": 0.7537,
      "step": 148400
    },
    {
      "epoch": 2.3312401883830454,
      "grad_norm": 4.244183540344238,
      "learning_rate": 4.85429748822606e-05,
      "loss": 0.7057,
      "step": 148500
    },
    {
      "epoch": 2.3328100470957613,
      "grad_norm": 3.3951401710510254,
      "learning_rate": 4.8541993720565146e-05,
      "loss": 0.7162,
      "step": 148600
    },
    {
      "epoch": 2.3343799058084773,
      "grad_norm": 3.872753143310547,
      "learning_rate": 4.8541012558869704e-05,
      "loss": 0.7102,
      "step": 148700
    },
    {
      "epoch": 2.3359497645211933,
      "grad_norm": 4.765328407287598,
      "learning_rate": 4.8540031397174255e-05,
      "loss": 0.6898,
      "step": 148800
    },
    {
      "epoch": 2.337519623233909,
      "grad_norm": 3.1752419471740723,
      "learning_rate": 4.853905023547881e-05,
      "loss": 0.6834,
      "step": 148900
    },
    {
      "epoch": 2.339089481946625,
      "grad_norm": 4.904603004455566,
      "learning_rate": 4.853806907378336e-05,
      "loss": 0.7221,
      "step": 149000
    },
    {
      "epoch": 2.340659340659341,
      "grad_norm": 3.539738178253174,
      "learning_rate": 4.8537087912087914e-05,
      "loss": 0.6777,
      "step": 149100
    },
    {
      "epoch": 2.3422291993720563,
      "grad_norm": 4.1053972244262695,
      "learning_rate": 4.8536106750392465e-05,
      "loss": 0.7344,
      "step": 149200
    },
    {
      "epoch": 2.3437990580847723,
      "grad_norm": 4.157251358032227,
      "learning_rate": 4.8535125588697016e-05,
      "loss": 0.7316,
      "step": 149300
    },
    {
      "epoch": 2.3453689167974883,
      "grad_norm": 4.561380863189697,
      "learning_rate": 4.8534144427001574e-05,
      "loss": 0.7235,
      "step": 149400
    },
    {
      "epoch": 2.3469387755102042,
      "grad_norm": 4.2983245849609375,
      "learning_rate": 4.8533163265306125e-05,
      "loss": 0.7191,
      "step": 149500
    },
    {
      "epoch": 2.3485086342229198,
      "grad_norm": 4.28715705871582,
      "learning_rate": 4.8532182103610676e-05,
      "loss": 0.7047,
      "step": 149600
    },
    {
      "epoch": 2.3500784929356358,
      "grad_norm": 5.0124711990356445,
      "learning_rate": 4.853120094191523e-05,
      "loss": 0.7202,
      "step": 149700
    },
    {
      "epoch": 2.3516483516483517,
      "grad_norm": 4.7173051834106445,
      "learning_rate": 4.8530219780219785e-05,
      "loss": 0.7085,
      "step": 149800
    },
    {
      "epoch": 2.3532182103610673,
      "grad_norm": 4.078597545623779,
      "learning_rate": 4.8529238618524336e-05,
      "loss": 0.703,
      "step": 149900
    },
    {
      "epoch": 2.3547880690737832,
      "grad_norm": 4.026644706726074,
      "learning_rate": 4.852825745682889e-05,
      "loss": 0.7227,
      "step": 150000
    },
    {
      "epoch": 2.356357927786499,
      "grad_norm": 3.6866352558135986,
      "learning_rate": 4.852727629513344e-05,
      "loss": 0.7041,
      "step": 150100
    },
    {
      "epoch": 2.357927786499215,
      "grad_norm": 3.733198881149292,
      "learning_rate": 4.8526295133437996e-05,
      "loss": 0.7711,
      "step": 150200
    },
    {
      "epoch": 2.359497645211931,
      "grad_norm": 4.4701056480407715,
      "learning_rate": 4.8525313971742547e-05,
      "loss": 0.6982,
      "step": 150300
    },
    {
      "epoch": 2.3610675039246467,
      "grad_norm": 4.748164176940918,
      "learning_rate": 4.85243328100471e-05,
      "loss": 0.7182,
      "step": 150400
    },
    {
      "epoch": 2.3626373626373627,
      "grad_norm": 4.447776794433594,
      "learning_rate": 4.852335164835165e-05,
      "loss": 0.7111,
      "step": 150500
    },
    {
      "epoch": 2.3642072213500787,
      "grad_norm": 4.045238494873047,
      "learning_rate": 4.8522370486656206e-05,
      "loss": 0.6771,
      "step": 150600
    },
    {
      "epoch": 2.365777080062794,
      "grad_norm": 3.6362829208374023,
      "learning_rate": 4.852138932496075e-05,
      "loss": 0.7029,
      "step": 150700
    },
    {
      "epoch": 2.36734693877551,
      "grad_norm": 4.844270706176758,
      "learning_rate": 4.852040816326531e-05,
      "loss": 0.7297,
      "step": 150800
    },
    {
      "epoch": 2.368916797488226,
      "grad_norm": 4.1913323402404785,
      "learning_rate": 4.851942700156986e-05,
      "loss": 0.7123,
      "step": 150900
    },
    {
      "epoch": 2.370486656200942,
      "grad_norm": 4.91420316696167,
      "learning_rate": 4.851844583987442e-05,
      "loss": 0.7045,
      "step": 151000
    },
    {
      "epoch": 2.3720565149136577,
      "grad_norm": 4.311274528503418,
      "learning_rate": 4.851746467817896e-05,
      "loss": 0.7185,
      "step": 151100
    },
    {
      "epoch": 2.3736263736263736,
      "grad_norm": 4.243536949157715,
      "learning_rate": 4.851648351648352e-05,
      "loss": 0.7076,
      "step": 151200
    },
    {
      "epoch": 2.3751962323390896,
      "grad_norm": 4.31722354888916,
      "learning_rate": 4.851550235478807e-05,
      "loss": 0.693,
      "step": 151300
    },
    {
      "epoch": 2.376766091051805,
      "grad_norm": 4.242668151855469,
      "learning_rate": 4.851452119309262e-05,
      "loss": 0.7535,
      "step": 151400
    },
    {
      "epoch": 2.378335949764521,
      "grad_norm": 3.691941738128662,
      "learning_rate": 4.851354003139718e-05,
      "loss": 0.739,
      "step": 151500
    },
    {
      "epoch": 2.379905808477237,
      "grad_norm": 3.816209316253662,
      "learning_rate": 4.851255886970173e-05,
      "loss": 0.6972,
      "step": 151600
    },
    {
      "epoch": 2.381475667189953,
      "grad_norm": 4.20884370803833,
      "learning_rate": 4.851157770800628e-05,
      "loss": 0.6967,
      "step": 151700
    },
    {
      "epoch": 2.3830455259026686,
      "grad_norm": 3.2652359008789062,
      "learning_rate": 4.851059654631083e-05,
      "loss": 0.7008,
      "step": 151800
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 4.257595539093018,
      "learning_rate": 4.850961538461539e-05,
      "loss": 0.7521,
      "step": 151900
    },
    {
      "epoch": 2.3861852433281006,
      "grad_norm": 3.098308801651001,
      "learning_rate": 4.850863422291994e-05,
      "loss": 0.686,
      "step": 152000
    },
    {
      "epoch": 2.387755102040816,
      "grad_norm": 4.930968284606934,
      "learning_rate": 4.850765306122449e-05,
      "loss": 0.7462,
      "step": 152100
    },
    {
      "epoch": 2.389324960753532,
      "grad_norm": 4.990589618682861,
      "learning_rate": 4.850667189952904e-05,
      "loss": 0.7705,
      "step": 152200
    },
    {
      "epoch": 2.390894819466248,
      "grad_norm": 5.802712917327881,
      "learning_rate": 4.85056907378336e-05,
      "loss": 0.712,
      "step": 152300
    },
    {
      "epoch": 2.392464678178964,
      "grad_norm": 4.657672882080078,
      "learning_rate": 4.850470957613815e-05,
      "loss": 0.715,
      "step": 152400
    },
    {
      "epoch": 2.39403453689168,
      "grad_norm": 4.6144890785217285,
      "learning_rate": 4.85037284144427e-05,
      "loss": 0.6871,
      "step": 152500
    },
    {
      "epoch": 2.3956043956043955,
      "grad_norm": 4.6686601638793945,
      "learning_rate": 4.850274725274725e-05,
      "loss": 0.713,
      "step": 152600
    },
    {
      "epoch": 2.3971742543171115,
      "grad_norm": 4.955989837646484,
      "learning_rate": 4.850176609105181e-05,
      "loss": 0.7639,
      "step": 152700
    },
    {
      "epoch": 2.3987441130298275,
      "grad_norm": 4.166932582855225,
      "learning_rate": 4.8500784929356355e-05,
      "loss": 0.7222,
      "step": 152800
    },
    {
      "epoch": 2.400313971742543,
      "grad_norm": 4.875722408294678,
      "learning_rate": 4.849980376766091e-05,
      "loss": 0.7075,
      "step": 152900
    },
    {
      "epoch": 2.401883830455259,
      "grad_norm": 4.13711404800415,
      "learning_rate": 4.8498822605965464e-05,
      "loss": 0.7388,
      "step": 153000
    },
    {
      "epoch": 2.403453689167975,
      "grad_norm": 4.340244770050049,
      "learning_rate": 4.849784144427002e-05,
      "loss": 0.6844,
      "step": 153100
    },
    {
      "epoch": 2.405023547880691,
      "grad_norm": 3.7313098907470703,
      "learning_rate": 4.8496860282574566e-05,
      "loss": 0.7288,
      "step": 153200
    },
    {
      "epoch": 2.4065934065934065,
      "grad_norm": 5.410903453826904,
      "learning_rate": 4.8495879120879123e-05,
      "loss": 0.7107,
      "step": 153300
    },
    {
      "epoch": 2.4081632653061225,
      "grad_norm": 4.403022766113281,
      "learning_rate": 4.8494897959183674e-05,
      "loss": 0.7307,
      "step": 153400
    },
    {
      "epoch": 2.4097331240188384,
      "grad_norm": 3.576876163482666,
      "learning_rate": 4.8493916797488225e-05,
      "loss": 0.7118,
      "step": 153500
    },
    {
      "epoch": 2.411302982731554,
      "grad_norm": 3.5816197395324707,
      "learning_rate": 4.849293563579278e-05,
      "loss": 0.7233,
      "step": 153600
    },
    {
      "epoch": 2.41287284144427,
      "grad_norm": 4.410873889923096,
      "learning_rate": 4.8491954474097334e-05,
      "loss": 0.6729,
      "step": 153700
    },
    {
      "epoch": 2.414442700156986,
      "grad_norm": 5.064492225646973,
      "learning_rate": 4.8490973312401885e-05,
      "loss": 0.6729,
      "step": 153800
    },
    {
      "epoch": 2.416012558869702,
      "grad_norm": 3.9214999675750732,
      "learning_rate": 4.8489992150706436e-05,
      "loss": 0.7128,
      "step": 153900
    },
    {
      "epoch": 2.4175824175824174,
      "grad_norm": 3.8891210556030273,
      "learning_rate": 4.8489010989010994e-05,
      "loss": 0.7327,
      "step": 154000
    },
    {
      "epoch": 2.4191522762951334,
      "grad_norm": 3.5188143253326416,
      "learning_rate": 4.8488029827315545e-05,
      "loss": 0.7147,
      "step": 154100
    },
    {
      "epoch": 2.4207221350078494,
      "grad_norm": 4.532094478607178,
      "learning_rate": 4.8487048665620096e-05,
      "loss": 0.6821,
      "step": 154200
    },
    {
      "epoch": 2.422291993720565,
      "grad_norm": 4.720531463623047,
      "learning_rate": 4.848606750392465e-05,
      "loss": 0.7119,
      "step": 154300
    },
    {
      "epoch": 2.423861852433281,
      "grad_norm": 4.066173076629639,
      "learning_rate": 4.8485086342229205e-05,
      "loss": 0.6628,
      "step": 154400
    },
    {
      "epoch": 2.425431711145997,
      "grad_norm": 4.759188652038574,
      "learning_rate": 4.8484105180533756e-05,
      "loss": 0.7259,
      "step": 154500
    },
    {
      "epoch": 2.427001569858713,
      "grad_norm": 5.303762435913086,
      "learning_rate": 4.8483124018838307e-05,
      "loss": 0.6985,
      "step": 154600
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 4.756664752960205,
      "learning_rate": 4.848214285714286e-05,
      "loss": 0.6965,
      "step": 154700
    },
    {
      "epoch": 2.4301412872841444,
      "grad_norm": 4.792757034301758,
      "learning_rate": 4.8481161695447415e-05,
      "loss": 0.7144,
      "step": 154800
    },
    {
      "epoch": 2.4317111459968603,
      "grad_norm": 3.102870225906372,
      "learning_rate": 4.848018053375196e-05,
      "loss": 0.6936,
      "step": 154900
    },
    {
      "epoch": 2.4332810047095763,
      "grad_norm": 3.7866811752319336,
      "learning_rate": 4.847919937205652e-05,
      "loss": 0.7113,
      "step": 155000
    },
    {
      "epoch": 2.434850863422292,
      "grad_norm": 3.765874147415161,
      "learning_rate": 4.847821821036107e-05,
      "loss": 0.721,
      "step": 155100
    },
    {
      "epoch": 2.436420722135008,
      "grad_norm": 4.2761406898498535,
      "learning_rate": 4.8477237048665626e-05,
      "loss": 0.7146,
      "step": 155200
    },
    {
      "epoch": 2.437990580847724,
      "grad_norm": 3.3485159873962402,
      "learning_rate": 4.847625588697017e-05,
      "loss": 0.7239,
      "step": 155300
    },
    {
      "epoch": 2.4395604395604398,
      "grad_norm": 4.463683128356934,
      "learning_rate": 4.847527472527473e-05,
      "loss": 0.7663,
      "step": 155400
    },
    {
      "epoch": 2.4411302982731553,
      "grad_norm": 5.384696960449219,
      "learning_rate": 4.847429356357928e-05,
      "loss": 0.7429,
      "step": 155500
    },
    {
      "epoch": 2.4427001569858713,
      "grad_norm": 3.9568960666656494,
      "learning_rate": 4.847331240188383e-05,
      "loss": 0.7588,
      "step": 155600
    },
    {
      "epoch": 2.4442700156985873,
      "grad_norm": 4.247960090637207,
      "learning_rate": 4.847233124018839e-05,
      "loss": 0.7307,
      "step": 155700
    },
    {
      "epoch": 2.445839874411303,
      "grad_norm": 4.341468811035156,
      "learning_rate": 4.847135007849294e-05,
      "loss": 0.7203,
      "step": 155800
    },
    {
      "epoch": 2.4474097331240188,
      "grad_norm": 3.852517604827881,
      "learning_rate": 4.847036891679749e-05,
      "loss": 0.6918,
      "step": 155900
    },
    {
      "epoch": 2.4489795918367347,
      "grad_norm": 3.7410898208618164,
      "learning_rate": 4.846938775510204e-05,
      "loss": 0.698,
      "step": 156000
    },
    {
      "epoch": 2.4505494505494507,
      "grad_norm": 4.187453746795654,
      "learning_rate": 4.84684065934066e-05,
      "loss": 0.7066,
      "step": 156100
    },
    {
      "epoch": 2.4521193092621663,
      "grad_norm": 2.39243483543396,
      "learning_rate": 4.846742543171115e-05,
      "loss": 0.6603,
      "step": 156200
    },
    {
      "epoch": 2.4536891679748822,
      "grad_norm": 3.9976003170013428,
      "learning_rate": 4.84664442700157e-05,
      "loss": 0.7453,
      "step": 156300
    },
    {
      "epoch": 2.455259026687598,
      "grad_norm": 3.696798324584961,
      "learning_rate": 4.846546310832025e-05,
      "loss": 0.6707,
      "step": 156400
    },
    {
      "epoch": 2.4568288854003137,
      "grad_norm": 3.5575473308563232,
      "learning_rate": 4.846448194662481e-05,
      "loss": 0.7387,
      "step": 156500
    },
    {
      "epoch": 2.4583987441130297,
      "grad_norm": 4.2697577476501465,
      "learning_rate": 4.846350078492936e-05,
      "loss": 0.7093,
      "step": 156600
    },
    {
      "epoch": 2.4599686028257457,
      "grad_norm": 4.055300235748291,
      "learning_rate": 4.846251962323391e-05,
      "loss": 0.7436,
      "step": 156700
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 4.312878131866455,
      "learning_rate": 4.846153846153846e-05,
      "loss": 0.706,
      "step": 156800
    },
    {
      "epoch": 2.463108320251177,
      "grad_norm": 4.279400825500488,
      "learning_rate": 4.846055729984302e-05,
      "loss": 0.7254,
      "step": 156900
    },
    {
      "epoch": 2.464678178963893,
      "grad_norm": 4.541538715362549,
      "learning_rate": 4.8459576138147564e-05,
      "loss": 0.7025,
      "step": 157000
    },
    {
      "epoch": 2.466248037676609,
      "grad_norm": 3.526071786880493,
      "learning_rate": 4.845859497645212e-05,
      "loss": 0.7638,
      "step": 157100
    },
    {
      "epoch": 2.467817896389325,
      "grad_norm": 2.7183098793029785,
      "learning_rate": 4.845761381475667e-05,
      "loss": 0.724,
      "step": 157200
    },
    {
      "epoch": 2.4693877551020407,
      "grad_norm": 4.095522880554199,
      "learning_rate": 4.845663265306123e-05,
      "loss": 0.7149,
      "step": 157300
    },
    {
      "epoch": 2.4709576138147566,
      "grad_norm": 3.4849131107330322,
      "learning_rate": 4.8455651491365775e-05,
      "loss": 0.7239,
      "step": 157400
    },
    {
      "epoch": 2.4725274725274726,
      "grad_norm": 3.460965394973755,
      "learning_rate": 4.845467032967033e-05,
      "loss": 0.7371,
      "step": 157500
    },
    {
      "epoch": 2.4740973312401886,
      "grad_norm": 4.458331108093262,
      "learning_rate": 4.845368916797488e-05,
      "loss": 0.7073,
      "step": 157600
    },
    {
      "epoch": 2.475667189952904,
      "grad_norm": 4.241382598876953,
      "learning_rate": 4.8452708006279434e-05,
      "loss": 0.6621,
      "step": 157700
    },
    {
      "epoch": 2.47723704866562,
      "grad_norm": 3.6820285320281982,
      "learning_rate": 4.845172684458399e-05,
      "loss": 0.6876,
      "step": 157800
    },
    {
      "epoch": 2.478806907378336,
      "grad_norm": 4.71167516708374,
      "learning_rate": 4.845074568288854e-05,
      "loss": 0.6939,
      "step": 157900
    },
    {
      "epoch": 2.4803767660910516,
      "grad_norm": 4.691235065460205,
      "learning_rate": 4.8449764521193094e-05,
      "loss": 0.6992,
      "step": 158000
    },
    {
      "epoch": 2.4819466248037676,
      "grad_norm": 4.650097846984863,
      "learning_rate": 4.8448783359497645e-05,
      "loss": 0.7511,
      "step": 158100
    },
    {
      "epoch": 2.4835164835164836,
      "grad_norm": 3.65376877784729,
      "learning_rate": 4.84478021978022e-05,
      "loss": 0.7504,
      "step": 158200
    },
    {
      "epoch": 2.4850863422291996,
      "grad_norm": 4.497293949127197,
      "learning_rate": 4.8446821036106754e-05,
      "loss": 0.6588,
      "step": 158300
    },
    {
      "epoch": 2.486656200941915,
      "grad_norm": 3.989990472793579,
      "learning_rate": 4.8445839874411305e-05,
      "loss": 0.6543,
      "step": 158400
    },
    {
      "epoch": 2.488226059654631,
      "grad_norm": 4.8265204429626465,
      "learning_rate": 4.8444858712715856e-05,
      "loss": 0.6853,
      "step": 158500
    },
    {
      "epoch": 2.489795918367347,
      "grad_norm": 4.268981456756592,
      "learning_rate": 4.8443877551020414e-05,
      "loss": 0.7106,
      "step": 158600
    },
    {
      "epoch": 2.4913657770800626,
      "grad_norm": 3.117323398590088,
      "learning_rate": 4.8442896389324965e-05,
      "loss": 0.7251,
      "step": 158700
    },
    {
      "epoch": 2.4929356357927785,
      "grad_norm": 4.213224411010742,
      "learning_rate": 4.8441915227629515e-05,
      "loss": 0.6974,
      "step": 158800
    },
    {
      "epoch": 2.4945054945054945,
      "grad_norm": 4.1890716552734375,
      "learning_rate": 4.8440934065934066e-05,
      "loss": 0.7207,
      "step": 158900
    },
    {
      "epoch": 2.4960753532182105,
      "grad_norm": 3.612565755844116,
      "learning_rate": 4.8439952904238624e-05,
      "loss": 0.7042,
      "step": 159000
    },
    {
      "epoch": 2.497645211930926,
      "grad_norm": 4.220539093017578,
      "learning_rate": 4.843897174254317e-05,
      "loss": 0.6617,
      "step": 159100
    },
    {
      "epoch": 2.499215070643642,
      "grad_norm": 3.507587194442749,
      "learning_rate": 4.8437990580847726e-05,
      "loss": 0.693,
      "step": 159200
    },
    {
      "epoch": 2.500784929356358,
      "grad_norm": 4.301030158996582,
      "learning_rate": 4.843700941915228e-05,
      "loss": 0.7374,
      "step": 159300
    },
    {
      "epoch": 2.5023547880690735,
      "grad_norm": 5.054028034210205,
      "learning_rate": 4.8436028257456835e-05,
      "loss": 0.6829,
      "step": 159400
    },
    {
      "epoch": 2.5039246467817895,
      "grad_norm": 3.1119139194488525,
      "learning_rate": 4.843504709576138e-05,
      "loss": 0.6922,
      "step": 159500
    },
    {
      "epoch": 2.5054945054945055,
      "grad_norm": 3.805905818939209,
      "learning_rate": 4.843406593406594e-05,
      "loss": 0.7302,
      "step": 159600
    },
    {
      "epoch": 2.5070643642072215,
      "grad_norm": 4.692047595977783,
      "learning_rate": 4.843308477237049e-05,
      "loss": 0.7253,
      "step": 159700
    },
    {
      "epoch": 2.5086342229199374,
      "grad_norm": 3.7894527912139893,
      "learning_rate": 4.843210361067504e-05,
      "loss": 0.7047,
      "step": 159800
    },
    {
      "epoch": 2.510204081632653,
      "grad_norm": 3.8887863159179688,
      "learning_rate": 4.84311224489796e-05,
      "loss": 0.7239,
      "step": 159900
    },
    {
      "epoch": 2.511773940345369,
      "grad_norm": 4.256416320800781,
      "learning_rate": 4.843014128728415e-05,
      "loss": 0.6627,
      "step": 160000
    },
    {
      "epoch": 2.513343799058085,
      "grad_norm": 3.382493734359741,
      "learning_rate": 4.84291601255887e-05,
      "loss": 0.7512,
      "step": 160100
    },
    {
      "epoch": 2.5149136577708004,
      "grad_norm": 3.946957588195801,
      "learning_rate": 4.842817896389325e-05,
      "loss": 0.7345,
      "step": 160200
    },
    {
      "epoch": 2.5164835164835164,
      "grad_norm": 4.498651504516602,
      "learning_rate": 4.842719780219781e-05,
      "loss": 0.7155,
      "step": 160300
    },
    {
      "epoch": 2.5180533751962324,
      "grad_norm": 3.464020013809204,
      "learning_rate": 4.842621664050236e-05,
      "loss": 0.7382,
      "step": 160400
    },
    {
      "epoch": 2.5196232339089484,
      "grad_norm": 3.0060648918151855,
      "learning_rate": 4.842523547880691e-05,
      "loss": 0.6924,
      "step": 160500
    },
    {
      "epoch": 2.521193092621664,
      "grad_norm": 4.467972278594971,
      "learning_rate": 4.842425431711146e-05,
      "loss": 0.7593,
      "step": 160600
    },
    {
      "epoch": 2.52276295133438,
      "grad_norm": 4.2225141525268555,
      "learning_rate": 4.842327315541602e-05,
      "loss": 0.7192,
      "step": 160700
    },
    {
      "epoch": 2.524332810047096,
      "grad_norm": 3.790287494659424,
      "learning_rate": 4.842229199372057e-05,
      "loss": 0.6946,
      "step": 160800
    },
    {
      "epoch": 2.5259026687598114,
      "grad_norm": 3.120614767074585,
      "learning_rate": 4.842131083202512e-05,
      "loss": 0.7118,
      "step": 160900
    },
    {
      "epoch": 2.5274725274725274,
      "grad_norm": 3.024409532546997,
      "learning_rate": 4.842032967032967e-05,
      "loss": 0.7232,
      "step": 161000
    },
    {
      "epoch": 2.5290423861852434,
      "grad_norm": 4.648998260498047,
      "learning_rate": 4.841934850863423e-05,
      "loss": 0.7332,
      "step": 161100
    },
    {
      "epoch": 2.5306122448979593,
      "grad_norm": 4.17644739151001,
      "learning_rate": 4.841836734693877e-05,
      "loss": 0.7506,
      "step": 161200
    },
    {
      "epoch": 2.5321821036106753,
      "grad_norm": 5.2884392738342285,
      "learning_rate": 4.841738618524333e-05,
      "loss": 0.7265,
      "step": 161300
    },
    {
      "epoch": 2.533751962323391,
      "grad_norm": 2.6238529682159424,
      "learning_rate": 4.841640502354788e-05,
      "loss": 0.6899,
      "step": 161400
    },
    {
      "epoch": 2.535321821036107,
      "grad_norm": 3.7436320781707764,
      "learning_rate": 4.841542386185244e-05,
      "loss": 0.6851,
      "step": 161500
    },
    {
      "epoch": 2.5368916797488223,
      "grad_norm": 4.608156204223633,
      "learning_rate": 4.8414442700156984e-05,
      "loss": 0.6761,
      "step": 161600
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 4.116640567779541,
      "learning_rate": 4.841346153846154e-05,
      "loss": 0.7327,
      "step": 161700
    },
    {
      "epoch": 2.5400313971742543,
      "grad_norm": 3.823760986328125,
      "learning_rate": 4.841248037676609e-05,
      "loss": 0.6911,
      "step": 161800
    },
    {
      "epoch": 2.5416012558869703,
      "grad_norm": 3.892338514328003,
      "learning_rate": 4.841149921507064e-05,
      "loss": 0.7109,
      "step": 161900
    },
    {
      "epoch": 2.5431711145996863,
      "grad_norm": 4.471756458282471,
      "learning_rate": 4.84105180533752e-05,
      "loss": 0.7311,
      "step": 162000
    },
    {
      "epoch": 2.544740973312402,
      "grad_norm": 2.9694786071777344,
      "learning_rate": 4.840953689167975e-05,
      "loss": 0.7012,
      "step": 162100
    },
    {
      "epoch": 2.5463108320251178,
      "grad_norm": 3.916623830795288,
      "learning_rate": 4.84085557299843e-05,
      "loss": 0.6937,
      "step": 162200
    },
    {
      "epoch": 2.5478806907378337,
      "grad_norm": 4.388896465301514,
      "learning_rate": 4.8407574568288854e-05,
      "loss": 0.7046,
      "step": 162300
    },
    {
      "epoch": 2.5494505494505493,
      "grad_norm": 4.974954605102539,
      "learning_rate": 4.840659340659341e-05,
      "loss": 0.7004,
      "step": 162400
    },
    {
      "epoch": 2.5510204081632653,
      "grad_norm": 4.463168144226074,
      "learning_rate": 4.840561224489796e-05,
      "loss": 0.7071,
      "step": 162500
    },
    {
      "epoch": 2.5525902668759812,
      "grad_norm": 3.5321645736694336,
      "learning_rate": 4.8404631083202514e-05,
      "loss": 0.6837,
      "step": 162600
    },
    {
      "epoch": 2.554160125588697,
      "grad_norm": 3.542165994644165,
      "learning_rate": 4.8403649921507065e-05,
      "loss": 0.682,
      "step": 162700
    },
    {
      "epoch": 2.5557299843014127,
      "grad_norm": 3.9739584922790527,
      "learning_rate": 4.840266875981162e-05,
      "loss": 0.7425,
      "step": 162800
    },
    {
      "epoch": 2.5572998430141287,
      "grad_norm": 1.9868435859680176,
      "learning_rate": 4.8401687598116173e-05,
      "loss": 0.6978,
      "step": 162900
    },
    {
      "epoch": 2.5588697017268447,
      "grad_norm": 4.303096294403076,
      "learning_rate": 4.8400706436420724e-05,
      "loss": 0.6814,
      "step": 163000
    },
    {
      "epoch": 2.5604395604395602,
      "grad_norm": 3.4438302516937256,
      "learning_rate": 4.8399725274725275e-05,
      "loss": 0.7172,
      "step": 163100
    },
    {
      "epoch": 2.562009419152276,
      "grad_norm": 2.6658456325531006,
      "learning_rate": 4.839874411302983e-05,
      "loss": 0.6838,
      "step": 163200
    },
    {
      "epoch": 2.563579277864992,
      "grad_norm": 4.658634662628174,
      "learning_rate": 4.839776295133438e-05,
      "loss": 0.6833,
      "step": 163300
    },
    {
      "epoch": 2.565149136577708,
      "grad_norm": 3.8148515224456787,
      "learning_rate": 4.8396781789638935e-05,
      "loss": 0.7376,
      "step": 163400
    },
    {
      "epoch": 2.566718995290424,
      "grad_norm": 4.52987003326416,
      "learning_rate": 4.8395800627943486e-05,
      "loss": 0.7323,
      "step": 163500
    },
    {
      "epoch": 2.5682888540031397,
      "grad_norm": 4.117944240570068,
      "learning_rate": 4.8394819466248044e-05,
      "loss": 0.7278,
      "step": 163600
    },
    {
      "epoch": 2.5698587127158556,
      "grad_norm": 3.2521655559539795,
      "learning_rate": 4.839383830455259e-05,
      "loss": 0.7274,
      "step": 163700
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 4.016576290130615,
      "learning_rate": 4.8392857142857146e-05,
      "loss": 0.6936,
      "step": 163800
    },
    {
      "epoch": 2.572998430141287,
      "grad_norm": 3.4647979736328125,
      "learning_rate": 4.83918759811617e-05,
      "loss": 0.6942,
      "step": 163900
    },
    {
      "epoch": 2.574568288854003,
      "grad_norm": 3.263556480407715,
      "learning_rate": 4.839089481946625e-05,
      "loss": 0.7013,
      "step": 164000
    },
    {
      "epoch": 2.576138147566719,
      "grad_norm": 4.526302337646484,
      "learning_rate": 4.8389913657770806e-05,
      "loss": 0.7003,
      "step": 164100
    },
    {
      "epoch": 2.577708006279435,
      "grad_norm": 4.048817157745361,
      "learning_rate": 4.8388932496075357e-05,
      "loss": 0.7169,
      "step": 164200
    },
    {
      "epoch": 2.5792778649921506,
      "grad_norm": 4.342134475708008,
      "learning_rate": 4.838795133437991e-05,
      "loss": 0.7461,
      "step": 164300
    },
    {
      "epoch": 2.5808477237048666,
      "grad_norm": 3.7842657566070557,
      "learning_rate": 4.838697017268446e-05,
      "loss": 0.6866,
      "step": 164400
    },
    {
      "epoch": 2.5824175824175826,
      "grad_norm": 3.8757312297821045,
      "learning_rate": 4.8385989010989016e-05,
      "loss": 0.7059,
      "step": 164500
    },
    {
      "epoch": 2.583987441130298,
      "grad_norm": 3.9625320434570312,
      "learning_rate": 4.838500784929357e-05,
      "loss": 0.7396,
      "step": 164600
    },
    {
      "epoch": 2.585557299843014,
      "grad_norm": 3.735710620880127,
      "learning_rate": 4.838402668759812e-05,
      "loss": 0.7624,
      "step": 164700
    },
    {
      "epoch": 2.58712715855573,
      "grad_norm": 3.907008409500122,
      "learning_rate": 4.838304552590267e-05,
      "loss": 0.7224,
      "step": 164800
    },
    {
      "epoch": 2.588697017268446,
      "grad_norm": 4.094396591186523,
      "learning_rate": 4.838206436420723e-05,
      "loss": 0.7226,
      "step": 164900
    },
    {
      "epoch": 2.5902668759811616,
      "grad_norm": 4.306395530700684,
      "learning_rate": 4.838108320251178e-05,
      "loss": 0.6958,
      "step": 165000
    },
    {
      "epoch": 2.5918367346938775,
      "grad_norm": 6.034765243530273,
      "learning_rate": 4.838010204081633e-05,
      "loss": 0.703,
      "step": 165100
    },
    {
      "epoch": 2.5934065934065935,
      "grad_norm": 3.441068410873413,
      "learning_rate": 4.837912087912088e-05,
      "loss": 0.6905,
      "step": 165200
    },
    {
      "epoch": 2.594976452119309,
      "grad_norm": 4.162937641143799,
      "learning_rate": 4.837813971742544e-05,
      "loss": 0.7655,
      "step": 165300
    },
    {
      "epoch": 2.596546310832025,
      "grad_norm": 4.402104377746582,
      "learning_rate": 4.837715855572998e-05,
      "loss": 0.7341,
      "step": 165400
    },
    {
      "epoch": 2.598116169544741,
      "grad_norm": 4.24057674407959,
      "learning_rate": 4.837617739403454e-05,
      "loss": 0.7011,
      "step": 165500
    },
    {
      "epoch": 2.599686028257457,
      "grad_norm": 3.6264724731445312,
      "learning_rate": 4.837519623233909e-05,
      "loss": 0.7195,
      "step": 165600
    },
    {
      "epoch": 2.601255886970173,
      "grad_norm": 4.339018821716309,
      "learning_rate": 4.837421507064365e-05,
      "loss": 0.7119,
      "step": 165700
    },
    {
      "epoch": 2.6028257456828885,
      "grad_norm": 3.5570805072784424,
      "learning_rate": 4.837323390894819e-05,
      "loss": 0.7457,
      "step": 165800
    },
    {
      "epoch": 2.6043956043956045,
      "grad_norm": 4.489461421966553,
      "learning_rate": 4.837225274725275e-05,
      "loss": 0.7232,
      "step": 165900
    },
    {
      "epoch": 2.60596546310832,
      "grad_norm": 3.992295026779175,
      "learning_rate": 4.83712715855573e-05,
      "loss": 0.7144,
      "step": 166000
    },
    {
      "epoch": 2.607535321821036,
      "grad_norm": 4.461860179901123,
      "learning_rate": 4.837029042386185e-05,
      "loss": 0.7115,
      "step": 166100
    },
    {
      "epoch": 2.609105180533752,
      "grad_norm": 2.98923921585083,
      "learning_rate": 4.836930926216641e-05,
      "loss": 0.7331,
      "step": 166200
    },
    {
      "epoch": 2.610675039246468,
      "grad_norm": 4.2869954109191895,
      "learning_rate": 4.836832810047096e-05,
      "loss": 0.7204,
      "step": 166300
    },
    {
      "epoch": 2.612244897959184,
      "grad_norm": 4.482283115386963,
      "learning_rate": 4.836734693877551e-05,
      "loss": 0.6852,
      "step": 166400
    },
    {
      "epoch": 2.6138147566718994,
      "grad_norm": 2.6750590801239014,
      "learning_rate": 4.836636577708006e-05,
      "loss": 0.6993,
      "step": 166500
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 3.727658748626709,
      "learning_rate": 4.836538461538462e-05,
      "loss": 0.7138,
      "step": 166600
    },
    {
      "epoch": 2.6169544740973314,
      "grad_norm": 4.7499003410339355,
      "learning_rate": 4.836440345368917e-05,
      "loss": 0.7356,
      "step": 166700
    },
    {
      "epoch": 2.618524332810047,
      "grad_norm": 3.896817684173584,
      "learning_rate": 4.836342229199372e-05,
      "loss": 0.6863,
      "step": 166800
    },
    {
      "epoch": 2.620094191522763,
      "grad_norm": 4.613949775695801,
      "learning_rate": 4.8362441130298274e-05,
      "loss": 0.6847,
      "step": 166900
    },
    {
      "epoch": 2.621664050235479,
      "grad_norm": 4.354069709777832,
      "learning_rate": 4.836145996860283e-05,
      "loss": 0.6884,
      "step": 167000
    },
    {
      "epoch": 2.623233908948195,
      "grad_norm": 3.8956172466278076,
      "learning_rate": 4.836047880690738e-05,
      "loss": 0.7309,
      "step": 167100
    },
    {
      "epoch": 2.6248037676609104,
      "grad_norm": 3.9516005516052246,
      "learning_rate": 4.8359497645211933e-05,
      "loss": 0.6657,
      "step": 167200
    },
    {
      "epoch": 2.6263736263736264,
      "grad_norm": 4.377869129180908,
      "learning_rate": 4.8358516483516484e-05,
      "loss": 0.7096,
      "step": 167300
    },
    {
      "epoch": 2.6279434850863423,
      "grad_norm": 4.136998176574707,
      "learning_rate": 4.835753532182104e-05,
      "loss": 0.7181,
      "step": 167400
    },
    {
      "epoch": 2.629513343799058,
      "grad_norm": 2.622858762741089,
      "learning_rate": 4.8356554160125586e-05,
      "loss": 0.6842,
      "step": 167500
    },
    {
      "epoch": 2.631083202511774,
      "grad_norm": 3.990797758102417,
      "learning_rate": 4.8355572998430144e-05,
      "loss": 0.6536,
      "step": 167600
    },
    {
      "epoch": 2.63265306122449,
      "grad_norm": 4.126570701599121,
      "learning_rate": 4.8354591836734695e-05,
      "loss": 0.7368,
      "step": 167700
    },
    {
      "epoch": 2.634222919937206,
      "grad_norm": 4.98611307144165,
      "learning_rate": 4.835361067503925e-05,
      "loss": 0.7222,
      "step": 167800
    },
    {
      "epoch": 2.6357927786499213,
      "grad_norm": 4.264703750610352,
      "learning_rate": 4.83526295133438e-05,
      "loss": 0.7067,
      "step": 167900
    },
    {
      "epoch": 2.6373626373626373,
      "grad_norm": 2.8257055282592773,
      "learning_rate": 4.8351648351648355e-05,
      "loss": 0.689,
      "step": 168000
    },
    {
      "epoch": 2.6389324960753533,
      "grad_norm": 4.2822394371032715,
      "learning_rate": 4.8350667189952906e-05,
      "loss": 0.7121,
      "step": 168100
    },
    {
      "epoch": 2.640502354788069,
      "grad_norm": 4.003798961639404,
      "learning_rate": 4.834968602825746e-05,
      "loss": 0.6863,
      "step": 168200
    },
    {
      "epoch": 2.642072213500785,
      "grad_norm": 3.3798577785491943,
      "learning_rate": 4.8348704866562015e-05,
      "loss": 0.7411,
      "step": 168300
    },
    {
      "epoch": 2.643642072213501,
      "grad_norm": 3.421505928039551,
      "learning_rate": 4.8347723704866566e-05,
      "loss": 0.7096,
      "step": 168400
    },
    {
      "epoch": 2.6452119309262168,
      "grad_norm": 4.535411834716797,
      "learning_rate": 4.8346742543171117e-05,
      "loss": 0.7219,
      "step": 168500
    },
    {
      "epoch": 2.6467817896389327,
      "grad_norm": 4.847121715545654,
      "learning_rate": 4.834576138147567e-05,
      "loss": 0.6816,
      "step": 168600
    },
    {
      "epoch": 2.6483516483516483,
      "grad_norm": 4.582287788391113,
      "learning_rate": 4.8344780219780225e-05,
      "loss": 0.6858,
      "step": 168700
    },
    {
      "epoch": 2.6499215070643642,
      "grad_norm": 4.5626301765441895,
      "learning_rate": 4.8343799058084776e-05,
      "loss": 0.7125,
      "step": 168800
    },
    {
      "epoch": 2.6514913657770802,
      "grad_norm": 3.8731625080108643,
      "learning_rate": 4.834281789638933e-05,
      "loss": 0.7337,
      "step": 168900
    },
    {
      "epoch": 2.6530612244897958,
      "grad_norm": 4.009683609008789,
      "learning_rate": 4.834183673469388e-05,
      "loss": 0.7479,
      "step": 169000
    },
    {
      "epoch": 2.6546310832025117,
      "grad_norm": 4.492085933685303,
      "learning_rate": 4.8340855572998436e-05,
      "loss": 0.6984,
      "step": 169100
    },
    {
      "epoch": 2.6562009419152277,
      "grad_norm": 3.447985887527466,
      "learning_rate": 4.833987441130299e-05,
      "loss": 0.7127,
      "step": 169200
    },
    {
      "epoch": 2.6577708006279437,
      "grad_norm": 4.267895221710205,
      "learning_rate": 4.833889324960754e-05,
      "loss": 0.7436,
      "step": 169300
    },
    {
      "epoch": 2.659340659340659,
      "grad_norm": 4.425050735473633,
      "learning_rate": 4.833791208791209e-05,
      "loss": 0.678,
      "step": 169400
    },
    {
      "epoch": 2.660910518053375,
      "grad_norm": 5.3042683601379395,
      "learning_rate": 4.833693092621665e-05,
      "loss": 0.7166,
      "step": 169500
    },
    {
      "epoch": 2.662480376766091,
      "grad_norm": 3.9922454357147217,
      "learning_rate": 4.833594976452119e-05,
      "loss": 0.6681,
      "step": 169600
    },
    {
      "epoch": 2.6640502354788067,
      "grad_norm": 3.6308610439300537,
      "learning_rate": 4.833496860282575e-05,
      "loss": 0.697,
      "step": 169700
    },
    {
      "epoch": 2.6656200941915227,
      "grad_norm": 3.849580764770508,
      "learning_rate": 4.83339874411303e-05,
      "loss": 0.6717,
      "step": 169800
    },
    {
      "epoch": 2.6671899529042387,
      "grad_norm": 3.7870965003967285,
      "learning_rate": 4.833300627943486e-05,
      "loss": 0.7285,
      "step": 169900
    },
    {
      "epoch": 2.6687598116169546,
      "grad_norm": 4.2259087562561035,
      "learning_rate": 4.83320251177394e-05,
      "loss": 0.7224,
      "step": 170000
    },
    {
      "epoch": 2.67032967032967,
      "grad_norm": 4.4084014892578125,
      "learning_rate": 4.833104395604396e-05,
      "loss": 0.7112,
      "step": 170100
    },
    {
      "epoch": 2.671899529042386,
      "grad_norm": 5.452540397644043,
      "learning_rate": 4.833006279434851e-05,
      "loss": 0.6588,
      "step": 170200
    },
    {
      "epoch": 2.673469387755102,
      "grad_norm": 4.854830741882324,
      "learning_rate": 4.832908163265306e-05,
      "loss": 0.7425,
      "step": 170300
    },
    {
      "epoch": 2.6750392464678177,
      "grad_norm": 3.6366240978240967,
      "learning_rate": 4.832810047095762e-05,
      "loss": 0.7178,
      "step": 170400
    },
    {
      "epoch": 2.6766091051805336,
      "grad_norm": 3.731379747390747,
      "learning_rate": 4.832711930926217e-05,
      "loss": 0.7294,
      "step": 170500
    },
    {
      "epoch": 2.6781789638932496,
      "grad_norm": 4.55043363571167,
      "learning_rate": 4.832613814756672e-05,
      "loss": 0.702,
      "step": 170600
    },
    {
      "epoch": 2.6797488226059656,
      "grad_norm": 4.21547269821167,
      "learning_rate": 4.832515698587127e-05,
      "loss": 0.7141,
      "step": 170700
    },
    {
      "epoch": 2.6813186813186816,
      "grad_norm": 4.498125076293945,
      "learning_rate": 4.832417582417583e-05,
      "loss": 0.6772,
      "step": 170800
    },
    {
      "epoch": 2.682888540031397,
      "grad_norm": 4.278616905212402,
      "learning_rate": 4.832319466248038e-05,
      "loss": 0.7164,
      "step": 170900
    },
    {
      "epoch": 2.684458398744113,
      "grad_norm": 3.8725810050964355,
      "learning_rate": 4.832221350078493e-05,
      "loss": 0.6641,
      "step": 171000
    },
    {
      "epoch": 2.686028257456829,
      "grad_norm": 3.191657066345215,
      "learning_rate": 4.832123233908948e-05,
      "loss": 0.6934,
      "step": 171100
    },
    {
      "epoch": 2.6875981161695446,
      "grad_norm": 4.403203964233398,
      "learning_rate": 4.832025117739404e-05,
      "loss": 0.7262,
      "step": 171200
    },
    {
      "epoch": 2.6891679748822606,
      "grad_norm": 3.9193899631500244,
      "learning_rate": 4.8319270015698585e-05,
      "loss": 0.7369,
      "step": 171300
    },
    {
      "epoch": 2.6907378335949765,
      "grad_norm": 4.375589370727539,
      "learning_rate": 4.831828885400314e-05,
      "loss": 0.7568,
      "step": 171400
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 3.7934141159057617,
      "learning_rate": 4.8317307692307693e-05,
      "loss": 0.7175,
      "step": 171500
    },
    {
      "epoch": 2.693877551020408,
      "grad_norm": 4.347580432891846,
      "learning_rate": 4.831632653061225e-05,
      "loss": 0.7378,
      "step": 171600
    },
    {
      "epoch": 2.695447409733124,
      "grad_norm": 3.7798168659210205,
      "learning_rate": 4.8315345368916795e-05,
      "loss": 0.7154,
      "step": 171700
    },
    {
      "epoch": 2.69701726844584,
      "grad_norm": 3.9933135509490967,
      "learning_rate": 4.831436420722135e-05,
      "loss": 0.7277,
      "step": 171800
    },
    {
      "epoch": 2.6985871271585555,
      "grad_norm": 4.2583489418029785,
      "learning_rate": 4.8313383045525904e-05,
      "loss": 0.733,
      "step": 171900
    },
    {
      "epoch": 2.7001569858712715,
      "grad_norm": 3.6262450218200684,
      "learning_rate": 4.8312401883830455e-05,
      "loss": 0.7142,
      "step": 172000
    },
    {
      "epoch": 2.7017268445839875,
      "grad_norm": 4.529608726501465,
      "learning_rate": 4.8311420722135006e-05,
      "loss": 0.7263,
      "step": 172100
    },
    {
      "epoch": 2.7032967032967035,
      "grad_norm": 3.416471004486084,
      "learning_rate": 4.8310439560439564e-05,
      "loss": 0.6838,
      "step": 172200
    },
    {
      "epoch": 2.704866562009419,
      "grad_norm": 4.276424884796143,
      "learning_rate": 4.8309458398744115e-05,
      "loss": 0.709,
      "step": 172300
    },
    {
      "epoch": 2.706436420722135,
      "grad_norm": 3.8270483016967773,
      "learning_rate": 4.8308477237048666e-05,
      "loss": 0.6973,
      "step": 172400
    },
    {
      "epoch": 2.708006279434851,
      "grad_norm": 3.073700189590454,
      "learning_rate": 4.8307496075353224e-05,
      "loss": 0.6675,
      "step": 172500
    },
    {
      "epoch": 2.7095761381475665,
      "grad_norm": 2.229605197906494,
      "learning_rate": 4.8306514913657775e-05,
      "loss": 0.6903,
      "step": 172600
    },
    {
      "epoch": 2.7111459968602825,
      "grad_norm": 4.328122138977051,
      "learning_rate": 4.8305533751962326e-05,
      "loss": 0.7086,
      "step": 172700
    },
    {
      "epoch": 2.7127158555729984,
      "grad_norm": 4.528508186340332,
      "learning_rate": 4.8304552590266877e-05,
      "loss": 0.7369,
      "step": 172800
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 3.1397712230682373,
      "learning_rate": 4.8303571428571434e-05,
      "loss": 0.6833,
      "step": 172900
    },
    {
      "epoch": 2.7158555729984304,
      "grad_norm": 4.300718784332275,
      "learning_rate": 4.8302590266875985e-05,
      "loss": 0.6303,
      "step": 173000
    },
    {
      "epoch": 2.717425431711146,
      "grad_norm": 4.466520309448242,
      "learning_rate": 4.8301609105180536e-05,
      "loss": 0.7644,
      "step": 173100
    },
    {
      "epoch": 2.718995290423862,
      "grad_norm": 4.537648677825928,
      "learning_rate": 4.830062794348509e-05,
      "loss": 0.6887,
      "step": 173200
    },
    {
      "epoch": 2.7205651491365774,
      "grad_norm": 3.602142095565796,
      "learning_rate": 4.8299646781789645e-05,
      "loss": 0.6791,
      "step": 173300
    },
    {
      "epoch": 2.7221350078492934,
      "grad_norm": 4.231595039367676,
      "learning_rate": 4.829866562009419e-05,
      "loss": 0.6828,
      "step": 173400
    },
    {
      "epoch": 2.7237048665620094,
      "grad_norm": 4.645932197570801,
      "learning_rate": 4.829768445839875e-05,
      "loss": 0.6867,
      "step": 173500
    },
    {
      "epoch": 2.7252747252747254,
      "grad_norm": 3.8230466842651367,
      "learning_rate": 4.82967032967033e-05,
      "loss": 0.7513,
      "step": 173600
    },
    {
      "epoch": 2.7268445839874413,
      "grad_norm": 4.479538440704346,
      "learning_rate": 4.8295722135007856e-05,
      "loss": 0.7525,
      "step": 173700
    },
    {
      "epoch": 2.728414442700157,
      "grad_norm": 3.6988818645477295,
      "learning_rate": 4.82947409733124e-05,
      "loss": 0.7378,
      "step": 173800
    },
    {
      "epoch": 2.729984301412873,
      "grad_norm": 4.346449375152588,
      "learning_rate": 4.829375981161696e-05,
      "loss": 0.7262,
      "step": 173900
    },
    {
      "epoch": 2.731554160125589,
      "grad_norm": 4.128853797912598,
      "learning_rate": 4.829277864992151e-05,
      "loss": 0.6798,
      "step": 174000
    },
    {
      "epoch": 2.7331240188383044,
      "grad_norm": 5.047815322875977,
      "learning_rate": 4.829179748822606e-05,
      "loss": 0.7403,
      "step": 174100
    },
    {
      "epoch": 2.7346938775510203,
      "grad_norm": 4.637297630310059,
      "learning_rate": 4.829081632653061e-05,
      "loss": 0.6988,
      "step": 174200
    },
    {
      "epoch": 2.7362637362637363,
      "grad_norm": 3.8183681964874268,
      "learning_rate": 4.828983516483517e-05,
      "loss": 0.7062,
      "step": 174300
    },
    {
      "epoch": 2.7378335949764523,
      "grad_norm": 3.2007949352264404,
      "learning_rate": 4.828885400313972e-05,
      "loss": 0.6895,
      "step": 174400
    },
    {
      "epoch": 2.739403453689168,
      "grad_norm": 4.390015125274658,
      "learning_rate": 4.828787284144427e-05,
      "loss": 0.6878,
      "step": 174500
    },
    {
      "epoch": 2.740973312401884,
      "grad_norm": 4.4993672370910645,
      "learning_rate": 4.828689167974882e-05,
      "loss": 0.6979,
      "step": 174600
    },
    {
      "epoch": 2.7425431711145998,
      "grad_norm": 3.7057318687438965,
      "learning_rate": 4.828591051805338e-05,
      "loss": 0.6812,
      "step": 174700
    },
    {
      "epoch": 2.7441130298273153,
      "grad_norm": 3.2861270904541016,
      "learning_rate": 4.828492935635793e-05,
      "loss": 0.7301,
      "step": 174800
    },
    {
      "epoch": 2.7456828885400313,
      "grad_norm": 4.517332077026367,
      "learning_rate": 4.828394819466248e-05,
      "loss": 0.7251,
      "step": 174900
    },
    {
      "epoch": 2.7472527472527473,
      "grad_norm": 4.0455193519592285,
      "learning_rate": 4.828296703296704e-05,
      "loss": 0.7284,
      "step": 175000
    },
    {
      "epoch": 2.7488226059654632,
      "grad_norm": 4.531797409057617,
      "learning_rate": 4.828198587127159e-05,
      "loss": 0.7109,
      "step": 175100
    },
    {
      "epoch": 2.750392464678179,
      "grad_norm": 4.537511825561523,
      "learning_rate": 4.828100470957614e-05,
      "loss": 0.692,
      "step": 175200
    },
    {
      "epoch": 2.7519623233908947,
      "grad_norm": 4.365380764007568,
      "learning_rate": 4.828002354788069e-05,
      "loss": 0.6876,
      "step": 175300
    },
    {
      "epoch": 2.7535321821036107,
      "grad_norm": 3.7495317459106445,
      "learning_rate": 4.827904238618525e-05,
      "loss": 0.7474,
      "step": 175400
    },
    {
      "epoch": 2.7551020408163263,
      "grad_norm": 3.4889419078826904,
      "learning_rate": 4.8278061224489794e-05,
      "loss": 0.7001,
      "step": 175500
    },
    {
      "epoch": 2.7566718995290422,
      "grad_norm": 4.501167297363281,
      "learning_rate": 4.827708006279435e-05,
      "loss": 0.7017,
      "step": 175600
    },
    {
      "epoch": 2.758241758241758,
      "grad_norm": 4.821932792663574,
      "learning_rate": 4.82760989010989e-05,
      "loss": 0.6831,
      "step": 175700
    },
    {
      "epoch": 2.759811616954474,
      "grad_norm": 4.46909761428833,
      "learning_rate": 4.827511773940346e-05,
      "loss": 0.705,
      "step": 175800
    },
    {
      "epoch": 2.76138147566719,
      "grad_norm": 3.7539279460906982,
      "learning_rate": 4.8274136577708004e-05,
      "loss": 0.68,
      "step": 175900
    },
    {
      "epoch": 2.7629513343799057,
      "grad_norm": 4.6806159019470215,
      "learning_rate": 4.827315541601256e-05,
      "loss": 0.6857,
      "step": 176000
    },
    {
      "epoch": 2.7645211930926217,
      "grad_norm": 4.202727794647217,
      "learning_rate": 4.827217425431711e-05,
      "loss": 0.6782,
      "step": 176100
    },
    {
      "epoch": 2.7660910518053377,
      "grad_norm": 3.899388313293457,
      "learning_rate": 4.8271193092621664e-05,
      "loss": 0.6622,
      "step": 176200
    },
    {
      "epoch": 2.767660910518053,
      "grad_norm": 4.106019020080566,
      "learning_rate": 4.8270211930926215e-05,
      "loss": 0.7037,
      "step": 176300
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 4.220741271972656,
      "learning_rate": 4.826923076923077e-05,
      "loss": 0.743,
      "step": 176400
    },
    {
      "epoch": 2.770800627943485,
      "grad_norm": 2.8826231956481934,
      "learning_rate": 4.8268249607535324e-05,
      "loss": 0.7,
      "step": 176500
    },
    {
      "epoch": 2.772370486656201,
      "grad_norm": 3.354323387145996,
      "learning_rate": 4.8267268445839875e-05,
      "loss": 0.6915,
      "step": 176600
    },
    {
      "epoch": 2.7739403453689166,
      "grad_norm": 4.636471748352051,
      "learning_rate": 4.8266287284144426e-05,
      "loss": 0.7071,
      "step": 176700
    },
    {
      "epoch": 2.7755102040816326,
      "grad_norm": 2.6010494232177734,
      "learning_rate": 4.8265306122448984e-05,
      "loss": 0.7398,
      "step": 176800
    },
    {
      "epoch": 2.7770800627943486,
      "grad_norm": 4.230368137359619,
      "learning_rate": 4.8264324960753535e-05,
      "loss": 0.6913,
      "step": 176900
    },
    {
      "epoch": 2.778649921507064,
      "grad_norm": 4.4668498039245605,
      "learning_rate": 4.8263343799058085e-05,
      "loss": 0.6762,
      "step": 177000
    },
    {
      "epoch": 2.78021978021978,
      "grad_norm": 3.881147861480713,
      "learning_rate": 4.826236263736264e-05,
      "loss": 0.7324,
      "step": 177100
    },
    {
      "epoch": 2.781789638932496,
      "grad_norm": 4.228194236755371,
      "learning_rate": 4.8261381475667194e-05,
      "loss": 0.7557,
      "step": 177200
    },
    {
      "epoch": 2.783359497645212,
      "grad_norm": 4.8045878410339355,
      "learning_rate": 4.8260400313971745e-05,
      "loss": 0.714,
      "step": 177300
    },
    {
      "epoch": 2.784929356357928,
      "grad_norm": 4.154351234436035,
      "learning_rate": 4.8259419152276296e-05,
      "loss": 0.6093,
      "step": 177400
    },
    {
      "epoch": 2.7864992150706436,
      "grad_norm": 4.115942001342773,
      "learning_rate": 4.8258437990580854e-05,
      "loss": 0.6891,
      "step": 177500
    },
    {
      "epoch": 2.7880690737833596,
      "grad_norm": 3.2779858112335205,
      "learning_rate": 4.82574568288854e-05,
      "loss": 0.6932,
      "step": 177600
    },
    {
      "epoch": 2.789638932496075,
      "grad_norm": 4.531918525695801,
      "learning_rate": 4.8256475667189956e-05,
      "loss": 0.6899,
      "step": 177700
    },
    {
      "epoch": 2.791208791208791,
      "grad_norm": 4.421391487121582,
      "learning_rate": 4.825549450549451e-05,
      "loss": 0.6889,
      "step": 177800
    },
    {
      "epoch": 2.792778649921507,
      "grad_norm": 3.8425252437591553,
      "learning_rate": 4.8254513343799065e-05,
      "loss": 0.6758,
      "step": 177900
    },
    {
      "epoch": 2.794348508634223,
      "grad_norm": 3.864567995071411,
      "learning_rate": 4.825353218210361e-05,
      "loss": 0.7144,
      "step": 178000
    },
    {
      "epoch": 2.795918367346939,
      "grad_norm": 4.542953968048096,
      "learning_rate": 4.8252551020408167e-05,
      "loss": 0.695,
      "step": 178100
    },
    {
      "epoch": 2.7974882260596545,
      "grad_norm": 4.177713394165039,
      "learning_rate": 4.825156985871272e-05,
      "loss": 0.7081,
      "step": 178200
    },
    {
      "epoch": 2.7990580847723705,
      "grad_norm": 4.01711368560791,
      "learning_rate": 4.825058869701727e-05,
      "loss": 0.7272,
      "step": 178300
    },
    {
      "epoch": 2.8006279434850865,
      "grad_norm": 3.3783750534057617,
      "learning_rate": 4.824960753532182e-05,
      "loss": 0.7001,
      "step": 178400
    },
    {
      "epoch": 2.802197802197802,
      "grad_norm": 4.024144172668457,
      "learning_rate": 4.824862637362638e-05,
      "loss": 0.7425,
      "step": 178500
    },
    {
      "epoch": 2.803767660910518,
      "grad_norm": 4.057023048400879,
      "learning_rate": 4.824764521193093e-05,
      "loss": 0.6789,
      "step": 178600
    },
    {
      "epoch": 2.805337519623234,
      "grad_norm": 4.104526996612549,
      "learning_rate": 4.824666405023548e-05,
      "loss": 0.6979,
      "step": 178700
    },
    {
      "epoch": 2.80690737833595,
      "grad_norm": 3.7509853839874268,
      "learning_rate": 4.824568288854003e-05,
      "loss": 0.7097,
      "step": 178800
    },
    {
      "epoch": 2.8084772370486655,
      "grad_norm": 2.681591749191284,
      "learning_rate": 4.824470172684459e-05,
      "loss": 0.6776,
      "step": 178900
    },
    {
      "epoch": 2.8100470957613815,
      "grad_norm": 4.5703558921813965,
      "learning_rate": 4.824372056514914e-05,
      "loss": 0.7015,
      "step": 179000
    },
    {
      "epoch": 2.8116169544740974,
      "grad_norm": 3.510470151901245,
      "learning_rate": 4.824273940345369e-05,
      "loss": 0.6889,
      "step": 179100
    },
    {
      "epoch": 2.813186813186813,
      "grad_norm": 5.074857711791992,
      "learning_rate": 4.824175824175825e-05,
      "loss": 0.7252,
      "step": 179200
    },
    {
      "epoch": 2.814756671899529,
      "grad_norm": 5.101118564605713,
      "learning_rate": 4.82407770800628e-05,
      "loss": 0.7092,
      "step": 179300
    },
    {
      "epoch": 2.816326530612245,
      "grad_norm": 4.524838447570801,
      "learning_rate": 4.823979591836735e-05,
      "loss": 0.6611,
      "step": 179400
    },
    {
      "epoch": 2.817896389324961,
      "grad_norm": 3.6106512546539307,
      "learning_rate": 4.82388147566719e-05,
      "loss": 0.6955,
      "step": 179500
    },
    {
      "epoch": 2.819466248037677,
      "grad_norm": 3.458310127258301,
      "learning_rate": 4.823783359497646e-05,
      "loss": 0.7025,
      "step": 179600
    },
    {
      "epoch": 2.8210361067503924,
      "grad_norm": 5.124490737915039,
      "learning_rate": 4.8236852433281e-05,
      "loss": 0.6731,
      "step": 179700
    },
    {
      "epoch": 2.8226059654631084,
      "grad_norm": 4.225178241729736,
      "learning_rate": 4.823587127158556e-05,
      "loss": 0.7046,
      "step": 179800
    },
    {
      "epoch": 2.824175824175824,
      "grad_norm": 3.928683280944824,
      "learning_rate": 4.823489010989011e-05,
      "loss": 0.7064,
      "step": 179900
    },
    {
      "epoch": 2.82574568288854,
      "grad_norm": 3.6823554039001465,
      "learning_rate": 4.823390894819467e-05,
      "loss": 0.6821,
      "step": 180000
    },
    {
      "epoch": 2.827315541601256,
      "grad_norm": 4.349119186401367,
      "learning_rate": 4.823292778649921e-05,
      "loss": 0.7344,
      "step": 180100
    },
    {
      "epoch": 2.828885400313972,
      "grad_norm": 4.191588878631592,
      "learning_rate": 4.823194662480377e-05,
      "loss": 0.6933,
      "step": 180200
    },
    {
      "epoch": 2.830455259026688,
      "grad_norm": 3.9894027709960938,
      "learning_rate": 4.823096546310832e-05,
      "loss": 0.6721,
      "step": 180300
    },
    {
      "epoch": 2.8320251177394034,
      "grad_norm": 4.587655544281006,
      "learning_rate": 4.822998430141287e-05,
      "loss": 0.7215,
      "step": 180400
    },
    {
      "epoch": 2.8335949764521193,
      "grad_norm": 3.7256174087524414,
      "learning_rate": 4.8229003139717424e-05,
      "loss": 0.7083,
      "step": 180500
    },
    {
      "epoch": 2.8351648351648353,
      "grad_norm": 3.6274969577789307,
      "learning_rate": 4.822802197802198e-05,
      "loss": 0.6979,
      "step": 180600
    },
    {
      "epoch": 2.836734693877551,
      "grad_norm": 3.824345111846924,
      "learning_rate": 4.822704081632653e-05,
      "loss": 0.7289,
      "step": 180700
    },
    {
      "epoch": 2.838304552590267,
      "grad_norm": 3.458674907684326,
      "learning_rate": 4.8226059654631084e-05,
      "loss": 0.6607,
      "step": 180800
    },
    {
      "epoch": 2.839874411302983,
      "grad_norm": 3.5549142360687256,
      "learning_rate": 4.8225078492935635e-05,
      "loss": 0.697,
      "step": 180900
    },
    {
      "epoch": 2.8414442700156988,
      "grad_norm": 5.348664283752441,
      "learning_rate": 4.822409733124019e-05,
      "loss": 0.7138,
      "step": 181000
    },
    {
      "epoch": 2.8430141287284143,
      "grad_norm": 4.044829368591309,
      "learning_rate": 4.8223116169544743e-05,
      "loss": 0.696,
      "step": 181100
    },
    {
      "epoch": 2.8445839874411303,
      "grad_norm": 4.287741661071777,
      "learning_rate": 4.8222135007849294e-05,
      "loss": 0.7264,
      "step": 181200
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 4.405102729797363,
      "learning_rate": 4.822115384615385e-05,
      "loss": 0.6991,
      "step": 181300
    },
    {
      "epoch": 2.847723704866562,
      "grad_norm": 3.562575340270996,
      "learning_rate": 4.82201726844584e-05,
      "loss": 0.7515,
      "step": 181400
    },
    {
      "epoch": 2.8492935635792778,
      "grad_norm": 3.783560276031494,
      "learning_rate": 4.8219191522762954e-05,
      "loss": 0.6668,
      "step": 181500
    },
    {
      "epoch": 2.8508634222919937,
      "grad_norm": 3.932407855987549,
      "learning_rate": 4.8218210361067505e-05,
      "loss": 0.7021,
      "step": 181600
    },
    {
      "epoch": 2.8524332810047097,
      "grad_norm": 4.859043121337891,
      "learning_rate": 4.821722919937206e-05,
      "loss": 0.7071,
      "step": 181700
    },
    {
      "epoch": 2.8540031397174257,
      "grad_norm": 4.4511399269104,
      "learning_rate": 4.821624803767661e-05,
      "loss": 0.7011,
      "step": 181800
    },
    {
      "epoch": 2.8555729984301412,
      "grad_norm": 3.3892221450805664,
      "learning_rate": 4.8215266875981165e-05,
      "loss": 0.6702,
      "step": 181900
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 4.588706016540527,
      "learning_rate": 4.8214285714285716e-05,
      "loss": 0.7064,
      "step": 182000
    },
    {
      "epoch": 2.8587127158555727,
      "grad_norm": 3.9084575176239014,
      "learning_rate": 4.8213304552590274e-05,
      "loss": 0.6963,
      "step": 182100
    },
    {
      "epoch": 2.8602825745682887,
      "grad_norm": 4.83244514465332,
      "learning_rate": 4.821232339089482e-05,
      "loss": 0.6733,
      "step": 182200
    },
    {
      "epoch": 2.8618524332810047,
      "grad_norm": 3.8416154384613037,
      "learning_rate": 4.8211342229199376e-05,
      "loss": 0.7161,
      "step": 182300
    },
    {
      "epoch": 2.8634222919937207,
      "grad_norm": 4.5097479820251465,
      "learning_rate": 4.8210361067503927e-05,
      "loss": 0.7078,
      "step": 182400
    },
    {
      "epoch": 2.8649921507064366,
      "grad_norm": 4.545917510986328,
      "learning_rate": 4.820937990580848e-05,
      "loss": 0.6854,
      "step": 182500
    },
    {
      "epoch": 2.866562009419152,
      "grad_norm": 2.678083896636963,
      "learning_rate": 4.820839874411303e-05,
      "loss": 0.6791,
      "step": 182600
    },
    {
      "epoch": 2.868131868131868,
      "grad_norm": 3.0312271118164062,
      "learning_rate": 4.8207417582417586e-05,
      "loss": 0.681,
      "step": 182700
    },
    {
      "epoch": 2.869701726844584,
      "grad_norm": 4.905115604400635,
      "learning_rate": 4.820643642072214e-05,
      "loss": 0.6992,
      "step": 182800
    },
    {
      "epoch": 2.8712715855572997,
      "grad_norm": 3.825082778930664,
      "learning_rate": 4.820545525902669e-05,
      "loss": 0.7292,
      "step": 182900
    },
    {
      "epoch": 2.8728414442700156,
      "grad_norm": 5.162566184997559,
      "learning_rate": 4.820447409733124e-05,
      "loss": 0.703,
      "step": 183000
    },
    {
      "epoch": 2.8744113029827316,
      "grad_norm": 3.948223114013672,
      "learning_rate": 4.82034929356358e-05,
      "loss": 0.7057,
      "step": 183100
    },
    {
      "epoch": 2.8759811616954476,
      "grad_norm": 4.396376132965088,
      "learning_rate": 4.820251177394035e-05,
      "loss": 0.7318,
      "step": 183200
    },
    {
      "epoch": 2.877551020408163,
      "grad_norm": 4.4753828048706055,
      "learning_rate": 4.82015306122449e-05,
      "loss": 0.7448,
      "step": 183300
    },
    {
      "epoch": 2.879120879120879,
      "grad_norm": 4.089789867401123,
      "learning_rate": 4.820054945054946e-05,
      "loss": 0.7235,
      "step": 183400
    },
    {
      "epoch": 2.880690737833595,
      "grad_norm": 3.9084811210632324,
      "learning_rate": 4.819956828885401e-05,
      "loss": 0.6822,
      "step": 183500
    },
    {
      "epoch": 2.8822605965463106,
      "grad_norm": 4.752087593078613,
      "learning_rate": 4.819858712715856e-05,
      "loss": 0.6973,
      "step": 183600
    },
    {
      "epoch": 2.8838304552590266,
      "grad_norm": 4.040282726287842,
      "learning_rate": 4.819760596546311e-05,
      "loss": 0.7154,
      "step": 183700
    },
    {
      "epoch": 2.8854003139717426,
      "grad_norm": 4.774153232574463,
      "learning_rate": 4.819662480376767e-05,
      "loss": 0.7204,
      "step": 183800
    },
    {
      "epoch": 2.8869701726844585,
      "grad_norm": 2.9095423221588135,
      "learning_rate": 4.819564364207221e-05,
      "loss": 0.6409,
      "step": 183900
    },
    {
      "epoch": 2.8885400313971745,
      "grad_norm": 4.940536022186279,
      "learning_rate": 4.819466248037677e-05,
      "loss": 0.7232,
      "step": 184000
    },
    {
      "epoch": 2.89010989010989,
      "grad_norm": 3.7032344341278076,
      "learning_rate": 4.819368131868132e-05,
      "loss": 0.7538,
      "step": 184100
    },
    {
      "epoch": 2.891679748822606,
      "grad_norm": 4.724015712738037,
      "learning_rate": 4.819270015698588e-05,
      "loss": 0.7046,
      "step": 184200
    },
    {
      "epoch": 2.8932496075353216,
      "grad_norm": 3.496257781982422,
      "learning_rate": 4.819171899529042e-05,
      "loss": 0.7095,
      "step": 184300
    },
    {
      "epoch": 2.8948194662480375,
      "grad_norm": 4.9724907875061035,
      "learning_rate": 4.819073783359498e-05,
      "loss": 0.6893,
      "step": 184400
    },
    {
      "epoch": 2.8963893249607535,
      "grad_norm": 3.715578556060791,
      "learning_rate": 4.818975667189953e-05,
      "loss": 0.7221,
      "step": 184500
    },
    {
      "epoch": 2.8979591836734695,
      "grad_norm": 4.456667423248291,
      "learning_rate": 4.818877551020408e-05,
      "loss": 0.711,
      "step": 184600
    },
    {
      "epoch": 2.8995290423861855,
      "grad_norm": 3.7876675128936768,
      "learning_rate": 4.818779434850863e-05,
      "loss": 0.6862,
      "step": 184700
    },
    {
      "epoch": 2.901098901098901,
      "grad_norm": 4.576122760772705,
      "learning_rate": 4.818681318681319e-05,
      "loss": 0.7225,
      "step": 184800
    },
    {
      "epoch": 2.902668759811617,
      "grad_norm": 4.666158676147461,
      "learning_rate": 4.818583202511774e-05,
      "loss": 0.6809,
      "step": 184900
    },
    {
      "epoch": 2.904238618524333,
      "grad_norm": 4.602603435516357,
      "learning_rate": 4.818485086342229e-05,
      "loss": 0.7094,
      "step": 185000
    },
    {
      "epoch": 2.9058084772370485,
      "grad_norm": 2.5646872520446777,
      "learning_rate": 4.8183869701726844e-05,
      "loss": 0.7072,
      "step": 185100
    },
    {
      "epoch": 2.9073783359497645,
      "grad_norm": 4.250153064727783,
      "learning_rate": 4.81828885400314e-05,
      "loss": 0.7258,
      "step": 185200
    },
    {
      "epoch": 2.9089481946624804,
      "grad_norm": 3.8130693435668945,
      "learning_rate": 4.818190737833595e-05,
      "loss": 0.746,
      "step": 185300
    },
    {
      "epoch": 2.9105180533751964,
      "grad_norm": 4.824798107147217,
      "learning_rate": 4.8180926216640503e-05,
      "loss": 0.7183,
      "step": 185400
    },
    {
      "epoch": 2.912087912087912,
      "grad_norm": 4.410651683807373,
      "learning_rate": 4.817994505494506e-05,
      "loss": 0.7164,
      "step": 185500
    },
    {
      "epoch": 2.913657770800628,
      "grad_norm": 4.254293441772461,
      "learning_rate": 4.817896389324961e-05,
      "loss": 0.7128,
      "step": 185600
    },
    {
      "epoch": 2.915227629513344,
      "grad_norm": 3.7037100791931152,
      "learning_rate": 4.817798273155416e-05,
      "loss": 0.6581,
      "step": 185700
    },
    {
      "epoch": 2.9167974882260594,
      "grad_norm": 3.6017465591430664,
      "learning_rate": 4.8177001569858714e-05,
      "loss": 0.6562,
      "step": 185800
    },
    {
      "epoch": 2.9183673469387754,
      "grad_norm": 4.097520351409912,
      "learning_rate": 4.817602040816327e-05,
      "loss": 0.6623,
      "step": 185900
    },
    {
      "epoch": 2.9199372056514914,
      "grad_norm": 4.176619529724121,
      "learning_rate": 4.8175039246467816e-05,
      "loss": 0.7051,
      "step": 186000
    },
    {
      "epoch": 2.9215070643642074,
      "grad_norm": 3.5826070308685303,
      "learning_rate": 4.8174058084772374e-05,
      "loss": 0.72,
      "step": 186100
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 4.078954219818115,
      "learning_rate": 4.8173076923076925e-05,
      "loss": 0.7233,
      "step": 186200
    },
    {
      "epoch": 2.924646781789639,
      "grad_norm": 2.772111654281616,
      "learning_rate": 4.817209576138148e-05,
      "loss": 0.6443,
      "step": 186300
    },
    {
      "epoch": 2.926216640502355,
      "grad_norm": 4.233848571777344,
      "learning_rate": 4.817111459968603e-05,
      "loss": 0.7225,
      "step": 186400
    },
    {
      "epoch": 2.9277864992150704,
      "grad_norm": 3.857605457305908,
      "learning_rate": 4.8170133437990585e-05,
      "loss": 0.6925,
      "step": 186500
    },
    {
      "epoch": 2.9293563579277864,
      "grad_norm": 3.4322078227996826,
      "learning_rate": 4.8169152276295136e-05,
      "loss": 0.7097,
      "step": 186600
    },
    {
      "epoch": 2.9309262166405023,
      "grad_norm": 3.902927875518799,
      "learning_rate": 4.8168171114599687e-05,
      "loss": 0.7338,
      "step": 186700
    },
    {
      "epoch": 2.9324960753532183,
      "grad_norm": 4.2389936447143555,
      "learning_rate": 4.816718995290424e-05,
      "loss": 0.7096,
      "step": 186800
    },
    {
      "epoch": 2.9340659340659343,
      "grad_norm": 3.996779203414917,
      "learning_rate": 4.8166208791208795e-05,
      "loss": 0.7373,
      "step": 186900
    },
    {
      "epoch": 2.93563579277865,
      "grad_norm": 3.3019227981567383,
      "learning_rate": 4.8165227629513346e-05,
      "loss": 0.7108,
      "step": 187000
    },
    {
      "epoch": 2.937205651491366,
      "grad_norm": 3.039077043533325,
      "learning_rate": 4.81642464678179e-05,
      "loss": 0.6656,
      "step": 187100
    },
    {
      "epoch": 2.938775510204082,
      "grad_norm": 4.294804573059082,
      "learning_rate": 4.816326530612245e-05,
      "loss": 0.7081,
      "step": 187200
    },
    {
      "epoch": 2.9403453689167973,
      "grad_norm": 4.115298748016357,
      "learning_rate": 4.8162284144427006e-05,
      "loss": 0.7541,
      "step": 187300
    },
    {
      "epoch": 2.9419152276295133,
      "grad_norm": 4.396572589874268,
      "learning_rate": 4.816130298273156e-05,
      "loss": 0.6951,
      "step": 187400
    },
    {
      "epoch": 2.9434850863422293,
      "grad_norm": 4.493447780609131,
      "learning_rate": 4.816032182103611e-05,
      "loss": 0.6981,
      "step": 187500
    },
    {
      "epoch": 2.9450549450549453,
      "grad_norm": 3.401179790496826,
      "learning_rate": 4.8159340659340666e-05,
      "loss": 0.6833,
      "step": 187600
    },
    {
      "epoch": 2.946624803767661,
      "grad_norm": 4.176111698150635,
      "learning_rate": 4.815835949764522e-05,
      "loss": 0.7233,
      "step": 187700
    },
    {
      "epoch": 2.9481946624803768,
      "grad_norm": 4.05111837387085,
      "learning_rate": 4.815737833594977e-05,
      "loss": 0.7221,
      "step": 187800
    },
    {
      "epoch": 2.9497645211930927,
      "grad_norm": 4.788556098937988,
      "learning_rate": 4.815639717425432e-05,
      "loss": 0.6883,
      "step": 187900
    },
    {
      "epoch": 2.9513343799058083,
      "grad_norm": 4.021585941314697,
      "learning_rate": 4.8155416012558876e-05,
      "loss": 0.6774,
      "step": 188000
    },
    {
      "epoch": 2.9529042386185242,
      "grad_norm": 3.8725602626800537,
      "learning_rate": 4.815443485086342e-05,
      "loss": 0.7154,
      "step": 188100
    },
    {
      "epoch": 2.9544740973312402,
      "grad_norm": 2.822605609893799,
      "learning_rate": 4.815345368916798e-05,
      "loss": 0.7165,
      "step": 188200
    },
    {
      "epoch": 2.956043956043956,
      "grad_norm": 3.4479870796203613,
      "learning_rate": 4.815247252747253e-05,
      "loss": 0.6928,
      "step": 188300
    },
    {
      "epoch": 2.957613814756672,
      "grad_norm": 3.732884168624878,
      "learning_rate": 4.815149136577709e-05,
      "loss": 0.6798,
      "step": 188400
    },
    {
      "epoch": 2.9591836734693877,
      "grad_norm": 3.760666608810425,
      "learning_rate": 4.815051020408163e-05,
      "loss": 0.7118,
      "step": 188500
    },
    {
      "epoch": 2.9607535321821037,
      "grad_norm": 3.485470771789551,
      "learning_rate": 4.814952904238619e-05,
      "loss": 0.7013,
      "step": 188600
    },
    {
      "epoch": 2.962323390894819,
      "grad_norm": 3.44577693939209,
      "learning_rate": 4.814854788069074e-05,
      "loss": 0.7199,
      "step": 188700
    },
    {
      "epoch": 2.963893249607535,
      "grad_norm": 3.993751049041748,
      "learning_rate": 4.814756671899529e-05,
      "loss": 0.6891,
      "step": 188800
    },
    {
      "epoch": 2.965463108320251,
      "grad_norm": 4.188378810882568,
      "learning_rate": 4.814658555729984e-05,
      "loss": 0.7412,
      "step": 188900
    },
    {
      "epoch": 2.967032967032967,
      "grad_norm": 4.056826114654541,
      "learning_rate": 4.81456043956044e-05,
      "loss": 0.7366,
      "step": 189000
    },
    {
      "epoch": 2.968602825745683,
      "grad_norm": 3.965843677520752,
      "learning_rate": 4.814462323390895e-05,
      "loss": 0.6824,
      "step": 189100
    },
    {
      "epoch": 2.9701726844583987,
      "grad_norm": 3.856165885925293,
      "learning_rate": 4.81436420722135e-05,
      "loss": 0.662,
      "step": 189200
    },
    {
      "epoch": 2.9717425431711146,
      "grad_norm": 3.991565227508545,
      "learning_rate": 4.814266091051805e-05,
      "loss": 0.704,
      "step": 189300
    },
    {
      "epoch": 2.9733124018838306,
      "grad_norm": 3.4109885692596436,
      "learning_rate": 4.814167974882261e-05,
      "loss": 0.726,
      "step": 189400
    },
    {
      "epoch": 2.974882260596546,
      "grad_norm": 3.675649642944336,
      "learning_rate": 4.814069858712716e-05,
      "loss": 0.7081,
      "step": 189500
    },
    {
      "epoch": 2.976452119309262,
      "grad_norm": 3.790125846862793,
      "learning_rate": 4.813971742543171e-05,
      "loss": 0.7343,
      "step": 189600
    },
    {
      "epoch": 2.978021978021978,
      "grad_norm": 4.105866432189941,
      "learning_rate": 4.813873626373627e-05,
      "loss": 0.6868,
      "step": 189700
    },
    {
      "epoch": 2.979591836734694,
      "grad_norm": 4.681905269622803,
      "learning_rate": 4.813775510204082e-05,
      "loss": 0.7407,
      "step": 189800
    },
    {
      "epoch": 2.9811616954474096,
      "grad_norm": 4.361865043640137,
      "learning_rate": 4.813677394034537e-05,
      "loss": 0.714,
      "step": 189900
    },
    {
      "epoch": 2.9827315541601256,
      "grad_norm": 4.129542350769043,
      "learning_rate": 4.813579277864992e-05,
      "loss": 0.6539,
      "step": 190000
    },
    {
      "epoch": 2.9843014128728416,
      "grad_norm": 4.738590717315674,
      "learning_rate": 4.813481161695448e-05,
      "loss": 0.7301,
      "step": 190100
    },
    {
      "epoch": 2.985871271585557,
      "grad_norm": 4.121333599090576,
      "learning_rate": 4.8133830455259025e-05,
      "loss": 0.6929,
      "step": 190200
    },
    {
      "epoch": 2.987441130298273,
      "grad_norm": 4.665836334228516,
      "learning_rate": 4.813284929356358e-05,
      "loss": 0.7408,
      "step": 190300
    },
    {
      "epoch": 2.989010989010989,
      "grad_norm": 4.415154933929443,
      "learning_rate": 4.8131868131868134e-05,
      "loss": 0.7111,
      "step": 190400
    },
    {
      "epoch": 2.990580847723705,
      "grad_norm": 4.631103515625,
      "learning_rate": 4.813088697017269e-05,
      "loss": 0.7347,
      "step": 190500
    },
    {
      "epoch": 2.9921507064364206,
      "grad_norm": 4.725419521331787,
      "learning_rate": 4.8129905808477236e-05,
      "loss": 0.668,
      "step": 190600
    },
    {
      "epoch": 2.9937205651491365,
      "grad_norm": 4.021218776702881,
      "learning_rate": 4.8128924646781794e-05,
      "loss": 0.7227,
      "step": 190700
    },
    {
      "epoch": 2.9952904238618525,
      "grad_norm": 3.8532707691192627,
      "learning_rate": 4.8127943485086345e-05,
      "loss": 0.7361,
      "step": 190800
    },
    {
      "epoch": 2.996860282574568,
      "grad_norm": 5.26715612411499,
      "learning_rate": 4.8126962323390896e-05,
      "loss": 0.7286,
      "step": 190900
    },
    {
      "epoch": 2.998430141287284,
      "grad_norm": 4.286890506744385,
      "learning_rate": 4.8125981161695446e-05,
      "loss": 0.6867,
      "step": 191000
    },
    {
      "epoch": 3.0,
      "grad_norm": 4.036189556121826,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 0.6573,
      "step": 191100
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.0369540452957153,
      "eval_runtime": 14.7705,
      "eval_samples_per_second": 227.007,
      "eval_steps_per_second": 227.007,
      "step": 191100
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.55019211769104,
      "eval_runtime": 280.5794,
      "eval_samples_per_second": 227.03,
      "eval_steps_per_second": 227.03,
      "step": 191100
    },
    {
      "epoch": 3.001569858712716,
      "grad_norm": 4.133156776428223,
      "learning_rate": 4.8124018838304555e-05,
      "loss": 0.6704,
      "step": 191200
    },
    {
      "epoch": 3.0031397174254315,
      "grad_norm": 4.652074813842773,
      "learning_rate": 4.8123037676609106e-05,
      "loss": 0.6817,
      "step": 191300
    },
    {
      "epoch": 3.0047095761381475,
      "grad_norm": 4.730746269226074,
      "learning_rate": 4.812205651491366e-05,
      "loss": 0.7163,
      "step": 191400
    },
    {
      "epoch": 3.0062794348508635,
      "grad_norm": 3.2052645683288574,
      "learning_rate": 4.8121075353218215e-05,
      "loss": 0.6912,
      "step": 191500
    },
    {
      "epoch": 3.0078492935635794,
      "grad_norm": 4.583684921264648,
      "learning_rate": 4.8120094191522766e-05,
      "loss": 0.7004,
      "step": 191600
    },
    {
      "epoch": 3.009419152276295,
      "grad_norm": 3.8467257022857666,
      "learning_rate": 4.811911302982732e-05,
      "loss": 0.675,
      "step": 191700
    },
    {
      "epoch": 3.010989010989011,
      "grad_norm": 4.407788276672363,
      "learning_rate": 4.8118131868131875e-05,
      "loss": 0.6877,
      "step": 191800
    },
    {
      "epoch": 3.012558869701727,
      "grad_norm": 3.724151611328125,
      "learning_rate": 4.8117150706436426e-05,
      "loss": 0.6886,
      "step": 191900
    },
    {
      "epoch": 3.014128728414443,
      "grad_norm": 4.762870788574219,
      "learning_rate": 4.811616954474098e-05,
      "loss": 0.6678,
      "step": 192000
    },
    {
      "epoch": 3.0156985871271584,
      "grad_norm": 5.092325210571289,
      "learning_rate": 4.811518838304553e-05,
      "loss": 0.7108,
      "step": 192100
    },
    {
      "epoch": 3.0172684458398744,
      "grad_norm": 3.3413734436035156,
      "learning_rate": 4.8114207221350085e-05,
      "loss": 0.6829,
      "step": 192200
    },
    {
      "epoch": 3.0188383045525904,
      "grad_norm": 4.295369625091553,
      "learning_rate": 4.811322605965463e-05,
      "loss": 0.6704,
      "step": 192300
    },
    {
      "epoch": 3.020408163265306,
      "grad_norm": 23.370685577392578,
      "learning_rate": 4.811224489795919e-05,
      "loss": 0.6768,
      "step": 192400
    },
    {
      "epoch": 3.021978021978022,
      "grad_norm": 4.092077255249023,
      "learning_rate": 4.811126373626374e-05,
      "loss": 0.6657,
      "step": 192500
    },
    {
      "epoch": 3.023547880690738,
      "grad_norm": 4.58964204788208,
      "learning_rate": 4.8110282574568296e-05,
      "loss": 0.7194,
      "step": 192600
    },
    {
      "epoch": 3.025117739403454,
      "grad_norm": 3.9247243404388428,
      "learning_rate": 4.810930141287284e-05,
      "loss": 0.7149,
      "step": 192700
    },
    {
      "epoch": 3.0266875981161694,
      "grad_norm": 5.6077961921691895,
      "learning_rate": 4.81083202511774e-05,
      "loss": 0.7003,
      "step": 192800
    },
    {
      "epoch": 3.0282574568288854,
      "grad_norm": 3.3341891765594482,
      "learning_rate": 4.810733908948195e-05,
      "loss": 0.7075,
      "step": 192900
    },
    {
      "epoch": 3.0298273155416013,
      "grad_norm": 3.721918821334839,
      "learning_rate": 4.81063579277865e-05,
      "loss": 0.6635,
      "step": 193000
    },
    {
      "epoch": 3.0313971742543173,
      "grad_norm": 3.786745071411133,
      "learning_rate": 4.810537676609105e-05,
      "loss": 0.7148,
      "step": 193100
    },
    {
      "epoch": 3.032967032967033,
      "grad_norm": 4.52922248840332,
      "learning_rate": 4.810439560439561e-05,
      "loss": 0.6966,
      "step": 193200
    },
    {
      "epoch": 3.034536891679749,
      "grad_norm": 4.808505058288574,
      "learning_rate": 4.810341444270016e-05,
      "loss": 0.6632,
      "step": 193300
    },
    {
      "epoch": 3.036106750392465,
      "grad_norm": 4.241766929626465,
      "learning_rate": 4.810243328100471e-05,
      "loss": 0.66,
      "step": 193400
    },
    {
      "epoch": 3.0376766091051803,
      "grad_norm": 3.00931978225708,
      "learning_rate": 4.810145211930926e-05,
      "loss": 0.6934,
      "step": 193500
    },
    {
      "epoch": 3.0392464678178963,
      "grad_norm": 3.9952845573425293,
      "learning_rate": 4.810047095761382e-05,
      "loss": 0.683,
      "step": 193600
    },
    {
      "epoch": 3.0408163265306123,
      "grad_norm": 1.9923311471939087,
      "learning_rate": 4.809948979591837e-05,
      "loss": 0.6902,
      "step": 193700
    },
    {
      "epoch": 3.0423861852433283,
      "grad_norm": 4.360772132873535,
      "learning_rate": 4.809850863422292e-05,
      "loss": 0.6418,
      "step": 193800
    },
    {
      "epoch": 3.043956043956044,
      "grad_norm": 2.8307223320007324,
      "learning_rate": 4.809752747252748e-05,
      "loss": 0.69,
      "step": 193900
    },
    {
      "epoch": 3.0455259026687598,
      "grad_norm": 4.034136772155762,
      "learning_rate": 4.809654631083202e-05,
      "loss": 0.6778,
      "step": 194000
    },
    {
      "epoch": 3.0470957613814758,
      "grad_norm": 3.2680962085723877,
      "learning_rate": 4.809556514913658e-05,
      "loss": 0.7199,
      "step": 194100
    },
    {
      "epoch": 3.0486656200941917,
      "grad_norm": 4.699560642242432,
      "learning_rate": 4.809458398744113e-05,
      "loss": 0.6952,
      "step": 194200
    },
    {
      "epoch": 3.0502354788069073,
      "grad_norm": 4.386282920837402,
      "learning_rate": 4.809360282574569e-05,
      "loss": 0.7294,
      "step": 194300
    },
    {
      "epoch": 3.0518053375196232,
      "grad_norm": 3.289034128189087,
      "learning_rate": 4.8092621664050234e-05,
      "loss": 0.7289,
      "step": 194400
    },
    {
      "epoch": 3.053375196232339,
      "grad_norm": 3.18230938911438,
      "learning_rate": 4.809164050235479e-05,
      "loss": 0.6854,
      "step": 194500
    },
    {
      "epoch": 3.0549450549450547,
      "grad_norm": 4.237265110015869,
      "learning_rate": 4.809065934065934e-05,
      "loss": 0.6734,
      "step": 194600
    },
    {
      "epoch": 3.0565149136577707,
      "grad_norm": 3.923238515853882,
      "learning_rate": 4.8089678178963894e-05,
      "loss": 0.7308,
      "step": 194700
    },
    {
      "epoch": 3.0580847723704867,
      "grad_norm": 3.522254705429077,
      "learning_rate": 4.8088697017268445e-05,
      "loss": 0.6861,
      "step": 194800
    },
    {
      "epoch": 3.0596546310832027,
      "grad_norm": 3.8023152351379395,
      "learning_rate": 4.8087715855573e-05,
      "loss": 0.7196,
      "step": 194900
    },
    {
      "epoch": 3.061224489795918,
      "grad_norm": 4.4151201248168945,
      "learning_rate": 4.8086734693877554e-05,
      "loss": 0.6858,
      "step": 195000
    },
    {
      "epoch": 3.062794348508634,
      "grad_norm": 3.909113883972168,
      "learning_rate": 4.8085753532182104e-05,
      "loss": 0.6622,
      "step": 195100
    },
    {
      "epoch": 3.06436420722135,
      "grad_norm": 3.9128644466400146,
      "learning_rate": 4.8084772370486655e-05,
      "loss": 0.6576,
      "step": 195200
    },
    {
      "epoch": 3.065934065934066,
      "grad_norm": 3.5027759075164795,
      "learning_rate": 4.808379120879121e-05,
      "loss": 0.6831,
      "step": 195300
    },
    {
      "epoch": 3.0675039246467817,
      "grad_norm": 4.168638229370117,
      "learning_rate": 4.8082810047095764e-05,
      "loss": 0.6985,
      "step": 195400
    },
    {
      "epoch": 3.0690737833594977,
      "grad_norm": 4.199560165405273,
      "learning_rate": 4.8081828885400315e-05,
      "loss": 0.7023,
      "step": 195500
    },
    {
      "epoch": 3.0706436420722136,
      "grad_norm": 4.771370887756348,
      "learning_rate": 4.8080847723704866e-05,
      "loss": 0.7148,
      "step": 195600
    },
    {
      "epoch": 3.072213500784929,
      "grad_norm": 3.82867169380188,
      "learning_rate": 4.8079866562009424e-05,
      "loss": 0.6772,
      "step": 195700
    },
    {
      "epoch": 3.073783359497645,
      "grad_norm": 3.4165894985198975,
      "learning_rate": 4.8078885400313975e-05,
      "loss": 0.6768,
      "step": 195800
    },
    {
      "epoch": 3.075353218210361,
      "grad_norm": 3.4058926105499268,
      "learning_rate": 4.8077904238618526e-05,
      "loss": 0.653,
      "step": 195900
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 4.754837989807129,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 0.7452,
      "step": 196000
    },
    {
      "epoch": 3.0784929356357926,
      "grad_norm": 3.637115955352783,
      "learning_rate": 4.807594191522763e-05,
      "loss": 0.6937,
      "step": 196100
    },
    {
      "epoch": 3.0800627943485086,
      "grad_norm": 4.75600004196167,
      "learning_rate": 4.8074960753532186e-05,
      "loss": 0.7124,
      "step": 196200
    },
    {
      "epoch": 3.0816326530612246,
      "grad_norm": 3.627723217010498,
      "learning_rate": 4.8073979591836737e-05,
      "loss": 0.713,
      "step": 196300
    },
    {
      "epoch": 3.0832025117739406,
      "grad_norm": 4.0438103675842285,
      "learning_rate": 4.8072998430141294e-05,
      "loss": 0.6978,
      "step": 196400
    },
    {
      "epoch": 3.084772370486656,
      "grad_norm": 4.344779968261719,
      "learning_rate": 4.807201726844584e-05,
      "loss": 0.7086,
      "step": 196500
    },
    {
      "epoch": 3.086342229199372,
      "grad_norm": 3.185119390487671,
      "learning_rate": 4.8071036106750396e-05,
      "loss": 0.7067,
      "step": 196600
    },
    {
      "epoch": 3.087912087912088,
      "grad_norm": 4.195657253265381,
      "learning_rate": 4.807005494505495e-05,
      "loss": 0.6977,
      "step": 196700
    },
    {
      "epoch": 3.0894819466248036,
      "grad_norm": 3.0973894596099854,
      "learning_rate": 4.80690737833595e-05,
      "loss": 0.6838,
      "step": 196800
    },
    {
      "epoch": 3.0910518053375196,
      "grad_norm": 5.333001613616943,
      "learning_rate": 4.806809262166405e-05,
      "loss": 0.6762,
      "step": 196900
    },
    {
      "epoch": 3.0926216640502355,
      "grad_norm": 4.2792253494262695,
      "learning_rate": 4.806711145996861e-05,
      "loss": 0.7039,
      "step": 197000
    },
    {
      "epoch": 3.0941915227629515,
      "grad_norm": 3.9101977348327637,
      "learning_rate": 4.806613029827316e-05,
      "loss": 0.6902,
      "step": 197100
    },
    {
      "epoch": 3.095761381475667,
      "grad_norm": 4.778287887573242,
      "learning_rate": 4.806514913657771e-05,
      "loss": 0.7108,
      "step": 197200
    },
    {
      "epoch": 3.097331240188383,
      "grad_norm": 4.8320746421813965,
      "learning_rate": 4.806416797488226e-05,
      "loss": 0.7185,
      "step": 197300
    },
    {
      "epoch": 3.098901098901099,
      "grad_norm": 5.063838958740234,
      "learning_rate": 4.806318681318682e-05,
      "loss": 0.7056,
      "step": 197400
    },
    {
      "epoch": 3.100470957613815,
      "grad_norm": 4.57328987121582,
      "learning_rate": 4.806220565149137e-05,
      "loss": 0.7126,
      "step": 197500
    },
    {
      "epoch": 3.1020408163265305,
      "grad_norm": 4.648008346557617,
      "learning_rate": 4.806122448979592e-05,
      "loss": 0.7297,
      "step": 197600
    },
    {
      "epoch": 3.1036106750392465,
      "grad_norm": 4.534456729888916,
      "learning_rate": 4.806024332810047e-05,
      "loss": 0.6833,
      "step": 197700
    },
    {
      "epoch": 3.1051805337519625,
      "grad_norm": 4.193495273590088,
      "learning_rate": 4.805926216640503e-05,
      "loss": 0.6791,
      "step": 197800
    },
    {
      "epoch": 3.106750392464678,
      "grad_norm": 4.8057942390441895,
      "learning_rate": 4.805828100470958e-05,
      "loss": 0.7097,
      "step": 197900
    },
    {
      "epoch": 3.108320251177394,
      "grad_norm": 3.2807748317718506,
      "learning_rate": 4.805729984301413e-05,
      "loss": 0.6845,
      "step": 198000
    },
    {
      "epoch": 3.10989010989011,
      "grad_norm": 4.445310115814209,
      "learning_rate": 4.805631868131869e-05,
      "loss": 0.6911,
      "step": 198100
    },
    {
      "epoch": 3.111459968602826,
      "grad_norm": 4.377899646759033,
      "learning_rate": 4.805533751962323e-05,
      "loss": 0.6903,
      "step": 198200
    },
    {
      "epoch": 3.1130298273155415,
      "grad_norm": 4.688360691070557,
      "learning_rate": 4.805435635792779e-05,
      "loss": 0.7067,
      "step": 198300
    },
    {
      "epoch": 3.1145996860282574,
      "grad_norm": 3.4981529712677,
      "learning_rate": 4.805337519623234e-05,
      "loss": 0.7074,
      "step": 198400
    },
    {
      "epoch": 3.1161695447409734,
      "grad_norm": 4.0092453956604,
      "learning_rate": 4.80523940345369e-05,
      "loss": 0.7127,
      "step": 198500
    },
    {
      "epoch": 3.1177394034536894,
      "grad_norm": 4.18446683883667,
      "learning_rate": 4.805141287284144e-05,
      "loss": 0.6861,
      "step": 198600
    },
    {
      "epoch": 3.119309262166405,
      "grad_norm": 3.6966986656188965,
      "learning_rate": 4.8050431711146e-05,
      "loss": 0.6893,
      "step": 198700
    },
    {
      "epoch": 3.120879120879121,
      "grad_norm": 3.78495454788208,
      "learning_rate": 4.804945054945055e-05,
      "loss": 0.6705,
      "step": 198800
    },
    {
      "epoch": 3.122448979591837,
      "grad_norm": 4.813541412353516,
      "learning_rate": 4.80484693877551e-05,
      "loss": 0.6699,
      "step": 198900
    },
    {
      "epoch": 3.1240188383045524,
      "grad_norm": 3.9024057388305664,
      "learning_rate": 4.8047488226059654e-05,
      "loss": 0.7233,
      "step": 199000
    },
    {
      "epoch": 3.1255886970172684,
      "grad_norm": 4.3390960693359375,
      "learning_rate": 4.804650706436421e-05,
      "loss": 0.6689,
      "step": 199100
    },
    {
      "epoch": 3.1271585557299844,
      "grad_norm": 3.9814975261688232,
      "learning_rate": 4.804552590266876e-05,
      "loss": 0.6836,
      "step": 199200
    },
    {
      "epoch": 3.1287284144427003,
      "grad_norm": 4.189142227172852,
      "learning_rate": 4.8044544740973313e-05,
      "loss": 0.6451,
      "step": 199300
    },
    {
      "epoch": 3.130298273155416,
      "grad_norm": 5.039839267730713,
      "learning_rate": 4.8043563579277864e-05,
      "loss": 0.6887,
      "step": 199400
    },
    {
      "epoch": 3.131868131868132,
      "grad_norm": 3.6917901039123535,
      "learning_rate": 4.804258241758242e-05,
      "loss": 0.6936,
      "step": 199500
    },
    {
      "epoch": 3.133437990580848,
      "grad_norm": 4.028990268707275,
      "learning_rate": 4.804160125588697e-05,
      "loss": 0.6795,
      "step": 199600
    },
    {
      "epoch": 3.1350078492935634,
      "grad_norm": 3.742753505706787,
      "learning_rate": 4.8040620094191524e-05,
      "loss": 0.6743,
      "step": 199700
    },
    {
      "epoch": 3.1365777080062793,
      "grad_norm": 3.610783576965332,
      "learning_rate": 4.8039638932496075e-05,
      "loss": 0.6985,
      "step": 199800
    },
    {
      "epoch": 3.1381475667189953,
      "grad_norm": 4.233772277832031,
      "learning_rate": 4.803865777080063e-05,
      "loss": 0.6733,
      "step": 199900
    },
    {
      "epoch": 3.1397174254317113,
      "grad_norm": 2.945734977722168,
      "learning_rate": 4.8037676609105184e-05,
      "loss": 0.6827,
      "step": 200000
    },
    {
      "epoch": 3.141287284144427,
      "grad_norm": 5.29433536529541,
      "learning_rate": 4.8036695447409735e-05,
      "loss": 0.7433,
      "step": 200100
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 4.121285438537598,
      "learning_rate": 4.803571428571429e-05,
      "loss": 0.6692,
      "step": 200200
    },
    {
      "epoch": 3.1444270015698588,
      "grad_norm": 3.728273630142212,
      "learning_rate": 4.803473312401884e-05,
      "loss": 0.678,
      "step": 200300
    },
    {
      "epoch": 3.1459968602825747,
      "grad_norm": 4.28792667388916,
      "learning_rate": 4.8033751962323395e-05,
      "loss": 0.7181,
      "step": 200400
    },
    {
      "epoch": 3.1475667189952903,
      "grad_norm": 4.240950107574463,
      "learning_rate": 4.8032770800627946e-05,
      "loss": 0.6872,
      "step": 200500
    },
    {
      "epoch": 3.1491365777080063,
      "grad_norm": 1.7603801488876343,
      "learning_rate": 4.80317896389325e-05,
      "loss": 0.7246,
      "step": 200600
    },
    {
      "epoch": 3.1507064364207222,
      "grad_norm": 3.916630744934082,
      "learning_rate": 4.803080847723705e-05,
      "loss": 0.6756,
      "step": 200700
    },
    {
      "epoch": 3.152276295133438,
      "grad_norm": 4.644760608673096,
      "learning_rate": 4.8029827315541605e-05,
      "loss": 0.6958,
      "step": 200800
    },
    {
      "epoch": 3.1538461538461537,
      "grad_norm": 4.041586399078369,
      "learning_rate": 4.8028846153846156e-05,
      "loss": 0.7007,
      "step": 200900
    },
    {
      "epoch": 3.1554160125588697,
      "grad_norm": 3.3801541328430176,
      "learning_rate": 4.802786499215071e-05,
      "loss": 0.6621,
      "step": 201000
    },
    {
      "epoch": 3.1569858712715857,
      "grad_norm": 3.590944766998291,
      "learning_rate": 4.802688383045526e-05,
      "loss": 0.7149,
      "step": 201100
    },
    {
      "epoch": 3.1585557299843012,
      "grad_norm": 3.5210964679718018,
      "learning_rate": 4.8025902668759816e-05,
      "loss": 0.6693,
      "step": 201200
    },
    {
      "epoch": 3.160125588697017,
      "grad_norm": 5.1150803565979,
      "learning_rate": 4.802492150706437e-05,
      "loss": 0.6833,
      "step": 201300
    },
    {
      "epoch": 3.161695447409733,
      "grad_norm": 4.974294185638428,
      "learning_rate": 4.802394034536892e-05,
      "loss": 0.7117,
      "step": 201400
    },
    {
      "epoch": 3.163265306122449,
      "grad_norm": 3.479217290878296,
      "learning_rate": 4.802295918367347e-05,
      "loss": 0.7112,
      "step": 201500
    },
    {
      "epoch": 3.1648351648351647,
      "grad_norm": 3.958580493927002,
      "learning_rate": 4.802197802197803e-05,
      "loss": 0.6884,
      "step": 201600
    },
    {
      "epoch": 3.1664050235478807,
      "grad_norm": 2.168565034866333,
      "learning_rate": 4.802099686028258e-05,
      "loss": 0.6827,
      "step": 201700
    },
    {
      "epoch": 3.1679748822605966,
      "grad_norm": 4.35714864730835,
      "learning_rate": 4.802001569858713e-05,
      "loss": 0.6781,
      "step": 201800
    },
    {
      "epoch": 3.169544740973312,
      "grad_norm": 3.5007264614105225,
      "learning_rate": 4.801903453689168e-05,
      "loss": 0.6946,
      "step": 201900
    },
    {
      "epoch": 3.171114599686028,
      "grad_norm": 3.506655693054199,
      "learning_rate": 4.801805337519624e-05,
      "loss": 0.6827,
      "step": 202000
    },
    {
      "epoch": 3.172684458398744,
      "grad_norm": 4.814579486846924,
      "learning_rate": 4.801707221350079e-05,
      "loss": 0.7191,
      "step": 202100
    },
    {
      "epoch": 3.17425431711146,
      "grad_norm": 4.352530479431152,
      "learning_rate": 4.801609105180534e-05,
      "loss": 0.6564,
      "step": 202200
    },
    {
      "epoch": 3.1758241758241756,
      "grad_norm": 4.045085906982422,
      "learning_rate": 4.80151098901099e-05,
      "loss": 0.6838,
      "step": 202300
    },
    {
      "epoch": 3.1773940345368916,
      "grad_norm": 4.142557621002197,
      "learning_rate": 4.801412872841444e-05,
      "loss": 0.6861,
      "step": 202400
    },
    {
      "epoch": 3.1789638932496076,
      "grad_norm": 4.297815799713135,
      "learning_rate": 4.8013147566719e-05,
      "loss": 0.7215,
      "step": 202500
    },
    {
      "epoch": 3.1805337519623236,
      "grad_norm": 4.233547687530518,
      "learning_rate": 4.801216640502355e-05,
      "loss": 0.6834,
      "step": 202600
    },
    {
      "epoch": 3.182103610675039,
      "grad_norm": 3.703941822052002,
      "learning_rate": 4.801118524332811e-05,
      "loss": 0.7552,
      "step": 202700
    },
    {
      "epoch": 3.183673469387755,
      "grad_norm": 4.448319435119629,
      "learning_rate": 4.801020408163265e-05,
      "loss": 0.7093,
      "step": 202800
    },
    {
      "epoch": 3.185243328100471,
      "grad_norm": 3.0585689544677734,
      "learning_rate": 4.800922291993721e-05,
      "loss": 0.6855,
      "step": 202900
    },
    {
      "epoch": 3.186813186813187,
      "grad_norm": 5.047140121459961,
      "learning_rate": 4.800824175824176e-05,
      "loss": 0.7288,
      "step": 203000
    },
    {
      "epoch": 3.1883830455259026,
      "grad_norm": 4.426783561706543,
      "learning_rate": 4.800726059654631e-05,
      "loss": 0.6942,
      "step": 203100
    },
    {
      "epoch": 3.1899529042386185,
      "grad_norm": 4.042941093444824,
      "learning_rate": 4.800627943485086e-05,
      "loss": 0.6615,
      "step": 203200
    },
    {
      "epoch": 3.1915227629513345,
      "grad_norm": 3.144592761993408,
      "learning_rate": 4.800529827315542e-05,
      "loss": 0.7257,
      "step": 203300
    },
    {
      "epoch": 3.19309262166405,
      "grad_norm": 4.587233543395996,
      "learning_rate": 4.800431711145997e-05,
      "loss": 0.6901,
      "step": 203400
    },
    {
      "epoch": 3.194662480376766,
      "grad_norm": 3.180276870727539,
      "learning_rate": 4.800333594976452e-05,
      "loss": 0.7006,
      "step": 203500
    },
    {
      "epoch": 3.196232339089482,
      "grad_norm": 4.277712821960449,
      "learning_rate": 4.8002354788069073e-05,
      "loss": 0.7052,
      "step": 203600
    },
    {
      "epoch": 3.197802197802198,
      "grad_norm": 3.4610350131988525,
      "learning_rate": 4.800137362637363e-05,
      "loss": 0.694,
      "step": 203700
    },
    {
      "epoch": 3.1993720565149135,
      "grad_norm": 4.866978645324707,
      "learning_rate": 4.800039246467818e-05,
      "loss": 0.7201,
      "step": 203800
    },
    {
      "epoch": 3.2009419152276295,
      "grad_norm": 4.112037181854248,
      "learning_rate": 4.799941130298273e-05,
      "loss": 0.6579,
      "step": 203900
    },
    {
      "epoch": 3.2025117739403455,
      "grad_norm": 3.9884297847747803,
      "learning_rate": 4.7998430141287284e-05,
      "loss": 0.6672,
      "step": 204000
    },
    {
      "epoch": 3.204081632653061,
      "grad_norm": 3.899305582046509,
      "learning_rate": 4.799744897959184e-05,
      "loss": 0.6582,
      "step": 204100
    },
    {
      "epoch": 3.205651491365777,
      "grad_norm": 4.406505584716797,
      "learning_rate": 4.799646781789639e-05,
      "loss": 0.6617,
      "step": 204200
    },
    {
      "epoch": 3.207221350078493,
      "grad_norm": 3.3862626552581787,
      "learning_rate": 4.7995486656200944e-05,
      "loss": 0.6739,
      "step": 204300
    },
    {
      "epoch": 3.208791208791209,
      "grad_norm": 3.3353068828582764,
      "learning_rate": 4.79945054945055e-05,
      "loss": 0.6757,
      "step": 204400
    },
    {
      "epoch": 3.2103610675039245,
      "grad_norm": 4.722851753234863,
      "learning_rate": 4.7993524332810046e-05,
      "loss": 0.6944,
      "step": 204500
    },
    {
      "epoch": 3.2119309262166404,
      "grad_norm": 4.014890193939209,
      "learning_rate": 4.7992543171114604e-05,
      "loss": 0.6906,
      "step": 204600
    },
    {
      "epoch": 3.2135007849293564,
      "grad_norm": 4.624723434448242,
      "learning_rate": 4.7991562009419155e-05,
      "loss": 0.7162,
      "step": 204700
    },
    {
      "epoch": 3.2150706436420724,
      "grad_norm": 4.185469150543213,
      "learning_rate": 4.799058084772371e-05,
      "loss": 0.6533,
      "step": 204800
    },
    {
      "epoch": 3.216640502354788,
      "grad_norm": 4.550415515899658,
      "learning_rate": 4.7989599686028257e-05,
      "loss": 0.6856,
      "step": 204900
    },
    {
      "epoch": 3.218210361067504,
      "grad_norm": 4.314126491546631,
      "learning_rate": 4.7988618524332814e-05,
      "loss": 0.7202,
      "step": 205000
    },
    {
      "epoch": 3.21978021978022,
      "grad_norm": 4.165397644042969,
      "learning_rate": 4.7987637362637365e-05,
      "loss": 0.6755,
      "step": 205100
    },
    {
      "epoch": 3.221350078492936,
      "grad_norm": 3.8716561794281006,
      "learning_rate": 4.7986656200941916e-05,
      "loss": 0.6768,
      "step": 205200
    },
    {
      "epoch": 3.2229199372056514,
      "grad_norm": 3.298008680343628,
      "learning_rate": 4.798567503924647e-05,
      "loss": 0.701,
      "step": 205300
    },
    {
      "epoch": 3.2244897959183674,
      "grad_norm": 2.4203097820281982,
      "learning_rate": 4.7984693877551025e-05,
      "loss": 0.6837,
      "step": 205400
    },
    {
      "epoch": 3.2260596546310834,
      "grad_norm": 4.65772819519043,
      "learning_rate": 4.7983712715855576e-05,
      "loss": 0.7372,
      "step": 205500
    },
    {
      "epoch": 3.227629513343799,
      "grad_norm": 4.161471366882324,
      "learning_rate": 4.798273155416013e-05,
      "loss": 0.7234,
      "step": 205600
    },
    {
      "epoch": 3.229199372056515,
      "grad_norm": 4.539589881896973,
      "learning_rate": 4.798175039246468e-05,
      "loss": 0.7093,
      "step": 205700
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 3.7908334732055664,
      "learning_rate": 4.7980769230769236e-05,
      "loss": 0.6636,
      "step": 205800
    },
    {
      "epoch": 3.232339089481947,
      "grad_norm": 3.6337897777557373,
      "learning_rate": 4.797978806907379e-05,
      "loss": 0.7323,
      "step": 205900
    },
    {
      "epoch": 3.2339089481946623,
      "grad_norm": 4.8512187004089355,
      "learning_rate": 4.797880690737834e-05,
      "loss": 0.6851,
      "step": 206000
    },
    {
      "epoch": 3.2354788069073783,
      "grad_norm": 3.5798540115356445,
      "learning_rate": 4.797782574568289e-05,
      "loss": 0.6764,
      "step": 206100
    },
    {
      "epoch": 3.2370486656200943,
      "grad_norm": 4.529168128967285,
      "learning_rate": 4.7976844583987446e-05,
      "loss": 0.6864,
      "step": 206200
    },
    {
      "epoch": 3.23861852433281,
      "grad_norm": 3.407867431640625,
      "learning_rate": 4.7975863422292e-05,
      "loss": 0.6727,
      "step": 206300
    },
    {
      "epoch": 3.240188383045526,
      "grad_norm": 3.8792498111724854,
      "learning_rate": 4.797488226059655e-05,
      "loss": 0.6912,
      "step": 206400
    },
    {
      "epoch": 3.241758241758242,
      "grad_norm": 4.287820339202881,
      "learning_rate": 4.7973901098901106e-05,
      "loss": 0.7242,
      "step": 206500
    },
    {
      "epoch": 3.2433281004709578,
      "grad_norm": 4.516058921813965,
      "learning_rate": 4.797291993720565e-05,
      "loss": 0.6773,
      "step": 206600
    },
    {
      "epoch": 3.2448979591836733,
      "grad_norm": 5.598888397216797,
      "learning_rate": 4.797193877551021e-05,
      "loss": 0.7085,
      "step": 206700
    },
    {
      "epoch": 3.2464678178963893,
      "grad_norm": 3.829332113265991,
      "learning_rate": 4.797095761381476e-05,
      "loss": 0.7217,
      "step": 206800
    },
    {
      "epoch": 3.2480376766091053,
      "grad_norm": 3.6929235458374023,
      "learning_rate": 4.796997645211932e-05,
      "loss": 0.7418,
      "step": 206900
    },
    {
      "epoch": 3.2496075353218212,
      "grad_norm": 3.827141761779785,
      "learning_rate": 4.796899529042386e-05,
      "loss": 0.7054,
      "step": 207000
    },
    {
      "epoch": 3.2511773940345368,
      "grad_norm": 4.752179145812988,
      "learning_rate": 4.796801412872842e-05,
      "loss": 0.6527,
      "step": 207100
    },
    {
      "epoch": 3.2527472527472527,
      "grad_norm": 4.0149407386779785,
      "learning_rate": 4.796703296703297e-05,
      "loss": 0.6662,
      "step": 207200
    },
    {
      "epoch": 3.2543171114599687,
      "grad_norm": 3.7168831825256348,
      "learning_rate": 4.796605180533752e-05,
      "loss": 0.6768,
      "step": 207300
    },
    {
      "epoch": 3.2558869701726847,
      "grad_norm": 4.445194721221924,
      "learning_rate": 4.796507064364207e-05,
      "loss": 0.6823,
      "step": 207400
    },
    {
      "epoch": 3.2574568288854002,
      "grad_norm": 3.8249340057373047,
      "learning_rate": 4.796408948194663e-05,
      "loss": 0.6786,
      "step": 207500
    },
    {
      "epoch": 3.259026687598116,
      "grad_norm": 3.947772264480591,
      "learning_rate": 4.796310832025118e-05,
      "loss": 0.6862,
      "step": 207600
    },
    {
      "epoch": 3.260596546310832,
      "grad_norm": 4.658844947814941,
      "learning_rate": 4.796212715855573e-05,
      "loss": 0.7042,
      "step": 207700
    },
    {
      "epoch": 3.2621664050235477,
      "grad_norm": 4.675922870635986,
      "learning_rate": 4.796114599686028e-05,
      "loss": 0.7274,
      "step": 207800
    },
    {
      "epoch": 3.2637362637362637,
      "grad_norm": 4.683229446411133,
      "learning_rate": 4.796016483516484e-05,
      "loss": 0.6947,
      "step": 207900
    },
    {
      "epoch": 3.2653061224489797,
      "grad_norm": 4.6421637535095215,
      "learning_rate": 4.795918367346939e-05,
      "loss": 0.7024,
      "step": 208000
    },
    {
      "epoch": 3.2668759811616956,
      "grad_norm": 3.9234297275543213,
      "learning_rate": 4.795820251177394e-05,
      "loss": 0.6919,
      "step": 208100
    },
    {
      "epoch": 3.268445839874411,
      "grad_norm": 3.621086835861206,
      "learning_rate": 4.795722135007849e-05,
      "loss": 0.6882,
      "step": 208200
    },
    {
      "epoch": 3.270015698587127,
      "grad_norm": 4.140910625457764,
      "learning_rate": 4.795624018838305e-05,
      "loss": 0.7273,
      "step": 208300
    },
    {
      "epoch": 3.271585557299843,
      "grad_norm": 3.713202476501465,
      "learning_rate": 4.79552590266876e-05,
      "loss": 0.6807,
      "step": 208400
    },
    {
      "epoch": 3.2731554160125587,
      "grad_norm": 4.478989601135254,
      "learning_rate": 4.795427786499215e-05,
      "loss": 0.7195,
      "step": 208500
    },
    {
      "epoch": 3.2747252747252746,
      "grad_norm": 3.440244197845459,
      "learning_rate": 4.795329670329671e-05,
      "loss": 0.7019,
      "step": 208600
    },
    {
      "epoch": 3.2762951334379906,
      "grad_norm": 3.7175703048706055,
      "learning_rate": 4.7952315541601255e-05,
      "loss": 0.7243,
      "step": 208700
    },
    {
      "epoch": 3.2778649921507066,
      "grad_norm": 3.8766286373138428,
      "learning_rate": 4.795133437990581e-05,
      "loss": 0.6636,
      "step": 208800
    },
    {
      "epoch": 3.279434850863422,
      "grad_norm": 3.752645492553711,
      "learning_rate": 4.7950353218210364e-05,
      "loss": 0.7051,
      "step": 208900
    },
    {
      "epoch": 3.281004709576138,
      "grad_norm": 4.268343448638916,
      "learning_rate": 4.794937205651492e-05,
      "loss": 0.6731,
      "step": 209000
    },
    {
      "epoch": 3.282574568288854,
      "grad_norm": 3.1846981048583984,
      "learning_rate": 4.7948390894819466e-05,
      "loss": 0.6893,
      "step": 209100
    },
    {
      "epoch": 3.2841444270015696,
      "grad_norm": 3.3723132610321045,
      "learning_rate": 4.794740973312402e-05,
      "loss": 0.6638,
      "step": 209200
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 5.220042705535889,
      "learning_rate": 4.7946428571428574e-05,
      "loss": 0.6689,
      "step": 209300
    },
    {
      "epoch": 3.2872841444270016,
      "grad_norm": 3.189896583557129,
      "learning_rate": 4.7945447409733125e-05,
      "loss": 0.6857,
      "step": 209400
    },
    {
      "epoch": 3.2888540031397175,
      "grad_norm": 4.353214740753174,
      "learning_rate": 4.7944466248037676e-05,
      "loss": 0.6846,
      "step": 209500
    },
    {
      "epoch": 3.2904238618524335,
      "grad_norm": 4.10971212387085,
      "learning_rate": 4.7943485086342234e-05,
      "loss": 0.7096,
      "step": 209600
    },
    {
      "epoch": 3.291993720565149,
      "grad_norm": 4.271107196807861,
      "learning_rate": 4.7942503924646785e-05,
      "loss": 0.6879,
      "step": 209700
    },
    {
      "epoch": 3.293563579277865,
      "grad_norm": 4.210685729980469,
      "learning_rate": 4.7941522762951336e-05,
      "loss": 0.661,
      "step": 209800
    },
    {
      "epoch": 3.295133437990581,
      "grad_norm": 3.923715353012085,
      "learning_rate": 4.794054160125589e-05,
      "loss": 0.7078,
      "step": 209900
    },
    {
      "epoch": 3.2967032967032965,
      "grad_norm": 4.020313739776611,
      "learning_rate": 4.7939560439560445e-05,
      "loss": 0.7165,
      "step": 210000
    },
    {
      "epoch": 3.2982731554160125,
      "grad_norm": 4.225745677947998,
      "learning_rate": 4.793857927786499e-05,
      "loss": 0.6874,
      "step": 210100
    },
    {
      "epoch": 3.2998430141287285,
      "grad_norm": 4.681971073150635,
      "learning_rate": 4.793759811616955e-05,
      "loss": 0.7163,
      "step": 210200
    },
    {
      "epoch": 3.3014128728414445,
      "grad_norm": 3.437471389770508,
      "learning_rate": 4.79366169544741e-05,
      "loss": 0.7276,
      "step": 210300
    },
    {
      "epoch": 3.30298273155416,
      "grad_norm": 4.211538791656494,
      "learning_rate": 4.7935635792778655e-05,
      "loss": 0.7093,
      "step": 210400
    },
    {
      "epoch": 3.304552590266876,
      "grad_norm": 5.091557502746582,
      "learning_rate": 4.7934654631083206e-05,
      "loss": 0.7048,
      "step": 210500
    },
    {
      "epoch": 3.306122448979592,
      "grad_norm": 5.010733127593994,
      "learning_rate": 4.793367346938776e-05,
      "loss": 0.6884,
      "step": 210600
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 4.32296085357666,
      "learning_rate": 4.7932692307692315e-05,
      "loss": 0.6637,
      "step": 210700
    },
    {
      "epoch": 3.3092621664050235,
      "grad_norm": 4.515056133270264,
      "learning_rate": 4.793171114599686e-05,
      "loss": 0.7145,
      "step": 210800
    },
    {
      "epoch": 3.3108320251177394,
      "grad_norm": 5.155807971954346,
      "learning_rate": 4.793072998430142e-05,
      "loss": 0.6936,
      "step": 210900
    },
    {
      "epoch": 3.3124018838304554,
      "grad_norm": 4.6141767501831055,
      "learning_rate": 4.792974882260597e-05,
      "loss": 0.6592,
      "step": 211000
    },
    {
      "epoch": 3.313971742543171,
      "grad_norm": 4.496230602264404,
      "learning_rate": 4.7928767660910526e-05,
      "loss": 0.6669,
      "step": 211100
    },
    {
      "epoch": 3.315541601255887,
      "grad_norm": 4.348483085632324,
      "learning_rate": 4.792778649921507e-05,
      "loss": 0.6477,
      "step": 211200
    },
    {
      "epoch": 3.317111459968603,
      "grad_norm": 5.601354122161865,
      "learning_rate": 4.792680533751963e-05,
      "loss": 0.685,
      "step": 211300
    },
    {
      "epoch": 3.3186813186813184,
      "grad_norm": 4.245335102081299,
      "learning_rate": 4.792582417582418e-05,
      "loss": 0.6756,
      "step": 211400
    },
    {
      "epoch": 3.3202511773940344,
      "grad_norm": 4.281673431396484,
      "learning_rate": 4.792484301412873e-05,
      "loss": 0.7156,
      "step": 211500
    },
    {
      "epoch": 3.3218210361067504,
      "grad_norm": 4.328022480010986,
      "learning_rate": 4.792386185243328e-05,
      "loss": 0.7177,
      "step": 211600
    },
    {
      "epoch": 3.3233908948194664,
      "grad_norm": 4.581658840179443,
      "learning_rate": 4.792288069073784e-05,
      "loss": 0.7127,
      "step": 211700
    },
    {
      "epoch": 3.3249607535321823,
      "grad_norm": 4.764644622802734,
      "learning_rate": 4.792189952904239e-05,
      "loss": 0.6722,
      "step": 211800
    },
    {
      "epoch": 3.326530612244898,
      "grad_norm": 4.687392234802246,
      "learning_rate": 4.792091836734694e-05,
      "loss": 0.6658,
      "step": 211900
    },
    {
      "epoch": 3.328100470957614,
      "grad_norm": 3.6554338932037354,
      "learning_rate": 4.791993720565149e-05,
      "loss": 0.6978,
      "step": 212000
    },
    {
      "epoch": 3.32967032967033,
      "grad_norm": 3.8632256984710693,
      "learning_rate": 4.791895604395605e-05,
      "loss": 0.6907,
      "step": 212100
    },
    {
      "epoch": 3.3312401883830454,
      "grad_norm": 5.13900089263916,
      "learning_rate": 4.791797488226059e-05,
      "loss": 0.6785,
      "step": 212200
    },
    {
      "epoch": 3.3328100470957613,
      "grad_norm": 3.2365853786468506,
      "learning_rate": 4.791699372056515e-05,
      "loss": 0.6813,
      "step": 212300
    },
    {
      "epoch": 3.3343799058084773,
      "grad_norm": 2.7702016830444336,
      "learning_rate": 4.79160125588697e-05,
      "loss": 0.7148,
      "step": 212400
    },
    {
      "epoch": 3.3359497645211933,
      "grad_norm": 4.037420749664307,
      "learning_rate": 4.791503139717426e-05,
      "loss": 0.69,
      "step": 212500
    },
    {
      "epoch": 3.337519623233909,
      "grad_norm": 3.3965201377868652,
      "learning_rate": 4.791405023547881e-05,
      "loss": 0.7144,
      "step": 212600
    },
    {
      "epoch": 3.339089481946625,
      "grad_norm": 4.543107509613037,
      "learning_rate": 4.791306907378336e-05,
      "loss": 0.7132,
      "step": 212700
    },
    {
      "epoch": 3.340659340659341,
      "grad_norm": 5.4719462394714355,
      "learning_rate": 4.791208791208792e-05,
      "loss": 0.7072,
      "step": 212800
    },
    {
      "epoch": 3.3422291993720563,
      "grad_norm": 3.376004219055176,
      "learning_rate": 4.7911106750392464e-05,
      "loss": 0.6842,
      "step": 212900
    },
    {
      "epoch": 3.3437990580847723,
      "grad_norm": 3.1474273204803467,
      "learning_rate": 4.791012558869702e-05,
      "loss": 0.6728,
      "step": 213000
    },
    {
      "epoch": 3.3453689167974883,
      "grad_norm": 4.199090003967285,
      "learning_rate": 4.790914442700157e-05,
      "loss": 0.6841,
      "step": 213100
    },
    {
      "epoch": 3.3469387755102042,
      "grad_norm": 4.764530658721924,
      "learning_rate": 4.790816326530613e-05,
      "loss": 0.7355,
      "step": 213200
    },
    {
      "epoch": 3.3485086342229198,
      "grad_norm": 3.754364252090454,
      "learning_rate": 4.7907182103610674e-05,
      "loss": 0.7061,
      "step": 213300
    },
    {
      "epoch": 3.3500784929356358,
      "grad_norm": 4.947940826416016,
      "learning_rate": 4.790620094191523e-05,
      "loss": 0.6885,
      "step": 213400
    },
    {
      "epoch": 3.3516483516483517,
      "grad_norm": 4.064205169677734,
      "learning_rate": 4.790521978021978e-05,
      "loss": 0.7233,
      "step": 213500
    },
    {
      "epoch": 3.3532182103610673,
      "grad_norm": 4.785403728485107,
      "learning_rate": 4.7904238618524334e-05,
      "loss": 0.6538,
      "step": 213600
    },
    {
      "epoch": 3.3547880690737832,
      "grad_norm": 3.754124641418457,
      "learning_rate": 4.7903257456828885e-05,
      "loss": 0.6499,
      "step": 213700
    },
    {
      "epoch": 3.356357927786499,
      "grad_norm": 4.371249198913574,
      "learning_rate": 4.790227629513344e-05,
      "loss": 0.7351,
      "step": 213800
    },
    {
      "epoch": 3.357927786499215,
      "grad_norm": 3.9318368434906006,
      "learning_rate": 4.7901295133437994e-05,
      "loss": 0.6859,
      "step": 213900
    },
    {
      "epoch": 3.359497645211931,
      "grad_norm": 3.1976985931396484,
      "learning_rate": 4.7900313971742545e-05,
      "loss": 0.7111,
      "step": 214000
    },
    {
      "epoch": 3.3610675039246467,
      "grad_norm": 4.110292434692383,
      "learning_rate": 4.7899332810047096e-05,
      "loss": 0.697,
      "step": 214100
    },
    {
      "epoch": 3.3626373626373627,
      "grad_norm": 3.89520263671875,
      "learning_rate": 4.7898351648351654e-05,
      "loss": 0.7149,
      "step": 214200
    },
    {
      "epoch": 3.3642072213500787,
      "grad_norm": 3.2158169746398926,
      "learning_rate": 4.78973704866562e-05,
      "loss": 0.7096,
      "step": 214300
    },
    {
      "epoch": 3.365777080062794,
      "grad_norm": 4.737054347991943,
      "learning_rate": 4.7896389324960756e-05,
      "loss": 0.7091,
      "step": 214400
    },
    {
      "epoch": 3.36734693877551,
      "grad_norm": 4.150075435638428,
      "learning_rate": 4.7895408163265307e-05,
      "loss": 0.698,
      "step": 214500
    },
    {
      "epoch": 3.368916797488226,
      "grad_norm": 3.9457950592041016,
      "learning_rate": 4.7894427001569864e-05,
      "loss": 0.7064,
      "step": 214600
    },
    {
      "epoch": 3.370486656200942,
      "grad_norm": 3.36194109916687,
      "learning_rate": 4.7893445839874415e-05,
      "loss": 0.6415,
      "step": 214700
    },
    {
      "epoch": 3.3720565149136577,
      "grad_norm": 4.351467609405518,
      "learning_rate": 4.7892464678178966e-05,
      "loss": 0.6149,
      "step": 214800
    },
    {
      "epoch": 3.3736263736263736,
      "grad_norm": 2.8573358058929443,
      "learning_rate": 4.7891483516483524e-05,
      "loss": 0.7263,
      "step": 214900
    },
    {
      "epoch": 3.3751962323390896,
      "grad_norm": 3.2432382106781006,
      "learning_rate": 4.789050235478807e-05,
      "loss": 0.6951,
      "step": 215000
    },
    {
      "epoch": 3.376766091051805,
      "grad_norm": 3.7690887451171875,
      "learning_rate": 4.7889521193092626e-05,
      "loss": 0.7165,
      "step": 215100
    },
    {
      "epoch": 3.378335949764521,
      "grad_norm": 4.808919429779053,
      "learning_rate": 4.788854003139718e-05,
      "loss": 0.7447,
      "step": 215200
    },
    {
      "epoch": 3.379905808477237,
      "grad_norm": 4.2885966300964355,
      "learning_rate": 4.7887558869701735e-05,
      "loss": 0.7066,
      "step": 215300
    },
    {
      "epoch": 3.381475667189953,
      "grad_norm": 3.9289674758911133,
      "learning_rate": 4.788657770800628e-05,
      "loss": 0.6429,
      "step": 215400
    },
    {
      "epoch": 3.3830455259026686,
      "grad_norm": 3.097151517868042,
      "learning_rate": 4.788559654631084e-05,
      "loss": 0.7303,
      "step": 215500
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 4.196166038513184,
      "learning_rate": 4.788461538461539e-05,
      "loss": 0.7164,
      "step": 215600
    },
    {
      "epoch": 3.3861852433281006,
      "grad_norm": 3.04011869430542,
      "learning_rate": 4.788363422291994e-05,
      "loss": 0.7128,
      "step": 215700
    },
    {
      "epoch": 3.387755102040816,
      "grad_norm": 3.5771305561065674,
      "learning_rate": 4.788265306122449e-05,
      "loss": 0.7247,
      "step": 215800
    },
    {
      "epoch": 3.389324960753532,
      "grad_norm": 4.702757835388184,
      "learning_rate": 4.788167189952905e-05,
      "loss": 0.7061,
      "step": 215900
    },
    {
      "epoch": 3.390894819466248,
      "grad_norm": 3.7025606632232666,
      "learning_rate": 4.78806907378336e-05,
      "loss": 0.7289,
      "step": 216000
    },
    {
      "epoch": 3.392464678178964,
      "grad_norm": 4.367710590362549,
      "learning_rate": 4.787970957613815e-05,
      "loss": 0.6685,
      "step": 216100
    },
    {
      "epoch": 3.39403453689168,
      "grad_norm": 3.556223154067993,
      "learning_rate": 4.78787284144427e-05,
      "loss": 0.6271,
      "step": 216200
    },
    {
      "epoch": 3.3956043956043955,
      "grad_norm": 4.174251079559326,
      "learning_rate": 4.787774725274726e-05,
      "loss": 0.7309,
      "step": 216300
    },
    {
      "epoch": 3.3971742543171115,
      "grad_norm": 4.420127868652344,
      "learning_rate": 4.78767660910518e-05,
      "loss": 0.6452,
      "step": 216400
    },
    {
      "epoch": 3.3987441130298275,
      "grad_norm": 3.642625570297241,
      "learning_rate": 4.787578492935636e-05,
      "loss": 0.6877,
      "step": 216500
    },
    {
      "epoch": 3.400313971742543,
      "grad_norm": 5.428676128387451,
      "learning_rate": 4.787480376766091e-05,
      "loss": 0.6714,
      "step": 216600
    },
    {
      "epoch": 3.401883830455259,
      "grad_norm": 5.189693927764893,
      "learning_rate": 4.787382260596546e-05,
      "loss": 0.714,
      "step": 216700
    },
    {
      "epoch": 3.403453689167975,
      "grad_norm": 3.232487440109253,
      "learning_rate": 4.787284144427002e-05,
      "loss": 0.6583,
      "step": 216800
    },
    {
      "epoch": 3.405023547880691,
      "grad_norm": 3.3971803188323975,
      "learning_rate": 4.787186028257457e-05,
      "loss": 0.6893,
      "step": 216900
    },
    {
      "epoch": 3.4065934065934065,
      "grad_norm": 3.3735454082489014,
      "learning_rate": 4.787087912087913e-05,
      "loss": 0.6808,
      "step": 217000
    },
    {
      "epoch": 3.4081632653061225,
      "grad_norm": 3.4411613941192627,
      "learning_rate": 4.786989795918367e-05,
      "loss": 0.691,
      "step": 217100
    },
    {
      "epoch": 3.4097331240188384,
      "grad_norm": 3.45717453956604,
      "learning_rate": 4.786891679748823e-05,
      "loss": 0.7252,
      "step": 217200
    },
    {
      "epoch": 3.411302982731554,
      "grad_norm": 4.157934188842773,
      "learning_rate": 4.786793563579278e-05,
      "loss": 0.6548,
      "step": 217300
    },
    {
      "epoch": 3.41287284144427,
      "grad_norm": 3.795490264892578,
      "learning_rate": 4.786695447409733e-05,
      "loss": 0.6885,
      "step": 217400
    },
    {
      "epoch": 3.414442700156986,
      "grad_norm": 3.751377820968628,
      "learning_rate": 4.7865973312401883e-05,
      "loss": 0.6683,
      "step": 217500
    },
    {
      "epoch": 3.416012558869702,
      "grad_norm": 4.472237586975098,
      "learning_rate": 4.786499215070644e-05,
      "loss": 0.6281,
      "step": 217600
    },
    {
      "epoch": 3.4175824175824174,
      "grad_norm": 4.188502311706543,
      "learning_rate": 4.786401098901099e-05,
      "loss": 0.6671,
      "step": 217700
    },
    {
      "epoch": 3.4191522762951334,
      "grad_norm": 4.575891971588135,
      "learning_rate": 4.786302982731554e-05,
      "loss": 0.6993,
      "step": 217800
    },
    {
      "epoch": 3.4207221350078494,
      "grad_norm": 3.7932653427124023,
      "learning_rate": 4.7862048665620094e-05,
      "loss": 0.7152,
      "step": 217900
    },
    {
      "epoch": 3.422291993720565,
      "grad_norm": 3.166539430618286,
      "learning_rate": 4.786106750392465e-05,
      "loss": 0.6952,
      "step": 218000
    },
    {
      "epoch": 3.423861852433281,
      "grad_norm": 3.8792901039123535,
      "learning_rate": 4.7860086342229196e-05,
      "loss": 0.7045,
      "step": 218100
    },
    {
      "epoch": 3.425431711145997,
      "grad_norm": 4.04907751083374,
      "learning_rate": 4.7859105180533754e-05,
      "loss": 0.6597,
      "step": 218200
    },
    {
      "epoch": 3.427001569858713,
      "grad_norm": 4.456523895263672,
      "learning_rate": 4.7858124018838305e-05,
      "loss": 0.7004,
      "step": 218300
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 3.1542158126831055,
      "learning_rate": 4.785714285714286e-05,
      "loss": 0.6995,
      "step": 218400
    },
    {
      "epoch": 3.4301412872841444,
      "grad_norm": 4.3637566566467285,
      "learning_rate": 4.785616169544741e-05,
      "loss": 0.6709,
      "step": 218500
    },
    {
      "epoch": 3.4317111459968603,
      "grad_norm": 5.240835189819336,
      "learning_rate": 4.7855180533751965e-05,
      "loss": 0.6862,
      "step": 218600
    },
    {
      "epoch": 3.4332810047095763,
      "grad_norm": 4.625594615936279,
      "learning_rate": 4.7854199372056516e-05,
      "loss": 0.6803,
      "step": 218700
    },
    {
      "epoch": 3.434850863422292,
      "grad_norm": 4.581073760986328,
      "learning_rate": 4.7853218210361067e-05,
      "loss": 0.6761,
      "step": 218800
    },
    {
      "epoch": 3.436420722135008,
      "grad_norm": 4.826159954071045,
      "learning_rate": 4.7852237048665624e-05,
      "loss": 0.6726,
      "step": 218900
    },
    {
      "epoch": 3.437990580847724,
      "grad_norm": 3.513436794281006,
      "learning_rate": 4.7851255886970175e-05,
      "loss": 0.7429,
      "step": 219000
    },
    {
      "epoch": 3.4395604395604398,
      "grad_norm": 3.750181198120117,
      "learning_rate": 4.7850274725274726e-05,
      "loss": 0.6928,
      "step": 219100
    },
    {
      "epoch": 3.4411302982731553,
      "grad_norm": 4.87345027923584,
      "learning_rate": 4.784929356357928e-05,
      "loss": 0.6901,
      "step": 219200
    },
    {
      "epoch": 3.4427001569858713,
      "grad_norm": 3.2250845432281494,
      "learning_rate": 4.7848312401883835e-05,
      "loss": 0.725,
      "step": 219300
    },
    {
      "epoch": 3.4442700156985873,
      "grad_norm": 4.136922359466553,
      "learning_rate": 4.7847331240188386e-05,
      "loss": 0.7143,
      "step": 219400
    },
    {
      "epoch": 3.445839874411303,
      "grad_norm": 3.780350923538208,
      "learning_rate": 4.784635007849294e-05,
      "loss": 0.7209,
      "step": 219500
    },
    {
      "epoch": 3.4474097331240188,
      "grad_norm": 3.39133882522583,
      "learning_rate": 4.784536891679749e-05,
      "loss": 0.6522,
      "step": 219600
    },
    {
      "epoch": 3.4489795918367347,
      "grad_norm": 4.381126880645752,
      "learning_rate": 4.7844387755102046e-05,
      "loss": 0.6763,
      "step": 219700
    },
    {
      "epoch": 3.4505494505494507,
      "grad_norm": 3.650371789932251,
      "learning_rate": 4.78434065934066e-05,
      "loss": 0.6732,
      "step": 219800
    },
    {
      "epoch": 3.4521193092621663,
      "grad_norm": 3.326218843460083,
      "learning_rate": 4.784242543171115e-05,
      "loss": 0.6802,
      "step": 219900
    },
    {
      "epoch": 3.4536891679748822,
      "grad_norm": 3.868427276611328,
      "learning_rate": 4.78414442700157e-05,
      "loss": 0.6873,
      "step": 220000
    },
    {
      "epoch": 3.455259026687598,
      "grad_norm": 4.239687442779541,
      "learning_rate": 4.7840463108320256e-05,
      "loss": 0.7201,
      "step": 220100
    },
    {
      "epoch": 3.4568288854003137,
      "grad_norm": 4.317376136779785,
      "learning_rate": 4.78394819466248e-05,
      "loss": 0.7038,
      "step": 220200
    },
    {
      "epoch": 3.4583987441130297,
      "grad_norm": 3.7604024410247803,
      "learning_rate": 4.783850078492936e-05,
      "loss": 0.6727,
      "step": 220300
    },
    {
      "epoch": 3.4599686028257457,
      "grad_norm": 3.353102684020996,
      "learning_rate": 4.783751962323391e-05,
      "loss": 0.6855,
      "step": 220400
    },
    {
      "epoch": 3.4615384615384617,
      "grad_norm": 3.5242061614990234,
      "learning_rate": 4.783653846153847e-05,
      "loss": 0.7129,
      "step": 220500
    },
    {
      "epoch": 3.463108320251177,
      "grad_norm": 4.2592620849609375,
      "learning_rate": 4.783555729984301e-05,
      "loss": 0.7084,
      "step": 220600
    },
    {
      "epoch": 3.464678178963893,
      "grad_norm": 4.1770710945129395,
      "learning_rate": 4.783457613814757e-05,
      "loss": 0.7151,
      "step": 220700
    },
    {
      "epoch": 3.466248037676609,
      "grad_norm": 4.07164192199707,
      "learning_rate": 4.783359497645212e-05,
      "loss": 0.7071,
      "step": 220800
    },
    {
      "epoch": 3.467817896389325,
      "grad_norm": 4.1988701820373535,
      "learning_rate": 4.783261381475667e-05,
      "loss": 0.675,
      "step": 220900
    },
    {
      "epoch": 3.4693877551020407,
      "grad_norm": 3.699130058288574,
      "learning_rate": 4.783163265306123e-05,
      "loss": 0.7108,
      "step": 221000
    },
    {
      "epoch": 3.4709576138147566,
      "grad_norm": 3.7885079383850098,
      "learning_rate": 4.783065149136578e-05,
      "loss": 0.7283,
      "step": 221100
    },
    {
      "epoch": 3.4725274725274726,
      "grad_norm": 3.844547986984253,
      "learning_rate": 4.782967032967033e-05,
      "loss": 0.6808,
      "step": 221200
    },
    {
      "epoch": 3.4740973312401886,
      "grad_norm": 2.4538497924804688,
      "learning_rate": 4.782868916797488e-05,
      "loss": 0.6753,
      "step": 221300
    },
    {
      "epoch": 3.475667189952904,
      "grad_norm": 3.0913333892822266,
      "learning_rate": 4.782770800627944e-05,
      "loss": 0.695,
      "step": 221400
    },
    {
      "epoch": 3.47723704866562,
      "grad_norm": 3.6291677951812744,
      "learning_rate": 4.782672684458399e-05,
      "loss": 0.699,
      "step": 221500
    },
    {
      "epoch": 3.478806907378336,
      "grad_norm": 4.483377933502197,
      "learning_rate": 4.782574568288854e-05,
      "loss": 0.6893,
      "step": 221600
    },
    {
      "epoch": 3.4803767660910516,
      "grad_norm": 5.081173896789551,
      "learning_rate": 4.782476452119309e-05,
      "loss": 0.713,
      "step": 221700
    },
    {
      "epoch": 3.4819466248037676,
      "grad_norm": 4.147609710693359,
      "learning_rate": 4.782378335949765e-05,
      "loss": 0.682,
      "step": 221800
    },
    {
      "epoch": 3.4835164835164836,
      "grad_norm": 3.539647102355957,
      "learning_rate": 4.78228021978022e-05,
      "loss": 0.6691,
      "step": 221900
    },
    {
      "epoch": 3.4850863422291996,
      "grad_norm": 4.393199920654297,
      "learning_rate": 4.782182103610675e-05,
      "loss": 0.6852,
      "step": 222000
    },
    {
      "epoch": 3.486656200941915,
      "grad_norm": 3.583676338195801,
      "learning_rate": 4.78208398744113e-05,
      "loss": 0.7076,
      "step": 222100
    },
    {
      "epoch": 3.488226059654631,
      "grad_norm": 4.680447578430176,
      "learning_rate": 4.781985871271586e-05,
      "loss": 0.7011,
      "step": 222200
    },
    {
      "epoch": 3.489795918367347,
      "grad_norm": 3.827868700027466,
      "learning_rate": 4.7818877551020405e-05,
      "loss": 0.7378,
      "step": 222300
    },
    {
      "epoch": 3.4913657770800626,
      "grad_norm": 4.2758965492248535,
      "learning_rate": 4.781789638932496e-05,
      "loss": 0.6846,
      "step": 222400
    },
    {
      "epoch": 3.4929356357927785,
      "grad_norm": 4.014399528503418,
      "learning_rate": 4.7816915227629514e-05,
      "loss": 0.7046,
      "step": 222500
    },
    {
      "epoch": 3.4945054945054945,
      "grad_norm": 4.686532974243164,
      "learning_rate": 4.781593406593407e-05,
      "loss": 0.6516,
      "step": 222600
    },
    {
      "epoch": 3.4960753532182105,
      "grad_norm": 4.494096279144287,
      "learning_rate": 4.7814952904238616e-05,
      "loss": 0.6939,
      "step": 222700
    },
    {
      "epoch": 3.497645211930926,
      "grad_norm": 4.035592555999756,
      "learning_rate": 4.7813971742543174e-05,
      "loss": 0.6798,
      "step": 222800
    },
    {
      "epoch": 3.499215070643642,
      "grad_norm": 4.727065086364746,
      "learning_rate": 4.7812990580847725e-05,
      "loss": 0.7183,
      "step": 222900
    },
    {
      "epoch": 3.500784929356358,
      "grad_norm": 3.82006573677063,
      "learning_rate": 4.7812009419152276e-05,
      "loss": 0.7034,
      "step": 223000
    },
    {
      "epoch": 3.5023547880690735,
      "grad_norm": 4.3005523681640625,
      "learning_rate": 4.781102825745683e-05,
      "loss": 0.6604,
      "step": 223100
    },
    {
      "epoch": 3.5039246467817895,
      "grad_norm": 5.038918495178223,
      "learning_rate": 4.7810047095761384e-05,
      "loss": 0.6404,
      "step": 223200
    },
    {
      "epoch": 3.5054945054945055,
      "grad_norm": 3.178699493408203,
      "learning_rate": 4.7809065934065935e-05,
      "loss": 0.7197,
      "step": 223300
    },
    {
      "epoch": 3.5070643642072215,
      "grad_norm": 4.969612121582031,
      "learning_rate": 4.7808084772370486e-05,
      "loss": 0.7406,
      "step": 223400
    },
    {
      "epoch": 3.5086342229199374,
      "grad_norm": 3.1650569438934326,
      "learning_rate": 4.7807103610675044e-05,
      "loss": 0.6993,
      "step": 223500
    },
    {
      "epoch": 3.510204081632653,
      "grad_norm": 3.771756410598755,
      "learning_rate": 4.7806122448979595e-05,
      "loss": 0.6768,
      "step": 223600
    },
    {
      "epoch": 3.511773940345369,
      "grad_norm": 5.390294075012207,
      "learning_rate": 4.7805141287284146e-05,
      "loss": 0.7157,
      "step": 223700
    },
    {
      "epoch": 3.513343799058085,
      "grad_norm": 4.750877380371094,
      "learning_rate": 4.78041601255887e-05,
      "loss": 0.6989,
      "step": 223800
    },
    {
      "epoch": 3.5149136577708004,
      "grad_norm": 5.250213623046875,
      "learning_rate": 4.7803178963893255e-05,
      "loss": 0.7198,
      "step": 223900
    },
    {
      "epoch": 3.5164835164835164,
      "grad_norm": 3.5712902545928955,
      "learning_rate": 4.7802197802197806e-05,
      "loss": 0.6884,
      "step": 224000
    },
    {
      "epoch": 3.5180533751962324,
      "grad_norm": 4.280002593994141,
      "learning_rate": 4.780121664050236e-05,
      "loss": 0.6971,
      "step": 224100
    },
    {
      "epoch": 3.5196232339089484,
      "grad_norm": 3.6416797637939453,
      "learning_rate": 4.780023547880691e-05,
      "loss": 0.6588,
      "step": 224200
    },
    {
      "epoch": 3.521193092621664,
      "grad_norm": 3.098189115524292,
      "learning_rate": 4.7799254317111465e-05,
      "loss": 0.6795,
      "step": 224300
    },
    {
      "epoch": 3.52276295133438,
      "grad_norm": 3.952065944671631,
      "learning_rate": 4.779827315541601e-05,
      "loss": 0.6909,
      "step": 224400
    },
    {
      "epoch": 3.524332810047096,
      "grad_norm": 3.67317271232605,
      "learning_rate": 4.779729199372057e-05,
      "loss": 0.6958,
      "step": 224500
    },
    {
      "epoch": 3.5259026687598114,
      "grad_norm": 4.301740646362305,
      "learning_rate": 4.779631083202512e-05,
      "loss": 0.6898,
      "step": 224600
    },
    {
      "epoch": 3.5274725274725274,
      "grad_norm": 3.5784852504730225,
      "learning_rate": 4.7795329670329676e-05,
      "loss": 0.689,
      "step": 224700
    },
    {
      "epoch": 3.5290423861852434,
      "grad_norm": 4.056456565856934,
      "learning_rate": 4.779434850863422e-05,
      "loss": 0.6916,
      "step": 224800
    },
    {
      "epoch": 3.5306122448979593,
      "grad_norm": 4.657114505767822,
      "learning_rate": 4.779336734693878e-05,
      "loss": 0.718,
      "step": 224900
    },
    {
      "epoch": 3.5321821036106753,
      "grad_norm": 4.878819465637207,
      "learning_rate": 4.779238618524333e-05,
      "loss": 0.6807,
      "step": 225000
    },
    {
      "epoch": 3.533751962323391,
      "grad_norm": 3.4901607036590576,
      "learning_rate": 4.779140502354788e-05,
      "loss": 0.6428,
      "step": 225100
    },
    {
      "epoch": 3.535321821036107,
      "grad_norm": 4.174561977386475,
      "learning_rate": 4.779042386185244e-05,
      "loss": 0.7172,
      "step": 225200
    },
    {
      "epoch": 3.5368916797488223,
      "grad_norm": 4.050912857055664,
      "learning_rate": 4.778944270015699e-05,
      "loss": 0.6814,
      "step": 225300
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 3.897526741027832,
      "learning_rate": 4.778846153846154e-05,
      "loss": 0.7199,
      "step": 225400
    },
    {
      "epoch": 3.5400313971742543,
      "grad_norm": 4.464590549468994,
      "learning_rate": 4.778748037676609e-05,
      "loss": 0.6961,
      "step": 225500
    },
    {
      "epoch": 3.5416012558869703,
      "grad_norm": 4.531529903411865,
      "learning_rate": 4.778649921507065e-05,
      "loss": 0.75,
      "step": 225600
    },
    {
      "epoch": 3.5431711145996863,
      "grad_norm": 3.463355779647827,
      "learning_rate": 4.77855180533752e-05,
      "loss": 0.6417,
      "step": 225700
    },
    {
      "epoch": 3.544740973312402,
      "grad_norm": 3.89846134185791,
      "learning_rate": 4.778453689167975e-05,
      "loss": 0.6603,
      "step": 225800
    },
    {
      "epoch": 3.5463108320251178,
      "grad_norm": 4.1738810539245605,
      "learning_rate": 4.77835557299843e-05,
      "loss": 0.7059,
      "step": 225900
    },
    {
      "epoch": 3.5478806907378337,
      "grad_norm": 4.637024402618408,
      "learning_rate": 4.778257456828886e-05,
      "loss": 0.7015,
      "step": 226000
    },
    {
      "epoch": 3.5494505494505493,
      "grad_norm": 4.783644199371338,
      "learning_rate": 4.778159340659341e-05,
      "loss": 0.6697,
      "step": 226100
    },
    {
      "epoch": 3.5510204081632653,
      "grad_norm": 2.692171335220337,
      "learning_rate": 4.778061224489796e-05,
      "loss": 0.6688,
      "step": 226200
    },
    {
      "epoch": 3.5525902668759812,
      "grad_norm": 4.346765041351318,
      "learning_rate": 4.777963108320251e-05,
      "loss": 0.709,
      "step": 226300
    },
    {
      "epoch": 3.554160125588697,
      "grad_norm": 4.247864246368408,
      "learning_rate": 4.777864992150707e-05,
      "loss": 0.6463,
      "step": 226400
    },
    {
      "epoch": 3.5557299843014127,
      "grad_norm": 3.809809923171997,
      "learning_rate": 4.7777668759811614e-05,
      "loss": 0.6825,
      "step": 226500
    },
    {
      "epoch": 3.5572998430141287,
      "grad_norm": 3.634667158126831,
      "learning_rate": 4.777668759811617e-05,
      "loss": 0.6734,
      "step": 226600
    },
    {
      "epoch": 3.5588697017268447,
      "grad_norm": 5.29538106918335,
      "learning_rate": 4.777570643642072e-05,
      "loss": 0.6759,
      "step": 226700
    },
    {
      "epoch": 3.5604395604395602,
      "grad_norm": 4.3552398681640625,
      "learning_rate": 4.777472527472528e-05,
      "loss": 0.6932,
      "step": 226800
    },
    {
      "epoch": 3.562009419152276,
      "grad_norm": 4.861806392669678,
      "learning_rate": 4.7773744113029825e-05,
      "loss": 0.7036,
      "step": 226900
    },
    {
      "epoch": 3.563579277864992,
      "grad_norm": 3.913529634475708,
      "learning_rate": 4.777276295133438e-05,
      "loss": 0.7102,
      "step": 227000
    },
    {
      "epoch": 3.565149136577708,
      "grad_norm": 4.876319885253906,
      "learning_rate": 4.7771781789638934e-05,
      "loss": 0.7103,
      "step": 227100
    },
    {
      "epoch": 3.566718995290424,
      "grad_norm": 4.102686405181885,
      "learning_rate": 4.7770800627943485e-05,
      "loss": 0.7109,
      "step": 227200
    },
    {
      "epoch": 3.5682888540031397,
      "grad_norm": 5.550441265106201,
      "learning_rate": 4.776981946624804e-05,
      "loss": 0.7192,
      "step": 227300
    },
    {
      "epoch": 3.5698587127158556,
      "grad_norm": 3.594783306121826,
      "learning_rate": 4.776883830455259e-05,
      "loss": 0.672,
      "step": 227400
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 3.4351115226745605,
      "learning_rate": 4.7767857142857144e-05,
      "loss": 0.7093,
      "step": 227500
    },
    {
      "epoch": 3.572998430141287,
      "grad_norm": 3.981306791305542,
      "learning_rate": 4.7766875981161695e-05,
      "loss": 0.6951,
      "step": 227600
    },
    {
      "epoch": 3.574568288854003,
      "grad_norm": 4.266631603240967,
      "learning_rate": 4.776589481946625e-05,
      "loss": 0.674,
      "step": 227700
    },
    {
      "epoch": 3.576138147566719,
      "grad_norm": 3.494598865509033,
      "learning_rate": 4.7764913657770804e-05,
      "loss": 0.6852,
      "step": 227800
    },
    {
      "epoch": 3.577708006279435,
      "grad_norm": 3.198509454727173,
      "learning_rate": 4.7763932496075355e-05,
      "loss": 0.655,
      "step": 227900
    },
    {
      "epoch": 3.5792778649921506,
      "grad_norm": 4.121933460235596,
      "learning_rate": 4.7762951334379906e-05,
      "loss": 0.7276,
      "step": 228000
    },
    {
      "epoch": 3.5808477237048666,
      "grad_norm": 3.9122860431671143,
      "learning_rate": 4.7761970172684464e-05,
      "loss": 0.6469,
      "step": 228100
    },
    {
      "epoch": 3.5824175824175826,
      "grad_norm": 4.032973289489746,
      "learning_rate": 4.7760989010989015e-05,
      "loss": 0.697,
      "step": 228200
    },
    {
      "epoch": 3.583987441130298,
      "grad_norm": 4.138460159301758,
      "learning_rate": 4.7760007849293566e-05,
      "loss": 0.727,
      "step": 228300
    },
    {
      "epoch": 3.585557299843014,
      "grad_norm": 4.039843559265137,
      "learning_rate": 4.775902668759812e-05,
      "loss": 0.6962,
      "step": 228400
    },
    {
      "epoch": 3.58712715855573,
      "grad_norm": 3.7112667560577393,
      "learning_rate": 4.7758045525902674e-05,
      "loss": 0.6879,
      "step": 228500
    },
    {
      "epoch": 3.588697017268446,
      "grad_norm": 4.366698265075684,
      "learning_rate": 4.775706436420722e-05,
      "loss": 0.6992,
      "step": 228600
    },
    {
      "epoch": 3.5902668759811616,
      "grad_norm": 5.600572109222412,
      "learning_rate": 4.7756083202511776e-05,
      "loss": 0.6342,
      "step": 228700
    },
    {
      "epoch": 3.5918367346938775,
      "grad_norm": 4.488298416137695,
      "learning_rate": 4.775510204081633e-05,
      "loss": 0.6787,
      "step": 228800
    },
    {
      "epoch": 3.5934065934065935,
      "grad_norm": 3.951507568359375,
      "learning_rate": 4.7754120879120885e-05,
      "loss": 0.727,
      "step": 228900
    },
    {
      "epoch": 3.594976452119309,
      "grad_norm": 3.9039535522460938,
      "learning_rate": 4.775313971742543e-05,
      "loss": 0.6919,
      "step": 229000
    },
    {
      "epoch": 3.596546310832025,
      "grad_norm": 3.4207828044891357,
      "learning_rate": 4.775215855572999e-05,
      "loss": 0.7181,
      "step": 229100
    },
    {
      "epoch": 3.598116169544741,
      "grad_norm": 4.585511684417725,
      "learning_rate": 4.775117739403454e-05,
      "loss": 0.6972,
      "step": 229200
    },
    {
      "epoch": 3.599686028257457,
      "grad_norm": 4.347285270690918,
      "learning_rate": 4.775019623233909e-05,
      "loss": 0.6967,
      "step": 229300
    },
    {
      "epoch": 3.601255886970173,
      "grad_norm": 4.319555759429932,
      "learning_rate": 4.774921507064365e-05,
      "loss": 0.7052,
      "step": 229400
    },
    {
      "epoch": 3.6028257456828885,
      "grad_norm": 4.9333953857421875,
      "learning_rate": 4.77482339089482e-05,
      "loss": 0.6841,
      "step": 229500
    },
    {
      "epoch": 3.6043956043956045,
      "grad_norm": 3.9019811153411865,
      "learning_rate": 4.774725274725275e-05,
      "loss": 0.7165,
      "step": 229600
    },
    {
      "epoch": 3.60596546310832,
      "grad_norm": 4.881191730499268,
      "learning_rate": 4.77462715855573e-05,
      "loss": 0.6712,
      "step": 229700
    },
    {
      "epoch": 3.607535321821036,
      "grad_norm": 3.478694438934326,
      "learning_rate": 4.774529042386186e-05,
      "loss": 0.6845,
      "step": 229800
    },
    {
      "epoch": 3.609105180533752,
      "grad_norm": 4.287099838256836,
      "learning_rate": 4.774430926216641e-05,
      "loss": 0.674,
      "step": 229900
    },
    {
      "epoch": 3.610675039246468,
      "grad_norm": 3.645693778991699,
      "learning_rate": 4.774332810047096e-05,
      "loss": 0.6772,
      "step": 230000
    },
    {
      "epoch": 3.612244897959184,
      "grad_norm": 3.8508198261260986,
      "learning_rate": 4.774234693877551e-05,
      "loss": 0.6924,
      "step": 230100
    },
    {
      "epoch": 3.6138147566718994,
      "grad_norm": 5.452381134033203,
      "learning_rate": 4.774136577708007e-05,
      "loss": 0.6858,
      "step": 230200
    },
    {
      "epoch": 3.6153846153846154,
      "grad_norm": 3.927577495574951,
      "learning_rate": 4.774038461538462e-05,
      "loss": 0.6627,
      "step": 230300
    },
    {
      "epoch": 3.6169544740973314,
      "grad_norm": 4.98906946182251,
      "learning_rate": 4.773940345368917e-05,
      "loss": 0.6897,
      "step": 230400
    },
    {
      "epoch": 3.618524332810047,
      "grad_norm": 5.066163063049316,
      "learning_rate": 4.773842229199372e-05,
      "loss": 0.6743,
      "step": 230500
    },
    {
      "epoch": 3.620094191522763,
      "grad_norm": 3.6577773094177246,
      "learning_rate": 4.773744113029828e-05,
      "loss": 0.7065,
      "step": 230600
    },
    {
      "epoch": 3.621664050235479,
      "grad_norm": 3.330843687057495,
      "learning_rate": 4.773645996860282e-05,
      "loss": 0.7205,
      "step": 230700
    },
    {
      "epoch": 3.623233908948195,
      "grad_norm": 3.9172463417053223,
      "learning_rate": 4.773547880690738e-05,
      "loss": 0.6709,
      "step": 230800
    },
    {
      "epoch": 3.6248037676609104,
      "grad_norm": 3.6632211208343506,
      "learning_rate": 4.773449764521193e-05,
      "loss": 0.695,
      "step": 230900
    },
    {
      "epoch": 3.6263736263736264,
      "grad_norm": 4.687806606292725,
      "learning_rate": 4.773351648351649e-05,
      "loss": 0.6895,
      "step": 231000
    },
    {
      "epoch": 3.6279434850863423,
      "grad_norm": 4.077116012573242,
      "learning_rate": 4.7732535321821034e-05,
      "loss": 0.6763,
      "step": 231100
    },
    {
      "epoch": 3.629513343799058,
      "grad_norm": 3.1819331645965576,
      "learning_rate": 4.773155416012559e-05,
      "loss": 0.6806,
      "step": 231200
    },
    {
      "epoch": 3.631083202511774,
      "grad_norm": 3.9189352989196777,
      "learning_rate": 4.773057299843014e-05,
      "loss": 0.6506,
      "step": 231300
    },
    {
      "epoch": 3.63265306122449,
      "grad_norm": 2.930842161178589,
      "learning_rate": 4.7729591836734693e-05,
      "loss": 0.6767,
      "step": 231400
    },
    {
      "epoch": 3.634222919937206,
      "grad_norm": 4.627021312713623,
      "learning_rate": 4.772861067503925e-05,
      "loss": 0.6897,
      "step": 231500
    },
    {
      "epoch": 3.6357927786499213,
      "grad_norm": 3.524167060852051,
      "learning_rate": 4.77276295133438e-05,
      "loss": 0.7288,
      "step": 231600
    },
    {
      "epoch": 3.6373626373626373,
      "grad_norm": 4.007374286651611,
      "learning_rate": 4.772664835164835e-05,
      "loss": 0.7242,
      "step": 231700
    },
    {
      "epoch": 3.6389324960753533,
      "grad_norm": 4.2353386878967285,
      "learning_rate": 4.7725667189952904e-05,
      "loss": 0.6497,
      "step": 231800
    },
    {
      "epoch": 3.640502354788069,
      "grad_norm": 4.398806095123291,
      "learning_rate": 4.772468602825746e-05,
      "loss": 0.671,
      "step": 231900
    },
    {
      "epoch": 3.642072213500785,
      "grad_norm": 5.020211219787598,
      "learning_rate": 4.772370486656201e-05,
      "loss": 0.6977,
      "step": 232000
    },
    {
      "epoch": 3.643642072213501,
      "grad_norm": 3.3754706382751465,
      "learning_rate": 4.7722723704866564e-05,
      "loss": 0.6927,
      "step": 232100
    },
    {
      "epoch": 3.6452119309262168,
      "grad_norm": 5.0176520347595215,
      "learning_rate": 4.7721742543171115e-05,
      "loss": 0.649,
      "step": 232200
    },
    {
      "epoch": 3.6467817896389327,
      "grad_norm": 2.8664467334747314,
      "learning_rate": 4.772076138147567e-05,
      "loss": 0.7343,
      "step": 232300
    },
    {
      "epoch": 3.6483516483516483,
      "grad_norm": 2.5946121215820312,
      "learning_rate": 4.7719780219780224e-05,
      "loss": 0.6917,
      "step": 232400
    },
    {
      "epoch": 3.6499215070643642,
      "grad_norm": 3.8393025398254395,
      "learning_rate": 4.7718799058084775e-05,
      "loss": 0.688,
      "step": 232500
    },
    {
      "epoch": 3.6514913657770802,
      "grad_norm": 4.6561360359191895,
      "learning_rate": 4.7717817896389326e-05,
      "loss": 0.6512,
      "step": 232600
    },
    {
      "epoch": 3.6530612244897958,
      "grad_norm": 3.3849096298217773,
      "learning_rate": 4.771683673469388e-05,
      "loss": 0.7291,
      "step": 232700
    },
    {
      "epoch": 3.6546310832025117,
      "grad_norm": 4.564155101776123,
      "learning_rate": 4.771585557299843e-05,
      "loss": 0.6863,
      "step": 232800
    },
    {
      "epoch": 3.6562009419152277,
      "grad_norm": 4.047068119049072,
      "learning_rate": 4.7714874411302985e-05,
      "loss": 0.7031,
      "step": 232900
    },
    {
      "epoch": 3.6577708006279437,
      "grad_norm": 3.4055898189544678,
      "learning_rate": 4.7713893249607536e-05,
      "loss": 0.655,
      "step": 233000
    },
    {
      "epoch": 3.659340659340659,
      "grad_norm": 4.955416202545166,
      "learning_rate": 4.7712912087912094e-05,
      "loss": 0.6801,
      "step": 233100
    },
    {
      "epoch": 3.660910518053375,
      "grad_norm": 2.9482765197753906,
      "learning_rate": 4.771193092621664e-05,
      "loss": 0.6815,
      "step": 233200
    },
    {
      "epoch": 3.662480376766091,
      "grad_norm": 3.6054627895355225,
      "learning_rate": 4.7710949764521196e-05,
      "loss": 0.6673,
      "step": 233300
    },
    {
      "epoch": 3.6640502354788067,
      "grad_norm": 4.300621509552002,
      "learning_rate": 4.770996860282575e-05,
      "loss": 0.7182,
      "step": 233400
    },
    {
      "epoch": 3.6656200941915227,
      "grad_norm": 4.10440731048584,
      "learning_rate": 4.77089874411303e-05,
      "loss": 0.6304,
      "step": 233500
    },
    {
      "epoch": 3.6671899529042387,
      "grad_norm": 4.010295391082764,
      "learning_rate": 4.7708006279434856e-05,
      "loss": 0.6892,
      "step": 233600
    },
    {
      "epoch": 3.6687598116169546,
      "grad_norm": 4.59210205078125,
      "learning_rate": 4.770702511773941e-05,
      "loss": 0.7083,
      "step": 233700
    },
    {
      "epoch": 3.67032967032967,
      "grad_norm": 3.043177366256714,
      "learning_rate": 4.770604395604396e-05,
      "loss": 0.696,
      "step": 233800
    },
    {
      "epoch": 3.671899529042386,
      "grad_norm": 4.246979236602783,
      "learning_rate": 4.770506279434851e-05,
      "loss": 0.6845,
      "step": 233900
    },
    {
      "epoch": 3.673469387755102,
      "grad_norm": 4.110363006591797,
      "learning_rate": 4.7704081632653066e-05,
      "loss": 0.6912,
      "step": 234000
    },
    {
      "epoch": 3.6750392464678177,
      "grad_norm": 3.593311309814453,
      "learning_rate": 4.770310047095762e-05,
      "loss": 0.6729,
      "step": 234100
    },
    {
      "epoch": 3.6766091051805336,
      "grad_norm": 4.459652900695801,
      "learning_rate": 4.770211930926217e-05,
      "loss": 0.6813,
      "step": 234200
    },
    {
      "epoch": 3.6781789638932496,
      "grad_norm": 4.6534743309021,
      "learning_rate": 4.770113814756672e-05,
      "loss": 0.6989,
      "step": 234300
    },
    {
      "epoch": 3.6797488226059656,
      "grad_norm": 4.4156904220581055,
      "learning_rate": 4.770015698587128e-05,
      "loss": 0.6377,
      "step": 234400
    },
    {
      "epoch": 3.6813186813186816,
      "grad_norm": 3.916687488555908,
      "learning_rate": 4.769917582417583e-05,
      "loss": 0.6609,
      "step": 234500
    },
    {
      "epoch": 3.682888540031397,
      "grad_norm": 3.103574275970459,
      "learning_rate": 4.769819466248038e-05,
      "loss": 0.7217,
      "step": 234600
    },
    {
      "epoch": 3.684458398744113,
      "grad_norm": 3.8520047664642334,
      "learning_rate": 4.769721350078493e-05,
      "loss": 0.6619,
      "step": 234700
    },
    {
      "epoch": 3.686028257456829,
      "grad_norm": 4.1017961502075195,
      "learning_rate": 4.769623233908949e-05,
      "loss": 0.6899,
      "step": 234800
    },
    {
      "epoch": 3.6875981161695446,
      "grad_norm": 4.562015056610107,
      "learning_rate": 4.769525117739403e-05,
      "loss": 0.6575,
      "step": 234900
    },
    {
      "epoch": 3.6891679748822606,
      "grad_norm": 4.485147953033447,
      "learning_rate": 4.769427001569859e-05,
      "loss": 0.6759,
      "step": 235000
    },
    {
      "epoch": 3.6907378335949765,
      "grad_norm": 3.787592649459839,
      "learning_rate": 4.769328885400314e-05,
      "loss": 0.6608,
      "step": 235100
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 3.3884575366973877,
      "learning_rate": 4.76923076923077e-05,
      "loss": 0.7036,
      "step": 235200
    },
    {
      "epoch": 3.693877551020408,
      "grad_norm": 4.249722003936768,
      "learning_rate": 4.769132653061224e-05,
      "loss": 0.6784,
      "step": 235300
    },
    {
      "epoch": 3.695447409733124,
      "grad_norm": 4.026219844818115,
      "learning_rate": 4.76903453689168e-05,
      "loss": 0.7074,
      "step": 235400
    },
    {
      "epoch": 3.69701726844584,
      "grad_norm": 4.867557525634766,
      "learning_rate": 4.768936420722135e-05,
      "loss": 0.6726,
      "step": 235500
    },
    {
      "epoch": 3.6985871271585555,
      "grad_norm": 3.1724276542663574,
      "learning_rate": 4.76883830455259e-05,
      "loss": 0.7221,
      "step": 235600
    },
    {
      "epoch": 3.7001569858712715,
      "grad_norm": 4.020410060882568,
      "learning_rate": 4.768740188383046e-05,
      "loss": 0.7122,
      "step": 235700
    },
    {
      "epoch": 3.7017268445839875,
      "grad_norm": 4.198667049407959,
      "learning_rate": 4.768642072213501e-05,
      "loss": 0.6918,
      "step": 235800
    },
    {
      "epoch": 3.7032967032967035,
      "grad_norm": 4.962991237640381,
      "learning_rate": 4.768543956043956e-05,
      "loss": 0.7115,
      "step": 235900
    },
    {
      "epoch": 3.704866562009419,
      "grad_norm": 3.8770546913146973,
      "learning_rate": 4.768445839874411e-05,
      "loss": 0.6788,
      "step": 236000
    },
    {
      "epoch": 3.706436420722135,
      "grad_norm": 2.8904082775115967,
      "learning_rate": 4.768347723704867e-05,
      "loss": 0.6679,
      "step": 236100
    },
    {
      "epoch": 3.708006279434851,
      "grad_norm": 3.2213547229766846,
      "learning_rate": 4.768249607535322e-05,
      "loss": 0.6623,
      "step": 236200
    },
    {
      "epoch": 3.7095761381475665,
      "grad_norm": 4.541619777679443,
      "learning_rate": 4.768151491365777e-05,
      "loss": 0.7051,
      "step": 236300
    },
    {
      "epoch": 3.7111459968602825,
      "grad_norm": 4.126660346984863,
      "learning_rate": 4.7680533751962324e-05,
      "loss": 0.7086,
      "step": 236400
    },
    {
      "epoch": 3.7127158555729984,
      "grad_norm": 4.0622639656066895,
      "learning_rate": 4.767955259026688e-05,
      "loss": 0.6891,
      "step": 236500
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 3.874770402908325,
      "learning_rate": 4.767857142857143e-05,
      "loss": 0.6592,
      "step": 236600
    },
    {
      "epoch": 3.7158555729984304,
      "grad_norm": 3.6094651222229004,
      "learning_rate": 4.7677590266875984e-05,
      "loss": 0.6623,
      "step": 236700
    },
    {
      "epoch": 3.717425431711146,
      "grad_norm": 3.6206774711608887,
      "learning_rate": 4.7676609105180535e-05,
      "loss": 0.6956,
      "step": 236800
    },
    {
      "epoch": 3.718995290423862,
      "grad_norm": 3.5374767780303955,
      "learning_rate": 4.767562794348509e-05,
      "loss": 0.6843,
      "step": 236900
    },
    {
      "epoch": 3.7205651491365774,
      "grad_norm": 3.8581113815307617,
      "learning_rate": 4.7674646781789637e-05,
      "loss": 0.6837,
      "step": 237000
    },
    {
      "epoch": 3.7221350078492934,
      "grad_norm": 4.745699882507324,
      "learning_rate": 4.7673665620094194e-05,
      "loss": 0.7215,
      "step": 237100
    },
    {
      "epoch": 3.7237048665620094,
      "grad_norm": 4.888535976409912,
      "learning_rate": 4.7672684458398745e-05,
      "loss": 0.695,
      "step": 237200
    },
    {
      "epoch": 3.7252747252747254,
      "grad_norm": 5.005124568939209,
      "learning_rate": 4.76717032967033e-05,
      "loss": 0.6879,
      "step": 237300
    },
    {
      "epoch": 3.7268445839874413,
      "grad_norm": 4.0859150886535645,
      "learning_rate": 4.767072213500785e-05,
      "loss": 0.6881,
      "step": 237400
    },
    {
      "epoch": 3.728414442700157,
      "grad_norm": 4.32421875,
      "learning_rate": 4.7669740973312405e-05,
      "loss": 0.6833,
      "step": 237500
    },
    {
      "epoch": 3.729984301412873,
      "grad_norm": 4.1114583015441895,
      "learning_rate": 4.7668759811616956e-05,
      "loss": 0.6933,
      "step": 237600
    },
    {
      "epoch": 3.731554160125589,
      "grad_norm": 3.0690104961395264,
      "learning_rate": 4.766777864992151e-05,
      "loss": 0.6802,
      "step": 237700
    },
    {
      "epoch": 3.7331240188383044,
      "grad_norm": 3.5142176151275635,
      "learning_rate": 4.7666797488226065e-05,
      "loss": 0.7081,
      "step": 237800
    },
    {
      "epoch": 3.7346938775510203,
      "grad_norm": 3.766014575958252,
      "learning_rate": 4.7665816326530616e-05,
      "loss": 0.6809,
      "step": 237900
    },
    {
      "epoch": 3.7362637362637363,
      "grad_norm": 3.3637101650238037,
      "learning_rate": 4.766483516483517e-05,
      "loss": 0.7017,
      "step": 238000
    },
    {
      "epoch": 3.7378335949764523,
      "grad_norm": 3.65096378326416,
      "learning_rate": 4.766385400313972e-05,
      "loss": 0.7295,
      "step": 238100
    },
    {
      "epoch": 3.739403453689168,
      "grad_norm": 3.908480405807495,
      "learning_rate": 4.7662872841444275e-05,
      "loss": 0.67,
      "step": 238200
    },
    {
      "epoch": 3.740973312401884,
      "grad_norm": 4.366663455963135,
      "learning_rate": 4.7661891679748826e-05,
      "loss": 0.7121,
      "step": 238300
    },
    {
      "epoch": 3.7425431711145998,
      "grad_norm": 3.1583683490753174,
      "learning_rate": 4.766091051805338e-05,
      "loss": 0.6818,
      "step": 238400
    },
    {
      "epoch": 3.7441130298273153,
      "grad_norm": 3.708124876022339,
      "learning_rate": 4.765992935635793e-05,
      "loss": 0.7106,
      "step": 238500
    },
    {
      "epoch": 3.7456828885400313,
      "grad_norm": 3.646629810333252,
      "learning_rate": 4.7658948194662486e-05,
      "loss": 0.6576,
      "step": 238600
    },
    {
      "epoch": 3.7472527472527473,
      "grad_norm": 4.795614719390869,
      "learning_rate": 4.765796703296704e-05,
      "loss": 0.7164,
      "step": 238700
    },
    {
      "epoch": 3.7488226059654632,
      "grad_norm": 3.6552939414978027,
      "learning_rate": 4.765698587127159e-05,
      "loss": 0.7223,
      "step": 238800
    },
    {
      "epoch": 3.750392464678179,
      "grad_norm": 4.855536937713623,
      "learning_rate": 4.765600470957614e-05,
      "loss": 0.6491,
      "step": 238900
    },
    {
      "epoch": 3.7519623233908947,
      "grad_norm": 4.335493087768555,
      "learning_rate": 4.76550235478807e-05,
      "loss": 0.7128,
      "step": 239000
    },
    {
      "epoch": 3.7535321821036107,
      "grad_norm": 4.0143938064575195,
      "learning_rate": 4.765404238618524e-05,
      "loss": 0.6843,
      "step": 239100
    },
    {
      "epoch": 3.7551020408163263,
      "grad_norm": 4.623118877410889,
      "learning_rate": 4.76530612244898e-05,
      "loss": 0.7198,
      "step": 239200
    },
    {
      "epoch": 3.7566718995290422,
      "grad_norm": 3.047985315322876,
      "learning_rate": 4.765208006279435e-05,
      "loss": 0.6861,
      "step": 239300
    },
    {
      "epoch": 3.758241758241758,
      "grad_norm": 4.324209690093994,
      "learning_rate": 4.76510989010989e-05,
      "loss": 0.68,
      "step": 239400
    },
    {
      "epoch": 3.759811616954474,
      "grad_norm": 3.979247570037842,
      "learning_rate": 4.765011773940345e-05,
      "loss": 0.7053,
      "step": 239500
    },
    {
      "epoch": 3.76138147566719,
      "grad_norm": 4.239807605743408,
      "learning_rate": 4.764913657770801e-05,
      "loss": 0.7021,
      "step": 239600
    },
    {
      "epoch": 3.7629513343799057,
      "grad_norm": 4.486297607421875,
      "learning_rate": 4.764815541601256e-05,
      "loss": 0.7113,
      "step": 239700
    },
    {
      "epoch": 3.7645211930926217,
      "grad_norm": 3.6892290115356445,
      "learning_rate": 4.764717425431711e-05,
      "loss": 0.7154,
      "step": 239800
    },
    {
      "epoch": 3.7660910518053377,
      "grad_norm": 4.043353080749512,
      "learning_rate": 4.764619309262167e-05,
      "loss": 0.6881,
      "step": 239900
    },
    {
      "epoch": 3.767660910518053,
      "grad_norm": 3.423354148864746,
      "learning_rate": 4.764521193092622e-05,
      "loss": 0.7022,
      "step": 240000
    },
    {
      "epoch": 3.769230769230769,
      "grad_norm": 4.679724216461182,
      "learning_rate": 4.764423076923077e-05,
      "loss": 0.7093,
      "step": 240100
    },
    {
      "epoch": 3.770800627943485,
      "grad_norm": 4.345253944396973,
      "learning_rate": 4.764324960753532e-05,
      "loss": 0.717,
      "step": 240200
    },
    {
      "epoch": 3.772370486656201,
      "grad_norm": 3.5818967819213867,
      "learning_rate": 4.764226844583988e-05,
      "loss": 0.6574,
      "step": 240300
    },
    {
      "epoch": 3.7739403453689166,
      "grad_norm": 3.95597505569458,
      "learning_rate": 4.764128728414443e-05,
      "loss": 0.6665,
      "step": 240400
    },
    {
      "epoch": 3.7755102040816326,
      "grad_norm": 4.388553142547607,
      "learning_rate": 4.764030612244898e-05,
      "loss": 0.7169,
      "step": 240500
    },
    {
      "epoch": 3.7770800627943486,
      "grad_norm": 3.6835851669311523,
      "learning_rate": 4.763932496075353e-05,
      "loss": 0.6849,
      "step": 240600
    },
    {
      "epoch": 3.778649921507064,
      "grad_norm": 4.331906795501709,
      "learning_rate": 4.763834379905809e-05,
      "loss": 0.6954,
      "step": 240700
    },
    {
      "epoch": 3.78021978021978,
      "grad_norm": 4.053897380828857,
      "learning_rate": 4.7637362637362635e-05,
      "loss": 0.6596,
      "step": 240800
    },
    {
      "epoch": 3.781789638932496,
      "grad_norm": 3.3056485652923584,
      "learning_rate": 4.763638147566719e-05,
      "loss": 0.7073,
      "step": 240900
    },
    {
      "epoch": 3.783359497645212,
      "grad_norm": 4.387648105621338,
      "learning_rate": 4.7635400313971744e-05,
      "loss": 0.7,
      "step": 241000
    },
    {
      "epoch": 3.784929356357928,
      "grad_norm": 3.6437828540802,
      "learning_rate": 4.76344191522763e-05,
      "loss": 0.688,
      "step": 241100
    },
    {
      "epoch": 3.7864992150706436,
      "grad_norm": 4.8755784034729,
      "learning_rate": 4.7633437990580846e-05,
      "loss": 0.6727,
      "step": 241200
    },
    {
      "epoch": 3.7880690737833596,
      "grad_norm": 4.5564703941345215,
      "learning_rate": 4.76324568288854e-05,
      "loss": 0.663,
      "step": 241300
    },
    {
      "epoch": 3.789638932496075,
      "grad_norm": 3.0793492794036865,
      "learning_rate": 4.7631475667189954e-05,
      "loss": 0.7089,
      "step": 241400
    },
    {
      "epoch": 3.791208791208791,
      "grad_norm": 3.3924546241760254,
      "learning_rate": 4.7630494505494505e-05,
      "loss": 0.6855,
      "step": 241500
    },
    {
      "epoch": 3.792778649921507,
      "grad_norm": 4.1591267585754395,
      "learning_rate": 4.7629513343799056e-05,
      "loss": 0.718,
      "step": 241600
    },
    {
      "epoch": 3.794348508634223,
      "grad_norm": 2.9173061847686768,
      "learning_rate": 4.7628532182103614e-05,
      "loss": 0.6441,
      "step": 241700
    },
    {
      "epoch": 3.795918367346939,
      "grad_norm": 4.191332817077637,
      "learning_rate": 4.7627551020408165e-05,
      "loss": 0.7053,
      "step": 241800
    },
    {
      "epoch": 3.7974882260596545,
      "grad_norm": 3.4156301021575928,
      "learning_rate": 4.7626569858712716e-05,
      "loss": 0.6942,
      "step": 241900
    },
    {
      "epoch": 3.7990580847723705,
      "grad_norm": 3.0482141971588135,
      "learning_rate": 4.7625588697017274e-05,
      "loss": 0.6933,
      "step": 242000
    },
    {
      "epoch": 3.8006279434850865,
      "grad_norm": 3.520942211151123,
      "learning_rate": 4.7624607535321825e-05,
      "loss": 0.6714,
      "step": 242100
    },
    {
      "epoch": 3.802197802197802,
      "grad_norm": 3.1074023246765137,
      "learning_rate": 4.7623626373626376e-05,
      "loss": 0.663,
      "step": 242200
    },
    {
      "epoch": 3.803767660910518,
      "grad_norm": 2.0947983264923096,
      "learning_rate": 4.762264521193093e-05,
      "loss": 0.7045,
      "step": 242300
    },
    {
      "epoch": 3.805337519623234,
      "grad_norm": 3.0715677738189697,
      "learning_rate": 4.7621664050235484e-05,
      "loss": 0.6793,
      "step": 242400
    },
    {
      "epoch": 3.80690737833595,
      "grad_norm": 3.716305732727051,
      "learning_rate": 4.7620682888540035e-05,
      "loss": 0.6864,
      "step": 242500
    },
    {
      "epoch": 3.8084772370486655,
      "grad_norm": 4.357000350952148,
      "learning_rate": 4.7619701726844586e-05,
      "loss": 0.7098,
      "step": 242600
    },
    {
      "epoch": 3.8100470957613815,
      "grad_norm": 5.171468734741211,
      "learning_rate": 4.761872056514914e-05,
      "loss": 0.6942,
      "step": 242700
    },
    {
      "epoch": 3.8116169544740974,
      "grad_norm": 4.014507293701172,
      "learning_rate": 4.7617739403453695e-05,
      "loss": 0.6759,
      "step": 242800
    },
    {
      "epoch": 3.813186813186813,
      "grad_norm": 3.9209797382354736,
      "learning_rate": 4.761675824175824e-05,
      "loss": 0.6855,
      "step": 242900
    },
    {
      "epoch": 3.814756671899529,
      "grad_norm": 4.227816104888916,
      "learning_rate": 4.76157770800628e-05,
      "loss": 0.6892,
      "step": 243000
    },
    {
      "epoch": 3.816326530612245,
      "grad_norm": 4.697353839874268,
      "learning_rate": 4.761479591836735e-05,
      "loss": 0.6793,
      "step": 243100
    },
    {
      "epoch": 3.817896389324961,
      "grad_norm": 4.495218276977539,
      "learning_rate": 4.7613814756671906e-05,
      "loss": 0.701,
      "step": 243200
    },
    {
      "epoch": 3.819466248037677,
      "grad_norm": 3.6297552585601807,
      "learning_rate": 4.761283359497645e-05,
      "loss": 0.653,
      "step": 243300
    },
    {
      "epoch": 3.8210361067503924,
      "grad_norm": 3.431245803833008,
      "learning_rate": 4.761185243328101e-05,
      "loss": 0.6621,
      "step": 243400
    },
    {
      "epoch": 3.8226059654631084,
      "grad_norm": 4.0326972007751465,
      "learning_rate": 4.761087127158556e-05,
      "loss": 0.7114,
      "step": 243500
    },
    {
      "epoch": 3.824175824175824,
      "grad_norm": 3.566188335418701,
      "learning_rate": 4.760989010989011e-05,
      "loss": 0.7614,
      "step": 243600
    },
    {
      "epoch": 3.82574568288854,
      "grad_norm": 4.695072174072266,
      "learning_rate": 4.760890894819466e-05,
      "loss": 0.6588,
      "step": 243700
    },
    {
      "epoch": 3.827315541601256,
      "grad_norm": 5.073031902313232,
      "learning_rate": 4.760792778649922e-05,
      "loss": 0.6973,
      "step": 243800
    },
    {
      "epoch": 3.828885400313972,
      "grad_norm": 4.2851338386535645,
      "learning_rate": 4.760694662480377e-05,
      "loss": 0.7062,
      "step": 243900
    },
    {
      "epoch": 3.830455259026688,
      "grad_norm": 4.1173200607299805,
      "learning_rate": 4.760596546310832e-05,
      "loss": 0.6646,
      "step": 244000
    },
    {
      "epoch": 3.8320251177394034,
      "grad_norm": 3.66601300239563,
      "learning_rate": 4.760498430141288e-05,
      "loss": 0.737,
      "step": 244100
    },
    {
      "epoch": 3.8335949764521193,
      "grad_norm": 4.204455375671387,
      "learning_rate": 4.760400313971743e-05,
      "loss": 0.6974,
      "step": 244200
    },
    {
      "epoch": 3.8351648351648353,
      "grad_norm": 3.8867499828338623,
      "learning_rate": 4.760302197802198e-05,
      "loss": 0.697,
      "step": 244300
    },
    {
      "epoch": 3.836734693877551,
      "grad_norm": 3.9635257720947266,
      "learning_rate": 4.760204081632653e-05,
      "loss": 0.7174,
      "step": 244400
    },
    {
      "epoch": 3.838304552590267,
      "grad_norm": 3.7982726097106934,
      "learning_rate": 4.760105965463109e-05,
      "loss": 0.7129,
      "step": 244500
    },
    {
      "epoch": 3.839874411302983,
      "grad_norm": 3.9761531352996826,
      "learning_rate": 4.760007849293564e-05,
      "loss": 0.6483,
      "step": 244600
    },
    {
      "epoch": 3.8414442700156988,
      "grad_norm": 3.6950788497924805,
      "learning_rate": 4.759909733124019e-05,
      "loss": 0.6764,
      "step": 244700
    },
    {
      "epoch": 3.8430141287284143,
      "grad_norm": 3.5429186820983887,
      "learning_rate": 4.759811616954474e-05,
      "loss": 0.715,
      "step": 244800
    },
    {
      "epoch": 3.8445839874411303,
      "grad_norm": 3.7545437812805176,
      "learning_rate": 4.75971350078493e-05,
      "loss": 0.6857,
      "step": 244900
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 2.4404137134552,
      "learning_rate": 4.7596153846153844e-05,
      "loss": 0.7747,
      "step": 245000
    },
    {
      "epoch": 3.847723704866562,
      "grad_norm": 4.129229545593262,
      "learning_rate": 4.75951726844584e-05,
      "loss": 0.6943,
      "step": 245100
    },
    {
      "epoch": 3.8492935635792778,
      "grad_norm": 4.377843856811523,
      "learning_rate": 4.759419152276295e-05,
      "loss": 0.6723,
      "step": 245200
    },
    {
      "epoch": 3.8508634222919937,
      "grad_norm": 4.82915735244751,
      "learning_rate": 4.759321036106751e-05,
      "loss": 0.6552,
      "step": 245300
    },
    {
      "epoch": 3.8524332810047097,
      "grad_norm": 4.065042972564697,
      "learning_rate": 4.7592229199372055e-05,
      "loss": 0.6835,
      "step": 245400
    },
    {
      "epoch": 3.8540031397174257,
      "grad_norm": 4.4250264167785645,
      "learning_rate": 4.759124803767661e-05,
      "loss": 0.6587,
      "step": 245500
    },
    {
      "epoch": 3.8555729984301412,
      "grad_norm": 4.673556804656982,
      "learning_rate": 4.759026687598116e-05,
      "loss": 0.7101,
      "step": 245600
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 3.7885959148406982,
      "learning_rate": 4.7589285714285714e-05,
      "loss": 0.7065,
      "step": 245700
    },
    {
      "epoch": 3.8587127158555727,
      "grad_norm": 3.7578866481781006,
      "learning_rate": 4.7588304552590265e-05,
      "loss": 0.7247,
      "step": 245800
    },
    {
      "epoch": 3.8602825745682887,
      "grad_norm": 3.660221815109253,
      "learning_rate": 4.758732339089482e-05,
      "loss": 0.6503,
      "step": 245900
    },
    {
      "epoch": 3.8618524332810047,
      "grad_norm": 4.642119407653809,
      "learning_rate": 4.7586342229199374e-05,
      "loss": 0.6862,
      "step": 246000
    },
    {
      "epoch": 3.8634222919937207,
      "grad_norm": 4.093353271484375,
      "learning_rate": 4.7585361067503925e-05,
      "loss": 0.739,
      "step": 246100
    },
    {
      "epoch": 3.8649921507064366,
      "grad_norm": 3.4898719787597656,
      "learning_rate": 4.758437990580848e-05,
      "loss": 0.6871,
      "step": 246200
    },
    {
      "epoch": 3.866562009419152,
      "grad_norm": 4.736448764801025,
      "learning_rate": 4.7583398744113034e-05,
      "loss": 0.692,
      "step": 246300
    },
    {
      "epoch": 3.868131868131868,
      "grad_norm": 3.366014003753662,
      "learning_rate": 4.7582417582417585e-05,
      "loss": 0.7187,
      "step": 246400
    },
    {
      "epoch": 3.869701726844584,
      "grad_norm": 3.687805652618408,
      "learning_rate": 4.7581436420722136e-05,
      "loss": 0.6997,
      "step": 246500
    },
    {
      "epoch": 3.8712715855572997,
      "grad_norm": 4.220821380615234,
      "learning_rate": 4.7580455259026693e-05,
      "loss": 0.6603,
      "step": 246600
    },
    {
      "epoch": 3.8728414442700156,
      "grad_norm": 3.4530739784240723,
      "learning_rate": 4.7579474097331244e-05,
      "loss": 0.7133,
      "step": 246700
    },
    {
      "epoch": 3.8744113029827316,
      "grad_norm": 3.483804225921631,
      "learning_rate": 4.7578492935635795e-05,
      "loss": 0.6866,
      "step": 246800
    },
    {
      "epoch": 3.8759811616954476,
      "grad_norm": 3.7649779319763184,
      "learning_rate": 4.7577511773940346e-05,
      "loss": 0.6669,
      "step": 246900
    },
    {
      "epoch": 3.877551020408163,
      "grad_norm": 5.206137657165527,
      "learning_rate": 4.7576530612244904e-05,
      "loss": 0.6928,
      "step": 247000
    },
    {
      "epoch": 3.879120879120879,
      "grad_norm": 3.409895658493042,
      "learning_rate": 4.757554945054945e-05,
      "loss": 0.6239,
      "step": 247100
    },
    {
      "epoch": 3.880690737833595,
      "grad_norm": 5.3336076736450195,
      "learning_rate": 4.7574568288854006e-05,
      "loss": 0.6421,
      "step": 247200
    },
    {
      "epoch": 3.8822605965463106,
      "grad_norm": 4.501804828643799,
      "learning_rate": 4.757358712715856e-05,
      "loss": 0.7134,
      "step": 247300
    },
    {
      "epoch": 3.8838304552590266,
      "grad_norm": 4.69303560256958,
      "learning_rate": 4.7572605965463115e-05,
      "loss": 0.7368,
      "step": 247400
    },
    {
      "epoch": 3.8854003139717426,
      "grad_norm": 2.5992839336395264,
      "learning_rate": 4.757162480376766e-05,
      "loss": 0.6617,
      "step": 247500
    },
    {
      "epoch": 3.8869701726844585,
      "grad_norm": 3.657256841659546,
      "learning_rate": 4.757064364207222e-05,
      "loss": 0.6535,
      "step": 247600
    },
    {
      "epoch": 3.8885400313971745,
      "grad_norm": 4.23308801651001,
      "learning_rate": 4.756966248037677e-05,
      "loss": 0.6667,
      "step": 247700
    },
    {
      "epoch": 3.89010989010989,
      "grad_norm": 4.582345962524414,
      "learning_rate": 4.756868131868132e-05,
      "loss": 0.7122,
      "step": 247800
    },
    {
      "epoch": 3.891679748822606,
      "grad_norm": 4.214420318603516,
      "learning_rate": 4.756770015698587e-05,
      "loss": 0.6414,
      "step": 247900
    },
    {
      "epoch": 3.8932496075353216,
      "grad_norm": 3.9537880420684814,
      "learning_rate": 4.756671899529043e-05,
      "loss": 0.673,
      "step": 248000
    },
    {
      "epoch": 3.8948194662480375,
      "grad_norm": 4.883286952972412,
      "learning_rate": 4.756573783359498e-05,
      "loss": 0.7172,
      "step": 248100
    },
    {
      "epoch": 3.8963893249607535,
      "grad_norm": 4.223415374755859,
      "learning_rate": 4.756475667189953e-05,
      "loss": 0.7381,
      "step": 248200
    },
    {
      "epoch": 3.8979591836734695,
      "grad_norm": 4.137406826019287,
      "learning_rate": 4.756377551020409e-05,
      "loss": 0.7407,
      "step": 248300
    },
    {
      "epoch": 3.8995290423861855,
      "grad_norm": 3.297464609146118,
      "learning_rate": 4.756279434850864e-05,
      "loss": 0.6495,
      "step": 248400
    },
    {
      "epoch": 3.901098901098901,
      "grad_norm": 3.9899330139160156,
      "learning_rate": 4.756181318681319e-05,
      "loss": 0.6488,
      "step": 248500
    },
    {
      "epoch": 3.902668759811617,
      "grad_norm": 3.642780065536499,
      "learning_rate": 4.756083202511774e-05,
      "loss": 0.6892,
      "step": 248600
    },
    {
      "epoch": 3.904238618524333,
      "grad_norm": 3.763911485671997,
      "learning_rate": 4.75598508634223e-05,
      "loss": 0.6832,
      "step": 248700
    },
    {
      "epoch": 3.9058084772370485,
      "grad_norm": 4.009809494018555,
      "learning_rate": 4.755886970172685e-05,
      "loss": 0.7325,
      "step": 248800
    },
    {
      "epoch": 3.9073783359497645,
      "grad_norm": 3.069417715072632,
      "learning_rate": 4.75578885400314e-05,
      "loss": 0.6556,
      "step": 248900
    },
    {
      "epoch": 3.9089481946624804,
      "grad_norm": 5.01071310043335,
      "learning_rate": 4.755690737833595e-05,
      "loss": 0.6872,
      "step": 249000
    },
    {
      "epoch": 3.9105180533751964,
      "grad_norm": 3.7713282108306885,
      "learning_rate": 4.755592621664051e-05,
      "loss": 0.6916,
      "step": 249100
    },
    {
      "epoch": 3.912087912087912,
      "grad_norm": 3.9430084228515625,
      "learning_rate": 4.755494505494505e-05,
      "loss": 0.648,
      "step": 249200
    },
    {
      "epoch": 3.913657770800628,
      "grad_norm": 4.471295356750488,
      "learning_rate": 4.755396389324961e-05,
      "loss": 0.6811,
      "step": 249300
    },
    {
      "epoch": 3.915227629513344,
      "grad_norm": 3.3953182697296143,
      "learning_rate": 4.755298273155416e-05,
      "loss": 0.6979,
      "step": 249400
    },
    {
      "epoch": 3.9167974882260594,
      "grad_norm": 4.397339820861816,
      "learning_rate": 4.755200156985872e-05,
      "loss": 0.6684,
      "step": 249500
    },
    {
      "epoch": 3.9183673469387754,
      "grad_norm": 4.414515495300293,
      "learning_rate": 4.7551020408163263e-05,
      "loss": 0.6749,
      "step": 249600
    },
    {
      "epoch": 3.9199372056514914,
      "grad_norm": 3.814574956893921,
      "learning_rate": 4.755003924646782e-05,
      "loss": 0.6944,
      "step": 249700
    },
    {
      "epoch": 3.9215070643642074,
      "grad_norm": 4.283221244812012,
      "learning_rate": 4.754905808477237e-05,
      "loss": 0.6448,
      "step": 249800
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 4.623457431793213,
      "learning_rate": 4.754807692307692e-05,
      "loss": 0.6378,
      "step": 249900
    },
    {
      "epoch": 3.924646781789639,
      "grad_norm": 4.400123119354248,
      "learning_rate": 4.7547095761381474e-05,
      "loss": 0.6846,
      "step": 250000
    },
    {
      "epoch": 3.926216640502355,
      "grad_norm": 3.1267006397247314,
      "learning_rate": 4.754611459968603e-05,
      "loss": 0.6886,
      "step": 250100
    },
    {
      "epoch": 3.9277864992150704,
      "grad_norm": 4.452840328216553,
      "learning_rate": 4.754513343799058e-05,
      "loss": 0.6901,
      "step": 250200
    },
    {
      "epoch": 3.9293563579277864,
      "grad_norm": 3.8437840938568115,
      "learning_rate": 4.7544152276295134e-05,
      "loss": 0.6593,
      "step": 250300
    },
    {
      "epoch": 3.9309262166405023,
      "grad_norm": 4.340795040130615,
      "learning_rate": 4.754317111459969e-05,
      "loss": 0.6999,
      "step": 250400
    },
    {
      "epoch": 3.9324960753532183,
      "grad_norm": 4.133960723876953,
      "learning_rate": 4.754218995290424e-05,
      "loss": 0.6515,
      "step": 250500
    },
    {
      "epoch": 3.9340659340659343,
      "grad_norm": 3.5292422771453857,
      "learning_rate": 4.7541208791208794e-05,
      "loss": 0.67,
      "step": 250600
    },
    {
      "epoch": 3.93563579277865,
      "grad_norm": 4.287811756134033,
      "learning_rate": 4.7540227629513345e-05,
      "loss": 0.663,
      "step": 250700
    },
    {
      "epoch": 3.937205651491366,
      "grad_norm": 4.127054691314697,
      "learning_rate": 4.75392464678179e-05,
      "loss": 0.7131,
      "step": 250800
    },
    {
      "epoch": 3.938775510204082,
      "grad_norm": 3.5598461627960205,
      "learning_rate": 4.753826530612245e-05,
      "loss": 0.6848,
      "step": 250900
    },
    {
      "epoch": 3.9403453689167973,
      "grad_norm": 3.218412160873413,
      "learning_rate": 4.7537284144427004e-05,
      "loss": 0.6733,
      "step": 251000
    },
    {
      "epoch": 3.9419152276295133,
      "grad_norm": 4.694291114807129,
      "learning_rate": 4.7536302982731555e-05,
      "loss": 0.7062,
      "step": 251100
    },
    {
      "epoch": 3.9434850863422293,
      "grad_norm": 4.6305365562438965,
      "learning_rate": 4.753532182103611e-05,
      "loss": 0.6671,
      "step": 251200
    },
    {
      "epoch": 3.9450549450549453,
      "grad_norm": 4.63388729095459,
      "learning_rate": 4.753434065934066e-05,
      "loss": 0.6705,
      "step": 251300
    },
    {
      "epoch": 3.946624803767661,
      "grad_norm": 3.868075370788574,
      "learning_rate": 4.7533359497645215e-05,
      "loss": 0.6621,
      "step": 251400
    },
    {
      "epoch": 3.9481946624803768,
      "grad_norm": 4.131197929382324,
      "learning_rate": 4.7532378335949766e-05,
      "loss": 0.6572,
      "step": 251500
    },
    {
      "epoch": 3.9497645211930927,
      "grad_norm": 4.215909004211426,
      "learning_rate": 4.7531397174254324e-05,
      "loss": 0.6697,
      "step": 251600
    },
    {
      "epoch": 3.9513343799058083,
      "grad_norm": 4.105564117431641,
      "learning_rate": 4.753041601255887e-05,
      "loss": 0.7085,
      "step": 251700
    },
    {
      "epoch": 3.9529042386185242,
      "grad_norm": 4.149998664855957,
      "learning_rate": 4.7529434850863426e-05,
      "loss": 0.7134,
      "step": 251800
    },
    {
      "epoch": 3.9544740973312402,
      "grad_norm": 4.087775707244873,
      "learning_rate": 4.752845368916798e-05,
      "loss": 0.6882,
      "step": 251900
    },
    {
      "epoch": 3.956043956043956,
      "grad_norm": 4.185553073883057,
      "learning_rate": 4.752747252747253e-05,
      "loss": 0.6797,
      "step": 252000
    },
    {
      "epoch": 3.957613814756672,
      "grad_norm": 3.6256632804870605,
      "learning_rate": 4.752649136577708e-05,
      "loss": 0.6954,
      "step": 252100
    },
    {
      "epoch": 3.9591836734693877,
      "grad_norm": 4.141051292419434,
      "learning_rate": 4.7525510204081636e-05,
      "loss": 0.6816,
      "step": 252200
    },
    {
      "epoch": 3.9607535321821037,
      "grad_norm": 4.059247016906738,
      "learning_rate": 4.752452904238619e-05,
      "loss": 0.6302,
      "step": 252300
    },
    {
      "epoch": 3.962323390894819,
      "grad_norm": 3.8980042934417725,
      "learning_rate": 4.752354788069074e-05,
      "loss": 0.6307,
      "step": 252400
    },
    {
      "epoch": 3.963893249607535,
      "grad_norm": 4.807615756988525,
      "learning_rate": 4.7522566718995296e-05,
      "loss": 0.6434,
      "step": 252500
    },
    {
      "epoch": 3.965463108320251,
      "grad_norm": 3.5339720249176025,
      "learning_rate": 4.752158555729985e-05,
      "loss": 0.6533,
      "step": 252600
    },
    {
      "epoch": 3.967032967032967,
      "grad_norm": 3.4444124698638916,
      "learning_rate": 4.75206043956044e-05,
      "loss": 0.7245,
      "step": 252700
    },
    {
      "epoch": 3.968602825745683,
      "grad_norm": 4.602416038513184,
      "learning_rate": 4.751962323390895e-05,
      "loss": 0.6963,
      "step": 252800
    },
    {
      "epoch": 3.9701726844583987,
      "grad_norm": 3.745795488357544,
      "learning_rate": 4.751864207221351e-05,
      "loss": 0.6681,
      "step": 252900
    },
    {
      "epoch": 3.9717425431711146,
      "grad_norm": 3.981602430343628,
      "learning_rate": 4.751766091051806e-05,
      "loss": 0.6451,
      "step": 253000
    },
    {
      "epoch": 3.9733124018838306,
      "grad_norm": 4.494424343109131,
      "learning_rate": 4.751667974882261e-05,
      "loss": 0.7142,
      "step": 253100
    },
    {
      "epoch": 3.974882260596546,
      "grad_norm": 4.5535478591918945,
      "learning_rate": 4.751569858712716e-05,
      "loss": 0.7252,
      "step": 253200
    },
    {
      "epoch": 3.976452119309262,
      "grad_norm": 4.147103309631348,
      "learning_rate": 4.751471742543172e-05,
      "loss": 0.694,
      "step": 253300
    },
    {
      "epoch": 3.978021978021978,
      "grad_norm": 3.8664567470550537,
      "learning_rate": 4.751373626373626e-05,
      "loss": 0.6744,
      "step": 253400
    },
    {
      "epoch": 3.979591836734694,
      "grad_norm": 4.09652853012085,
      "learning_rate": 4.751275510204082e-05,
      "loss": 0.7115,
      "step": 253500
    },
    {
      "epoch": 3.9811616954474096,
      "grad_norm": 3.9364781379699707,
      "learning_rate": 4.751177394034537e-05,
      "loss": 0.6815,
      "step": 253600
    },
    {
      "epoch": 3.9827315541601256,
      "grad_norm": 4.77299690246582,
      "learning_rate": 4.751079277864993e-05,
      "loss": 0.6676,
      "step": 253700
    },
    {
      "epoch": 3.9843014128728416,
      "grad_norm": 3.804586172103882,
      "learning_rate": 4.750981161695447e-05,
      "loss": 0.6684,
      "step": 253800
    },
    {
      "epoch": 3.985871271585557,
      "grad_norm": 3.4918863773345947,
      "learning_rate": 4.750883045525903e-05,
      "loss": 0.6697,
      "step": 253900
    },
    {
      "epoch": 3.987441130298273,
      "grad_norm": 4.613541126251221,
      "learning_rate": 4.750784929356358e-05,
      "loss": 0.6661,
      "step": 254000
    },
    {
      "epoch": 3.989010989010989,
      "grad_norm": 4.159115791320801,
      "learning_rate": 4.750686813186813e-05,
      "loss": 0.6767,
      "step": 254100
    },
    {
      "epoch": 3.990580847723705,
      "grad_norm": 3.4979844093322754,
      "learning_rate": 4.750588697017268e-05,
      "loss": 0.7001,
      "step": 254200
    },
    {
      "epoch": 3.9921507064364206,
      "grad_norm": 3.6873319149017334,
      "learning_rate": 4.750490580847724e-05,
      "loss": 0.7058,
      "step": 254300
    },
    {
      "epoch": 3.9937205651491365,
      "grad_norm": 3.9393324851989746,
      "learning_rate": 4.750392464678179e-05,
      "loss": 0.6433,
      "step": 254400
    },
    {
      "epoch": 3.9952904238618525,
      "grad_norm": 3.6136252880096436,
      "learning_rate": 4.750294348508634e-05,
      "loss": 0.6764,
      "step": 254500
    },
    {
      "epoch": 3.996860282574568,
      "grad_norm": 4.232896327972412,
      "learning_rate": 4.7501962323390894e-05,
      "loss": 0.687,
      "step": 254600
    },
    {
      "epoch": 3.998430141287284,
      "grad_norm": 4.203738689422607,
      "learning_rate": 4.750098116169545e-05,
      "loss": 0.6885,
      "step": 254700
    },
    {
      "epoch": 4.0,
      "grad_norm": 4.2277727127075195,
      "learning_rate": 4.75e-05,
      "loss": 0.7046,
      "step": 254800
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.0351793766021729,
      "eval_runtime": 14.638,
      "eval_samples_per_second": 229.062,
      "eval_steps_per_second": 229.062,
      "step": 254800
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5359585881233215,
      "eval_runtime": 281.66,
      "eval_samples_per_second": 226.159,
      "eval_steps_per_second": 226.159,
      "step": 254800
    },
    {
      "epoch": 4.001569858712716,
      "grad_norm": 4.652597904205322,
      "learning_rate": 4.7499018838304554e-05,
      "loss": 0.6883,
      "step": 254900
    },
    {
      "epoch": 4.003139717425432,
      "grad_norm": 4.473802089691162,
      "learning_rate": 4.749803767660911e-05,
      "loss": 0.659,
      "step": 255000
    },
    {
      "epoch": 4.004709576138148,
      "grad_norm": 4.421225547790527,
      "learning_rate": 4.749705651491366e-05,
      "loss": 0.7047,
      "step": 255100
    },
    {
      "epoch": 4.006279434850863,
      "grad_norm": 3.649533987045288,
      "learning_rate": 4.749607535321821e-05,
      "loss": 0.6699,
      "step": 255200
    },
    {
      "epoch": 4.007849293563579,
      "grad_norm": 3.423956871032715,
      "learning_rate": 4.7495094191522764e-05,
      "loss": 0.6724,
      "step": 255300
    },
    {
      "epoch": 4.009419152276295,
      "grad_norm": 4.009871959686279,
      "learning_rate": 4.749411302982732e-05,
      "loss": 0.7282,
      "step": 255400
    },
    {
      "epoch": 4.010989010989011,
      "grad_norm": 3.26784348487854,
      "learning_rate": 4.7493131868131866e-05,
      "loss": 0.6547,
      "step": 255500
    },
    {
      "epoch": 4.012558869701727,
      "grad_norm": 5.00852108001709,
      "learning_rate": 4.7492150706436424e-05,
      "loss": 0.6945,
      "step": 255600
    },
    {
      "epoch": 4.014128728414443,
      "grad_norm": 4.189489364624023,
      "learning_rate": 4.7491169544740975e-05,
      "loss": 0.732,
      "step": 255700
    },
    {
      "epoch": 4.015698587127159,
      "grad_norm": 3.7587037086486816,
      "learning_rate": 4.749018838304553e-05,
      "loss": 0.6664,
      "step": 255800
    },
    {
      "epoch": 4.017268445839874,
      "grad_norm": 3.5895981788635254,
      "learning_rate": 4.748920722135008e-05,
      "loss": 0.6887,
      "step": 255900
    },
    {
      "epoch": 4.01883830455259,
      "grad_norm": 3.8509328365325928,
      "learning_rate": 4.7488226059654635e-05,
      "loss": 0.6697,
      "step": 256000
    },
    {
      "epoch": 4.020408163265306,
      "grad_norm": 4.560542583465576,
      "learning_rate": 4.7487244897959186e-05,
      "loss": 0.6679,
      "step": 256100
    },
    {
      "epoch": 4.021978021978022,
      "grad_norm": 4.668769359588623,
      "learning_rate": 4.748626373626374e-05,
      "loss": 0.6583,
      "step": 256200
    },
    {
      "epoch": 4.023547880690738,
      "grad_norm": 3.8709611892700195,
      "learning_rate": 4.748528257456829e-05,
      "loss": 0.7058,
      "step": 256300
    },
    {
      "epoch": 4.025117739403454,
      "grad_norm": 3.425837993621826,
      "learning_rate": 4.7484301412872845e-05,
      "loss": 0.589,
      "step": 256400
    },
    {
      "epoch": 4.02668759811617,
      "grad_norm": 4.167825222015381,
      "learning_rate": 4.7483320251177396e-05,
      "loss": 0.7065,
      "step": 256500
    },
    {
      "epoch": 4.028257456828886,
      "grad_norm": 4.7050299644470215,
      "learning_rate": 4.748233908948195e-05,
      "loss": 0.7077,
      "step": 256600
    },
    {
      "epoch": 4.029827315541601,
      "grad_norm": 4.5900797843933105,
      "learning_rate": 4.74813579277865e-05,
      "loss": 0.6771,
      "step": 256700
    },
    {
      "epoch": 4.031397174254317,
      "grad_norm": 4.051831245422363,
      "learning_rate": 4.7480376766091056e-05,
      "loss": 0.6586,
      "step": 256800
    },
    {
      "epoch": 4.032967032967033,
      "grad_norm": 4.991147994995117,
      "learning_rate": 4.747939560439561e-05,
      "loss": 0.6958,
      "step": 256900
    },
    {
      "epoch": 4.034536891679749,
      "grad_norm": 4.355092525482178,
      "learning_rate": 4.747841444270016e-05,
      "loss": 0.659,
      "step": 257000
    },
    {
      "epoch": 4.036106750392465,
      "grad_norm": 4.848819255828857,
      "learning_rate": 4.7477433281004716e-05,
      "loss": 0.6958,
      "step": 257100
    },
    {
      "epoch": 4.037676609105181,
      "grad_norm": 4.078448295593262,
      "learning_rate": 4.747645211930927e-05,
      "loss": 0.6554,
      "step": 257200
    },
    {
      "epoch": 4.039246467817897,
      "grad_norm": 3.550682783126831,
      "learning_rate": 4.747547095761382e-05,
      "loss": 0.6298,
      "step": 257300
    },
    {
      "epoch": 4.040816326530612,
      "grad_norm": 4.584277629852295,
      "learning_rate": 4.747448979591837e-05,
      "loss": 0.6659,
      "step": 257400
    },
    {
      "epoch": 4.042386185243328,
      "grad_norm": 4.343369960784912,
      "learning_rate": 4.7473508634222927e-05,
      "loss": 0.6814,
      "step": 257500
    },
    {
      "epoch": 4.043956043956044,
      "grad_norm": 5.031847953796387,
      "learning_rate": 4.747252747252747e-05,
      "loss": 0.671,
      "step": 257600
    },
    {
      "epoch": 4.04552590266876,
      "grad_norm": 4.493315696716309,
      "learning_rate": 4.747154631083203e-05,
      "loss": 0.6997,
      "step": 257700
    },
    {
      "epoch": 4.047095761381476,
      "grad_norm": 4.292971134185791,
      "learning_rate": 4.747056514913658e-05,
      "loss": 0.6701,
      "step": 257800
    },
    {
      "epoch": 4.048665620094192,
      "grad_norm": 4.678450107574463,
      "learning_rate": 4.746958398744114e-05,
      "loss": 0.6516,
      "step": 257900
    },
    {
      "epoch": 4.050235478806908,
      "grad_norm": 3.729290008544922,
      "learning_rate": 4.746860282574568e-05,
      "loss": 0.6631,
      "step": 258000
    },
    {
      "epoch": 4.051805337519623,
      "grad_norm": 4.327523708343506,
      "learning_rate": 4.746762166405024e-05,
      "loss": 0.6827,
      "step": 258100
    },
    {
      "epoch": 4.053375196232339,
      "grad_norm": 4.180704593658447,
      "learning_rate": 4.746664050235479e-05,
      "loss": 0.7583,
      "step": 258200
    },
    {
      "epoch": 4.054945054945055,
      "grad_norm": 3.947230577468872,
      "learning_rate": 4.746565934065934e-05,
      "loss": 0.6841,
      "step": 258300
    },
    {
      "epoch": 4.056514913657771,
      "grad_norm": 6.477135181427002,
      "learning_rate": 4.746467817896389e-05,
      "loss": 0.6546,
      "step": 258400
    },
    {
      "epoch": 4.058084772370487,
      "grad_norm": 4.452513694763184,
      "learning_rate": 4.746369701726845e-05,
      "loss": 0.6776,
      "step": 258500
    },
    {
      "epoch": 4.059654631083203,
      "grad_norm": 3.9585049152374268,
      "learning_rate": 4.7462715855573e-05,
      "loss": 0.6625,
      "step": 258600
    },
    {
      "epoch": 4.061224489795919,
      "grad_norm": 3.566209077835083,
      "learning_rate": 4.746173469387755e-05,
      "loss": 0.6949,
      "step": 258700
    },
    {
      "epoch": 4.062794348508635,
      "grad_norm": 4.17811393737793,
      "learning_rate": 4.74607535321821e-05,
      "loss": 0.6598,
      "step": 258800
    },
    {
      "epoch": 4.06436420722135,
      "grad_norm": 4.464097023010254,
      "learning_rate": 4.745977237048666e-05,
      "loss": 0.6543,
      "step": 258900
    },
    {
      "epoch": 4.065934065934066,
      "grad_norm": 4.497383117675781,
      "learning_rate": 4.745879120879121e-05,
      "loss": 0.6781,
      "step": 259000
    },
    {
      "epoch": 4.067503924646782,
      "grad_norm": 4.464968681335449,
      "learning_rate": 4.745781004709576e-05,
      "loss": 0.6891,
      "step": 259100
    },
    {
      "epoch": 4.069073783359498,
      "grad_norm": 3.5341737270355225,
      "learning_rate": 4.745682888540032e-05,
      "loss": 0.6338,
      "step": 259200
    },
    {
      "epoch": 4.070643642072214,
      "grad_norm": 4.278619289398193,
      "learning_rate": 4.745584772370487e-05,
      "loss": 0.7074,
      "step": 259300
    },
    {
      "epoch": 4.07221350078493,
      "grad_norm": 3.486733913421631,
      "learning_rate": 4.745486656200942e-05,
      "loss": 0.6687,
      "step": 259400
    },
    {
      "epoch": 4.073783359497646,
      "grad_norm": 4.703190803527832,
      "learning_rate": 4.745388540031397e-05,
      "loss": 0.7026,
      "step": 259500
    },
    {
      "epoch": 4.075353218210361,
      "grad_norm": 2.956265926361084,
      "learning_rate": 4.745290423861853e-05,
      "loss": 0.6672,
      "step": 259600
    },
    {
      "epoch": 4.076923076923077,
      "grad_norm": 4.285943508148193,
      "learning_rate": 4.7451923076923075e-05,
      "loss": 0.6698,
      "step": 259700
    },
    {
      "epoch": 4.078492935635793,
      "grad_norm": 3.535538911819458,
      "learning_rate": 4.745094191522763e-05,
      "loss": 0.6606,
      "step": 259800
    },
    {
      "epoch": 4.080062794348509,
      "grad_norm": 3.4179601669311523,
      "learning_rate": 4.7449960753532184e-05,
      "loss": 0.6947,
      "step": 259900
    },
    {
      "epoch": 4.081632653061225,
      "grad_norm": 3.1264286041259766,
      "learning_rate": 4.744897959183674e-05,
      "loss": 0.6935,
      "step": 260000
    },
    {
      "epoch": 4.083202511773941,
      "grad_norm": 3.4268972873687744,
      "learning_rate": 4.7447998430141286e-05,
      "loss": 0.6565,
      "step": 260100
    },
    {
      "epoch": 4.0847723704866565,
      "grad_norm": 3.51084566116333,
      "learning_rate": 4.7447017268445844e-05,
      "loss": 0.6598,
      "step": 260200
    },
    {
      "epoch": 4.086342229199372,
      "grad_norm": 4.6614766120910645,
      "learning_rate": 4.7446036106750395e-05,
      "loss": 0.68,
      "step": 260300
    },
    {
      "epoch": 4.087912087912088,
      "grad_norm": 4.457892417907715,
      "learning_rate": 4.7445054945054946e-05,
      "loss": 0.6758,
      "step": 260400
    },
    {
      "epoch": 4.089481946624804,
      "grad_norm": 3.222954273223877,
      "learning_rate": 4.74440737833595e-05,
      "loss": 0.6767,
      "step": 260500
    },
    {
      "epoch": 4.0910518053375196,
      "grad_norm": 4.1093220710754395,
      "learning_rate": 4.7443092621664054e-05,
      "loss": 0.6864,
      "step": 260600
    },
    {
      "epoch": 4.0926216640502355,
      "grad_norm": 4.353708267211914,
      "learning_rate": 4.7442111459968605e-05,
      "loss": 0.6974,
      "step": 260700
    },
    {
      "epoch": 4.0941915227629515,
      "grad_norm": 3.9501101970672607,
      "learning_rate": 4.7441130298273156e-05,
      "loss": 0.6785,
      "step": 260800
    },
    {
      "epoch": 4.0957613814756675,
      "grad_norm": 4.332975387573242,
      "learning_rate": 4.744014913657771e-05,
      "loss": 0.7005,
      "step": 260900
    },
    {
      "epoch": 4.0973312401883835,
      "grad_norm": 3.7465312480926514,
      "learning_rate": 4.7439167974882265e-05,
      "loss": 0.6901,
      "step": 261000
    },
    {
      "epoch": 4.0989010989010985,
      "grad_norm": 3.5787510871887207,
      "learning_rate": 4.7438186813186816e-05,
      "loss": 0.6525,
      "step": 261100
    },
    {
      "epoch": 4.1004709576138145,
      "grad_norm": 4.514106750488281,
      "learning_rate": 4.743720565149137e-05,
      "loss": 0.627,
      "step": 261200
    },
    {
      "epoch": 4.1020408163265305,
      "grad_norm": 3.5358171463012695,
      "learning_rate": 4.7436224489795925e-05,
      "loss": 0.6635,
      "step": 261300
    },
    {
      "epoch": 4.1036106750392465,
      "grad_norm": 4.242919921875,
      "learning_rate": 4.7435243328100476e-05,
      "loss": 0.736,
      "step": 261400
    },
    {
      "epoch": 4.1051805337519625,
      "grad_norm": 2.896510601043701,
      "learning_rate": 4.743426216640503e-05,
      "loss": 0.6688,
      "step": 261500
    },
    {
      "epoch": 4.106750392464678,
      "grad_norm": 4.713226795196533,
      "learning_rate": 4.743328100470958e-05,
      "loss": 0.6674,
      "step": 261600
    },
    {
      "epoch": 4.108320251177394,
      "grad_norm": 3.4386277198791504,
      "learning_rate": 4.7432299843014136e-05,
      "loss": 0.6404,
      "step": 261700
    },
    {
      "epoch": 4.1098901098901095,
      "grad_norm": 3.5711352825164795,
      "learning_rate": 4.743131868131868e-05,
      "loss": 0.6886,
      "step": 261800
    },
    {
      "epoch": 4.1114599686028255,
      "grad_norm": 4.252394199371338,
      "learning_rate": 4.743033751962324e-05,
      "loss": 0.7279,
      "step": 261900
    },
    {
      "epoch": 4.1130298273155415,
      "grad_norm": 4.266851902008057,
      "learning_rate": 4.742935635792779e-05,
      "loss": 0.693,
      "step": 262000
    },
    {
      "epoch": 4.114599686028257,
      "grad_norm": 2.964576482772827,
      "learning_rate": 4.742837519623234e-05,
      "loss": 0.6615,
      "step": 262100
    },
    {
      "epoch": 4.116169544740973,
      "grad_norm": 4.477406024932861,
      "learning_rate": 4.742739403453689e-05,
      "loss": 0.6948,
      "step": 262200
    },
    {
      "epoch": 4.117739403453689,
      "grad_norm": 4.156589984893799,
      "learning_rate": 4.742641287284145e-05,
      "loss": 0.713,
      "step": 262300
    },
    {
      "epoch": 4.119309262166405,
      "grad_norm": 3.3142752647399902,
      "learning_rate": 4.7425431711146e-05,
      "loss": 0.6975,
      "step": 262400
    },
    {
      "epoch": 4.1208791208791204,
      "grad_norm": 3.7178261280059814,
      "learning_rate": 4.742445054945055e-05,
      "loss": 0.634,
      "step": 262500
    },
    {
      "epoch": 4.122448979591836,
      "grad_norm": 4.398045539855957,
      "learning_rate": 4.74234693877551e-05,
      "loss": 0.6813,
      "step": 262600
    },
    {
      "epoch": 4.124018838304552,
      "grad_norm": 4.4338059425354,
      "learning_rate": 4.742248822605966e-05,
      "loss": 0.665,
      "step": 262700
    },
    {
      "epoch": 4.125588697017268,
      "grad_norm": 3.8700103759765625,
      "learning_rate": 4.742150706436421e-05,
      "loss": 0.6731,
      "step": 262800
    },
    {
      "epoch": 4.127158555729984,
      "grad_norm": 3.9259886741638184,
      "learning_rate": 4.742052590266876e-05,
      "loss": 0.6988,
      "step": 262900
    },
    {
      "epoch": 4.1287284144427,
      "grad_norm": 4.242665767669678,
      "learning_rate": 4.741954474097331e-05,
      "loss": 0.6763,
      "step": 263000
    },
    {
      "epoch": 4.130298273155416,
      "grad_norm": 2.934295177459717,
      "learning_rate": 4.741856357927787e-05,
      "loss": 0.6694,
      "step": 263100
    },
    {
      "epoch": 4.131868131868132,
      "grad_norm": 3.5737879276275635,
      "learning_rate": 4.741758241758242e-05,
      "loss": 0.6844,
      "step": 263200
    },
    {
      "epoch": 4.133437990580847,
      "grad_norm": 4.007653713226318,
      "learning_rate": 4.741660125588697e-05,
      "loss": 0.6644,
      "step": 263300
    },
    {
      "epoch": 4.135007849293563,
      "grad_norm": 4.156789302825928,
      "learning_rate": 4.741562009419153e-05,
      "loss": 0.6811,
      "step": 263400
    },
    {
      "epoch": 4.136577708006279,
      "grad_norm": 4.594313621520996,
      "learning_rate": 4.7414638932496074e-05,
      "loss": 0.6493,
      "step": 263500
    },
    {
      "epoch": 4.138147566718995,
      "grad_norm": 4.743560314178467,
      "learning_rate": 4.741365777080063e-05,
      "loss": 0.7046,
      "step": 263600
    },
    {
      "epoch": 4.139717425431711,
      "grad_norm": 4.216530799865723,
      "learning_rate": 4.741267660910518e-05,
      "loss": 0.7139,
      "step": 263700
    },
    {
      "epoch": 4.141287284144427,
      "grad_norm": 4.316485404968262,
      "learning_rate": 4.741169544740974e-05,
      "loss": 0.6796,
      "step": 263800
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 4.177703857421875,
      "learning_rate": 4.7410714285714284e-05,
      "loss": 0.6888,
      "step": 263900
    },
    {
      "epoch": 4.144427001569858,
      "grad_norm": 4.221890449523926,
      "learning_rate": 4.740973312401884e-05,
      "loss": 0.6805,
      "step": 264000
    },
    {
      "epoch": 4.145996860282574,
      "grad_norm": 4.673862457275391,
      "learning_rate": 4.740875196232339e-05,
      "loss": 0.6794,
      "step": 264100
    },
    {
      "epoch": 4.14756671899529,
      "grad_norm": 4.927812576293945,
      "learning_rate": 4.7407770800627944e-05,
      "loss": 0.6934,
      "step": 264200
    },
    {
      "epoch": 4.149136577708006,
      "grad_norm": 3.4154303073883057,
      "learning_rate": 4.7406789638932495e-05,
      "loss": 0.6741,
      "step": 264300
    },
    {
      "epoch": 4.150706436420722,
      "grad_norm": 4.150356769561768,
      "learning_rate": 4.740580847723705e-05,
      "loss": 0.718,
      "step": 264400
    },
    {
      "epoch": 4.152276295133438,
      "grad_norm": 4.220087051391602,
      "learning_rate": 4.7404827315541604e-05,
      "loss": 0.7004,
      "step": 264500
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 3.4627060890197754,
      "learning_rate": 4.7403846153846155e-05,
      "loss": 0.6861,
      "step": 264600
    },
    {
      "epoch": 4.155416012558869,
      "grad_norm": 4.992697715759277,
      "learning_rate": 4.7402864992150706e-05,
      "loss": 0.6693,
      "step": 264700
    },
    {
      "epoch": 4.156985871271585,
      "grad_norm": 3.1079962253570557,
      "learning_rate": 4.740188383045526e-05,
      "loss": 0.6694,
      "step": 264800
    },
    {
      "epoch": 4.158555729984301,
      "grad_norm": 4.7777838706970215,
      "learning_rate": 4.7400902668759814e-05,
      "loss": 0.6734,
      "step": 264900
    },
    {
      "epoch": 4.160125588697017,
      "grad_norm": 4.102451324462891,
      "learning_rate": 4.7399921507064365e-05,
      "loss": 0.6685,
      "step": 265000
    },
    {
      "epoch": 4.161695447409733,
      "grad_norm": 4.765937805175781,
      "learning_rate": 4.7398940345368916e-05,
      "loss": 0.6848,
      "step": 265100
    },
    {
      "epoch": 4.163265306122449,
      "grad_norm": 3.9578027725219727,
      "learning_rate": 4.7397959183673474e-05,
      "loss": 0.6754,
      "step": 265200
    },
    {
      "epoch": 4.164835164835165,
      "grad_norm": 4.246636867523193,
      "learning_rate": 4.7396978021978025e-05,
      "loss": 0.6886,
      "step": 265300
    },
    {
      "epoch": 4.166405023547881,
      "grad_norm": 5.423036098480225,
      "learning_rate": 4.7395996860282576e-05,
      "loss": 0.6431,
      "step": 265400
    },
    {
      "epoch": 4.167974882260596,
      "grad_norm": 4.544802665710449,
      "learning_rate": 4.7395015698587134e-05,
      "loss": 0.6629,
      "step": 265500
    },
    {
      "epoch": 4.169544740973312,
      "grad_norm": 4.334299564361572,
      "learning_rate": 4.739403453689168e-05,
      "loss": 0.6857,
      "step": 265600
    },
    {
      "epoch": 4.171114599686028,
      "grad_norm": 3.8768105506896973,
      "learning_rate": 4.7393053375196236e-05,
      "loss": 0.7176,
      "step": 265700
    },
    {
      "epoch": 4.172684458398744,
      "grad_norm": 4.464404582977295,
      "learning_rate": 4.739207221350079e-05,
      "loss": 0.7111,
      "step": 265800
    },
    {
      "epoch": 4.17425431711146,
      "grad_norm": 5.05095100402832,
      "learning_rate": 4.7391091051805345e-05,
      "loss": 0.6943,
      "step": 265900
    },
    {
      "epoch": 4.175824175824176,
      "grad_norm": 4.118257522583008,
      "learning_rate": 4.739010989010989e-05,
      "loss": 0.6754,
      "step": 266000
    },
    {
      "epoch": 4.177394034536892,
      "grad_norm": 3.7404751777648926,
      "learning_rate": 4.7389128728414446e-05,
      "loss": 0.7042,
      "step": 266100
    },
    {
      "epoch": 4.178963893249607,
      "grad_norm": 4.610115051269531,
      "learning_rate": 4.7388147566719e-05,
      "loss": 0.6897,
      "step": 266200
    },
    {
      "epoch": 4.180533751962323,
      "grad_norm": 3.7864487171173096,
      "learning_rate": 4.738716640502355e-05,
      "loss": 0.6373,
      "step": 266300
    },
    {
      "epoch": 4.182103610675039,
      "grad_norm": 4.403534412384033,
      "learning_rate": 4.73861852433281e-05,
      "loss": 0.6715,
      "step": 266400
    },
    {
      "epoch": 4.183673469387755,
      "grad_norm": 3.2147576808929443,
      "learning_rate": 4.738520408163266e-05,
      "loss": 0.6999,
      "step": 266500
    },
    {
      "epoch": 4.185243328100471,
      "grad_norm": 3.9481735229492188,
      "learning_rate": 4.738422291993721e-05,
      "loss": 0.6551,
      "step": 266600
    },
    {
      "epoch": 4.186813186813187,
      "grad_norm": 4.737280368804932,
      "learning_rate": 4.738324175824176e-05,
      "loss": 0.7124,
      "step": 266700
    },
    {
      "epoch": 4.188383045525903,
      "grad_norm": 3.4166738986968994,
      "learning_rate": 4.738226059654631e-05,
      "loss": 0.6817,
      "step": 266800
    },
    {
      "epoch": 4.189952904238618,
      "grad_norm": 3.2787094116210938,
      "learning_rate": 4.738127943485087e-05,
      "loss": 0.7114,
      "step": 266900
    },
    {
      "epoch": 4.191522762951334,
      "grad_norm": 3.6831183433532715,
      "learning_rate": 4.738029827315542e-05,
      "loss": 0.6896,
      "step": 267000
    },
    {
      "epoch": 4.19309262166405,
      "grad_norm": 3.4388022422790527,
      "learning_rate": 4.737931711145997e-05,
      "loss": 0.6365,
      "step": 267100
    },
    {
      "epoch": 4.194662480376766,
      "grad_norm": 4.535200595855713,
      "learning_rate": 4.737833594976452e-05,
      "loss": 0.7065,
      "step": 267200
    },
    {
      "epoch": 4.196232339089482,
      "grad_norm": 3.33835506439209,
      "learning_rate": 4.737735478806908e-05,
      "loss": 0.6918,
      "step": 267300
    },
    {
      "epoch": 4.197802197802198,
      "grad_norm": 4.0304436683654785,
      "learning_rate": 4.737637362637363e-05,
      "loss": 0.6463,
      "step": 267400
    },
    {
      "epoch": 4.199372056514914,
      "grad_norm": 3.732553005218506,
      "learning_rate": 4.737539246467818e-05,
      "loss": 0.6611,
      "step": 267500
    },
    {
      "epoch": 4.20094191522763,
      "grad_norm": 4.089311122894287,
      "learning_rate": 4.737441130298274e-05,
      "loss": 0.6878,
      "step": 267600
    },
    {
      "epoch": 4.202511773940345,
      "grad_norm": 4.370321273803711,
      "learning_rate": 4.737343014128728e-05,
      "loss": 0.6995,
      "step": 267700
    },
    {
      "epoch": 4.204081632653061,
      "grad_norm": 4.479807376861572,
      "learning_rate": 4.737244897959184e-05,
      "loss": 0.6555,
      "step": 267800
    },
    {
      "epoch": 4.205651491365777,
      "grad_norm": 2.9668707847595215,
      "learning_rate": 4.737146781789639e-05,
      "loss": 0.6566,
      "step": 267900
    },
    {
      "epoch": 4.207221350078493,
      "grad_norm": 4.021942615509033,
      "learning_rate": 4.737048665620095e-05,
      "loss": 0.6543,
      "step": 268000
    },
    {
      "epoch": 4.208791208791209,
      "grad_norm": 3.7587828636169434,
      "learning_rate": 4.736950549450549e-05,
      "loss": 0.6577,
      "step": 268100
    },
    {
      "epoch": 4.210361067503925,
      "grad_norm": 4.482262134552002,
      "learning_rate": 4.736852433281005e-05,
      "loss": 0.6304,
      "step": 268200
    },
    {
      "epoch": 4.211930926216641,
      "grad_norm": 3.727091073989868,
      "learning_rate": 4.73675431711146e-05,
      "loss": 0.6833,
      "step": 268300
    },
    {
      "epoch": 4.213500784929356,
      "grad_norm": 3.9050674438476562,
      "learning_rate": 4.736656200941915e-05,
      "loss": 0.6958,
      "step": 268400
    },
    {
      "epoch": 4.215070643642072,
      "grad_norm": 3.674335241317749,
      "learning_rate": 4.7365580847723704e-05,
      "loss": 0.6986,
      "step": 268500
    },
    {
      "epoch": 4.216640502354788,
      "grad_norm": 4.6823601722717285,
      "learning_rate": 4.736459968602826e-05,
      "loss": 0.6984,
      "step": 268600
    },
    {
      "epoch": 4.218210361067504,
      "grad_norm": 3.1742818355560303,
      "learning_rate": 4.736361852433281e-05,
      "loss": 0.6779,
      "step": 268700
    },
    {
      "epoch": 4.21978021978022,
      "grad_norm": 4.317164421081543,
      "learning_rate": 4.7362637362637364e-05,
      "loss": 0.6572,
      "step": 268800
    },
    {
      "epoch": 4.221350078492936,
      "grad_norm": 3.8170523643493652,
      "learning_rate": 4.7361656200941915e-05,
      "loss": 0.7151,
      "step": 268900
    },
    {
      "epoch": 4.222919937205652,
      "grad_norm": 4.351582050323486,
      "learning_rate": 4.736067503924647e-05,
      "loss": 0.6668,
      "step": 269000
    },
    {
      "epoch": 4.224489795918367,
      "grad_norm": 4.22592306137085,
      "learning_rate": 4.735969387755102e-05,
      "loss": 0.6658,
      "step": 269100
    },
    {
      "epoch": 4.226059654631083,
      "grad_norm": 4.175691604614258,
      "learning_rate": 4.7358712715855574e-05,
      "loss": 0.6869,
      "step": 269200
    },
    {
      "epoch": 4.227629513343799,
      "grad_norm": 4.722596645355225,
      "learning_rate": 4.7357731554160125e-05,
      "loss": 0.7211,
      "step": 269300
    },
    {
      "epoch": 4.229199372056515,
      "grad_norm": 3.648923635482788,
      "learning_rate": 4.735675039246468e-05,
      "loss": 0.6985,
      "step": 269400
    },
    {
      "epoch": 4.230769230769231,
      "grad_norm": 3.417931079864502,
      "learning_rate": 4.7355769230769234e-05,
      "loss": 0.6789,
      "step": 269500
    },
    {
      "epoch": 4.232339089481947,
      "grad_norm": 4.277522563934326,
      "learning_rate": 4.7354788069073785e-05,
      "loss": 0.7213,
      "step": 269600
    },
    {
      "epoch": 4.233908948194663,
      "grad_norm": 3.273987293243408,
      "learning_rate": 4.735380690737834e-05,
      "loss": 0.7359,
      "step": 269700
    },
    {
      "epoch": 4.235478806907379,
      "grad_norm": 5.245060920715332,
      "learning_rate": 4.735282574568289e-05,
      "loss": 0.6619,
      "step": 269800
    },
    {
      "epoch": 4.237048665620094,
      "grad_norm": 3.1064109802246094,
      "learning_rate": 4.7351844583987445e-05,
      "loss": 0.708,
      "step": 269900
    },
    {
      "epoch": 4.23861852433281,
      "grad_norm": 3.5815956592559814,
      "learning_rate": 4.7350863422291996e-05,
      "loss": 0.6828,
      "step": 270000
    },
    {
      "epoch": 4.240188383045526,
      "grad_norm": 4.49785852432251,
      "learning_rate": 4.7349882260596553e-05,
      "loss": 0.6432,
      "step": 270100
    },
    {
      "epoch": 4.241758241758242,
      "grad_norm": 3.5908422470092773,
      "learning_rate": 4.73489010989011e-05,
      "loss": 0.6772,
      "step": 270200
    },
    {
      "epoch": 4.243328100470958,
      "grad_norm": 4.232873439788818,
      "learning_rate": 4.7347919937205655e-05,
      "loss": 0.7253,
      "step": 270300
    },
    {
      "epoch": 4.244897959183674,
      "grad_norm": 3.985429048538208,
      "learning_rate": 4.7346938775510206e-05,
      "loss": 0.6481,
      "step": 270400
    },
    {
      "epoch": 4.24646781789639,
      "grad_norm": 4.560540676116943,
      "learning_rate": 4.734595761381476e-05,
      "loss": 0.679,
      "step": 270500
    },
    {
      "epoch": 4.248037676609105,
      "grad_norm": 4.014875888824463,
      "learning_rate": 4.734497645211931e-05,
      "loss": 0.6506,
      "step": 270600
    },
    {
      "epoch": 4.249607535321821,
      "grad_norm": 4.70601224899292,
      "learning_rate": 4.7343995290423866e-05,
      "loss": 0.6565,
      "step": 270700
    },
    {
      "epoch": 4.251177394034537,
      "grad_norm": 4.508305072784424,
      "learning_rate": 4.734301412872842e-05,
      "loss": 0.6902,
      "step": 270800
    },
    {
      "epoch": 4.252747252747253,
      "grad_norm": 4.8132171630859375,
      "learning_rate": 4.734203296703297e-05,
      "loss": 0.6394,
      "step": 270900
    },
    {
      "epoch": 4.254317111459969,
      "grad_norm": 3.857975482940674,
      "learning_rate": 4.734105180533752e-05,
      "loss": 0.6693,
      "step": 271000
    },
    {
      "epoch": 4.255886970172685,
      "grad_norm": 3.8961634635925293,
      "learning_rate": 4.734007064364208e-05,
      "loss": 0.7003,
      "step": 271100
    },
    {
      "epoch": 4.257456828885401,
      "grad_norm": 3.2538836002349854,
      "learning_rate": 4.733908948194663e-05,
      "loss": 0.6951,
      "step": 271200
    },
    {
      "epoch": 4.259026687598116,
      "grad_norm": 4.447409629821777,
      "learning_rate": 4.733810832025118e-05,
      "loss": 0.6862,
      "step": 271300
    },
    {
      "epoch": 4.260596546310832,
      "grad_norm": 3.7799196243286133,
      "learning_rate": 4.733712715855573e-05,
      "loss": 0.6599,
      "step": 271400
    },
    {
      "epoch": 4.262166405023548,
      "grad_norm": 4.229420185089111,
      "learning_rate": 4.733614599686029e-05,
      "loss": 0.6875,
      "step": 271500
    },
    {
      "epoch": 4.263736263736264,
      "grad_norm": 3.8635332584381104,
      "learning_rate": 4.733516483516484e-05,
      "loss": 0.6539,
      "step": 271600
    },
    {
      "epoch": 4.26530612244898,
      "grad_norm": 4.644077777862549,
      "learning_rate": 4.733418367346939e-05,
      "loss": 0.6831,
      "step": 271700
    },
    {
      "epoch": 4.266875981161696,
      "grad_norm": 2.665104866027832,
      "learning_rate": 4.733320251177395e-05,
      "loss": 0.67,
      "step": 271800
    },
    {
      "epoch": 4.268445839874412,
      "grad_norm": 4.423269748687744,
      "learning_rate": 4.733222135007849e-05,
      "loss": 0.6321,
      "step": 271900
    },
    {
      "epoch": 4.270015698587127,
      "grad_norm": 4.0022053718566895,
      "learning_rate": 4.733124018838305e-05,
      "loss": 0.6369,
      "step": 272000
    },
    {
      "epoch": 4.271585557299843,
      "grad_norm": 3.0974130630493164,
      "learning_rate": 4.73302590266876e-05,
      "loss": 0.7127,
      "step": 272100
    },
    {
      "epoch": 4.273155416012559,
      "grad_norm": 4.650594711303711,
      "learning_rate": 4.732927786499216e-05,
      "loss": 0.7244,
      "step": 272200
    },
    {
      "epoch": 4.274725274725275,
      "grad_norm": 4.2395429611206055,
      "learning_rate": 4.73282967032967e-05,
      "loss": 0.695,
      "step": 272300
    },
    {
      "epoch": 4.276295133437991,
      "grad_norm": 4.10556697845459,
      "learning_rate": 4.732731554160126e-05,
      "loss": 0.6841,
      "step": 272400
    },
    {
      "epoch": 4.277864992150707,
      "grad_norm": 4.339336395263672,
      "learning_rate": 4.732633437990581e-05,
      "loss": 0.7019,
      "step": 272500
    },
    {
      "epoch": 4.279434850863423,
      "grad_norm": 3.325669288635254,
      "learning_rate": 4.732535321821036e-05,
      "loss": 0.6673,
      "step": 272600
    },
    {
      "epoch": 4.2810047095761385,
      "grad_norm": 2.954998016357422,
      "learning_rate": 4.732437205651491e-05,
      "loss": 0.7128,
      "step": 272700
    },
    {
      "epoch": 4.282574568288854,
      "grad_norm": 5.345810413360596,
      "learning_rate": 4.732339089481947e-05,
      "loss": 0.6503,
      "step": 272800
    },
    {
      "epoch": 4.28414442700157,
      "grad_norm": 4.440383434295654,
      "learning_rate": 4.732240973312402e-05,
      "loss": 0.7068,
      "step": 272900
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 4.672170162200928,
      "learning_rate": 4.732142857142857e-05,
      "loss": 0.6273,
      "step": 273000
    },
    {
      "epoch": 4.287284144427002,
      "grad_norm": 4.253457069396973,
      "learning_rate": 4.7320447409733124e-05,
      "loss": 0.6544,
      "step": 273100
    },
    {
      "epoch": 4.2888540031397175,
      "grad_norm": 4.292364597320557,
      "learning_rate": 4.731946624803768e-05,
      "loss": 0.6799,
      "step": 273200
    },
    {
      "epoch": 4.2904238618524335,
      "grad_norm": 3.510920763015747,
      "learning_rate": 4.731848508634223e-05,
      "loss": 0.6406,
      "step": 273300
    },
    {
      "epoch": 4.2919937205651495,
      "grad_norm": 4.070984840393066,
      "learning_rate": 4.731750392464678e-05,
      "loss": 0.6688,
      "step": 273400
    },
    {
      "epoch": 4.293563579277865,
      "grad_norm": 4.1145710945129395,
      "learning_rate": 4.7316522762951334e-05,
      "loss": 0.6798,
      "step": 273500
    },
    {
      "epoch": 4.295133437990581,
      "grad_norm": 4.897317409515381,
      "learning_rate": 4.731554160125589e-05,
      "loss": 0.6511,
      "step": 273600
    },
    {
      "epoch": 4.2967032967032965,
      "grad_norm": 4.7121195793151855,
      "learning_rate": 4.731456043956044e-05,
      "loss": 0.6555,
      "step": 273700
    },
    {
      "epoch": 4.2982731554160125,
      "grad_norm": 4.848147392272949,
      "learning_rate": 4.7313579277864994e-05,
      "loss": 0.6904,
      "step": 273800
    },
    {
      "epoch": 4.2998430141287285,
      "grad_norm": 4.857992172241211,
      "learning_rate": 4.731259811616955e-05,
      "loss": 0.6674,
      "step": 273900
    },
    {
      "epoch": 4.3014128728414445,
      "grad_norm": 3.346876621246338,
      "learning_rate": 4.7311616954474096e-05,
      "loss": 0.6912,
      "step": 274000
    },
    {
      "epoch": 4.3029827315541604,
      "grad_norm": 3.9647319316864014,
      "learning_rate": 4.7310635792778654e-05,
      "loss": 0.6764,
      "step": 274100
    },
    {
      "epoch": 4.304552590266876,
      "grad_norm": 3.324350357055664,
      "learning_rate": 4.7309654631083205e-05,
      "loss": 0.6815,
      "step": 274200
    },
    {
      "epoch": 4.3061224489795915,
      "grad_norm": 3.343475818634033,
      "learning_rate": 4.730867346938776e-05,
      "loss": 0.6654,
      "step": 274300
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 3.995119094848633,
      "learning_rate": 4.730769230769231e-05,
      "loss": 0.6938,
      "step": 274400
    },
    {
      "epoch": 4.3092621664050235,
      "grad_norm": 4.459177494049072,
      "learning_rate": 4.7306711145996864e-05,
      "loss": 0.6657,
      "step": 274500
    },
    {
      "epoch": 4.310832025117739,
      "grad_norm": 4.318443775177002,
      "learning_rate": 4.7305729984301415e-05,
      "loss": 0.6455,
      "step": 274600
    },
    {
      "epoch": 4.312401883830455,
      "grad_norm": 3.9016025066375732,
      "learning_rate": 4.7304748822605966e-05,
      "loss": 0.6398,
      "step": 274700
    },
    {
      "epoch": 4.313971742543171,
      "grad_norm": 4.621609687805176,
      "learning_rate": 4.730376766091052e-05,
      "loss": 0.7038,
      "step": 274800
    },
    {
      "epoch": 4.315541601255887,
      "grad_norm": 3.9087862968444824,
      "learning_rate": 4.7302786499215075e-05,
      "loss": 0.709,
      "step": 274900
    },
    {
      "epoch": 4.3171114599686025,
      "grad_norm": 3.638404369354248,
      "learning_rate": 4.7301805337519626e-05,
      "loss": 0.6707,
      "step": 275000
    },
    {
      "epoch": 4.318681318681318,
      "grad_norm": 5.272716999053955,
      "learning_rate": 4.730082417582418e-05,
      "loss": 0.6815,
      "step": 275100
    },
    {
      "epoch": 4.320251177394034,
      "grad_norm": 3.518348455429077,
      "learning_rate": 4.729984301412873e-05,
      "loss": 0.6948,
      "step": 275200
    },
    {
      "epoch": 4.32182103610675,
      "grad_norm": 3.466677665710449,
      "learning_rate": 4.7298861852433286e-05,
      "loss": 0.6365,
      "step": 275300
    },
    {
      "epoch": 4.323390894819466,
      "grad_norm": 4.550163745880127,
      "learning_rate": 4.729788069073784e-05,
      "loss": 0.6626,
      "step": 275400
    },
    {
      "epoch": 4.324960753532182,
      "grad_norm": 4.513132095336914,
      "learning_rate": 4.729689952904239e-05,
      "loss": 0.7282,
      "step": 275500
    },
    {
      "epoch": 4.326530612244898,
      "grad_norm": 4.420587062835693,
      "learning_rate": 4.729591836734694e-05,
      "loss": 0.687,
      "step": 275600
    },
    {
      "epoch": 4.328100470957613,
      "grad_norm": 3.7653167247772217,
      "learning_rate": 4.7294937205651497e-05,
      "loss": 0.6789,
      "step": 275700
    },
    {
      "epoch": 4.329670329670329,
      "grad_norm": 3.9975199699401855,
      "learning_rate": 4.729395604395605e-05,
      "loss": 0.648,
      "step": 275800
    },
    {
      "epoch": 4.331240188383045,
      "grad_norm": 3.6646153926849365,
      "learning_rate": 4.72929748822606e-05,
      "loss": 0.6924,
      "step": 275900
    },
    {
      "epoch": 4.332810047095761,
      "grad_norm": 3.964341640472412,
      "learning_rate": 4.7291993720565156e-05,
      "loss": 0.6838,
      "step": 276000
    },
    {
      "epoch": 4.334379905808477,
      "grad_norm": 5.312320232391357,
      "learning_rate": 4.72910125588697e-05,
      "loss": 0.6872,
      "step": 276100
    },
    {
      "epoch": 4.335949764521193,
      "grad_norm": 4.8357391357421875,
      "learning_rate": 4.729003139717426e-05,
      "loss": 0.6899,
      "step": 276200
    },
    {
      "epoch": 4.337519623233909,
      "grad_norm": 3.9009928703308105,
      "learning_rate": 4.728905023547881e-05,
      "loss": 0.6566,
      "step": 276300
    },
    {
      "epoch": 4.339089481946624,
      "grad_norm": 3.8670313358306885,
      "learning_rate": 4.728806907378337e-05,
      "loss": 0.6491,
      "step": 276400
    },
    {
      "epoch": 4.34065934065934,
      "grad_norm": 4.62401819229126,
      "learning_rate": 4.728708791208791e-05,
      "loss": 0.7059,
      "step": 276500
    },
    {
      "epoch": 4.342229199372056,
      "grad_norm": 4.564660549163818,
      "learning_rate": 4.728610675039247e-05,
      "loss": 0.6504,
      "step": 276600
    },
    {
      "epoch": 4.343799058084772,
      "grad_norm": 3.418977975845337,
      "learning_rate": 4.728512558869702e-05,
      "loss": 0.6777,
      "step": 276700
    },
    {
      "epoch": 4.345368916797488,
      "grad_norm": 3.3036065101623535,
      "learning_rate": 4.728414442700157e-05,
      "loss": 0.7051,
      "step": 276800
    },
    {
      "epoch": 4.346938775510204,
      "grad_norm": 4.082484722137451,
      "learning_rate": 4.728316326530612e-05,
      "loss": 0.6663,
      "step": 276900
    },
    {
      "epoch": 4.34850863422292,
      "grad_norm": 2.474083185195923,
      "learning_rate": 4.728218210361068e-05,
      "loss": 0.6643,
      "step": 277000
    },
    {
      "epoch": 4.350078492935636,
      "grad_norm": 3.5971100330352783,
      "learning_rate": 4.728120094191523e-05,
      "loss": 0.6667,
      "step": 277100
    },
    {
      "epoch": 4.351648351648351,
      "grad_norm": 3.9134743213653564,
      "learning_rate": 4.728021978021978e-05,
      "loss": 0.6843,
      "step": 277200
    },
    {
      "epoch": 4.353218210361067,
      "grad_norm": 4.962235450744629,
      "learning_rate": 4.727923861852433e-05,
      "loss": 0.6548,
      "step": 277300
    },
    {
      "epoch": 4.354788069073783,
      "grad_norm": 4.082308769226074,
      "learning_rate": 4.727825745682889e-05,
      "loss": 0.6654,
      "step": 277400
    },
    {
      "epoch": 4.356357927786499,
      "grad_norm": 4.111719131469727,
      "learning_rate": 4.727727629513344e-05,
      "loss": 0.6554,
      "step": 277500
    },
    {
      "epoch": 4.357927786499215,
      "grad_norm": 4.009565830230713,
      "learning_rate": 4.727629513343799e-05,
      "loss": 0.672,
      "step": 277600
    },
    {
      "epoch": 4.359497645211931,
      "grad_norm": 3.796727418899536,
      "learning_rate": 4.727531397174254e-05,
      "loss": 0.674,
      "step": 277700
    },
    {
      "epoch": 4.361067503924647,
      "grad_norm": 4.477943420410156,
      "learning_rate": 4.72743328100471e-05,
      "loss": 0.7013,
      "step": 277800
    },
    {
      "epoch": 4.362637362637362,
      "grad_norm": 3.933851718902588,
      "learning_rate": 4.727335164835165e-05,
      "loss": 0.6787,
      "step": 277900
    },
    {
      "epoch": 4.364207221350078,
      "grad_norm": 3.417968988418579,
      "learning_rate": 4.72723704866562e-05,
      "loss": 0.6561,
      "step": 278000
    },
    {
      "epoch": 4.365777080062794,
      "grad_norm": 3.5638020038604736,
      "learning_rate": 4.727138932496076e-05,
      "loss": 0.7058,
      "step": 278100
    },
    {
      "epoch": 4.36734693877551,
      "grad_norm": 4.163540840148926,
      "learning_rate": 4.7270408163265305e-05,
      "loss": 0.6413,
      "step": 278200
    },
    {
      "epoch": 4.368916797488226,
      "grad_norm": 4.11171293258667,
      "learning_rate": 4.726942700156986e-05,
      "loss": 0.676,
      "step": 278300
    },
    {
      "epoch": 4.370486656200942,
      "grad_norm": 4.407525539398193,
      "learning_rate": 4.7268445839874414e-05,
      "loss": 0.6704,
      "step": 278400
    },
    {
      "epoch": 4.372056514913658,
      "grad_norm": 3.6430282592773438,
      "learning_rate": 4.726746467817897e-05,
      "loss": 0.6754,
      "step": 278500
    },
    {
      "epoch": 4.373626373626374,
      "grad_norm": 2.7906055450439453,
      "learning_rate": 4.7266483516483516e-05,
      "loss": 0.7045,
      "step": 278600
    },
    {
      "epoch": 4.375196232339089,
      "grad_norm": 3.592686653137207,
      "learning_rate": 4.7265502354788073e-05,
      "loss": 0.6924,
      "step": 278700
    },
    {
      "epoch": 4.376766091051805,
      "grad_norm": 3.4248344898223877,
      "learning_rate": 4.7264521193092624e-05,
      "loss": 0.6669,
      "step": 278800
    },
    {
      "epoch": 4.378335949764521,
      "grad_norm": 3.9714596271514893,
      "learning_rate": 4.7263540031397175e-05,
      "loss": 0.7098,
      "step": 278900
    },
    {
      "epoch": 4.379905808477237,
      "grad_norm": 3.9093105792999268,
      "learning_rate": 4.7262558869701726e-05,
      "loss": 0.6753,
      "step": 279000
    },
    {
      "epoch": 4.381475667189953,
      "grad_norm": 3.0989701747894287,
      "learning_rate": 4.7261577708006284e-05,
      "loss": 0.7026,
      "step": 279100
    },
    {
      "epoch": 4.383045525902669,
      "grad_norm": 4.337825775146484,
      "learning_rate": 4.7260596546310835e-05,
      "loss": 0.6898,
      "step": 279200
    },
    {
      "epoch": 4.384615384615385,
      "grad_norm": 4.5941691398620605,
      "learning_rate": 4.7259615384615386e-05,
      "loss": 0.6754,
      "step": 279300
    },
    {
      "epoch": 4.3861852433281,
      "grad_norm": 4.119044303894043,
      "learning_rate": 4.725863422291994e-05,
      "loss": 0.6716,
      "step": 279400
    },
    {
      "epoch": 4.387755102040816,
      "grad_norm": 4.950125694274902,
      "learning_rate": 4.7257653061224495e-05,
      "loss": 0.6648,
      "step": 279500
    },
    {
      "epoch": 4.389324960753532,
      "grad_norm": 4.502707481384277,
      "learning_rate": 4.7256671899529046e-05,
      "loss": 0.7029,
      "step": 279600
    },
    {
      "epoch": 4.390894819466248,
      "grad_norm": 3.696927547454834,
      "learning_rate": 4.72556907378336e-05,
      "loss": 0.6531,
      "step": 279700
    },
    {
      "epoch": 4.392464678178964,
      "grad_norm": 4.712969779968262,
      "learning_rate": 4.725470957613815e-05,
      "loss": 0.7093,
      "step": 279800
    },
    {
      "epoch": 4.39403453689168,
      "grad_norm": 4.365996360778809,
      "learning_rate": 4.7253728414442706e-05,
      "loss": 0.6905,
      "step": 279900
    },
    {
      "epoch": 4.395604395604396,
      "grad_norm": 3.877267837524414,
      "learning_rate": 4.7252747252747257e-05,
      "loss": 0.6707,
      "step": 280000
    },
    {
      "epoch": 4.397174254317111,
      "grad_norm": 4.180154800415039,
      "learning_rate": 4.725176609105181e-05,
      "loss": 0.6942,
      "step": 280100
    },
    {
      "epoch": 4.398744113029827,
      "grad_norm": 4.671648025512695,
      "learning_rate": 4.7250784929356365e-05,
      "loss": 0.6688,
      "step": 280200
    },
    {
      "epoch": 4.400313971742543,
      "grad_norm": 4.170560359954834,
      "learning_rate": 4.724980376766091e-05,
      "loss": 0.6888,
      "step": 280300
    },
    {
      "epoch": 4.401883830455259,
      "grad_norm": 3.7037293910980225,
      "learning_rate": 4.724882260596547e-05,
      "loss": 0.6896,
      "step": 280400
    },
    {
      "epoch": 4.403453689167975,
      "grad_norm": 4.103370666503906,
      "learning_rate": 4.724784144427002e-05,
      "loss": 0.6961,
      "step": 280500
    },
    {
      "epoch": 4.405023547880691,
      "grad_norm": 3.59649920463562,
      "learning_rate": 4.7246860282574576e-05,
      "loss": 0.6795,
      "step": 280600
    },
    {
      "epoch": 4.406593406593407,
      "grad_norm": 4.172338008880615,
      "learning_rate": 4.724587912087912e-05,
      "loss": 0.663,
      "step": 280700
    },
    {
      "epoch": 4.408163265306122,
      "grad_norm": 3.773350238800049,
      "learning_rate": 4.724489795918368e-05,
      "loss": 0.6529,
      "step": 280800
    },
    {
      "epoch": 4.409733124018838,
      "grad_norm": 3.6588478088378906,
      "learning_rate": 4.724391679748823e-05,
      "loss": 0.6197,
      "step": 280900
    },
    {
      "epoch": 4.411302982731554,
      "grad_norm": 4.51162052154541,
      "learning_rate": 4.724293563579278e-05,
      "loss": 0.664,
      "step": 281000
    },
    {
      "epoch": 4.41287284144427,
      "grad_norm": 3.8780579566955566,
      "learning_rate": 4.724195447409733e-05,
      "loss": 0.6862,
      "step": 281100
    },
    {
      "epoch": 4.414442700156986,
      "grad_norm": 3.7979021072387695,
      "learning_rate": 4.724097331240189e-05,
      "loss": 0.6847,
      "step": 281200
    },
    {
      "epoch": 4.416012558869702,
      "grad_norm": 3.960148334503174,
      "learning_rate": 4.723999215070644e-05,
      "loss": 0.6797,
      "step": 281300
    },
    {
      "epoch": 4.417582417582418,
      "grad_norm": 4.183413982391357,
      "learning_rate": 4.723901098901099e-05,
      "loss": 0.6708,
      "step": 281400
    },
    {
      "epoch": 4.419152276295134,
      "grad_norm": 3.229950428009033,
      "learning_rate": 4.723802982731554e-05,
      "loss": 0.6809,
      "step": 281500
    },
    {
      "epoch": 4.420722135007849,
      "grad_norm": 5.175266742706299,
      "learning_rate": 4.72370486656201e-05,
      "loss": 0.6866,
      "step": 281600
    },
    {
      "epoch": 4.422291993720565,
      "grad_norm": 4.563126087188721,
      "learning_rate": 4.723606750392465e-05,
      "loss": 0.6924,
      "step": 281700
    },
    {
      "epoch": 4.423861852433281,
      "grad_norm": 4.186872959136963,
      "learning_rate": 4.72350863422292e-05,
      "loss": 0.6992,
      "step": 281800
    },
    {
      "epoch": 4.425431711145997,
      "grad_norm": 3.3960063457489014,
      "learning_rate": 4.723410518053375e-05,
      "loss": 0.6945,
      "step": 281900
    },
    {
      "epoch": 4.427001569858713,
      "grad_norm": 3.7048771381378174,
      "learning_rate": 4.723312401883831e-05,
      "loss": 0.679,
      "step": 282000
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 4.691397666931152,
      "learning_rate": 4.723214285714286e-05,
      "loss": 0.669,
      "step": 282100
    },
    {
      "epoch": 4.430141287284145,
      "grad_norm": 3.9942851066589355,
      "learning_rate": 4.723116169544741e-05,
      "loss": 0.692,
      "step": 282200
    },
    {
      "epoch": 4.43171114599686,
      "grad_norm": 3.791820764541626,
      "learning_rate": 4.723018053375197e-05,
      "loss": 0.6903,
      "step": 282300
    },
    {
      "epoch": 4.433281004709576,
      "grad_norm": 3.9380099773406982,
      "learning_rate": 4.7229199372056514e-05,
      "loss": 0.6903,
      "step": 282400
    },
    {
      "epoch": 4.434850863422292,
      "grad_norm": 3.307124376296997,
      "learning_rate": 4.722821821036107e-05,
      "loss": 0.6683,
      "step": 282500
    },
    {
      "epoch": 4.436420722135008,
      "grad_norm": 4.640432834625244,
      "learning_rate": 4.722723704866562e-05,
      "loss": 0.6772,
      "step": 282600
    },
    {
      "epoch": 4.437990580847724,
      "grad_norm": 3.922910213470459,
      "learning_rate": 4.722625588697018e-05,
      "loss": 0.6885,
      "step": 282700
    },
    {
      "epoch": 4.43956043956044,
      "grad_norm": 4.520632743835449,
      "learning_rate": 4.7225274725274725e-05,
      "loss": 0.67,
      "step": 282800
    },
    {
      "epoch": 4.441130298273156,
      "grad_norm": 4.192996501922607,
      "learning_rate": 4.722429356357928e-05,
      "loss": 0.7217,
      "step": 282900
    },
    {
      "epoch": 4.442700156985872,
      "grad_norm": 3.535147190093994,
      "learning_rate": 4.722331240188383e-05,
      "loss": 0.6751,
      "step": 283000
    },
    {
      "epoch": 4.444270015698587,
      "grad_norm": 3.458132266998291,
      "learning_rate": 4.7222331240188384e-05,
      "loss": 0.6573,
      "step": 283100
    },
    {
      "epoch": 4.445839874411303,
      "grad_norm": 3.7139086723327637,
      "learning_rate": 4.7221350078492935e-05,
      "loss": 0.7113,
      "step": 283200
    },
    {
      "epoch": 4.447409733124019,
      "grad_norm": 3.1788673400878906,
      "learning_rate": 4.722036891679749e-05,
      "loss": 0.6384,
      "step": 283300
    },
    {
      "epoch": 4.448979591836735,
      "grad_norm": 4.175965785980225,
      "learning_rate": 4.7219387755102044e-05,
      "loss": 0.6848,
      "step": 283400
    },
    {
      "epoch": 4.450549450549451,
      "grad_norm": 1.6006017923355103,
      "learning_rate": 4.7218406593406595e-05,
      "loss": 0.6543,
      "step": 283500
    },
    {
      "epoch": 4.452119309262167,
      "grad_norm": 4.301729679107666,
      "learning_rate": 4.7217425431711146e-05,
      "loss": 0.5996,
      "step": 283600
    },
    {
      "epoch": 4.453689167974883,
      "grad_norm": 4.074408531188965,
      "learning_rate": 4.7216444270015704e-05,
      "loss": 0.6545,
      "step": 283700
    },
    {
      "epoch": 4.455259026687598,
      "grad_norm": 4.456742763519287,
      "learning_rate": 4.7215463108320255e-05,
      "loss": 0.6729,
      "step": 283800
    },
    {
      "epoch": 4.456828885400314,
      "grad_norm": 3.4231069087982178,
      "learning_rate": 4.7214481946624806e-05,
      "loss": 0.6779,
      "step": 283900
    },
    {
      "epoch": 4.45839874411303,
      "grad_norm": 4.171637535095215,
      "learning_rate": 4.721350078492936e-05,
      "loss": 0.6742,
      "step": 284000
    },
    {
      "epoch": 4.459968602825746,
      "grad_norm": 4.440673828125,
      "learning_rate": 4.7212519623233915e-05,
      "loss": 0.7044,
      "step": 284100
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 4.035192489624023,
      "learning_rate": 4.7211538461538465e-05,
      "loss": 0.6405,
      "step": 284200
    },
    {
      "epoch": 4.463108320251178,
      "grad_norm": 3.6815571784973145,
      "learning_rate": 4.7210557299843016e-05,
      "loss": 0.6731,
      "step": 284300
    },
    {
      "epoch": 4.464678178963894,
      "grad_norm": 3.958411931991577,
      "learning_rate": 4.7209576138147574e-05,
      "loss": 0.6615,
      "step": 284400
    },
    {
      "epoch": 4.466248037676609,
      "grad_norm": 3.755383014678955,
      "learning_rate": 4.720859497645212e-05,
      "loss": 0.6778,
      "step": 284500
    },
    {
      "epoch": 4.467817896389325,
      "grad_norm": 3.578469753265381,
      "learning_rate": 4.7207613814756676e-05,
      "loss": 0.664,
      "step": 284600
    },
    {
      "epoch": 4.469387755102041,
      "grad_norm": 3.8740530014038086,
      "learning_rate": 4.720663265306123e-05,
      "loss": 0.6883,
      "step": 284700
    },
    {
      "epoch": 4.470957613814757,
      "grad_norm": 4.5648322105407715,
      "learning_rate": 4.720565149136578e-05,
      "loss": 0.6977,
      "step": 284800
    },
    {
      "epoch": 4.472527472527473,
      "grad_norm": 2.686384677886963,
      "learning_rate": 4.720467032967033e-05,
      "loss": 0.6604,
      "step": 284900
    },
    {
      "epoch": 4.474097331240189,
      "grad_norm": 3.9306039810180664,
      "learning_rate": 4.720368916797489e-05,
      "loss": 0.6601,
      "step": 285000
    },
    {
      "epoch": 4.475667189952905,
      "grad_norm": 4.714221000671387,
      "learning_rate": 4.720270800627944e-05,
      "loss": 0.6739,
      "step": 285100
    },
    {
      "epoch": 4.47723704866562,
      "grad_norm": 2.980924367904663,
      "learning_rate": 4.720172684458399e-05,
      "loss": 0.6588,
      "step": 285200
    },
    {
      "epoch": 4.478806907378336,
      "grad_norm": 3.721318244934082,
      "learning_rate": 4.720074568288854e-05,
      "loss": 0.6714,
      "step": 285300
    },
    {
      "epoch": 4.480376766091052,
      "grad_norm": 2.7798027992248535,
      "learning_rate": 4.71997645211931e-05,
      "loss": 0.6591,
      "step": 285400
    },
    {
      "epoch": 4.481946624803768,
      "grad_norm": 4.408758163452148,
      "learning_rate": 4.719878335949764e-05,
      "loss": 0.6575,
      "step": 285500
    },
    {
      "epoch": 4.483516483516484,
      "grad_norm": 3.095017671585083,
      "learning_rate": 4.71978021978022e-05,
      "loss": 0.6381,
      "step": 285600
    },
    {
      "epoch": 4.4850863422291996,
      "grad_norm": 3.8953144550323486,
      "learning_rate": 4.719682103610675e-05,
      "loss": 0.6765,
      "step": 285700
    },
    {
      "epoch": 4.4866562009419155,
      "grad_norm": 3.381359338760376,
      "learning_rate": 4.719583987441131e-05,
      "loss": 0.6469,
      "step": 285800
    },
    {
      "epoch": 4.488226059654631,
      "grad_norm": 3.955670118331909,
      "learning_rate": 4.719485871271586e-05,
      "loss": 0.6876,
      "step": 285900
    },
    {
      "epoch": 4.489795918367347,
      "grad_norm": 4.285511493682861,
      "learning_rate": 4.719387755102041e-05,
      "loss": 0.6909,
      "step": 286000
    },
    {
      "epoch": 4.491365777080063,
      "grad_norm": 4.48303747177124,
      "learning_rate": 4.719289638932496e-05,
      "loss": 0.6462,
      "step": 286100
    },
    {
      "epoch": 4.4929356357927785,
      "grad_norm": 3.071065902709961,
      "learning_rate": 4.719191522762951e-05,
      "loss": 0.6995,
      "step": 286200
    },
    {
      "epoch": 4.4945054945054945,
      "grad_norm": 4.179928302764893,
      "learning_rate": 4.719093406593407e-05,
      "loss": 0.7118,
      "step": 286300
    },
    {
      "epoch": 4.4960753532182105,
      "grad_norm": 4.571705341339111,
      "learning_rate": 4.718995290423862e-05,
      "loss": 0.6634,
      "step": 286400
    },
    {
      "epoch": 4.4976452119309265,
      "grad_norm": 4.113286018371582,
      "learning_rate": 4.718897174254318e-05,
      "loss": 0.6903,
      "step": 286500
    },
    {
      "epoch": 4.4992150706436425,
      "grad_norm": 3.4205968379974365,
      "learning_rate": 4.718799058084772e-05,
      "loss": 0.6985,
      "step": 286600
    },
    {
      "epoch": 4.5007849293563575,
      "grad_norm": 4.5610480308532715,
      "learning_rate": 4.718700941915228e-05,
      "loss": 0.7082,
      "step": 286700
    },
    {
      "epoch": 4.5023547880690735,
      "grad_norm": 4.712759017944336,
      "learning_rate": 4.718602825745683e-05,
      "loss": 0.6654,
      "step": 286800
    },
    {
      "epoch": 4.5039246467817895,
      "grad_norm": 3.5302796363830566,
      "learning_rate": 4.718504709576138e-05,
      "loss": 0.6657,
      "step": 286900
    },
    {
      "epoch": 4.5054945054945055,
      "grad_norm": 3.552281141281128,
      "learning_rate": 4.7184065934065934e-05,
      "loss": 0.715,
      "step": 287000
    },
    {
      "epoch": 4.5070643642072215,
      "grad_norm": 3.195446014404297,
      "learning_rate": 4.718308477237049e-05,
      "loss": 0.6892,
      "step": 287100
    },
    {
      "epoch": 4.508634222919937,
      "grad_norm": 3.194026470184326,
      "learning_rate": 4.718210361067504e-05,
      "loss": 0.6591,
      "step": 287200
    },
    {
      "epoch": 4.510204081632653,
      "grad_norm": 4.26465368270874,
      "learning_rate": 4.718112244897959e-05,
      "loss": 0.6997,
      "step": 287300
    },
    {
      "epoch": 4.511773940345369,
      "grad_norm": 4.077883720397949,
      "learning_rate": 4.7180141287284144e-05,
      "loss": 0.6962,
      "step": 287400
    },
    {
      "epoch": 4.5133437990580845,
      "grad_norm": 4.177895545959473,
      "learning_rate": 4.71791601255887e-05,
      "loss": 0.6817,
      "step": 287500
    },
    {
      "epoch": 4.5149136577708004,
      "grad_norm": 4.715366840362549,
      "learning_rate": 4.7178178963893246e-05,
      "loss": 0.6854,
      "step": 287600
    },
    {
      "epoch": 4.516483516483516,
      "grad_norm": 4.1739821434021,
      "learning_rate": 4.7177197802197804e-05,
      "loss": 0.7044,
      "step": 287700
    },
    {
      "epoch": 4.518053375196232,
      "grad_norm": 3.6713593006134033,
      "learning_rate": 4.7176216640502355e-05,
      "loss": 0.6763,
      "step": 287800
    },
    {
      "epoch": 4.519623233908948,
      "grad_norm": 3.302339553833008,
      "learning_rate": 4.717523547880691e-05,
      "loss": 0.6535,
      "step": 287900
    },
    {
      "epoch": 4.521193092621664,
      "grad_norm": 3.41715669631958,
      "learning_rate": 4.7174254317111464e-05,
      "loss": 0.6924,
      "step": 288000
    },
    {
      "epoch": 4.52276295133438,
      "grad_norm": 4.5643839836120605,
      "learning_rate": 4.7173273155416015e-05,
      "loss": 0.6223,
      "step": 288100
    },
    {
      "epoch": 4.524332810047095,
      "grad_norm": 4.317234516143799,
      "learning_rate": 4.7172291993720566e-05,
      "loss": 0.6782,
      "step": 288200
    },
    {
      "epoch": 4.525902668759811,
      "grad_norm": 4.010705471038818,
      "learning_rate": 4.717131083202512e-05,
      "loss": 0.6321,
      "step": 288300
    },
    {
      "epoch": 4.527472527472527,
      "grad_norm": 3.599325180053711,
      "learning_rate": 4.7170329670329674e-05,
      "loss": 0.6955,
      "step": 288400
    },
    {
      "epoch": 4.529042386185243,
      "grad_norm": 4.352548122406006,
      "learning_rate": 4.7169348508634225e-05,
      "loss": 0.6651,
      "step": 288500
    },
    {
      "epoch": 4.530612244897959,
      "grad_norm": 3.5428144931793213,
      "learning_rate": 4.716836734693878e-05,
      "loss": 0.656,
      "step": 288600
    },
    {
      "epoch": 4.532182103610675,
      "grad_norm": 4.1319355964660645,
      "learning_rate": 4.716738618524333e-05,
      "loss": 0.7274,
      "step": 288700
    },
    {
      "epoch": 4.533751962323391,
      "grad_norm": 4.088152885437012,
      "learning_rate": 4.7166405023547885e-05,
      "loss": 0.6727,
      "step": 288800
    },
    {
      "epoch": 4.535321821036106,
      "grad_norm": 3.4749011993408203,
      "learning_rate": 4.7165423861852436e-05,
      "loss": 0.6642,
      "step": 288900
    },
    {
      "epoch": 4.536891679748822,
      "grad_norm": 4.224738121032715,
      "learning_rate": 4.716444270015699e-05,
      "loss": 0.6577,
      "step": 289000
    },
    {
      "epoch": 4.538461538461538,
      "grad_norm": 2.613027334213257,
      "learning_rate": 4.716346153846154e-05,
      "loss": 0.6728,
      "step": 289100
    },
    {
      "epoch": 4.540031397174254,
      "grad_norm": 3.6837265491485596,
      "learning_rate": 4.7162480376766096e-05,
      "loss": 0.6427,
      "step": 289200
    },
    {
      "epoch": 4.54160125588697,
      "grad_norm": 3.26383638381958,
      "learning_rate": 4.716149921507065e-05,
      "loss": 0.6857,
      "step": 289300
    },
    {
      "epoch": 4.543171114599686,
      "grad_norm": 3.623631000518799,
      "learning_rate": 4.71605180533752e-05,
      "loss": 0.6288,
      "step": 289400
    },
    {
      "epoch": 4.544740973312402,
      "grad_norm": 4.304204940795898,
      "learning_rate": 4.715953689167975e-05,
      "loss": 0.6923,
      "step": 289500
    },
    {
      "epoch": 4.546310832025117,
      "grad_norm": 4.67928409576416,
      "learning_rate": 4.7158555729984307e-05,
      "loss": 0.69,
      "step": 289600
    },
    {
      "epoch": 4.547880690737833,
      "grad_norm": 3.6097538471221924,
      "learning_rate": 4.715757456828885e-05,
      "loss": 0.6596,
      "step": 289700
    },
    {
      "epoch": 4.549450549450549,
      "grad_norm": 3.2071638107299805,
      "learning_rate": 4.715659340659341e-05,
      "loss": 0.6593,
      "step": 289800
    },
    {
      "epoch": 4.551020408163265,
      "grad_norm": 5.005595684051514,
      "learning_rate": 4.715561224489796e-05,
      "loss": 0.6629,
      "step": 289900
    },
    {
      "epoch": 4.552590266875981,
      "grad_norm": 5.083735942840576,
      "learning_rate": 4.715463108320252e-05,
      "loss": 0.6671,
      "step": 290000
    },
    {
      "epoch": 4.554160125588697,
      "grad_norm": 3.744908094406128,
      "learning_rate": 4.715364992150706e-05,
      "loss": 0.6747,
      "step": 290100
    },
    {
      "epoch": 4.555729984301413,
      "grad_norm": 4.271833896636963,
      "learning_rate": 4.715266875981162e-05,
      "loss": 0.6706,
      "step": 290200
    },
    {
      "epoch": 4.557299843014128,
      "grad_norm": 4.589900970458984,
      "learning_rate": 4.715168759811617e-05,
      "loss": 0.67,
      "step": 290300
    },
    {
      "epoch": 4.558869701726844,
      "grad_norm": 3.829991579055786,
      "learning_rate": 4.715070643642072e-05,
      "loss": 0.6666,
      "step": 290400
    },
    {
      "epoch": 4.56043956043956,
      "grad_norm": 4.816482067108154,
      "learning_rate": 4.714972527472528e-05,
      "loss": 0.666,
      "step": 290500
    },
    {
      "epoch": 4.562009419152276,
      "grad_norm": 3.3753244876861572,
      "learning_rate": 4.714874411302983e-05,
      "loss": 0.686,
      "step": 290600
    },
    {
      "epoch": 4.563579277864992,
      "grad_norm": 4.220893383026123,
      "learning_rate": 4.714776295133439e-05,
      "loss": 0.7206,
      "step": 290700
    },
    {
      "epoch": 4.565149136577708,
      "grad_norm": 3.293107271194458,
      "learning_rate": 4.714678178963893e-05,
      "loss": 0.6462,
      "step": 290800
    },
    {
      "epoch": 4.566718995290424,
      "grad_norm": 4.953923225402832,
      "learning_rate": 4.714580062794349e-05,
      "loss": 0.6278,
      "step": 290900
    },
    {
      "epoch": 4.568288854003139,
      "grad_norm": 4.246881484985352,
      "learning_rate": 4.714481946624804e-05,
      "loss": 0.6511,
      "step": 291000
    },
    {
      "epoch": 4.569858712715855,
      "grad_norm": 3.2393627166748047,
      "learning_rate": 4.714383830455259e-05,
      "loss": 0.6979,
      "step": 291100
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 4.704121112823486,
      "learning_rate": 4.714285714285714e-05,
      "loss": 0.6998,
      "step": 291200
    },
    {
      "epoch": 4.572998430141287,
      "grad_norm": 4.751491546630859,
      "learning_rate": 4.71418759811617e-05,
      "loss": 0.6798,
      "step": 291300
    },
    {
      "epoch": 4.574568288854003,
      "grad_norm": 3.675246000289917,
      "learning_rate": 4.714089481946625e-05,
      "loss": 0.7227,
      "step": 291400
    },
    {
      "epoch": 4.576138147566719,
      "grad_norm": 4.40720272064209,
      "learning_rate": 4.71399136577708e-05,
      "loss": 0.6653,
      "step": 291500
    },
    {
      "epoch": 4.577708006279435,
      "grad_norm": 4.983305931091309,
      "learning_rate": 4.713893249607535e-05,
      "loss": 0.6828,
      "step": 291600
    },
    {
      "epoch": 4.579277864992151,
      "grad_norm": 4.2581963539123535,
      "learning_rate": 4.713795133437991e-05,
      "loss": 0.7318,
      "step": 291700
    },
    {
      "epoch": 4.580847723704867,
      "grad_norm": 5.698441028594971,
      "learning_rate": 4.7136970172684455e-05,
      "loss": 0.6584,
      "step": 291800
    },
    {
      "epoch": 4.582417582417582,
      "grad_norm": 4.63938570022583,
      "learning_rate": 4.713598901098901e-05,
      "loss": 0.7046,
      "step": 291900
    },
    {
      "epoch": 4.583987441130298,
      "grad_norm": 4.561948299407959,
      "learning_rate": 4.7135007849293564e-05,
      "loss": 0.6971,
      "step": 292000
    },
    {
      "epoch": 4.585557299843014,
      "grad_norm": 3.950939655303955,
      "learning_rate": 4.713402668759812e-05,
      "loss": 0.6515,
      "step": 292100
    },
    {
      "epoch": 4.58712715855573,
      "grad_norm": 4.308842658996582,
      "learning_rate": 4.7133045525902666e-05,
      "loss": 0.67,
      "step": 292200
    },
    {
      "epoch": 4.588697017268446,
      "grad_norm": 3.8765954971313477,
      "learning_rate": 4.7132064364207224e-05,
      "loss": 0.642,
      "step": 292300
    },
    {
      "epoch": 4.590266875981162,
      "grad_norm": 3.968050479888916,
      "learning_rate": 4.7131083202511775e-05,
      "loss": 0.688,
      "step": 292400
    },
    {
      "epoch": 4.591836734693878,
      "grad_norm": 3.9059336185455322,
      "learning_rate": 4.7130102040816326e-05,
      "loss": 0.6433,
      "step": 292500
    },
    {
      "epoch": 4.593406593406593,
      "grad_norm": 3.174269437789917,
      "learning_rate": 4.7129120879120883e-05,
      "loss": 0.6756,
      "step": 292600
    },
    {
      "epoch": 4.594976452119309,
      "grad_norm": 3.8179261684417725,
      "learning_rate": 4.7128139717425434e-05,
      "loss": 0.6577,
      "step": 292700
    },
    {
      "epoch": 4.596546310832025,
      "grad_norm": 4.289277076721191,
      "learning_rate": 4.712715855572999e-05,
      "loss": 0.6731,
      "step": 292800
    },
    {
      "epoch": 4.598116169544741,
      "grad_norm": 4.107276439666748,
      "learning_rate": 4.7126177394034536e-05,
      "loss": 0.6962,
      "step": 292900
    },
    {
      "epoch": 4.599686028257457,
      "grad_norm": 4.7359185218811035,
      "learning_rate": 4.7125196232339094e-05,
      "loss": 0.6925,
      "step": 293000
    },
    {
      "epoch": 4.601255886970173,
      "grad_norm": 3.3622548580169678,
      "learning_rate": 4.7124215070643645e-05,
      "loss": 0.7012,
      "step": 293100
    },
    {
      "epoch": 4.602825745682889,
      "grad_norm": 4.95927619934082,
      "learning_rate": 4.7123233908948196e-05,
      "loss": 0.7109,
      "step": 293200
    },
    {
      "epoch": 4.604395604395604,
      "grad_norm": 4.360296249389648,
      "learning_rate": 4.712225274725275e-05,
      "loss": 0.6498,
      "step": 293300
    },
    {
      "epoch": 4.60596546310832,
      "grad_norm": 5.046242713928223,
      "learning_rate": 4.7121271585557305e-05,
      "loss": 0.7005,
      "step": 293400
    },
    {
      "epoch": 4.607535321821036,
      "grad_norm": 3.563018321990967,
      "learning_rate": 4.7120290423861856e-05,
      "loss": 0.6794,
      "step": 293500
    },
    {
      "epoch": 4.609105180533752,
      "grad_norm": 4.313770294189453,
      "learning_rate": 4.711930926216641e-05,
      "loss": 0.647,
      "step": 293600
    },
    {
      "epoch": 4.610675039246468,
      "grad_norm": 4.397770881652832,
      "learning_rate": 4.711832810047096e-05,
      "loss": 0.7155,
      "step": 293700
    },
    {
      "epoch": 4.612244897959184,
      "grad_norm": 3.289773464202881,
      "learning_rate": 4.7117346938775516e-05,
      "loss": 0.6417,
      "step": 293800
    },
    {
      "epoch": 4.6138147566719,
      "grad_norm": 4.540371417999268,
      "learning_rate": 4.711636577708006e-05,
      "loss": 0.6612,
      "step": 293900
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 3.5849766731262207,
      "learning_rate": 4.711538461538462e-05,
      "loss": 0.6638,
      "step": 294000
    },
    {
      "epoch": 4.616954474097331,
      "grad_norm": 3.8784594535827637,
      "learning_rate": 4.711440345368917e-05,
      "loss": 0.6637,
      "step": 294100
    },
    {
      "epoch": 4.618524332810047,
      "grad_norm": 4.381464958190918,
      "learning_rate": 4.7113422291993726e-05,
      "loss": 0.714,
      "step": 294200
    },
    {
      "epoch": 4.620094191522763,
      "grad_norm": 4.532561779022217,
      "learning_rate": 4.711244113029827e-05,
      "loss": 0.6981,
      "step": 294300
    },
    {
      "epoch": 4.621664050235479,
      "grad_norm": 3.8347878456115723,
      "learning_rate": 4.711145996860283e-05,
      "loss": 0.6787,
      "step": 294400
    },
    {
      "epoch": 4.623233908948195,
      "grad_norm": 3.3082072734832764,
      "learning_rate": 4.711047880690738e-05,
      "loss": 0.651,
      "step": 294500
    },
    {
      "epoch": 4.624803767660911,
      "grad_norm": 4.14057731628418,
      "learning_rate": 4.710949764521193e-05,
      "loss": 0.6638,
      "step": 294600
    },
    {
      "epoch": 4.626373626373626,
      "grad_norm": 3.411313056945801,
      "learning_rate": 4.710851648351649e-05,
      "loss": 0.6596,
      "step": 294700
    },
    {
      "epoch": 4.627943485086342,
      "grad_norm": 4.1598711013793945,
      "learning_rate": 4.710753532182104e-05,
      "loss": 0.6836,
      "step": 294800
    },
    {
      "epoch": 4.629513343799058,
      "grad_norm": 2.675616979598999,
      "learning_rate": 4.71065541601256e-05,
      "loss": 0.6771,
      "step": 294900
    },
    {
      "epoch": 4.631083202511774,
      "grad_norm": 4.22223424911499,
      "learning_rate": 4.710557299843014e-05,
      "loss": 0.6929,
      "step": 295000
    },
    {
      "epoch": 4.63265306122449,
      "grad_norm": 3.9938178062438965,
      "learning_rate": 4.71045918367347e-05,
      "loss": 0.6757,
      "step": 295100
    },
    {
      "epoch": 4.634222919937206,
      "grad_norm": 4.042769432067871,
      "learning_rate": 4.710361067503925e-05,
      "loss": 0.6826,
      "step": 295200
    },
    {
      "epoch": 4.635792778649922,
      "grad_norm": 4.800907611846924,
      "learning_rate": 4.71026295133438e-05,
      "loss": 0.6567,
      "step": 295300
    },
    {
      "epoch": 4.637362637362637,
      "grad_norm": 3.901155471801758,
      "learning_rate": 4.710164835164835e-05,
      "loss": 0.6628,
      "step": 295400
    },
    {
      "epoch": 4.638932496075353,
      "grad_norm": 3.30061674118042,
      "learning_rate": 4.710066718995291e-05,
      "loss": 0.7134,
      "step": 295500
    },
    {
      "epoch": 4.640502354788069,
      "grad_norm": 3.6017441749572754,
      "learning_rate": 4.709968602825746e-05,
      "loss": 0.6253,
      "step": 295600
    },
    {
      "epoch": 4.642072213500785,
      "grad_norm": 4.064005374908447,
      "learning_rate": 4.709870486656201e-05,
      "loss": 0.69,
      "step": 295700
    },
    {
      "epoch": 4.643642072213501,
      "grad_norm": 4.573694705963135,
      "learning_rate": 4.709772370486656e-05,
      "loss": 0.6822,
      "step": 295800
    },
    {
      "epoch": 4.645211930926217,
      "grad_norm": 3.5792789459228516,
      "learning_rate": 4.709674254317112e-05,
      "loss": 0.6623,
      "step": 295900
    },
    {
      "epoch": 4.646781789638933,
      "grad_norm": 3.9597082138061523,
      "learning_rate": 4.7095761381475664e-05,
      "loss": 0.6824,
      "step": 296000
    },
    {
      "epoch": 4.648351648351649,
      "grad_norm": 4.356625556945801,
      "learning_rate": 4.709478021978022e-05,
      "loss": 0.6919,
      "step": 296100
    },
    {
      "epoch": 4.649921507064365,
      "grad_norm": 4.4317307472229,
      "learning_rate": 4.709379905808477e-05,
      "loss": 0.672,
      "step": 296200
    },
    {
      "epoch": 4.65149136577708,
      "grad_norm": 3.9809539318084717,
      "learning_rate": 4.709281789638933e-05,
      "loss": 0.6435,
      "step": 296300
    },
    {
      "epoch": 4.653061224489796,
      "grad_norm": 3.7955262660980225,
      "learning_rate": 4.7091836734693875e-05,
      "loss": 0.7124,
      "step": 296400
    },
    {
      "epoch": 4.654631083202512,
      "grad_norm": 3.4896152019500732,
      "learning_rate": 4.709085557299843e-05,
      "loss": 0.6797,
      "step": 296500
    },
    {
      "epoch": 4.656200941915228,
      "grad_norm": 3.5174617767333984,
      "learning_rate": 4.7089874411302984e-05,
      "loss": 0.6775,
      "step": 296600
    },
    {
      "epoch": 4.657770800627944,
      "grad_norm": 5.038230895996094,
      "learning_rate": 4.7088893249607535e-05,
      "loss": 0.6967,
      "step": 296700
    },
    {
      "epoch": 4.65934065934066,
      "grad_norm": 3.7017645835876465,
      "learning_rate": 4.708791208791209e-05,
      "loss": 0.6884,
      "step": 296800
    },
    {
      "epoch": 4.660910518053376,
      "grad_norm": 5.081867218017578,
      "learning_rate": 4.7086930926216643e-05,
      "loss": 0.6565,
      "step": 296900
    },
    {
      "epoch": 4.662480376766091,
      "grad_norm": 3.3381826877593994,
      "learning_rate": 4.7085949764521194e-05,
      "loss": 0.6878,
      "step": 297000
    },
    {
      "epoch": 4.664050235478807,
      "grad_norm": 4.537021160125732,
      "learning_rate": 4.7084968602825745e-05,
      "loss": 0.6815,
      "step": 297100
    },
    {
      "epoch": 4.665620094191523,
      "grad_norm": 4.191314697265625,
      "learning_rate": 4.70839874411303e-05,
      "loss": 0.6953,
      "step": 297200
    },
    {
      "epoch": 4.667189952904239,
      "grad_norm": 3.72737717628479,
      "learning_rate": 4.7083006279434854e-05,
      "loss": 0.6965,
      "step": 297300
    },
    {
      "epoch": 4.668759811616955,
      "grad_norm": 4.157914638519287,
      "learning_rate": 4.7082025117739405e-05,
      "loss": 0.6543,
      "step": 297400
    },
    {
      "epoch": 4.670329670329671,
      "grad_norm": 4.313502788543701,
      "learning_rate": 4.7081043956043956e-05,
      "loss": 0.6766,
      "step": 297500
    },
    {
      "epoch": 4.671899529042387,
      "grad_norm": 4.269144535064697,
      "learning_rate": 4.7080062794348514e-05,
      "loss": 0.6483,
      "step": 297600
    },
    {
      "epoch": 4.673469387755102,
      "grad_norm": 4.59192419052124,
      "learning_rate": 4.7079081632653065e-05,
      "loss": 0.6598,
      "step": 297700
    },
    {
      "epoch": 4.675039246467818,
      "grad_norm": 4.221196174621582,
      "learning_rate": 4.7078100470957616e-05,
      "loss": 0.6709,
      "step": 297800
    },
    {
      "epoch": 4.676609105180534,
      "grad_norm": 3.089789390563965,
      "learning_rate": 4.707711930926217e-05,
      "loss": 0.696,
      "step": 297900
    },
    {
      "epoch": 4.67817896389325,
      "grad_norm": 5.034912586212158,
      "learning_rate": 4.7076138147566725e-05,
      "loss": 0.6594,
      "step": 298000
    },
    {
      "epoch": 4.679748822605966,
      "grad_norm": 2.8415794372558594,
      "learning_rate": 4.707515698587127e-05,
      "loss": 0.6706,
      "step": 298100
    },
    {
      "epoch": 4.681318681318682,
      "grad_norm": 4.3535542488098145,
      "learning_rate": 4.7074175824175826e-05,
      "loss": 0.7212,
      "step": 298200
    },
    {
      "epoch": 4.6828885400313975,
      "grad_norm": 3.8680531978607178,
      "learning_rate": 4.707319466248038e-05,
      "loss": 0.6481,
      "step": 298300
    },
    {
      "epoch": 4.684458398744113,
      "grad_norm": 3.5587916374206543,
      "learning_rate": 4.7072213500784935e-05,
      "loss": 0.7013,
      "step": 298400
    },
    {
      "epoch": 4.686028257456829,
      "grad_norm": 4.935722827911377,
      "learning_rate": 4.707123233908948e-05,
      "loss": 0.7209,
      "step": 298500
    },
    {
      "epoch": 4.687598116169545,
      "grad_norm": 3.991694211959839,
      "learning_rate": 4.707025117739404e-05,
      "loss": 0.6766,
      "step": 298600
    },
    {
      "epoch": 4.689167974882261,
      "grad_norm": 4.2006916999816895,
      "learning_rate": 4.706927001569859e-05,
      "loss": 0.7021,
      "step": 298700
    },
    {
      "epoch": 4.6907378335949765,
      "grad_norm": 3.442377805709839,
      "learning_rate": 4.706828885400314e-05,
      "loss": 0.6374,
      "step": 298800
    },
    {
      "epoch": 4.6923076923076925,
      "grad_norm": 3.4888784885406494,
      "learning_rate": 4.70673076923077e-05,
      "loss": 0.6612,
      "step": 298900
    },
    {
      "epoch": 4.6938775510204085,
      "grad_norm": 4.294275283813477,
      "learning_rate": 4.706632653061225e-05,
      "loss": 0.6931,
      "step": 299000
    },
    {
      "epoch": 4.695447409733124,
      "grad_norm": 4.7136006355285645,
      "learning_rate": 4.70653453689168e-05,
      "loss": 0.627,
      "step": 299100
    },
    {
      "epoch": 4.6970172684458396,
      "grad_norm": 2.9293293952941895,
      "learning_rate": 4.706436420722135e-05,
      "loss": 0.7427,
      "step": 299200
    },
    {
      "epoch": 4.6985871271585555,
      "grad_norm": 4.03350305557251,
      "learning_rate": 4.706338304552591e-05,
      "loss": 0.6818,
      "step": 299300
    },
    {
      "epoch": 4.7001569858712715,
      "grad_norm": 4.070458889007568,
      "learning_rate": 4.706240188383046e-05,
      "loss": 0.6494,
      "step": 299400
    },
    {
      "epoch": 4.7017268445839875,
      "grad_norm": 3.4781455993652344,
      "learning_rate": 4.706142072213501e-05,
      "loss": 0.6699,
      "step": 299500
    },
    {
      "epoch": 4.7032967032967035,
      "grad_norm": 4.155882358551025,
      "learning_rate": 4.706043956043956e-05,
      "loss": 0.7333,
      "step": 299600
    },
    {
      "epoch": 4.704866562009419,
      "grad_norm": 3.2412548065185547,
      "learning_rate": 4.705945839874412e-05,
      "loss": 0.6672,
      "step": 299700
    },
    {
      "epoch": 4.7064364207221345,
      "grad_norm": 3.3004980087280273,
      "learning_rate": 4.705847723704867e-05,
      "loss": 0.6536,
      "step": 299800
    },
    {
      "epoch": 4.7080062794348505,
      "grad_norm": 4.0152363777160645,
      "learning_rate": 4.705749607535322e-05,
      "loss": 0.6743,
      "step": 299900
    },
    {
      "epoch": 4.7095761381475665,
      "grad_norm": 4.722228527069092,
      "learning_rate": 4.705651491365777e-05,
      "loss": 0.6845,
      "step": 300000
    },
    {
      "epoch": 4.7111459968602825,
      "grad_norm": 4.299479007720947,
      "learning_rate": 4.705553375196233e-05,
      "loss": 0.6951,
      "step": 300100
    },
    {
      "epoch": 4.712715855572998,
      "grad_norm": 3.7449779510498047,
      "learning_rate": 4.705455259026687e-05,
      "loss": 0.712,
      "step": 300200
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 3.8778815269470215,
      "learning_rate": 4.705357142857143e-05,
      "loss": 0.6542,
      "step": 300300
    },
    {
      "epoch": 4.71585557299843,
      "grad_norm": 4.395393371582031,
      "learning_rate": 4.705259026687598e-05,
      "loss": 0.6796,
      "step": 300400
    },
    {
      "epoch": 4.717425431711146,
      "grad_norm": 3.52555775642395,
      "learning_rate": 4.705160910518054e-05,
      "loss": 0.7016,
      "step": 300500
    },
    {
      "epoch": 4.718995290423862,
      "grad_norm": 4.140084266662598,
      "learning_rate": 4.7050627943485084e-05,
      "loss": 0.6481,
      "step": 300600
    },
    {
      "epoch": 4.720565149136577,
      "grad_norm": 3.4186019897460938,
      "learning_rate": 4.704964678178964e-05,
      "loss": 0.6502,
      "step": 300700
    },
    {
      "epoch": 4.722135007849293,
      "grad_norm": 3.7579658031463623,
      "learning_rate": 4.704866562009419e-05,
      "loss": 0.6452,
      "step": 300800
    },
    {
      "epoch": 4.723704866562009,
      "grad_norm": 4.088251113891602,
      "learning_rate": 4.7047684458398744e-05,
      "loss": 0.6745,
      "step": 300900
    },
    {
      "epoch": 4.725274725274725,
      "grad_norm": 5.283199787139893,
      "learning_rate": 4.70467032967033e-05,
      "loss": 0.6919,
      "step": 301000
    },
    {
      "epoch": 4.726844583987441,
      "grad_norm": 4.154319763183594,
      "learning_rate": 4.704572213500785e-05,
      "loss": 0.686,
      "step": 301100
    },
    {
      "epoch": 4.728414442700157,
      "grad_norm": 4.42350435256958,
      "learning_rate": 4.70447409733124e-05,
      "loss": 0.6567,
      "step": 301200
    },
    {
      "epoch": 4.729984301412873,
      "grad_norm": 5.263411045074463,
      "learning_rate": 4.7043759811616954e-05,
      "loss": 0.6684,
      "step": 301300
    },
    {
      "epoch": 4.731554160125588,
      "grad_norm": 3.7327983379364014,
      "learning_rate": 4.704277864992151e-05,
      "loss": 0.6935,
      "step": 301400
    },
    {
      "epoch": 4.733124018838304,
      "grad_norm": 4.04680871963501,
      "learning_rate": 4.704179748822606e-05,
      "loss": 0.6118,
      "step": 301500
    },
    {
      "epoch": 4.73469387755102,
      "grad_norm": 4.455444812774658,
      "learning_rate": 4.7040816326530614e-05,
      "loss": 0.6288,
      "step": 301600
    },
    {
      "epoch": 4.736263736263736,
      "grad_norm": 4.487597942352295,
      "learning_rate": 4.7039835164835165e-05,
      "loss": 0.6892,
      "step": 301700
    },
    {
      "epoch": 4.737833594976452,
      "grad_norm": 4.303781032562256,
      "learning_rate": 4.703885400313972e-05,
      "loss": 0.6396,
      "step": 301800
    },
    {
      "epoch": 4.739403453689168,
      "grad_norm": 3.4089341163635254,
      "learning_rate": 4.7037872841444274e-05,
      "loss": 0.7083,
      "step": 301900
    },
    {
      "epoch": 4.740973312401884,
      "grad_norm": 3.1110386848449707,
      "learning_rate": 4.7036891679748825e-05,
      "loss": 0.6844,
      "step": 302000
    },
    {
      "epoch": 4.742543171114599,
      "grad_norm": 4.011343002319336,
      "learning_rate": 4.7035910518053376e-05,
      "loss": 0.6484,
      "step": 302100
    },
    {
      "epoch": 4.744113029827315,
      "grad_norm": 3.8393337726593018,
      "learning_rate": 4.7034929356357934e-05,
      "loss": 0.7006,
      "step": 302200
    },
    {
      "epoch": 4.745682888540031,
      "grad_norm": 3.3312432765960693,
      "learning_rate": 4.703394819466248e-05,
      "loss": 0.7121,
      "step": 302300
    },
    {
      "epoch": 4.747252747252747,
      "grad_norm": 3.184554100036621,
      "learning_rate": 4.7032967032967035e-05,
      "loss": 0.6419,
      "step": 302400
    },
    {
      "epoch": 4.748822605965463,
      "grad_norm": 3.90944766998291,
      "learning_rate": 4.7031985871271586e-05,
      "loss": 0.6966,
      "step": 302500
    },
    {
      "epoch": 4.750392464678179,
      "grad_norm": 3.9492340087890625,
      "learning_rate": 4.7031004709576144e-05,
      "loss": 0.6706,
      "step": 302600
    },
    {
      "epoch": 4.751962323390895,
      "grad_norm": 4.442107200622559,
      "learning_rate": 4.703002354788069e-05,
      "loss": 0.7014,
      "step": 302700
    },
    {
      "epoch": 4.75353218210361,
      "grad_norm": 3.4604177474975586,
      "learning_rate": 4.7029042386185246e-05,
      "loss": 0.6802,
      "step": 302800
    },
    {
      "epoch": 4.755102040816326,
      "grad_norm": 4.025793075561523,
      "learning_rate": 4.70280612244898e-05,
      "loss": 0.6763,
      "step": 302900
    },
    {
      "epoch": 4.756671899529042,
      "grad_norm": 3.971118688583374,
      "learning_rate": 4.702708006279435e-05,
      "loss": 0.6062,
      "step": 303000
    },
    {
      "epoch": 4.758241758241758,
      "grad_norm": 3.7719905376434326,
      "learning_rate": 4.7026098901098906e-05,
      "loss": 0.6911,
      "step": 303100
    },
    {
      "epoch": 4.759811616954474,
      "grad_norm": 5.159405708312988,
      "learning_rate": 4.702511773940346e-05,
      "loss": 0.6947,
      "step": 303200
    },
    {
      "epoch": 4.76138147566719,
      "grad_norm": 3.955939769744873,
      "learning_rate": 4.702413657770801e-05,
      "loss": 0.6421,
      "step": 303300
    },
    {
      "epoch": 4.762951334379906,
      "grad_norm": 4.321470737457275,
      "learning_rate": 4.702315541601256e-05,
      "loss": 0.6896,
      "step": 303400
    },
    {
      "epoch": 4.764521193092621,
      "grad_norm": 4.20265007019043,
      "learning_rate": 4.7022174254317117e-05,
      "loss": 0.6485,
      "step": 303500
    },
    {
      "epoch": 4.766091051805337,
      "grad_norm": 4.936803340911865,
      "learning_rate": 4.702119309262167e-05,
      "loss": 0.6329,
      "step": 303600
    },
    {
      "epoch": 4.767660910518053,
      "grad_norm": 3.7521767616271973,
      "learning_rate": 4.702021193092622e-05,
      "loss": 0.6976,
      "step": 303700
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 3.7460782527923584,
      "learning_rate": 4.701923076923077e-05,
      "loss": 0.6944,
      "step": 303800
    },
    {
      "epoch": 4.770800627943485,
      "grad_norm": 4.077909469604492,
      "learning_rate": 4.701824960753533e-05,
      "loss": 0.6978,
      "step": 303900
    },
    {
      "epoch": 4.772370486656201,
      "grad_norm": 4.251471519470215,
      "learning_rate": 4.701726844583988e-05,
      "loss": 0.7184,
      "step": 304000
    },
    {
      "epoch": 4.773940345368917,
      "grad_norm": 3.833857774734497,
      "learning_rate": 4.701628728414443e-05,
      "loss": 0.6621,
      "step": 304100
    },
    {
      "epoch": 4.775510204081632,
      "grad_norm": 5.295615196228027,
      "learning_rate": 4.701530612244898e-05,
      "loss": 0.6552,
      "step": 304200
    },
    {
      "epoch": 4.777080062794348,
      "grad_norm": 4.027021884918213,
      "learning_rate": 4.701432496075354e-05,
      "loss": 0.661,
      "step": 304300
    },
    {
      "epoch": 4.778649921507064,
      "grad_norm": 3.9470255374908447,
      "learning_rate": 4.701334379905808e-05,
      "loss": 0.6925,
      "step": 304400
    },
    {
      "epoch": 4.78021978021978,
      "grad_norm": 4.609099864959717,
      "learning_rate": 4.701236263736264e-05,
      "loss": 0.6267,
      "step": 304500
    },
    {
      "epoch": 4.781789638932496,
      "grad_norm": 3.5311429500579834,
      "learning_rate": 4.701138147566719e-05,
      "loss": 0.6403,
      "step": 304600
    },
    {
      "epoch": 4.783359497645212,
      "grad_norm": 3.722991943359375,
      "learning_rate": 4.701040031397175e-05,
      "loss": 0.6501,
      "step": 304700
    },
    {
      "epoch": 4.784929356357928,
      "grad_norm": 3.8659467697143555,
      "learning_rate": 4.700941915227629e-05,
      "loss": 0.6754,
      "step": 304800
    },
    {
      "epoch": 4.786499215070644,
      "grad_norm": 3.9785051345825195,
      "learning_rate": 4.700843799058085e-05,
      "loss": 0.6795,
      "step": 304900
    },
    {
      "epoch": 4.78806907378336,
      "grad_norm": 4.648197174072266,
      "learning_rate": 4.70074568288854e-05,
      "loss": 0.6814,
      "step": 305000
    },
    {
      "epoch": 4.789638932496075,
      "grad_norm": 3.7689149379730225,
      "learning_rate": 4.700647566718995e-05,
      "loss": 0.6719,
      "step": 305100
    },
    {
      "epoch": 4.791208791208791,
      "grad_norm": 3.883761167526245,
      "learning_rate": 4.700549450549451e-05,
      "loss": 0.662,
      "step": 305200
    },
    {
      "epoch": 4.792778649921507,
      "grad_norm": 4.101100444793701,
      "learning_rate": 4.700451334379906e-05,
      "loss": 0.7026,
      "step": 305300
    },
    {
      "epoch": 4.794348508634223,
      "grad_norm": 4.029136657714844,
      "learning_rate": 4.700353218210361e-05,
      "loss": 0.6906,
      "step": 305400
    },
    {
      "epoch": 4.795918367346939,
      "grad_norm": 3.9799768924713135,
      "learning_rate": 4.700255102040816e-05,
      "loss": 0.6797,
      "step": 305500
    },
    {
      "epoch": 4.797488226059655,
      "grad_norm": 4.36775016784668,
      "learning_rate": 4.700156985871272e-05,
      "loss": 0.6853,
      "step": 305600
    },
    {
      "epoch": 4.799058084772371,
      "grad_norm": 4.098068714141846,
      "learning_rate": 4.700058869701727e-05,
      "loss": 0.6872,
      "step": 305700
    },
    {
      "epoch": 4.800627943485086,
      "grad_norm": 2.8540453910827637,
      "learning_rate": 4.699960753532182e-05,
      "loss": 0.682,
      "step": 305800
    },
    {
      "epoch": 4.802197802197802,
      "grad_norm": 3.9762585163116455,
      "learning_rate": 4.6998626373626374e-05,
      "loss": 0.7034,
      "step": 305900
    },
    {
      "epoch": 4.803767660910518,
      "grad_norm": 3.4198811054229736,
      "learning_rate": 4.699764521193093e-05,
      "loss": 0.6813,
      "step": 306000
    },
    {
      "epoch": 4.805337519623234,
      "grad_norm": 4.079990386962891,
      "learning_rate": 4.699666405023548e-05,
      "loss": 0.6804,
      "step": 306100
    },
    {
      "epoch": 4.80690737833595,
      "grad_norm": 4.864233016967773,
      "learning_rate": 4.6995682888540034e-05,
      "loss": 0.6502,
      "step": 306200
    },
    {
      "epoch": 4.808477237048666,
      "grad_norm": 4.106625556945801,
      "learning_rate": 4.6994701726844585e-05,
      "loss": 0.6735,
      "step": 306300
    },
    {
      "epoch": 4.810047095761382,
      "grad_norm": 4.043144702911377,
      "learning_rate": 4.699372056514914e-05,
      "loss": 0.66,
      "step": 306400
    },
    {
      "epoch": 4.811616954474097,
      "grad_norm": 3.707141876220703,
      "learning_rate": 4.699273940345369e-05,
      "loss": 0.7061,
      "step": 306500
    },
    {
      "epoch": 4.813186813186813,
      "grad_norm": 4.16402006149292,
      "learning_rate": 4.6991758241758244e-05,
      "loss": 0.6475,
      "step": 306600
    },
    {
      "epoch": 4.814756671899529,
      "grad_norm": 3.6913371086120605,
      "learning_rate": 4.6990777080062795e-05,
      "loss": 0.6969,
      "step": 306700
    },
    {
      "epoch": 4.816326530612245,
      "grad_norm": 4.254756927490234,
      "learning_rate": 4.698979591836735e-05,
      "loss": 0.6696,
      "step": 306800
    },
    {
      "epoch": 4.817896389324961,
      "grad_norm": 2.9493207931518555,
      "learning_rate": 4.69888147566719e-05,
      "loss": 0.675,
      "step": 306900
    },
    {
      "epoch": 4.819466248037677,
      "grad_norm": 4.271134376525879,
      "learning_rate": 4.6987833594976455e-05,
      "loss": 0.6762,
      "step": 307000
    },
    {
      "epoch": 4.821036106750393,
      "grad_norm": 4.114038944244385,
      "learning_rate": 4.6986852433281006e-05,
      "loss": 0.7026,
      "step": 307100
    },
    {
      "epoch": 4.822605965463108,
      "grad_norm": 4.011929512023926,
      "learning_rate": 4.698587127158556e-05,
      "loss": 0.684,
      "step": 307200
    },
    {
      "epoch": 4.824175824175824,
      "grad_norm": 3.897146224975586,
      "learning_rate": 4.6984890109890115e-05,
      "loss": 0.6556,
      "step": 307300
    },
    {
      "epoch": 4.82574568288854,
      "grad_norm": 3.3840482234954834,
      "learning_rate": 4.6983908948194666e-05,
      "loss": 0.6641,
      "step": 307400
    },
    {
      "epoch": 4.827315541601256,
      "grad_norm": 3.9979588985443115,
      "learning_rate": 4.698292778649922e-05,
      "loss": 0.6848,
      "step": 307500
    },
    {
      "epoch": 4.828885400313972,
      "grad_norm": 3.2818315029144287,
      "learning_rate": 4.698194662480377e-05,
      "loss": 0.6453,
      "step": 307600
    },
    {
      "epoch": 4.830455259026688,
      "grad_norm": 2.859806776046753,
      "learning_rate": 4.6980965463108326e-05,
      "loss": 0.6637,
      "step": 307700
    },
    {
      "epoch": 4.832025117739404,
      "grad_norm": 4.642868995666504,
      "learning_rate": 4.6979984301412877e-05,
      "loss": 0.6389,
      "step": 307800
    },
    {
      "epoch": 4.833594976452119,
      "grad_norm": 2.9993324279785156,
      "learning_rate": 4.697900313971743e-05,
      "loss": 0.6996,
      "step": 307900
    },
    {
      "epoch": 4.835164835164835,
      "grad_norm": 3.8832247257232666,
      "learning_rate": 4.697802197802198e-05,
      "loss": 0.6503,
      "step": 308000
    },
    {
      "epoch": 4.836734693877551,
      "grad_norm": 3.0977115631103516,
      "learning_rate": 4.6977040816326536e-05,
      "loss": 0.6541,
      "step": 308100
    },
    {
      "epoch": 4.838304552590267,
      "grad_norm": 4.022557735443115,
      "learning_rate": 4.697605965463108e-05,
      "loss": 0.6848,
      "step": 308200
    },
    {
      "epoch": 4.839874411302983,
      "grad_norm": 4.354294300079346,
      "learning_rate": 4.697507849293564e-05,
      "loss": 0.6993,
      "step": 308300
    },
    {
      "epoch": 4.841444270015699,
      "grad_norm": 3.2843096256256104,
      "learning_rate": 4.697409733124019e-05,
      "loss": 0.6725,
      "step": 308400
    },
    {
      "epoch": 4.843014128728415,
      "grad_norm": 3.3959414958953857,
      "learning_rate": 4.697311616954475e-05,
      "loss": 0.6623,
      "step": 308500
    },
    {
      "epoch": 4.84458398744113,
      "grad_norm": 4.175196647644043,
      "learning_rate": 4.697213500784929e-05,
      "loss": 0.6895,
      "step": 308600
    },
    {
      "epoch": 4.846153846153846,
      "grad_norm": 3.5398621559143066,
      "learning_rate": 4.697115384615385e-05,
      "loss": 0.6642,
      "step": 308700
    },
    {
      "epoch": 4.847723704866562,
      "grad_norm": 5.479466915130615,
      "learning_rate": 4.69701726844584e-05,
      "loss": 0.6864,
      "step": 308800
    },
    {
      "epoch": 4.849293563579278,
      "grad_norm": 3.8167784214019775,
      "learning_rate": 4.696919152276295e-05,
      "loss": 0.6603,
      "step": 308900
    },
    {
      "epoch": 4.850863422291994,
      "grad_norm": 4.373660564422607,
      "learning_rate": 4.69682103610675e-05,
      "loss": 0.6841,
      "step": 309000
    },
    {
      "epoch": 4.85243328100471,
      "grad_norm": 4.655967712402344,
      "learning_rate": 4.696722919937206e-05,
      "loss": 0.6724,
      "step": 309100
    },
    {
      "epoch": 4.854003139717426,
      "grad_norm": 4.418309211730957,
      "learning_rate": 4.696624803767661e-05,
      "loss": 0.6843,
      "step": 309200
    },
    {
      "epoch": 4.855572998430142,
      "grad_norm": 3.122331142425537,
      "learning_rate": 4.696526687598116e-05,
      "loss": 0.726,
      "step": 309300
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 4.430784702301025,
      "learning_rate": 4.696428571428572e-05,
      "loss": 0.6886,
      "step": 309400
    },
    {
      "epoch": 4.858712715855573,
      "grad_norm": 3.549574851989746,
      "learning_rate": 4.696330455259027e-05,
      "loss": 0.6552,
      "step": 309500
    },
    {
      "epoch": 4.860282574568289,
      "grad_norm": 3.1286911964416504,
      "learning_rate": 4.696232339089482e-05,
      "loss": 0.694,
      "step": 309600
    },
    {
      "epoch": 4.861852433281005,
      "grad_norm": 4.551493167877197,
      "learning_rate": 4.696134222919937e-05,
      "loss": 0.6814,
      "step": 309700
    },
    {
      "epoch": 4.863422291993721,
      "grad_norm": 3.483138084411621,
      "learning_rate": 4.696036106750393e-05,
      "loss": 0.6954,
      "step": 309800
    },
    {
      "epoch": 4.864992150706437,
      "grad_norm": 4.306197643280029,
      "learning_rate": 4.695937990580848e-05,
      "loss": 0.6267,
      "step": 309900
    },
    {
      "epoch": 4.866562009419153,
      "grad_norm": 3.072192668914795,
      "learning_rate": 4.695839874411303e-05,
      "loss": 0.6842,
      "step": 310000
    },
    {
      "epoch": 4.868131868131869,
      "grad_norm": 4.466462135314941,
      "learning_rate": 4.695741758241758e-05,
      "loss": 0.7134,
      "step": 310100
    },
    {
      "epoch": 4.869701726844584,
      "grad_norm": 4.322058200836182,
      "learning_rate": 4.695643642072214e-05,
      "loss": 0.6342,
      "step": 310200
    },
    {
      "epoch": 4.8712715855573,
      "grad_norm": 4.6572747230529785,
      "learning_rate": 4.6955455259026685e-05,
      "loss": 0.6517,
      "step": 310300
    },
    {
      "epoch": 4.872841444270016,
      "grad_norm": 3.4166219234466553,
      "learning_rate": 4.695447409733124e-05,
      "loss": 0.6703,
      "step": 310400
    },
    {
      "epoch": 4.874411302982732,
      "grad_norm": 5.352090358734131,
      "learning_rate": 4.6953492935635794e-05,
      "loss": 0.648,
      "step": 310500
    },
    {
      "epoch": 4.875981161695448,
      "grad_norm": 3.816391944885254,
      "learning_rate": 4.695251177394035e-05,
      "loss": 0.6758,
      "step": 310600
    },
    {
      "epoch": 4.877551020408164,
      "grad_norm": 3.440355062484741,
      "learning_rate": 4.6951530612244896e-05,
      "loss": 0.6894,
      "step": 310700
    },
    {
      "epoch": 4.8791208791208796,
      "grad_norm": 3.336853504180908,
      "learning_rate": 4.6950549450549453e-05,
      "loss": 0.6397,
      "step": 310800
    },
    {
      "epoch": 4.880690737833595,
      "grad_norm": 4.841472625732422,
      "learning_rate": 4.6949568288854004e-05,
      "loss": 0.6953,
      "step": 310900
    },
    {
      "epoch": 4.882260596546311,
      "grad_norm": 3.720869541168213,
      "learning_rate": 4.6948587127158555e-05,
      "loss": 0.6726,
      "step": 311000
    },
    {
      "epoch": 4.883830455259027,
      "grad_norm": 3.5842316150665283,
      "learning_rate": 4.6947605965463106e-05,
      "loss": 0.6531,
      "step": 311100
    },
    {
      "epoch": 4.885400313971743,
      "grad_norm": 3.665050983428955,
      "learning_rate": 4.6946624803767664e-05,
      "loss": 0.6692,
      "step": 311200
    },
    {
      "epoch": 4.8869701726844585,
      "grad_norm": 3.380307674407959,
      "learning_rate": 4.6945643642072215e-05,
      "loss": 0.6854,
      "step": 311300
    },
    {
      "epoch": 4.8885400313971745,
      "grad_norm": 3.9227867126464844,
      "learning_rate": 4.6944662480376766e-05,
      "loss": 0.6608,
      "step": 311400
    },
    {
      "epoch": 4.8901098901098905,
      "grad_norm": 2.899013042449951,
      "learning_rate": 4.6943681318681324e-05,
      "loss": 0.6887,
      "step": 311500
    },
    {
      "epoch": 4.891679748822606,
      "grad_norm": 3.9743895530700684,
      "learning_rate": 4.6942700156985875e-05,
      "loss": 0.6785,
      "step": 311600
    },
    {
      "epoch": 4.893249607535322,
      "grad_norm": 4.038234233856201,
      "learning_rate": 4.6941718995290426e-05,
      "loss": 0.7067,
      "step": 311700
    },
    {
      "epoch": 4.8948194662480375,
      "grad_norm": 4.462766170501709,
      "learning_rate": 4.694073783359498e-05,
      "loss": 0.6493,
      "step": 311800
    },
    {
      "epoch": 4.8963893249607535,
      "grad_norm": 4.447329044342041,
      "learning_rate": 4.6939756671899535e-05,
      "loss": 0.631,
      "step": 311900
    },
    {
      "epoch": 4.8979591836734695,
      "grad_norm": 3.328228235244751,
      "learning_rate": 4.6938775510204086e-05,
      "loss": 0.646,
      "step": 312000
    },
    {
      "epoch": 4.8995290423861855,
      "grad_norm": 3.9237277507781982,
      "learning_rate": 4.6937794348508637e-05,
      "loss": 0.651,
      "step": 312100
    },
    {
      "epoch": 4.9010989010989015,
      "grad_norm": 5.009263515472412,
      "learning_rate": 4.693681318681319e-05,
      "loss": 0.6598,
      "step": 312200
    },
    {
      "epoch": 4.9026687598116165,
      "grad_norm": 4.105545520782471,
      "learning_rate": 4.6935832025117745e-05,
      "loss": 0.6857,
      "step": 312300
    },
    {
      "epoch": 4.9042386185243325,
      "grad_norm": 4.30509090423584,
      "learning_rate": 4.693485086342229e-05,
      "loss": 0.6779,
      "step": 312400
    },
    {
      "epoch": 4.9058084772370485,
      "grad_norm": 3.1533634662628174,
      "learning_rate": 4.693386970172685e-05,
      "loss": 0.673,
      "step": 312500
    },
    {
      "epoch": 4.9073783359497645,
      "grad_norm": 4.643099784851074,
      "learning_rate": 4.69328885400314e-05,
      "loss": 0.6785,
      "step": 312600
    },
    {
      "epoch": 4.9089481946624804,
      "grad_norm": 4.087155342102051,
      "learning_rate": 4.6931907378335956e-05,
      "loss": 0.6723,
      "step": 312700
    },
    {
      "epoch": 4.910518053375196,
      "grad_norm": 3.7905960083007812,
      "learning_rate": 4.69309262166405e-05,
      "loss": 0.7017,
      "step": 312800
    },
    {
      "epoch": 4.912087912087912,
      "grad_norm": 3.6476709842681885,
      "learning_rate": 4.692994505494506e-05,
      "loss": 0.7028,
      "step": 312900
    },
    {
      "epoch": 4.9136577708006275,
      "grad_norm": 4.042351722717285,
      "learning_rate": 4.692896389324961e-05,
      "loss": 0.7156,
      "step": 313000
    },
    {
      "epoch": 4.9152276295133435,
      "grad_norm": 3.519981861114502,
      "learning_rate": 4.692798273155416e-05,
      "loss": 0.6638,
      "step": 313100
    },
    {
      "epoch": 4.916797488226059,
      "grad_norm": 3.3178319931030273,
      "learning_rate": 4.692700156985871e-05,
      "loss": 0.6833,
      "step": 313200
    },
    {
      "epoch": 4.918367346938775,
      "grad_norm": 3.0280959606170654,
      "learning_rate": 4.692602040816327e-05,
      "loss": 0.6806,
      "step": 313300
    },
    {
      "epoch": 4.919937205651491,
      "grad_norm": 3.427962303161621,
      "learning_rate": 4.692503924646782e-05,
      "loss": 0.6662,
      "step": 313400
    },
    {
      "epoch": 4.921507064364207,
      "grad_norm": 3.4183928966522217,
      "learning_rate": 4.692405808477237e-05,
      "loss": 0.6606,
      "step": 313500
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 4.185716152191162,
      "learning_rate": 4.692307692307693e-05,
      "loss": 0.6433,
      "step": 313600
    },
    {
      "epoch": 4.924646781789638,
      "grad_norm": 4.965280532836914,
      "learning_rate": 4.692209576138148e-05,
      "loss": 0.7005,
      "step": 313700
    },
    {
      "epoch": 4.926216640502354,
      "grad_norm": 3.6015284061431885,
      "learning_rate": 4.692111459968603e-05,
      "loss": 0.6879,
      "step": 313800
    },
    {
      "epoch": 4.92778649921507,
      "grad_norm": 4.320811748504639,
      "learning_rate": 4.692013343799058e-05,
      "loss": 0.6642,
      "step": 313900
    },
    {
      "epoch": 4.929356357927786,
      "grad_norm": 2.886357069015503,
      "learning_rate": 4.691915227629514e-05,
      "loss": 0.708,
      "step": 314000
    },
    {
      "epoch": 4.930926216640502,
      "grad_norm": 4.002133369445801,
      "learning_rate": 4.691817111459969e-05,
      "loss": 0.6719,
      "step": 314100
    },
    {
      "epoch": 4.932496075353218,
      "grad_norm": 3.070521354675293,
      "learning_rate": 4.691718995290424e-05,
      "loss": 0.6639,
      "step": 314200
    },
    {
      "epoch": 4.934065934065934,
      "grad_norm": 4.023017883300781,
      "learning_rate": 4.691620879120879e-05,
      "loss": 0.6442,
      "step": 314300
    },
    {
      "epoch": 4.93563579277865,
      "grad_norm": 4.97515869140625,
      "learning_rate": 4.691522762951335e-05,
      "loss": 0.6759,
      "step": 314400
    },
    {
      "epoch": 4.937205651491366,
      "grad_norm": 4.223485946655273,
      "learning_rate": 4.6914246467817894e-05,
      "loss": 0.7059,
      "step": 314500
    },
    {
      "epoch": 4.938775510204081,
      "grad_norm": 2.1544268131256104,
      "learning_rate": 4.691326530612245e-05,
      "loss": 0.6712,
      "step": 314600
    },
    {
      "epoch": 4.940345368916797,
      "grad_norm": 3.812105894088745,
      "learning_rate": 4.6912284144427e-05,
      "loss": 0.6525,
      "step": 314700
    },
    {
      "epoch": 4.941915227629513,
      "grad_norm": 4.453269004821777,
      "learning_rate": 4.691130298273156e-05,
      "loss": 0.6731,
      "step": 314800
    },
    {
      "epoch": 4.943485086342229,
      "grad_norm": 2.9883129596710205,
      "learning_rate": 4.6910321821036105e-05,
      "loss": 0.6685,
      "step": 314900
    },
    {
      "epoch": 4.945054945054945,
      "grad_norm": 1.824498176574707,
      "learning_rate": 4.690934065934066e-05,
      "loss": 0.6886,
      "step": 315000
    },
    {
      "epoch": 4.946624803767661,
      "grad_norm": 2.820533514022827,
      "learning_rate": 4.6908359497645213e-05,
      "loss": 0.7115,
      "step": 315100
    },
    {
      "epoch": 4.948194662480377,
      "grad_norm": 3.039663314819336,
      "learning_rate": 4.6907378335949764e-05,
      "loss": 0.6456,
      "step": 315200
    },
    {
      "epoch": 4.949764521193092,
      "grad_norm": 3.8210256099700928,
      "learning_rate": 4.6906397174254315e-05,
      "loss": 0.6869,
      "step": 315300
    },
    {
      "epoch": 4.951334379905808,
      "grad_norm": 2.4554965496063232,
      "learning_rate": 4.690541601255887e-05,
      "loss": 0.6856,
      "step": 315400
    },
    {
      "epoch": 4.952904238618524,
      "grad_norm": 3.3040366172790527,
      "learning_rate": 4.6904434850863424e-05,
      "loss": 0.6431,
      "step": 315500
    },
    {
      "epoch": 4.95447409733124,
      "grad_norm": 3.5071535110473633,
      "learning_rate": 4.6903453689167975e-05,
      "loss": 0.6446,
      "step": 315600
    },
    {
      "epoch": 4.956043956043956,
      "grad_norm": 4.1980414390563965,
      "learning_rate": 4.690247252747253e-05,
      "loss": 0.6684,
      "step": 315700
    },
    {
      "epoch": 4.957613814756672,
      "grad_norm": 3.684361457824707,
      "learning_rate": 4.6901491365777084e-05,
      "loss": 0.6543,
      "step": 315800
    },
    {
      "epoch": 4.959183673469388,
      "grad_norm": 3.794440269470215,
      "learning_rate": 4.6900510204081635e-05,
      "loss": 0.6512,
      "step": 315900
    },
    {
      "epoch": 4.960753532182103,
      "grad_norm": 4.5481743812561035,
      "learning_rate": 4.6899529042386186e-05,
      "loss": 0.6478,
      "step": 316000
    },
    {
      "epoch": 4.962323390894819,
      "grad_norm": 4.4414262771606445,
      "learning_rate": 4.6898547880690744e-05,
      "loss": 0.658,
      "step": 316100
    },
    {
      "epoch": 4.963893249607535,
      "grad_norm": 4.8871049880981445,
      "learning_rate": 4.6897566718995295e-05,
      "loss": 0.7004,
      "step": 316200
    },
    {
      "epoch": 4.965463108320251,
      "grad_norm": 3.819378614425659,
      "learning_rate": 4.6896585557299846e-05,
      "loss": 0.6622,
      "step": 316300
    },
    {
      "epoch": 4.967032967032967,
      "grad_norm": 3.299570322036743,
      "learning_rate": 4.6895604395604396e-05,
      "loss": 0.6352,
      "step": 316400
    },
    {
      "epoch": 4.968602825745683,
      "grad_norm": 4.98503303527832,
      "learning_rate": 4.6894623233908954e-05,
      "loss": 0.658,
      "step": 316500
    },
    {
      "epoch": 4.970172684458399,
      "grad_norm": 3.8731000423431396,
      "learning_rate": 4.68936420722135e-05,
      "loss": 0.6613,
      "step": 316600
    },
    {
      "epoch": 4.971742543171114,
      "grad_norm": 3.288029670715332,
      "learning_rate": 4.6892660910518056e-05,
      "loss": 0.7058,
      "step": 316700
    },
    {
      "epoch": 4.97331240188383,
      "grad_norm": 4.217562675476074,
      "learning_rate": 4.689167974882261e-05,
      "loss": 0.6602,
      "step": 316800
    },
    {
      "epoch": 4.974882260596546,
      "grad_norm": 3.49578595161438,
      "learning_rate": 4.6890698587127165e-05,
      "loss": 0.6909,
      "step": 316900
    },
    {
      "epoch": 4.976452119309262,
      "grad_norm": 3.706855535507202,
      "learning_rate": 4.688971742543171e-05,
      "loss": 0.6782,
      "step": 317000
    },
    {
      "epoch": 4.978021978021978,
      "grad_norm": 2.9755406379699707,
      "learning_rate": 4.688873626373627e-05,
      "loss": 0.6577,
      "step": 317100
    },
    {
      "epoch": 4.979591836734694,
      "grad_norm": 4.0371599197387695,
      "learning_rate": 4.688775510204082e-05,
      "loss": 0.7015,
      "step": 317200
    },
    {
      "epoch": 4.98116169544741,
      "grad_norm": 3.8819961547851562,
      "learning_rate": 4.688677394034537e-05,
      "loss": 0.666,
      "step": 317300
    },
    {
      "epoch": 4.982731554160125,
      "grad_norm": 5.40482234954834,
      "learning_rate": 4.688579277864992e-05,
      "loss": 0.6605,
      "step": 317400
    },
    {
      "epoch": 4.984301412872841,
      "grad_norm": 3.185529947280884,
      "learning_rate": 4.688481161695448e-05,
      "loss": 0.6778,
      "step": 317500
    },
    {
      "epoch": 4.985871271585557,
      "grad_norm": 4.017259120941162,
      "learning_rate": 4.688383045525903e-05,
      "loss": 0.7008,
      "step": 317600
    },
    {
      "epoch": 4.987441130298273,
      "grad_norm": 3.9433982372283936,
      "learning_rate": 4.688284929356358e-05,
      "loss": 0.6803,
      "step": 317700
    },
    {
      "epoch": 4.989010989010989,
      "grad_norm": 3.3236372470855713,
      "learning_rate": 4.688186813186814e-05,
      "loss": 0.6587,
      "step": 317800
    },
    {
      "epoch": 4.990580847723705,
      "grad_norm": 4.844809055328369,
      "learning_rate": 4.688088697017269e-05,
      "loss": 0.6968,
      "step": 317900
    },
    {
      "epoch": 4.992150706436421,
      "grad_norm": 3.734311819076538,
      "learning_rate": 4.687990580847724e-05,
      "loss": 0.6721,
      "step": 318000
    },
    {
      "epoch": 4.993720565149136,
      "grad_norm": 4.881266117095947,
      "learning_rate": 4.687892464678179e-05,
      "loss": 0.631,
      "step": 318100
    },
    {
      "epoch": 4.995290423861852,
      "grad_norm": 3.784034013748169,
      "learning_rate": 4.687794348508635e-05,
      "loss": 0.7128,
      "step": 318200
    },
    {
      "epoch": 4.996860282574568,
      "grad_norm": 3.753624200820923,
      "learning_rate": 4.68769623233909e-05,
      "loss": 0.6436,
      "step": 318300
    },
    {
      "epoch": 4.998430141287284,
      "grad_norm": 3.5077760219573975,
      "learning_rate": 4.687598116169545e-05,
      "loss": 0.6674,
      "step": 318400
    },
    {
      "epoch": 5.0,
      "grad_norm": 3.9231975078582764,
      "learning_rate": 4.6875e-05,
      "loss": 0.7272,
      "step": 318500
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.021451473236084,
      "eval_runtime": 15.3348,
      "eval_samples_per_second": 218.652,
      "eval_steps_per_second": 218.652,
      "step": 318500
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5247576236724854,
      "eval_runtime": 291.7464,
      "eval_samples_per_second": 218.34,
      "eval_steps_per_second": 218.34,
      "step": 318500
    },
    {
      "epoch": 5.001569858712716,
      "grad_norm": 3.8899219036102295,
      "learning_rate": 4.687401883830456e-05,
      "loss": 0.6493,
      "step": 318600
    },
    {
      "epoch": 5.003139717425432,
      "grad_norm": 3.761291265487671,
      "learning_rate": 4.68730376766091e-05,
      "loss": 0.6758,
      "step": 318700
    },
    {
      "epoch": 5.004709576138148,
      "grad_norm": 4.149172782897949,
      "learning_rate": 4.687205651491366e-05,
      "loss": 0.623,
      "step": 318800
    },
    {
      "epoch": 5.006279434850863,
      "grad_norm": 4.002706050872803,
      "learning_rate": 4.687107535321821e-05,
      "loss": 0.644,
      "step": 318900
    },
    {
      "epoch": 5.007849293563579,
      "grad_norm": 4.105700492858887,
      "learning_rate": 4.687009419152277e-05,
      "loss": 0.7203,
      "step": 319000
    },
    {
      "epoch": 5.009419152276295,
      "grad_norm": 5.531335353851318,
      "learning_rate": 4.6869113029827314e-05,
      "loss": 0.6445,
      "step": 319100
    },
    {
      "epoch": 5.010989010989011,
      "grad_norm": 2.6235013008117676,
      "learning_rate": 4.686813186813187e-05,
      "loss": 0.6848,
      "step": 319200
    },
    {
      "epoch": 5.012558869701727,
      "grad_norm": 4.201631546020508,
      "learning_rate": 4.686715070643642e-05,
      "loss": 0.6751,
      "step": 319300
    },
    {
      "epoch": 5.014128728414443,
      "grad_norm": 3.1810660362243652,
      "learning_rate": 4.686616954474097e-05,
      "loss": 0.6751,
      "step": 319400
    },
    {
      "epoch": 5.015698587127159,
      "grad_norm": 3.47135066986084,
      "learning_rate": 4.6865188383045524e-05,
      "loss": 0.6454,
      "step": 319500
    },
    {
      "epoch": 5.017268445839874,
      "grad_norm": 3.4526405334472656,
      "learning_rate": 4.686420722135008e-05,
      "loss": 0.6587,
      "step": 319600
    },
    {
      "epoch": 5.01883830455259,
      "grad_norm": 3.338526487350464,
      "learning_rate": 4.686322605965463e-05,
      "loss": 0.6676,
      "step": 319700
    },
    {
      "epoch": 5.020408163265306,
      "grad_norm": 3.4724442958831787,
      "learning_rate": 4.6862244897959184e-05,
      "loss": 0.6441,
      "step": 319800
    },
    {
      "epoch": 5.021978021978022,
      "grad_norm": 2.436013698577881,
      "learning_rate": 4.686126373626374e-05,
      "loss": 0.6782,
      "step": 319900
    },
    {
      "epoch": 5.023547880690738,
      "grad_norm": 3.6659133434295654,
      "learning_rate": 4.686028257456829e-05,
      "loss": 0.6881,
      "step": 320000
    },
    {
      "epoch": 5.025117739403454,
      "grad_norm": 3.4519548416137695,
      "learning_rate": 4.6859301412872844e-05,
      "loss": 0.6347,
      "step": 320100
    },
    {
      "epoch": 5.02668759811617,
      "grad_norm": 5.240691184997559,
      "learning_rate": 4.6858320251177395e-05,
      "loss": 0.6799,
      "step": 320200
    },
    {
      "epoch": 5.028257456828886,
      "grad_norm": 3.8283073902130127,
      "learning_rate": 4.685733908948195e-05,
      "loss": 0.6987,
      "step": 320300
    },
    {
      "epoch": 5.029827315541601,
      "grad_norm": 4.798040866851807,
      "learning_rate": 4.6856357927786504e-05,
      "loss": 0.6849,
      "step": 320400
    },
    {
      "epoch": 5.031397174254317,
      "grad_norm": 5.168225288391113,
      "learning_rate": 4.6855376766091054e-05,
      "loss": 0.669,
      "step": 320500
    },
    {
      "epoch": 5.032967032967033,
      "grad_norm": 3.602501392364502,
      "learning_rate": 4.6854395604395605e-05,
      "loss": 0.7128,
      "step": 320600
    },
    {
      "epoch": 5.034536891679749,
      "grad_norm": 3.646557092666626,
      "learning_rate": 4.685341444270016e-05,
      "loss": 0.6462,
      "step": 320700
    },
    {
      "epoch": 5.036106750392465,
      "grad_norm": 3.1352956295013428,
      "learning_rate": 4.685243328100471e-05,
      "loss": 0.6072,
      "step": 320800
    },
    {
      "epoch": 5.037676609105181,
      "grad_norm": 4.4778900146484375,
      "learning_rate": 4.6851452119309265e-05,
      "loss": 0.6661,
      "step": 320900
    },
    {
      "epoch": 5.039246467817897,
      "grad_norm": 3.187004566192627,
      "learning_rate": 4.6850470957613816e-05,
      "loss": 0.6781,
      "step": 321000
    },
    {
      "epoch": 5.040816326530612,
      "grad_norm": 4.143573760986328,
      "learning_rate": 4.6849489795918374e-05,
      "loss": 0.672,
      "step": 321100
    },
    {
      "epoch": 5.042386185243328,
      "grad_norm": 4.673798084259033,
      "learning_rate": 4.684850863422292e-05,
      "loss": 0.6611,
      "step": 321200
    },
    {
      "epoch": 5.043956043956044,
      "grad_norm": 4.5276408195495605,
      "learning_rate": 4.6847527472527476e-05,
      "loss": 0.6819,
      "step": 321300
    },
    {
      "epoch": 5.04552590266876,
      "grad_norm": 4.179312229156494,
      "learning_rate": 4.684654631083203e-05,
      "loss": 0.6589,
      "step": 321400
    },
    {
      "epoch": 5.047095761381476,
      "grad_norm": 4.929173469543457,
      "learning_rate": 4.684556514913658e-05,
      "loss": 0.6516,
      "step": 321500
    },
    {
      "epoch": 5.048665620094192,
      "grad_norm": 4.6728644371032715,
      "learning_rate": 4.684458398744113e-05,
      "loss": 0.6471,
      "step": 321600
    },
    {
      "epoch": 5.050235478806908,
      "grad_norm": 3.9253883361816406,
      "learning_rate": 4.6843602825745687e-05,
      "loss": 0.658,
      "step": 321700
    },
    {
      "epoch": 5.051805337519623,
      "grad_norm": 4.552691459655762,
      "learning_rate": 4.684262166405024e-05,
      "loss": 0.6294,
      "step": 321800
    },
    {
      "epoch": 5.053375196232339,
      "grad_norm": 4.4280476570129395,
      "learning_rate": 4.684164050235479e-05,
      "loss": 0.6648,
      "step": 321900
    },
    {
      "epoch": 5.054945054945055,
      "grad_norm": 3.714308261871338,
      "learning_rate": 4.6840659340659346e-05,
      "loss": 0.6414,
      "step": 322000
    },
    {
      "epoch": 5.056514913657771,
      "grad_norm": 5.640679359436035,
      "learning_rate": 4.68396781789639e-05,
      "loss": 0.6207,
      "step": 322100
    },
    {
      "epoch": 5.058084772370487,
      "grad_norm": 3.7521719932556152,
      "learning_rate": 4.683869701726845e-05,
      "loss": 0.6601,
      "step": 322200
    },
    {
      "epoch": 5.059654631083203,
      "grad_norm": 4.391061305999756,
      "learning_rate": 4.6837715855573e-05,
      "loss": 0.6517,
      "step": 322300
    },
    {
      "epoch": 5.061224489795919,
      "grad_norm": 4.295456886291504,
      "learning_rate": 4.683673469387756e-05,
      "loss": 0.6526,
      "step": 322400
    },
    {
      "epoch": 5.062794348508635,
      "grad_norm": 3.788799285888672,
      "learning_rate": 4.683575353218211e-05,
      "loss": 0.6404,
      "step": 322500
    },
    {
      "epoch": 5.06436420722135,
      "grad_norm": 3.6494946479797363,
      "learning_rate": 4.683477237048666e-05,
      "loss": 0.656,
      "step": 322600
    },
    {
      "epoch": 5.065934065934066,
      "grad_norm": 5.4038987159729,
      "learning_rate": 4.683379120879121e-05,
      "loss": 0.6676,
      "step": 322700
    },
    {
      "epoch": 5.067503924646782,
      "grad_norm": 3.942432403564453,
      "learning_rate": 4.683281004709577e-05,
      "loss": 0.6743,
      "step": 322800
    },
    {
      "epoch": 5.069073783359498,
      "grad_norm": 3.8175272941589355,
      "learning_rate": 4.683182888540031e-05,
      "loss": 0.6757,
      "step": 322900
    },
    {
      "epoch": 5.070643642072214,
      "grad_norm": 4.457486629486084,
      "learning_rate": 4.683084772370487e-05,
      "loss": 0.6637,
      "step": 323000
    },
    {
      "epoch": 5.07221350078493,
      "grad_norm": 3.756697177886963,
      "learning_rate": 4.682986656200942e-05,
      "loss": 0.6638,
      "step": 323100
    },
    {
      "epoch": 5.073783359497646,
      "grad_norm": 4.085574150085449,
      "learning_rate": 4.682888540031398e-05,
      "loss": 0.6804,
      "step": 323200
    },
    {
      "epoch": 5.075353218210361,
      "grad_norm": 4.372395992279053,
      "learning_rate": 4.682790423861852e-05,
      "loss": 0.6234,
      "step": 323300
    },
    {
      "epoch": 5.076923076923077,
      "grad_norm": 3.640660047531128,
      "learning_rate": 4.682692307692308e-05,
      "loss": 0.6702,
      "step": 323400
    },
    {
      "epoch": 5.078492935635793,
      "grad_norm": 4.268948078155518,
      "learning_rate": 4.682594191522763e-05,
      "loss": 0.6824,
      "step": 323500
    },
    {
      "epoch": 5.080062794348509,
      "grad_norm": 4.158763885498047,
      "learning_rate": 4.682496075353218e-05,
      "loss": 0.6535,
      "step": 323600
    },
    {
      "epoch": 5.081632653061225,
      "grad_norm": 4.341344833374023,
      "learning_rate": 4.682397959183673e-05,
      "loss": 0.6568,
      "step": 323700
    },
    {
      "epoch": 5.083202511773941,
      "grad_norm": 3.4825682640075684,
      "learning_rate": 4.682299843014129e-05,
      "loss": 0.641,
      "step": 323800
    },
    {
      "epoch": 5.0847723704866565,
      "grad_norm": 4.286757946014404,
      "learning_rate": 4.682201726844584e-05,
      "loss": 0.6966,
      "step": 323900
    },
    {
      "epoch": 5.086342229199372,
      "grad_norm": 3.673963785171509,
      "learning_rate": 4.682103610675039e-05,
      "loss": 0.6532,
      "step": 324000
    },
    {
      "epoch": 5.087912087912088,
      "grad_norm": 4.662906169891357,
      "learning_rate": 4.682005494505495e-05,
      "loss": 0.6832,
      "step": 324100
    },
    {
      "epoch": 5.089481946624804,
      "grad_norm": 4.542591571807861,
      "learning_rate": 4.68190737833595e-05,
      "loss": 0.637,
      "step": 324200
    },
    {
      "epoch": 5.0910518053375196,
      "grad_norm": 4.675800800323486,
      "learning_rate": 4.681809262166405e-05,
      "loss": 0.637,
      "step": 324300
    },
    {
      "epoch": 5.0926216640502355,
      "grad_norm": 3.0784409046173096,
      "learning_rate": 4.6817111459968604e-05,
      "loss": 0.6765,
      "step": 324400
    },
    {
      "epoch": 5.0941915227629515,
      "grad_norm": 4.683157920837402,
      "learning_rate": 4.681613029827316e-05,
      "loss": 0.6526,
      "step": 324500
    },
    {
      "epoch": 5.0957613814756675,
      "grad_norm": 4.081713676452637,
      "learning_rate": 4.681514913657771e-05,
      "loss": 0.654,
      "step": 324600
    },
    {
      "epoch": 5.0973312401883835,
      "grad_norm": 4.407011985778809,
      "learning_rate": 4.6814167974882263e-05,
      "loss": 0.6257,
      "step": 324700
    },
    {
      "epoch": 5.0989010989010985,
      "grad_norm": 5.10946798324585,
      "learning_rate": 4.6813186813186814e-05,
      "loss": 0.6313,
      "step": 324800
    },
    {
      "epoch": 5.1004709576138145,
      "grad_norm": 4.0366530418396,
      "learning_rate": 4.681220565149137e-05,
      "loss": 0.6779,
      "step": 324900
    },
    {
      "epoch": 5.1020408163265305,
      "grad_norm": 3.556621789932251,
      "learning_rate": 4.6811224489795916e-05,
      "loss": 0.694,
      "step": 325000
    },
    {
      "epoch": 5.1036106750392465,
      "grad_norm": 3.0131676197052,
      "learning_rate": 4.6810243328100474e-05,
      "loss": 0.6598,
      "step": 325100
    },
    {
      "epoch": 5.1051805337519625,
      "grad_norm": 3.3924055099487305,
      "learning_rate": 4.6809262166405025e-05,
      "loss": 0.697,
      "step": 325200
    },
    {
      "epoch": 5.106750392464678,
      "grad_norm": 2.7948501110076904,
      "learning_rate": 4.680828100470958e-05,
      "loss": 0.6428,
      "step": 325300
    },
    {
      "epoch": 5.108320251177394,
      "grad_norm": 3.9757680892944336,
      "learning_rate": 4.680729984301413e-05,
      "loss": 0.6702,
      "step": 325400
    },
    {
      "epoch": 5.1098901098901095,
      "grad_norm": 3.970337390899658,
      "learning_rate": 4.6806318681318685e-05,
      "loss": 0.6762,
      "step": 325500
    },
    {
      "epoch": 5.1114599686028255,
      "grad_norm": 4.169715881347656,
      "learning_rate": 4.6805337519623236e-05,
      "loss": 0.6057,
      "step": 325600
    },
    {
      "epoch": 5.1130298273155415,
      "grad_norm": 3.6584253311157227,
      "learning_rate": 4.680435635792779e-05,
      "loss": 0.6568,
      "step": 325700
    },
    {
      "epoch": 5.114599686028257,
      "grad_norm": 3.651970386505127,
      "learning_rate": 4.680337519623234e-05,
      "loss": 0.6739,
      "step": 325800
    },
    {
      "epoch": 5.116169544740973,
      "grad_norm": 4.575229167938232,
      "learning_rate": 4.6802394034536896e-05,
      "loss": 0.6649,
      "step": 325900
    },
    {
      "epoch": 5.117739403453689,
      "grad_norm": 4.7632951736450195,
      "learning_rate": 4.6801412872841447e-05,
      "loss": 0.6896,
      "step": 326000
    },
    {
      "epoch": 5.119309262166405,
      "grad_norm": 3.7498552799224854,
      "learning_rate": 4.6800431711146e-05,
      "loss": 0.6478,
      "step": 326100
    },
    {
      "epoch": 5.1208791208791204,
      "grad_norm": 3.736820936203003,
      "learning_rate": 4.6799450549450555e-05,
      "loss": 0.6324,
      "step": 326200
    },
    {
      "epoch": 5.122448979591836,
      "grad_norm": 4.239162445068359,
      "learning_rate": 4.6798469387755106e-05,
      "loss": 0.6335,
      "step": 326300
    },
    {
      "epoch": 5.124018838304552,
      "grad_norm": 3.8917276859283447,
      "learning_rate": 4.679748822605966e-05,
      "loss": 0.6902,
      "step": 326400
    },
    {
      "epoch": 5.125588697017268,
      "grad_norm": 2.99218487739563,
      "learning_rate": 4.679650706436421e-05,
      "loss": 0.6481,
      "step": 326500
    },
    {
      "epoch": 5.127158555729984,
      "grad_norm": 3.752988338470459,
      "learning_rate": 4.6795525902668766e-05,
      "loss": 0.6595,
      "step": 326600
    },
    {
      "epoch": 5.1287284144427,
      "grad_norm": 4.356729030609131,
      "learning_rate": 4.679454474097332e-05,
      "loss": 0.6663,
      "step": 326700
    },
    {
      "epoch": 5.130298273155416,
      "grad_norm": 2.3822896480560303,
      "learning_rate": 4.679356357927787e-05,
      "loss": 0.6614,
      "step": 326800
    },
    {
      "epoch": 5.131868131868132,
      "grad_norm": 4.182880878448486,
      "learning_rate": 4.679258241758242e-05,
      "loss": 0.7076,
      "step": 326900
    },
    {
      "epoch": 5.133437990580847,
      "grad_norm": 4.10748815536499,
      "learning_rate": 4.679160125588698e-05,
      "loss": 0.655,
      "step": 327000
    },
    {
      "epoch": 5.135007849293563,
      "grad_norm": 4.558518886566162,
      "learning_rate": 4.679062009419152e-05,
      "loss": 0.6673,
      "step": 327100
    },
    {
      "epoch": 5.136577708006279,
      "grad_norm": 5.195411205291748,
      "learning_rate": 4.678963893249608e-05,
      "loss": 0.6676,
      "step": 327200
    },
    {
      "epoch": 5.138147566718995,
      "grad_norm": 3.945733070373535,
      "learning_rate": 4.678865777080063e-05,
      "loss": 0.6762,
      "step": 327300
    },
    {
      "epoch": 5.139717425431711,
      "grad_norm": 3.5418167114257812,
      "learning_rate": 4.678767660910519e-05,
      "loss": 0.6775,
      "step": 327400
    },
    {
      "epoch": 5.141287284144427,
      "grad_norm": 4.706028461456299,
      "learning_rate": 4.678669544740973e-05,
      "loss": 0.6437,
      "step": 327500
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 4.00163459777832,
      "learning_rate": 4.678571428571429e-05,
      "loss": 0.6294,
      "step": 327600
    },
    {
      "epoch": 5.144427001569858,
      "grad_norm": 4.054046630859375,
      "learning_rate": 4.678473312401884e-05,
      "loss": 0.6558,
      "step": 327700
    },
    {
      "epoch": 5.145996860282574,
      "grad_norm": 3.4377057552337646,
      "learning_rate": 4.678375196232339e-05,
      "loss": 0.6871,
      "step": 327800
    },
    {
      "epoch": 5.14756671899529,
      "grad_norm": 4.041555404663086,
      "learning_rate": 4.678277080062794e-05,
      "loss": 0.6718,
      "step": 327900
    },
    {
      "epoch": 5.149136577708006,
      "grad_norm": 4.551620960235596,
      "learning_rate": 4.67817896389325e-05,
      "loss": 0.6957,
      "step": 328000
    },
    {
      "epoch": 5.150706436420722,
      "grad_norm": 4.048908233642578,
      "learning_rate": 4.678080847723705e-05,
      "loss": 0.6463,
      "step": 328100
    },
    {
      "epoch": 5.152276295133438,
      "grad_norm": 4.126847267150879,
      "learning_rate": 4.67798273155416e-05,
      "loss": 0.6632,
      "step": 328200
    },
    {
      "epoch": 5.153846153846154,
      "grad_norm": 4.401174545288086,
      "learning_rate": 4.677884615384616e-05,
      "loss": 0.6643,
      "step": 328300
    },
    {
      "epoch": 5.155416012558869,
      "grad_norm": 3.4675114154815674,
      "learning_rate": 4.677786499215071e-05,
      "loss": 0.6565,
      "step": 328400
    },
    {
      "epoch": 5.156985871271585,
      "grad_norm": 5.144802093505859,
      "learning_rate": 4.677688383045526e-05,
      "loss": 0.6577,
      "step": 328500
    },
    {
      "epoch": 5.158555729984301,
      "grad_norm": 3.943711757659912,
      "learning_rate": 4.677590266875981e-05,
      "loss": 0.6535,
      "step": 328600
    },
    {
      "epoch": 5.160125588697017,
      "grad_norm": 4.9992523193359375,
      "learning_rate": 4.677492150706437e-05,
      "loss": 0.7013,
      "step": 328700
    },
    {
      "epoch": 5.161695447409733,
      "grad_norm": 3.9164645671844482,
      "learning_rate": 4.677394034536892e-05,
      "loss": 0.6435,
      "step": 328800
    },
    {
      "epoch": 5.163265306122449,
      "grad_norm": 3.9685721397399902,
      "learning_rate": 4.677295918367347e-05,
      "loss": 0.6753,
      "step": 328900
    },
    {
      "epoch": 5.164835164835165,
      "grad_norm": 7.174018859863281,
      "learning_rate": 4.6771978021978023e-05,
      "loss": 0.6203,
      "step": 329000
    },
    {
      "epoch": 5.166405023547881,
      "grad_norm": 4.208096981048584,
      "learning_rate": 4.677099686028258e-05,
      "loss": 0.653,
      "step": 329100
    },
    {
      "epoch": 5.167974882260596,
      "grad_norm": 4.248377323150635,
      "learning_rate": 4.6770015698587125e-05,
      "loss": 0.6879,
      "step": 329200
    },
    {
      "epoch": 5.169544740973312,
      "grad_norm": 3.4788734912872314,
      "learning_rate": 4.676903453689168e-05,
      "loss": 0.622,
      "step": 329300
    },
    {
      "epoch": 5.171114599686028,
      "grad_norm": 4.304073333740234,
      "learning_rate": 4.6768053375196234e-05,
      "loss": 0.6814,
      "step": 329400
    },
    {
      "epoch": 5.172684458398744,
      "grad_norm": 4.160642623901367,
      "learning_rate": 4.676707221350079e-05,
      "loss": 0.6648,
      "step": 329500
    },
    {
      "epoch": 5.17425431711146,
      "grad_norm": 3.6399216651916504,
      "learning_rate": 4.6766091051805336e-05,
      "loss": 0.6551,
      "step": 329600
    },
    {
      "epoch": 5.175824175824176,
      "grad_norm": 3.6923842430114746,
      "learning_rate": 4.6765109890109894e-05,
      "loss": 0.6775,
      "step": 329700
    },
    {
      "epoch": 5.177394034536892,
      "grad_norm": 3.8419065475463867,
      "learning_rate": 4.6764128728414445e-05,
      "loss": 0.6683,
      "step": 329800
    },
    {
      "epoch": 5.178963893249607,
      "grad_norm": 2.997887372970581,
      "learning_rate": 4.6763147566718996e-05,
      "loss": 0.6629,
      "step": 329900
    },
    {
      "epoch": 5.180533751962323,
      "grad_norm": 3.9138028621673584,
      "learning_rate": 4.676216640502355e-05,
      "loss": 0.6823,
      "step": 330000
    },
    {
      "epoch": 5.182103610675039,
      "grad_norm": 4.262688159942627,
      "learning_rate": 4.6761185243328105e-05,
      "loss": 0.6444,
      "step": 330100
    },
    {
      "epoch": 5.183673469387755,
      "grad_norm": 3.957052230834961,
      "learning_rate": 4.6760204081632656e-05,
      "loss": 0.6389,
      "step": 330200
    },
    {
      "epoch": 5.185243328100471,
      "grad_norm": 4.35952615737915,
      "learning_rate": 4.6759222919937207e-05,
      "loss": 0.6257,
      "step": 330300
    },
    {
      "epoch": 5.186813186813187,
      "grad_norm": 5.227481842041016,
      "learning_rate": 4.6758241758241764e-05,
      "loss": 0.6806,
      "step": 330400
    },
    {
      "epoch": 5.188383045525903,
      "grad_norm": 4.529896259307861,
      "learning_rate": 4.6757260596546315e-05,
      "loss": 0.6573,
      "step": 330500
    },
    {
      "epoch": 5.189952904238618,
      "grad_norm": 4.178516387939453,
      "learning_rate": 4.6756279434850866e-05,
      "loss": 0.6613,
      "step": 330600
    },
    {
      "epoch": 5.191522762951334,
      "grad_norm": 4.427163124084473,
      "learning_rate": 4.675529827315542e-05,
      "loss": 0.6867,
      "step": 330700
    },
    {
      "epoch": 5.19309262166405,
      "grad_norm": 4.269001483917236,
      "learning_rate": 4.6754317111459975e-05,
      "loss": 0.673,
      "step": 330800
    },
    {
      "epoch": 5.194662480376766,
      "grad_norm": 4.1773552894592285,
      "learning_rate": 4.675333594976452e-05,
      "loss": 0.6509,
      "step": 330900
    },
    {
      "epoch": 5.196232339089482,
      "grad_norm": 4.742835998535156,
      "learning_rate": 4.675235478806908e-05,
      "loss": 0.6396,
      "step": 331000
    },
    {
      "epoch": 5.197802197802198,
      "grad_norm": 4.413194179534912,
      "learning_rate": 4.675137362637363e-05,
      "loss": 0.6527,
      "step": 331100
    },
    {
      "epoch": 5.199372056514914,
      "grad_norm": 3.9214696884155273,
      "learning_rate": 4.6750392464678186e-05,
      "loss": 0.694,
      "step": 331200
    },
    {
      "epoch": 5.20094191522763,
      "grad_norm": 3.677924394607544,
      "learning_rate": 4.674941130298273e-05,
      "loss": 0.6687,
      "step": 331300
    },
    {
      "epoch": 5.202511773940345,
      "grad_norm": 2.819831609725952,
      "learning_rate": 4.674843014128729e-05,
      "loss": 0.6552,
      "step": 331400
    },
    {
      "epoch": 5.204081632653061,
      "grad_norm": 3.0791125297546387,
      "learning_rate": 4.674744897959184e-05,
      "loss": 0.6635,
      "step": 331500
    },
    {
      "epoch": 5.205651491365777,
      "grad_norm": 4.432744979858398,
      "learning_rate": 4.674646781789639e-05,
      "loss": 0.6818,
      "step": 331600
    },
    {
      "epoch": 5.207221350078493,
      "grad_norm": 3.4621732234954834,
      "learning_rate": 4.674548665620094e-05,
      "loss": 0.6626,
      "step": 331700
    },
    {
      "epoch": 5.208791208791209,
      "grad_norm": 3.8325107097625732,
      "learning_rate": 4.67445054945055e-05,
      "loss": 0.661,
      "step": 331800
    },
    {
      "epoch": 5.210361067503925,
      "grad_norm": 3.6310436725616455,
      "learning_rate": 4.674352433281005e-05,
      "loss": 0.7195,
      "step": 331900
    },
    {
      "epoch": 5.211930926216641,
      "grad_norm": 3.4905169010162354,
      "learning_rate": 4.67425431711146e-05,
      "loss": 0.6774,
      "step": 332000
    },
    {
      "epoch": 5.213500784929356,
      "grad_norm": 3.3326239585876465,
      "learning_rate": 4.674156200941915e-05,
      "loss": 0.6809,
      "step": 332100
    },
    {
      "epoch": 5.215070643642072,
      "grad_norm": 3.3005752563476562,
      "learning_rate": 4.674058084772371e-05,
      "loss": 0.6812,
      "step": 332200
    },
    {
      "epoch": 5.216640502354788,
      "grad_norm": 4.284373760223389,
      "learning_rate": 4.673959968602826e-05,
      "loss": 0.6472,
      "step": 332300
    },
    {
      "epoch": 5.218210361067504,
      "grad_norm": 3.765693187713623,
      "learning_rate": 4.673861852433281e-05,
      "loss": 0.6294,
      "step": 332400
    },
    {
      "epoch": 5.21978021978022,
      "grad_norm": 4.557390213012695,
      "learning_rate": 4.673763736263736e-05,
      "loss": 0.6513,
      "step": 332500
    },
    {
      "epoch": 5.221350078492936,
      "grad_norm": 3.8662946224212646,
      "learning_rate": 4.673665620094192e-05,
      "loss": 0.6518,
      "step": 332600
    },
    {
      "epoch": 5.222919937205652,
      "grad_norm": 4.272804260253906,
      "learning_rate": 4.673567503924647e-05,
      "loss": 0.6804,
      "step": 332700
    },
    {
      "epoch": 5.224489795918367,
      "grad_norm": 3.8986761569976807,
      "learning_rate": 4.673469387755102e-05,
      "loss": 0.6849,
      "step": 332800
    },
    {
      "epoch": 5.226059654631083,
      "grad_norm": 3.9841248989105225,
      "learning_rate": 4.673371271585558e-05,
      "loss": 0.6425,
      "step": 332900
    },
    {
      "epoch": 5.227629513343799,
      "grad_norm": 3.342165470123291,
      "learning_rate": 4.6732731554160124e-05,
      "loss": 0.6985,
      "step": 333000
    },
    {
      "epoch": 5.229199372056515,
      "grad_norm": 3.6751413345336914,
      "learning_rate": 4.673175039246468e-05,
      "loss": 0.6972,
      "step": 333100
    },
    {
      "epoch": 5.230769230769231,
      "grad_norm": 4.481664657592773,
      "learning_rate": 4.673076923076923e-05,
      "loss": 0.6409,
      "step": 333200
    },
    {
      "epoch": 5.232339089481947,
      "grad_norm": 1.7653312683105469,
      "learning_rate": 4.672978806907379e-05,
      "loss": 0.6669,
      "step": 333300
    },
    {
      "epoch": 5.233908948194663,
      "grad_norm": 3.4322497844696045,
      "learning_rate": 4.6728806907378334e-05,
      "loss": 0.6668,
      "step": 333400
    },
    {
      "epoch": 5.235478806907379,
      "grad_norm": 3.1909632682800293,
      "learning_rate": 4.672782574568289e-05,
      "loss": 0.6761,
      "step": 333500
    },
    {
      "epoch": 5.237048665620094,
      "grad_norm": 4.109644889831543,
      "learning_rate": 4.672684458398744e-05,
      "loss": 0.6572,
      "step": 333600
    },
    {
      "epoch": 5.23861852433281,
      "grad_norm": 2.876797676086426,
      "learning_rate": 4.6725863422291994e-05,
      "loss": 0.6724,
      "step": 333700
    },
    {
      "epoch": 5.240188383045526,
      "grad_norm": 3.5581021308898926,
      "learning_rate": 4.6724882260596545e-05,
      "loss": 0.7254,
      "step": 333800
    },
    {
      "epoch": 5.241758241758242,
      "grad_norm": 4.667768955230713,
      "learning_rate": 4.67239010989011e-05,
      "loss": 0.6397,
      "step": 333900
    },
    {
      "epoch": 5.243328100470958,
      "grad_norm": 4.42426872253418,
      "learning_rate": 4.6722919937205654e-05,
      "loss": 0.6405,
      "step": 334000
    },
    {
      "epoch": 5.244897959183674,
      "grad_norm": 4.376983165740967,
      "learning_rate": 4.6721938775510205e-05,
      "loss": 0.6714,
      "step": 334100
    },
    {
      "epoch": 5.24646781789639,
      "grad_norm": 4.515996932983398,
      "learning_rate": 4.6720957613814756e-05,
      "loss": 0.664,
      "step": 334200
    },
    {
      "epoch": 5.248037676609105,
      "grad_norm": 4.0845136642456055,
      "learning_rate": 4.6719976452119314e-05,
      "loss": 0.6763,
      "step": 334300
    },
    {
      "epoch": 5.249607535321821,
      "grad_norm": 4.374002933502197,
      "learning_rate": 4.6718995290423865e-05,
      "loss": 0.6519,
      "step": 334400
    },
    {
      "epoch": 5.251177394034537,
      "grad_norm": 3.157345771789551,
      "learning_rate": 4.6718014128728415e-05,
      "loss": 0.6613,
      "step": 334500
    },
    {
      "epoch": 5.252747252747253,
      "grad_norm": 3.6911561489105225,
      "learning_rate": 4.6717032967032966e-05,
      "loss": 0.6346,
      "step": 334600
    },
    {
      "epoch": 5.254317111459969,
      "grad_norm": 4.2307515144348145,
      "learning_rate": 4.6716051805337524e-05,
      "loss": 0.7049,
      "step": 334700
    },
    {
      "epoch": 5.255886970172685,
      "grad_norm": 3.429110527038574,
      "learning_rate": 4.6715070643642075e-05,
      "loss": 0.6618,
      "step": 334800
    },
    {
      "epoch": 5.257456828885401,
      "grad_norm": 4.771365165710449,
      "learning_rate": 4.6714089481946626e-05,
      "loss": 0.7114,
      "step": 334900
    },
    {
      "epoch": 5.259026687598116,
      "grad_norm": 3.224839925765991,
      "learning_rate": 4.6713108320251184e-05,
      "loss": 0.7028,
      "step": 335000
    },
    {
      "epoch": 5.260596546310832,
      "grad_norm": 4.736693382263184,
      "learning_rate": 4.671212715855573e-05,
      "loss": 0.6648,
      "step": 335100
    },
    {
      "epoch": 5.262166405023548,
      "grad_norm": 3.5509324073791504,
      "learning_rate": 4.6711145996860286e-05,
      "loss": 0.6716,
      "step": 335200
    },
    {
      "epoch": 5.263736263736264,
      "grad_norm": 3.8924922943115234,
      "learning_rate": 4.671016483516484e-05,
      "loss": 0.6782,
      "step": 335300
    },
    {
      "epoch": 5.26530612244898,
      "grad_norm": 4.793665885925293,
      "learning_rate": 4.6709183673469395e-05,
      "loss": 0.6618,
      "step": 335400
    },
    {
      "epoch": 5.266875981161696,
      "grad_norm": 4.022476673126221,
      "learning_rate": 4.670820251177394e-05,
      "loss": 0.6847,
      "step": 335500
    },
    {
      "epoch": 5.268445839874412,
      "grad_norm": 4.505865097045898,
      "learning_rate": 4.67072213500785e-05,
      "loss": 0.6686,
      "step": 335600
    },
    {
      "epoch": 5.270015698587127,
      "grad_norm": 3.8007431030273438,
      "learning_rate": 4.670624018838305e-05,
      "loss": 0.6615,
      "step": 335700
    },
    {
      "epoch": 5.271585557299843,
      "grad_norm": 4.13673210144043,
      "learning_rate": 4.67052590266876e-05,
      "loss": 0.658,
      "step": 335800
    },
    {
      "epoch": 5.273155416012559,
      "grad_norm": 4.043201446533203,
      "learning_rate": 4.670427786499215e-05,
      "loss": 0.6576,
      "step": 335900
    },
    {
      "epoch": 5.274725274725275,
      "grad_norm": 3.678246259689331,
      "learning_rate": 4.670329670329671e-05,
      "loss": 0.7263,
      "step": 336000
    },
    {
      "epoch": 5.276295133437991,
      "grad_norm": 4.72843599319458,
      "learning_rate": 4.670231554160126e-05,
      "loss": 0.6641,
      "step": 336100
    },
    {
      "epoch": 5.277864992150707,
      "grad_norm": 3.393179416656494,
      "learning_rate": 4.670133437990581e-05,
      "loss": 0.6612,
      "step": 336200
    },
    {
      "epoch": 5.279434850863423,
      "grad_norm": 4.036086559295654,
      "learning_rate": 4.670035321821036e-05,
      "loss": 0.6487,
      "step": 336300
    },
    {
      "epoch": 5.2810047095761385,
      "grad_norm": 3.2833642959594727,
      "learning_rate": 4.669937205651492e-05,
      "loss": 0.6817,
      "step": 336400
    },
    {
      "epoch": 5.282574568288854,
      "grad_norm": 3.946735143661499,
      "learning_rate": 4.669839089481947e-05,
      "loss": 0.6406,
      "step": 336500
    },
    {
      "epoch": 5.28414442700157,
      "grad_norm": 3.0479631423950195,
      "learning_rate": 4.669740973312402e-05,
      "loss": 0.6599,
      "step": 336600
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 4.8882622718811035,
      "learning_rate": 4.669642857142857e-05,
      "loss": 0.6918,
      "step": 336700
    },
    {
      "epoch": 5.287284144427002,
      "grad_norm": 3.8720223903656006,
      "learning_rate": 4.669544740973313e-05,
      "loss": 0.6451,
      "step": 336800
    },
    {
      "epoch": 5.2888540031397175,
      "grad_norm": 3.9640228748321533,
      "learning_rate": 4.669446624803768e-05,
      "loss": 0.6447,
      "step": 336900
    },
    {
      "epoch": 5.2904238618524335,
      "grad_norm": 3.6137924194335938,
      "learning_rate": 4.669348508634223e-05,
      "loss": 0.7117,
      "step": 337000
    },
    {
      "epoch": 5.2919937205651495,
      "grad_norm": 3.24678635597229,
      "learning_rate": 4.669250392464679e-05,
      "loss": 0.674,
      "step": 337100
    },
    {
      "epoch": 5.293563579277865,
      "grad_norm": 3.1232264041900635,
      "learning_rate": 4.669152276295133e-05,
      "loss": 0.6914,
      "step": 337200
    },
    {
      "epoch": 5.295133437990581,
      "grad_norm": 3.826810836791992,
      "learning_rate": 4.669054160125589e-05,
      "loss": 0.6631,
      "step": 337300
    },
    {
      "epoch": 5.2967032967032965,
      "grad_norm": 4.49258279800415,
      "learning_rate": 4.668956043956044e-05,
      "loss": 0.6405,
      "step": 337400
    },
    {
      "epoch": 5.2982731554160125,
      "grad_norm": 3.439079999923706,
      "learning_rate": 4.6688579277865e-05,
      "loss": 0.657,
      "step": 337500
    },
    {
      "epoch": 5.2998430141287285,
      "grad_norm": 3.523379325866699,
      "learning_rate": 4.668759811616954e-05,
      "loss": 0.6449,
      "step": 337600
    },
    {
      "epoch": 5.3014128728414445,
      "grad_norm": 3.161262273788452,
      "learning_rate": 4.66866169544741e-05,
      "loss": 0.662,
      "step": 337700
    },
    {
      "epoch": 5.3029827315541604,
      "grad_norm": 3.899930953979492,
      "learning_rate": 4.668563579277865e-05,
      "loss": 0.6313,
      "step": 337800
    },
    {
      "epoch": 5.304552590266876,
      "grad_norm": 1.6669495105743408,
      "learning_rate": 4.66846546310832e-05,
      "loss": 0.6754,
      "step": 337900
    },
    {
      "epoch": 5.3061224489795915,
      "grad_norm": 4.334996700286865,
      "learning_rate": 4.6683673469387754e-05,
      "loss": 0.6241,
      "step": 338000
    },
    {
      "epoch": 5.3076923076923075,
      "grad_norm": 3.5603249073028564,
      "learning_rate": 4.668269230769231e-05,
      "loss": 0.7027,
      "step": 338100
    },
    {
      "epoch": 5.3092621664050235,
      "grad_norm": 3.184553623199463,
      "learning_rate": 4.668171114599686e-05,
      "loss": 0.7179,
      "step": 338200
    },
    {
      "epoch": 5.310832025117739,
      "grad_norm": 4.364271640777588,
      "learning_rate": 4.6680729984301414e-05,
      "loss": 0.6833,
      "step": 338300
    },
    {
      "epoch": 5.312401883830455,
      "grad_norm": 4.312171459197998,
      "learning_rate": 4.6679748822605965e-05,
      "loss": 0.7021,
      "step": 338400
    },
    {
      "epoch": 5.313971742543171,
      "grad_norm": 3.784696102142334,
      "learning_rate": 4.667876766091052e-05,
      "loss": 0.6505,
      "step": 338500
    },
    {
      "epoch": 5.315541601255887,
      "grad_norm": 5.42485237121582,
      "learning_rate": 4.6677786499215073e-05,
      "loss": 0.6462,
      "step": 338600
    },
    {
      "epoch": 5.3171114599686025,
      "grad_norm": 4.192597389221191,
      "learning_rate": 4.6676805337519624e-05,
      "loss": 0.6466,
      "step": 338700
    },
    {
      "epoch": 5.318681318681318,
      "grad_norm": 3.9957406520843506,
      "learning_rate": 4.6675824175824175e-05,
      "loss": 0.6415,
      "step": 338800
    },
    {
      "epoch": 5.320251177394034,
      "grad_norm": 3.6679582595825195,
      "learning_rate": 4.667484301412873e-05,
      "loss": 0.658,
      "step": 338900
    },
    {
      "epoch": 5.32182103610675,
      "grad_norm": 2.6486122608184814,
      "learning_rate": 4.6673861852433284e-05,
      "loss": 0.6455,
      "step": 339000
    },
    {
      "epoch": 5.323390894819466,
      "grad_norm": 3.5644760131835938,
      "learning_rate": 4.6672880690737835e-05,
      "loss": 0.6484,
      "step": 339100
    },
    {
      "epoch": 5.324960753532182,
      "grad_norm": 3.9896490573883057,
      "learning_rate": 4.667189952904239e-05,
      "loss": 0.6824,
      "step": 339200
    },
    {
      "epoch": 5.326530612244898,
      "grad_norm": 4.529460906982422,
      "learning_rate": 4.667091836734694e-05,
      "loss": 0.6692,
      "step": 339300
    },
    {
      "epoch": 5.328100470957613,
      "grad_norm": 4.000598907470703,
      "learning_rate": 4.6669937205651495e-05,
      "loss": 0.6464,
      "step": 339400
    },
    {
      "epoch": 5.329670329670329,
      "grad_norm": 4.278095722198486,
      "learning_rate": 4.6668956043956046e-05,
      "loss": 0.6619,
      "step": 339500
    },
    {
      "epoch": 5.331240188383045,
      "grad_norm": 4.1789751052856445,
      "learning_rate": 4.6667974882260604e-05,
      "loss": 0.6538,
      "step": 339600
    },
    {
      "epoch": 5.332810047095761,
      "grad_norm": 3.8569345474243164,
      "learning_rate": 4.666699372056515e-05,
      "loss": 0.6646,
      "step": 339700
    },
    {
      "epoch": 5.334379905808477,
      "grad_norm": 4.769703388214111,
      "learning_rate": 4.6666012558869706e-05,
      "loss": 0.6491,
      "step": 339800
    },
    {
      "epoch": 5.335949764521193,
      "grad_norm": 4.105967044830322,
      "learning_rate": 4.6665031397174257e-05,
      "loss": 0.6872,
      "step": 339900
    },
    {
      "epoch": 5.337519623233909,
      "grad_norm": 4.118862152099609,
      "learning_rate": 4.666405023547881e-05,
      "loss": 0.6562,
      "step": 340000
    },
    {
      "epoch": 5.339089481946624,
      "grad_norm": 3.681178569793701,
      "learning_rate": 4.666306907378336e-05,
      "loss": 0.6646,
      "step": 340100
    },
    {
      "epoch": 5.34065934065934,
      "grad_norm": 4.541553020477295,
      "learning_rate": 4.6662087912087916e-05,
      "loss": 0.6599,
      "step": 340200
    },
    {
      "epoch": 5.342229199372056,
      "grad_norm": 4.161733627319336,
      "learning_rate": 4.666110675039247e-05,
      "loss": 0.7137,
      "step": 340300
    },
    {
      "epoch": 5.343799058084772,
      "grad_norm": 3.9387989044189453,
      "learning_rate": 4.666012558869702e-05,
      "loss": 0.6642,
      "step": 340400
    },
    {
      "epoch": 5.345368916797488,
      "grad_norm": 4.087608814239502,
      "learning_rate": 4.665914442700157e-05,
      "loss": 0.658,
      "step": 340500
    },
    {
      "epoch": 5.346938775510204,
      "grad_norm": 3.049556255340576,
      "learning_rate": 4.665816326530613e-05,
      "loss": 0.6358,
      "step": 340600
    },
    {
      "epoch": 5.34850863422292,
      "grad_norm": 3.309182643890381,
      "learning_rate": 4.665718210361068e-05,
      "loss": 0.7087,
      "step": 340700
    },
    {
      "epoch": 5.350078492935636,
      "grad_norm": 5.0361328125,
      "learning_rate": 4.665620094191523e-05,
      "loss": 0.6342,
      "step": 340800
    },
    {
      "epoch": 5.351648351648351,
      "grad_norm": 3.0567634105682373,
      "learning_rate": 4.665521978021978e-05,
      "loss": 0.6723,
      "step": 340900
    },
    {
      "epoch": 5.353218210361067,
      "grad_norm": 4.36506986618042,
      "learning_rate": 4.665423861852434e-05,
      "loss": 0.6439,
      "step": 341000
    },
    {
      "epoch": 5.354788069073783,
      "grad_norm": 2.938774347305298,
      "learning_rate": 4.665325745682889e-05,
      "loss": 0.6156,
      "step": 341100
    },
    {
      "epoch": 5.356357927786499,
      "grad_norm": 4.181194305419922,
      "learning_rate": 4.665227629513344e-05,
      "loss": 0.6395,
      "step": 341200
    },
    {
      "epoch": 5.357927786499215,
      "grad_norm": 3.8971364498138428,
      "learning_rate": 4.6651295133438e-05,
      "loss": 0.6721,
      "step": 341300
    },
    {
      "epoch": 5.359497645211931,
      "grad_norm": 2.9118764400482178,
      "learning_rate": 4.665031397174254e-05,
      "loss": 0.6514,
      "step": 341400
    },
    {
      "epoch": 5.361067503924647,
      "grad_norm": 3.616655111312866,
      "learning_rate": 4.66493328100471e-05,
      "loss": 0.6781,
      "step": 341500
    },
    {
      "epoch": 5.362637362637362,
      "grad_norm": 3.7806804180145264,
      "learning_rate": 4.664835164835165e-05,
      "loss": 0.6335,
      "step": 341600
    },
    {
      "epoch": 5.364207221350078,
      "grad_norm": 3.90643310546875,
      "learning_rate": 4.664737048665621e-05,
      "loss": 0.6515,
      "step": 341700
    },
    {
      "epoch": 5.365777080062794,
      "grad_norm": 3.8871631622314453,
      "learning_rate": 4.664638932496075e-05,
      "loss": 0.6674,
      "step": 341800
    },
    {
      "epoch": 5.36734693877551,
      "grad_norm": 4.113308906555176,
      "learning_rate": 4.664540816326531e-05,
      "loss": 0.6112,
      "step": 341900
    },
    {
      "epoch": 5.368916797488226,
      "grad_norm": 4.266454219818115,
      "learning_rate": 4.664442700156986e-05,
      "loss": 0.6732,
      "step": 342000
    },
    {
      "epoch": 5.370486656200942,
      "grad_norm": 3.6816492080688477,
      "learning_rate": 4.664344583987441e-05,
      "loss": 0.6301,
      "step": 342100
    },
    {
      "epoch": 5.372056514913658,
      "grad_norm": 3.785098075866699,
      "learning_rate": 4.664246467817896e-05,
      "loss": 0.6818,
      "step": 342200
    },
    {
      "epoch": 5.373626373626374,
      "grad_norm": 3.89668869972229,
      "learning_rate": 4.664148351648352e-05,
      "loss": 0.6387,
      "step": 342300
    },
    {
      "epoch": 5.375196232339089,
      "grad_norm": 2.649013042449951,
      "learning_rate": 4.664050235478807e-05,
      "loss": 0.694,
      "step": 342400
    },
    {
      "epoch": 5.376766091051805,
      "grad_norm": 3.9241812229156494,
      "learning_rate": 4.663952119309262e-05,
      "loss": 0.633,
      "step": 342500
    },
    {
      "epoch": 5.378335949764521,
      "grad_norm": 3.118502140045166,
      "learning_rate": 4.6638540031397174e-05,
      "loss": 0.6461,
      "step": 342600
    },
    {
      "epoch": 5.379905808477237,
      "grad_norm": 4.726030349731445,
      "learning_rate": 4.663755886970173e-05,
      "loss": 0.7111,
      "step": 342700
    },
    {
      "epoch": 5.381475667189953,
      "grad_norm": 4.06329870223999,
      "learning_rate": 4.663657770800628e-05,
      "loss": 0.6635,
      "step": 342800
    },
    {
      "epoch": 5.383045525902669,
      "grad_norm": 4.103984355926514,
      "learning_rate": 4.6635596546310833e-05,
      "loss": 0.6486,
      "step": 342900
    },
    {
      "epoch": 5.384615384615385,
      "grad_norm": 4.432331085205078,
      "learning_rate": 4.6634615384615384e-05,
      "loss": 0.6605,
      "step": 343000
    },
    {
      "epoch": 5.3861852433281,
      "grad_norm": 3.8492345809936523,
      "learning_rate": 4.663363422291994e-05,
      "loss": 0.6635,
      "step": 343100
    },
    {
      "epoch": 5.387755102040816,
      "grad_norm": 2.8219897747039795,
      "learning_rate": 4.663265306122449e-05,
      "loss": 0.614,
      "step": 343200
    },
    {
      "epoch": 5.389324960753532,
      "grad_norm": 2.872979164123535,
      "learning_rate": 4.6631671899529044e-05,
      "loss": 0.6491,
      "step": 343300
    },
    {
      "epoch": 5.390894819466248,
      "grad_norm": 4.118152618408203,
      "learning_rate": 4.66306907378336e-05,
      "loss": 0.6653,
      "step": 343400
    },
    {
      "epoch": 5.392464678178964,
      "grad_norm": 4.573215484619141,
      "learning_rate": 4.6629709576138146e-05,
      "loss": 0.6418,
      "step": 343500
    },
    {
      "epoch": 5.39403453689168,
      "grad_norm": 4.275957107543945,
      "learning_rate": 4.6628728414442704e-05,
      "loss": 0.685,
      "step": 343600
    },
    {
      "epoch": 5.395604395604396,
      "grad_norm": 4.364469051361084,
      "learning_rate": 4.6627747252747255e-05,
      "loss": 0.6798,
      "step": 343700
    },
    {
      "epoch": 5.397174254317111,
      "grad_norm": 4.073806285858154,
      "learning_rate": 4.662676609105181e-05,
      "loss": 0.7214,
      "step": 343800
    },
    {
      "epoch": 5.398744113029827,
      "grad_norm": 2.9556772708892822,
      "learning_rate": 4.662578492935636e-05,
      "loss": 0.699,
      "step": 343900
    },
    {
      "epoch": 5.400313971742543,
      "grad_norm": 3.8185386657714844,
      "learning_rate": 4.6624803767660915e-05,
      "loss": 0.6695,
      "step": 344000
    },
    {
      "epoch": 5.401883830455259,
      "grad_norm": 4.355503559112549,
      "learning_rate": 4.6623822605965466e-05,
      "loss": 0.6652,
      "step": 344100
    },
    {
      "epoch": 5.403453689167975,
      "grad_norm": 4.9467549324035645,
      "learning_rate": 4.6622841444270017e-05,
      "loss": 0.6416,
      "step": 344200
    },
    {
      "epoch": 5.405023547880691,
      "grad_norm": 4.235684394836426,
      "learning_rate": 4.662186028257457e-05,
      "loss": 0.637,
      "step": 344300
    },
    {
      "epoch": 5.406593406593407,
      "grad_norm": 3.7317042350769043,
      "learning_rate": 4.6620879120879125e-05,
      "loss": 0.6361,
      "step": 344400
    },
    {
      "epoch": 5.408163265306122,
      "grad_norm": 4.352004528045654,
      "learning_rate": 4.6619897959183676e-05,
      "loss": 0.6653,
      "step": 344500
    },
    {
      "epoch": 5.409733124018838,
      "grad_norm": 4.444324493408203,
      "learning_rate": 4.661891679748823e-05,
      "loss": 0.6946,
      "step": 344600
    },
    {
      "epoch": 5.411302982731554,
      "grad_norm": 4.397879123687744,
      "learning_rate": 4.661793563579278e-05,
      "loss": 0.7053,
      "step": 344700
    },
    {
      "epoch": 5.41287284144427,
      "grad_norm": 4.123907566070557,
      "learning_rate": 4.6616954474097336e-05,
      "loss": 0.6749,
      "step": 344800
    },
    {
      "epoch": 5.414442700156986,
      "grad_norm": 3.9857177734375,
      "learning_rate": 4.661597331240189e-05,
      "loss": 0.6774,
      "step": 344900
    },
    {
      "epoch": 5.416012558869702,
      "grad_norm": 4.213938236236572,
      "learning_rate": 4.661499215070644e-05,
      "loss": 0.6419,
      "step": 345000
    },
    {
      "epoch": 5.417582417582418,
      "grad_norm": 3.48478364944458,
      "learning_rate": 4.661401098901099e-05,
      "loss": 0.6756,
      "step": 345100
    },
    {
      "epoch": 5.419152276295134,
      "grad_norm": 3.008223056793213,
      "learning_rate": 4.661302982731555e-05,
      "loss": 0.6625,
      "step": 345200
    },
    {
      "epoch": 5.420722135007849,
      "grad_norm": 5.0880889892578125,
      "learning_rate": 4.66120486656201e-05,
      "loss": 0.6401,
      "step": 345300
    },
    {
      "epoch": 5.422291993720565,
      "grad_norm": 3.8351593017578125,
      "learning_rate": 4.661106750392465e-05,
      "loss": 0.6882,
      "step": 345400
    },
    {
      "epoch": 5.423861852433281,
      "grad_norm": 4.745454788208008,
      "learning_rate": 4.6610086342229206e-05,
      "loss": 0.7123,
      "step": 345500
    },
    {
      "epoch": 5.425431711145997,
      "grad_norm": 4.202686309814453,
      "learning_rate": 4.660910518053375e-05,
      "loss": 0.6429,
      "step": 345600
    },
    {
      "epoch": 5.427001569858713,
      "grad_norm": 4.491085529327393,
      "learning_rate": 4.660812401883831e-05,
      "loss": 0.6737,
      "step": 345700
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 3.593479871749878,
      "learning_rate": 4.660714285714286e-05,
      "loss": 0.6563,
      "step": 345800
    },
    {
      "epoch": 5.430141287284145,
      "grad_norm": 4.399837970733643,
      "learning_rate": 4.660616169544742e-05,
      "loss": 0.6654,
      "step": 345900
    },
    {
      "epoch": 5.43171114599686,
      "grad_norm": 4.495527267456055,
      "learning_rate": 4.660518053375196e-05,
      "loss": 0.6576,
      "step": 346000
    },
    {
      "epoch": 5.433281004709576,
      "grad_norm": 4.611325740814209,
      "learning_rate": 4.660419937205652e-05,
      "loss": 0.686,
      "step": 346100
    },
    {
      "epoch": 5.434850863422292,
      "grad_norm": 3.407758951187134,
      "learning_rate": 4.660321821036107e-05,
      "loss": 0.6577,
      "step": 346200
    },
    {
      "epoch": 5.436420722135008,
      "grad_norm": 3.7011423110961914,
      "learning_rate": 4.660223704866562e-05,
      "loss": 0.636,
      "step": 346300
    },
    {
      "epoch": 5.437990580847724,
      "grad_norm": 4.176114559173584,
      "learning_rate": 4.660125588697017e-05,
      "loss": 0.7148,
      "step": 346400
    },
    {
      "epoch": 5.43956043956044,
      "grad_norm": 3.729356050491333,
      "learning_rate": 4.660027472527473e-05,
      "loss": 0.6502,
      "step": 346500
    },
    {
      "epoch": 5.441130298273156,
      "grad_norm": 3.40828013420105,
      "learning_rate": 4.659929356357928e-05,
      "loss": 0.6555,
      "step": 346600
    },
    {
      "epoch": 5.442700156985872,
      "grad_norm": 3.436732769012451,
      "learning_rate": 4.659831240188383e-05,
      "loss": 0.6515,
      "step": 346700
    },
    {
      "epoch": 5.444270015698587,
      "grad_norm": 4.819643974304199,
      "learning_rate": 4.659733124018838e-05,
      "loss": 0.6485,
      "step": 346800
    },
    {
      "epoch": 5.445839874411303,
      "grad_norm": 5.132820129394531,
      "learning_rate": 4.659635007849294e-05,
      "loss": 0.6364,
      "step": 346900
    },
    {
      "epoch": 5.447409733124019,
      "grad_norm": 3.6934616565704346,
      "learning_rate": 4.659536891679749e-05,
      "loss": 0.6214,
      "step": 347000
    },
    {
      "epoch": 5.448979591836735,
      "grad_norm": 4.209964275360107,
      "learning_rate": 4.659438775510204e-05,
      "loss": 0.6368,
      "step": 347100
    },
    {
      "epoch": 5.450549450549451,
      "grad_norm": 4.9085211753845215,
      "learning_rate": 4.6593406593406593e-05,
      "loss": 0.6639,
      "step": 347200
    },
    {
      "epoch": 5.452119309262167,
      "grad_norm": 3.713698387145996,
      "learning_rate": 4.659242543171115e-05,
      "loss": 0.6798,
      "step": 347300
    },
    {
      "epoch": 5.453689167974883,
      "grad_norm": 2.9701855182647705,
      "learning_rate": 4.65914442700157e-05,
      "loss": 0.6462,
      "step": 347400
    },
    {
      "epoch": 5.455259026687598,
      "grad_norm": 3.7234225273132324,
      "learning_rate": 4.659046310832025e-05,
      "loss": 0.6543,
      "step": 347500
    },
    {
      "epoch": 5.456828885400314,
      "grad_norm": 3.1575939655303955,
      "learning_rate": 4.658948194662481e-05,
      "loss": 0.6754,
      "step": 347600
    },
    {
      "epoch": 5.45839874411303,
      "grad_norm": 4.408132553100586,
      "learning_rate": 4.6588500784929355e-05,
      "loss": 0.6591,
      "step": 347700
    },
    {
      "epoch": 5.459968602825746,
      "grad_norm": 5.062893390655518,
      "learning_rate": 4.658751962323391e-05,
      "loss": 0.6408,
      "step": 347800
    },
    {
      "epoch": 5.461538461538462,
      "grad_norm": 3.530402660369873,
      "learning_rate": 4.6586538461538464e-05,
      "loss": 0.6714,
      "step": 347900
    },
    {
      "epoch": 5.463108320251178,
      "grad_norm": 4.2326226234436035,
      "learning_rate": 4.658555729984302e-05,
      "loss": 0.6557,
      "step": 348000
    },
    {
      "epoch": 5.464678178963894,
      "grad_norm": 4.017530918121338,
      "learning_rate": 4.6584576138147566e-05,
      "loss": 0.6443,
      "step": 348100
    },
    {
      "epoch": 5.466248037676609,
      "grad_norm": 4.102180480957031,
      "learning_rate": 4.6583594976452124e-05,
      "loss": 0.6724,
      "step": 348200
    },
    {
      "epoch": 5.467817896389325,
      "grad_norm": 4.050705432891846,
      "learning_rate": 4.6582613814756675e-05,
      "loss": 0.683,
      "step": 348300
    },
    {
      "epoch": 5.469387755102041,
      "grad_norm": 4.328230381011963,
      "learning_rate": 4.6581632653061226e-05,
      "loss": 0.6448,
      "step": 348400
    },
    {
      "epoch": 5.470957613814757,
      "grad_norm": 4.305591583251953,
      "learning_rate": 4.6580651491365777e-05,
      "loss": 0.6954,
      "step": 348500
    },
    {
      "epoch": 5.472527472527473,
      "grad_norm": 4.101687431335449,
      "learning_rate": 4.6579670329670334e-05,
      "loss": 0.692,
      "step": 348600
    },
    {
      "epoch": 5.474097331240189,
      "grad_norm": 3.63566255569458,
      "learning_rate": 4.6578689167974885e-05,
      "loss": 0.6352,
      "step": 348700
    },
    {
      "epoch": 5.475667189952905,
      "grad_norm": 4.719802379608154,
      "learning_rate": 4.6577708006279436e-05,
      "loss": 0.6796,
      "step": 348800
    },
    {
      "epoch": 5.47723704866562,
      "grad_norm": 3.8147151470184326,
      "learning_rate": 4.657672684458399e-05,
      "loss": 0.6478,
      "step": 348900
    },
    {
      "epoch": 5.478806907378336,
      "grad_norm": 3.631514310836792,
      "learning_rate": 4.6575745682888545e-05,
      "loss": 0.6552,
      "step": 349000
    },
    {
      "epoch": 5.480376766091052,
      "grad_norm": 3.0429327487945557,
      "learning_rate": 4.6574764521193096e-05,
      "loss": 0.6557,
      "step": 349100
    },
    {
      "epoch": 5.481946624803768,
      "grad_norm": 4.57565975189209,
      "learning_rate": 4.657378335949765e-05,
      "loss": 0.6889,
      "step": 349200
    },
    {
      "epoch": 5.483516483516484,
      "grad_norm": 3.80686616897583,
      "learning_rate": 4.65728021978022e-05,
      "loss": 0.6323,
      "step": 349300
    },
    {
      "epoch": 5.4850863422291996,
      "grad_norm": 4.3930864334106445,
      "learning_rate": 4.6571821036106756e-05,
      "loss": 0.6665,
      "step": 349400
    },
    {
      "epoch": 5.4866562009419155,
      "grad_norm": 3.9693188667297363,
      "learning_rate": 4.657083987441131e-05,
      "loss": 0.6373,
      "step": 349500
    },
    {
      "epoch": 5.488226059654631,
      "grad_norm": 3.9709508419036865,
      "learning_rate": 4.656985871271586e-05,
      "loss": 0.6587,
      "step": 349600
    },
    {
      "epoch": 5.489795918367347,
      "grad_norm": 4.301069736480713,
      "learning_rate": 4.6568877551020415e-05,
      "loss": 0.6603,
      "step": 349700
    },
    {
      "epoch": 5.491365777080063,
      "grad_norm": 3.9920735359191895,
      "learning_rate": 4.656789638932496e-05,
      "loss": 0.664,
      "step": 349800
    },
    {
      "epoch": 5.4929356357927785,
      "grad_norm": 3.6813769340515137,
      "learning_rate": 4.656691522762952e-05,
      "loss": 0.6506,
      "step": 349900
    },
    {
      "epoch": 5.4945054945054945,
      "grad_norm": 2.594759941101074,
      "learning_rate": 4.656593406593407e-05,
      "loss": 0.6593,
      "step": 350000
    },
    {
      "epoch": 5.4960753532182105,
      "grad_norm": 3.528402328491211,
      "learning_rate": 4.6564952904238626e-05,
      "loss": 0.633,
      "step": 350100
    },
    {
      "epoch": 5.4976452119309265,
      "grad_norm": 4.8264641761779785,
      "learning_rate": 4.656397174254317e-05,
      "loss": 0.6436,
      "step": 350200
    },
    {
      "epoch": 5.4992150706436425,
      "grad_norm": 3.840022087097168,
      "learning_rate": 4.656299058084773e-05,
      "loss": 0.6403,
      "step": 350300
    },
    {
      "epoch": 5.5007849293563575,
      "grad_norm": 4.415582656860352,
      "learning_rate": 4.656200941915228e-05,
      "loss": 0.6717,
      "step": 350400
    },
    {
      "epoch": 5.5023547880690735,
      "grad_norm": 3.958989381790161,
      "learning_rate": 4.656102825745683e-05,
      "loss": 0.6879,
      "step": 350500
    },
    {
      "epoch": 5.5039246467817895,
      "grad_norm": 3.505143165588379,
      "learning_rate": 4.656004709576138e-05,
      "loss": 0.6735,
      "step": 350600
    },
    {
      "epoch": 5.5054945054945055,
      "grad_norm": 3.3269505500793457,
      "learning_rate": 4.655906593406594e-05,
      "loss": 0.6723,
      "step": 350700
    },
    {
      "epoch": 5.5070643642072215,
      "grad_norm": 3.6489412784576416,
      "learning_rate": 4.655808477237049e-05,
      "loss": 0.6409,
      "step": 350800
    },
    {
      "epoch": 5.508634222919937,
      "grad_norm": 3.5117950439453125,
      "learning_rate": 4.655710361067504e-05,
      "loss": 0.6906,
      "step": 350900
    },
    {
      "epoch": 5.510204081632653,
      "grad_norm": 4.716924667358398,
      "learning_rate": 4.655612244897959e-05,
      "loss": 0.6821,
      "step": 351000
    },
    {
      "epoch": 5.511773940345369,
      "grad_norm": 4.815706729888916,
      "learning_rate": 4.655514128728415e-05,
      "loss": 0.6532,
      "step": 351100
    },
    {
      "epoch": 5.5133437990580845,
      "grad_norm": 4.7390007972717285,
      "learning_rate": 4.65541601255887e-05,
      "loss": 0.6836,
      "step": 351200
    },
    {
      "epoch": 5.5149136577708004,
      "grad_norm": 2.4299893379211426,
      "learning_rate": 4.655317896389325e-05,
      "loss": 0.6749,
      "step": 351300
    },
    {
      "epoch": 5.516483516483516,
      "grad_norm": 3.8045012950897217,
      "learning_rate": 4.65521978021978e-05,
      "loss": 0.6434,
      "step": 351400
    },
    {
      "epoch": 5.518053375196232,
      "grad_norm": 4.25892972946167,
      "learning_rate": 4.655121664050236e-05,
      "loss": 0.6891,
      "step": 351500
    },
    {
      "epoch": 5.519623233908948,
      "grad_norm": 3.5039541721343994,
      "learning_rate": 4.655023547880691e-05,
      "loss": 0.6737,
      "step": 351600
    },
    {
      "epoch": 5.521193092621664,
      "grad_norm": 4.249258995056152,
      "learning_rate": 4.654925431711146e-05,
      "loss": 0.6826,
      "step": 351700
    },
    {
      "epoch": 5.52276295133438,
      "grad_norm": 4.384886741638184,
      "learning_rate": 4.654827315541602e-05,
      "loss": 0.6268,
      "step": 351800
    },
    {
      "epoch": 5.524332810047095,
      "grad_norm": 4.9598541259765625,
      "learning_rate": 4.6547291993720564e-05,
      "loss": 0.7066,
      "step": 351900
    },
    {
      "epoch": 5.525902668759811,
      "grad_norm": 3.9141836166381836,
      "learning_rate": 4.654631083202512e-05,
      "loss": 0.6558,
      "step": 352000
    },
    {
      "epoch": 5.527472527472527,
      "grad_norm": 4.086628437042236,
      "learning_rate": 4.654532967032967e-05,
      "loss": 0.6732,
      "step": 352100
    },
    {
      "epoch": 5.529042386185243,
      "grad_norm": 4.243972301483154,
      "learning_rate": 4.654434850863423e-05,
      "loss": 0.6732,
      "step": 352200
    },
    {
      "epoch": 5.530612244897959,
      "grad_norm": 3.470395565032959,
      "learning_rate": 4.6543367346938775e-05,
      "loss": 0.6738,
      "step": 352300
    },
    {
      "epoch": 5.532182103610675,
      "grad_norm": 3.25649094581604,
      "learning_rate": 4.654238618524333e-05,
      "loss": 0.6475,
      "step": 352400
    },
    {
      "epoch": 5.533751962323391,
      "grad_norm": 4.454104423522949,
      "learning_rate": 4.6541405023547884e-05,
      "loss": 0.6362,
      "step": 352500
    },
    {
      "epoch": 5.535321821036106,
      "grad_norm": 3.637272596359253,
      "learning_rate": 4.6540423861852435e-05,
      "loss": 0.6577,
      "step": 352600
    },
    {
      "epoch": 5.536891679748822,
      "grad_norm": 3.7758231163024902,
      "learning_rate": 4.6539442700156985e-05,
      "loss": 0.7147,
      "step": 352700
    },
    {
      "epoch": 5.538461538461538,
      "grad_norm": 4.257971286773682,
      "learning_rate": 4.653846153846154e-05,
      "loss": 0.7259,
      "step": 352800
    },
    {
      "epoch": 5.540031397174254,
      "grad_norm": 4.01359748840332,
      "learning_rate": 4.653748037676609e-05,
      "loss": 0.6831,
      "step": 352900
    },
    {
      "epoch": 5.54160125588697,
      "grad_norm": 3.290703773498535,
      "learning_rate": 4.6536499215070645e-05,
      "loss": 0.6335,
      "step": 353000
    },
    {
      "epoch": 5.543171114599686,
      "grad_norm": 4.915111541748047,
      "learning_rate": 4.6535518053375196e-05,
      "loss": 0.649,
      "step": 353100
    },
    {
      "epoch": 5.544740973312402,
      "grad_norm": 4.558431625366211,
      "learning_rate": 4.6534536891679754e-05,
      "loss": 0.6825,
      "step": 353200
    },
    {
      "epoch": 5.546310832025117,
      "grad_norm": 4.467582702636719,
      "learning_rate": 4.6533555729984305e-05,
      "loss": 0.6621,
      "step": 353300
    },
    {
      "epoch": 5.547880690737833,
      "grad_norm": 3.6082687377929688,
      "learning_rate": 4.6532574568288856e-05,
      "loss": 0.6822,
      "step": 353400
    },
    {
      "epoch": 5.549450549450549,
      "grad_norm": 4.965259552001953,
      "learning_rate": 4.653159340659341e-05,
      "loss": 0.6816,
      "step": 353500
    },
    {
      "epoch": 5.551020408163265,
      "grad_norm": 3.4185242652893066,
      "learning_rate": 4.653061224489796e-05,
      "loss": 0.6745,
      "step": 353600
    },
    {
      "epoch": 5.552590266875981,
      "grad_norm": 4.023850440979004,
      "learning_rate": 4.6529631083202516e-05,
      "loss": 0.6714,
      "step": 353700
    },
    {
      "epoch": 5.554160125588697,
      "grad_norm": 3.7046163082122803,
      "learning_rate": 4.652864992150707e-05,
      "loss": 0.6693,
      "step": 353800
    },
    {
      "epoch": 5.555729984301413,
      "grad_norm": 4.510915756225586,
      "learning_rate": 4.6527668759811624e-05,
      "loss": 0.6578,
      "step": 353900
    },
    {
      "epoch": 5.557299843014128,
      "grad_norm": 3.3449227809906006,
      "learning_rate": 4.652668759811617e-05,
      "loss": 0.6472,
      "step": 354000
    },
    {
      "epoch": 5.558869701726844,
      "grad_norm": 3.753309726715088,
      "learning_rate": 4.6525706436420726e-05,
      "loss": 0.6445,
      "step": 354100
    },
    {
      "epoch": 5.56043956043956,
      "grad_norm": 3.1468069553375244,
      "learning_rate": 4.652472527472528e-05,
      "loss": 0.6389,
      "step": 354200
    },
    {
      "epoch": 5.562009419152276,
      "grad_norm": 3.6814181804656982,
      "learning_rate": 4.652374411302983e-05,
      "loss": 0.6652,
      "step": 354300
    },
    {
      "epoch": 5.563579277864992,
      "grad_norm": 4.358278274536133,
      "learning_rate": 4.652276295133438e-05,
      "loss": 0.6935,
      "step": 354400
    },
    {
      "epoch": 5.565149136577708,
      "grad_norm": 4.460705280303955,
      "learning_rate": 4.652178178963894e-05,
      "loss": 0.6369,
      "step": 354500
    },
    {
      "epoch": 5.566718995290424,
      "grad_norm": 4.855538845062256,
      "learning_rate": 4.652080062794349e-05,
      "loss": 0.6664,
      "step": 354600
    },
    {
      "epoch": 5.568288854003139,
      "grad_norm": 3.7632434368133545,
      "learning_rate": 4.651981946624804e-05,
      "loss": 0.6818,
      "step": 354700
    },
    {
      "epoch": 5.569858712715855,
      "grad_norm": 3.38041615486145,
      "learning_rate": 4.651883830455259e-05,
      "loss": 0.6459,
      "step": 354800
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 4.178966999053955,
      "learning_rate": 4.651785714285715e-05,
      "loss": 0.6926,
      "step": 354900
    },
    {
      "epoch": 5.572998430141287,
      "grad_norm": 3.77398681640625,
      "learning_rate": 4.651687598116169e-05,
      "loss": 0.6367,
      "step": 355000
    },
    {
      "epoch": 5.574568288854003,
      "grad_norm": 4.689624309539795,
      "learning_rate": 4.651589481946625e-05,
      "loss": 0.6606,
      "step": 355100
    },
    {
      "epoch": 5.576138147566719,
      "grad_norm": 3.9890942573547363,
      "learning_rate": 4.65149136577708e-05,
      "loss": 0.709,
      "step": 355200
    },
    {
      "epoch": 5.577708006279435,
      "grad_norm": 4.480832576751709,
      "learning_rate": 4.651393249607536e-05,
      "loss": 0.6762,
      "step": 355300
    },
    {
      "epoch": 5.579277864992151,
      "grad_norm": 3.953087091445923,
      "learning_rate": 4.651295133437991e-05,
      "loss": 0.6685,
      "step": 355400
    },
    {
      "epoch": 5.580847723704867,
      "grad_norm": 4.330842971801758,
      "learning_rate": 4.651197017268446e-05,
      "loss": 0.6889,
      "step": 355500
    },
    {
      "epoch": 5.582417582417582,
      "grad_norm": 4.258101463317871,
      "learning_rate": 4.651098901098901e-05,
      "loss": 0.673,
      "step": 355600
    },
    {
      "epoch": 5.583987441130298,
      "grad_norm": 3.6900155544281006,
      "learning_rate": 4.651000784929356e-05,
      "loss": 0.6786,
      "step": 355700
    },
    {
      "epoch": 5.585557299843014,
      "grad_norm": 3.882009267807007,
      "learning_rate": 4.650902668759812e-05,
      "loss": 0.7329,
      "step": 355800
    },
    {
      "epoch": 5.58712715855573,
      "grad_norm": 3.805756092071533,
      "learning_rate": 4.650804552590267e-05,
      "loss": 0.6879,
      "step": 355900
    },
    {
      "epoch": 5.588697017268446,
      "grad_norm": 4.841065406799316,
      "learning_rate": 4.650706436420723e-05,
      "loss": 0.7126,
      "step": 356000
    },
    {
      "epoch": 5.590266875981162,
      "grad_norm": 4.720370292663574,
      "learning_rate": 4.650608320251177e-05,
      "loss": 0.6852,
      "step": 356100
    },
    {
      "epoch": 5.591836734693878,
      "grad_norm": 3.7315380573272705,
      "learning_rate": 4.650510204081633e-05,
      "loss": 0.6486,
      "step": 356200
    },
    {
      "epoch": 5.593406593406593,
      "grad_norm": 4.426881313323975,
      "learning_rate": 4.650412087912088e-05,
      "loss": 0.6685,
      "step": 356300
    },
    {
      "epoch": 5.594976452119309,
      "grad_norm": 3.738783121109009,
      "learning_rate": 4.650313971742543e-05,
      "loss": 0.7038,
      "step": 356400
    },
    {
      "epoch": 5.596546310832025,
      "grad_norm": 3.4657976627349854,
      "learning_rate": 4.6502158555729984e-05,
      "loss": 0.6877,
      "step": 356500
    },
    {
      "epoch": 5.598116169544741,
      "grad_norm": 3.3131442070007324,
      "learning_rate": 4.650117739403454e-05,
      "loss": 0.6666,
      "step": 356600
    },
    {
      "epoch": 5.599686028257457,
      "grad_norm": 3.88067889213562,
      "learning_rate": 4.650019623233909e-05,
      "loss": 0.6508,
      "step": 356700
    },
    {
      "epoch": 5.601255886970173,
      "grad_norm": 3.5245134830474854,
      "learning_rate": 4.6499215070643643e-05,
      "loss": 0.6638,
      "step": 356800
    },
    {
      "epoch": 5.602825745682889,
      "grad_norm": 4.101591110229492,
      "learning_rate": 4.6498233908948194e-05,
      "loss": 0.7118,
      "step": 356900
    },
    {
      "epoch": 5.604395604395604,
      "grad_norm": 4.50521993637085,
      "learning_rate": 4.649725274725275e-05,
      "loss": 0.6935,
      "step": 357000
    },
    {
      "epoch": 5.60596546310832,
      "grad_norm": 3.3908913135528564,
      "learning_rate": 4.6496271585557296e-05,
      "loss": 0.6449,
      "step": 357100
    },
    {
      "epoch": 5.607535321821036,
      "grad_norm": 4.374117374420166,
      "learning_rate": 4.6495290423861854e-05,
      "loss": 0.6379,
      "step": 357200
    },
    {
      "epoch": 5.609105180533752,
      "grad_norm": 3.6457207202911377,
      "learning_rate": 4.6494309262166405e-05,
      "loss": 0.6799,
      "step": 357300
    },
    {
      "epoch": 5.610675039246468,
      "grad_norm": 3.2695670127868652,
      "learning_rate": 4.649332810047096e-05,
      "loss": 0.6626,
      "step": 357400
    },
    {
      "epoch": 5.612244897959184,
      "grad_norm": 4.085814952850342,
      "learning_rate": 4.6492346938775514e-05,
      "loss": 0.654,
      "step": 357500
    },
    {
      "epoch": 5.6138147566719,
      "grad_norm": 4.597539901733398,
      "learning_rate": 4.6491365777080065e-05,
      "loss": 0.6404,
      "step": 357600
    },
    {
      "epoch": 5.615384615384615,
      "grad_norm": 4.430565357208252,
      "learning_rate": 4.6490384615384616e-05,
      "loss": 0.6876,
      "step": 357700
    },
    {
      "epoch": 5.616954474097331,
      "grad_norm": 4.307713031768799,
      "learning_rate": 4.648940345368917e-05,
      "loss": 0.697,
      "step": 357800
    },
    {
      "epoch": 5.618524332810047,
      "grad_norm": 4.242955207824707,
      "learning_rate": 4.6488422291993725e-05,
      "loss": 0.6679,
      "step": 357900
    },
    {
      "epoch": 5.620094191522763,
      "grad_norm": 4.832005023956299,
      "learning_rate": 4.6487441130298276e-05,
      "loss": 0.6299,
      "step": 358000
    },
    {
      "epoch": 5.621664050235479,
      "grad_norm": 5.058720588684082,
      "learning_rate": 4.648645996860283e-05,
      "loss": 0.6395,
      "step": 358100
    },
    {
      "epoch": 5.623233908948195,
      "grad_norm": 4.051327228546143,
      "learning_rate": 4.648547880690738e-05,
      "loss": 0.6484,
      "step": 358200
    },
    {
      "epoch": 5.624803767660911,
      "grad_norm": 3.9856903553009033,
      "learning_rate": 4.6484497645211935e-05,
      "loss": 0.656,
      "step": 358300
    },
    {
      "epoch": 5.626373626373626,
      "grad_norm": 3.772324800491333,
      "learning_rate": 4.6483516483516486e-05,
      "loss": 0.6213,
      "step": 358400
    },
    {
      "epoch": 5.627943485086342,
      "grad_norm": 3.4850354194641113,
      "learning_rate": 4.648253532182104e-05,
      "loss": 0.6669,
      "step": 358500
    },
    {
      "epoch": 5.629513343799058,
      "grad_norm": 3.6699671745300293,
      "learning_rate": 4.648155416012559e-05,
      "loss": 0.6675,
      "step": 358600
    },
    {
      "epoch": 5.631083202511774,
      "grad_norm": 4.7226057052612305,
      "learning_rate": 4.6480572998430146e-05,
      "loss": 0.7019,
      "step": 358700
    },
    {
      "epoch": 5.63265306122449,
      "grad_norm": 4.166373252868652,
      "learning_rate": 4.64795918367347e-05,
      "loss": 0.6539,
      "step": 358800
    },
    {
      "epoch": 5.634222919937206,
      "grad_norm": 4.336981773376465,
      "learning_rate": 4.647861067503925e-05,
      "loss": 0.6301,
      "step": 358900
    },
    {
      "epoch": 5.635792778649922,
      "grad_norm": 4.007297515869141,
      "learning_rate": 4.64776295133438e-05,
      "loss": 0.6739,
      "step": 359000
    },
    {
      "epoch": 5.637362637362637,
      "grad_norm": 4.458922386169434,
      "learning_rate": 4.647664835164836e-05,
      "loss": 0.6708,
      "step": 359100
    },
    {
      "epoch": 5.638932496075353,
      "grad_norm": 3.121837615966797,
      "learning_rate": 4.64756671899529e-05,
      "loss": 0.677,
      "step": 359200
    },
    {
      "epoch": 5.640502354788069,
      "grad_norm": 4.263079643249512,
      "learning_rate": 4.647468602825746e-05,
      "loss": 0.6443,
      "step": 359300
    },
    {
      "epoch": 5.642072213500785,
      "grad_norm": 3.2478461265563965,
      "learning_rate": 4.647370486656201e-05,
      "loss": 0.6682,
      "step": 359400
    },
    {
      "epoch": 5.643642072213501,
      "grad_norm": 6.708268165588379,
      "learning_rate": 4.647272370486657e-05,
      "loss": 0.6653,
      "step": 359500
    },
    {
      "epoch": 5.645211930926217,
      "grad_norm": 3.215080738067627,
      "learning_rate": 4.647174254317112e-05,
      "loss": 0.6565,
      "step": 359600
    },
    {
      "epoch": 5.646781789638933,
      "grad_norm": 3.8892276287078857,
      "learning_rate": 4.647076138147567e-05,
      "loss": 0.682,
      "step": 359700
    },
    {
      "epoch": 5.648351648351649,
      "grad_norm": 3.1383657455444336,
      "learning_rate": 4.646978021978022e-05,
      "loss": 0.7036,
      "step": 359800
    },
    {
      "epoch": 5.649921507064365,
      "grad_norm": 4.3141770362854,
      "learning_rate": 4.646879905808477e-05,
      "loss": 0.6393,
      "step": 359900
    },
    {
      "epoch": 5.65149136577708,
      "grad_norm": 4.815280437469482,
      "learning_rate": 4.646781789638933e-05,
      "loss": 0.6465,
      "step": 360000
    },
    {
      "epoch": 5.653061224489796,
      "grad_norm": 3.7528398036956787,
      "learning_rate": 4.646683673469388e-05,
      "loss": 0.664,
      "step": 360100
    },
    {
      "epoch": 5.654631083202512,
      "grad_norm": 4.020031452178955,
      "learning_rate": 4.646585557299844e-05,
      "loss": 0.6767,
      "step": 360200
    },
    {
      "epoch": 5.656200941915228,
      "grad_norm": 3.8104970455169678,
      "learning_rate": 4.646487441130298e-05,
      "loss": 0.6797,
      "step": 360300
    },
    {
      "epoch": 5.657770800627944,
      "grad_norm": 3.2275636196136475,
      "learning_rate": 4.646389324960754e-05,
      "loss": 0.6586,
      "step": 360400
    },
    {
      "epoch": 5.65934065934066,
      "grad_norm": 4.347494125366211,
      "learning_rate": 4.646291208791209e-05,
      "loss": 0.7012,
      "step": 360500
    },
    {
      "epoch": 5.660910518053376,
      "grad_norm": 4.469321250915527,
      "learning_rate": 4.646193092621664e-05,
      "loss": 0.6527,
      "step": 360600
    },
    {
      "epoch": 5.662480376766091,
      "grad_norm": 4.442033767700195,
      "learning_rate": 4.646094976452119e-05,
      "loss": 0.662,
      "step": 360700
    },
    {
      "epoch": 5.664050235478807,
      "grad_norm": 3.5943150520324707,
      "learning_rate": 4.645996860282575e-05,
      "loss": 0.6652,
      "step": 360800
    },
    {
      "epoch": 5.665620094191523,
      "grad_norm": 3.9497804641723633,
      "learning_rate": 4.64589874411303e-05,
      "loss": 0.6552,
      "step": 360900
    },
    {
      "epoch": 5.667189952904239,
      "grad_norm": 4.429511070251465,
      "learning_rate": 4.645800627943485e-05,
      "loss": 0.6807,
      "step": 361000
    },
    {
      "epoch": 5.668759811616955,
      "grad_norm": 3.576270341873169,
      "learning_rate": 4.6457025117739403e-05,
      "loss": 0.7226,
      "step": 361100
    },
    {
      "epoch": 5.670329670329671,
      "grad_norm": 4.293335914611816,
      "learning_rate": 4.645604395604396e-05,
      "loss": 0.6234,
      "step": 361200
    },
    {
      "epoch": 5.671899529042387,
      "grad_norm": 4.018089294433594,
      "learning_rate": 4.6455062794348505e-05,
      "loss": 0.7063,
      "step": 361300
    },
    {
      "epoch": 5.673469387755102,
      "grad_norm": 3.824526071548462,
      "learning_rate": 4.645408163265306e-05,
      "loss": 0.675,
      "step": 361400
    },
    {
      "epoch": 5.675039246467818,
      "grad_norm": 4.560312271118164,
      "learning_rate": 4.6453100470957614e-05,
      "loss": 0.6548,
      "step": 361500
    },
    {
      "epoch": 5.676609105180534,
      "grad_norm": 2.9982337951660156,
      "learning_rate": 4.645211930926217e-05,
      "loss": 0.6564,
      "step": 361600
    },
    {
      "epoch": 5.67817896389325,
      "grad_norm": 3.7719497680664062,
      "learning_rate": 4.645113814756672e-05,
      "loss": 0.6714,
      "step": 361700
    },
    {
      "epoch": 5.679748822605966,
      "grad_norm": 4.503504753112793,
      "learning_rate": 4.6450156985871274e-05,
      "loss": 0.6548,
      "step": 361800
    },
    {
      "epoch": 5.681318681318682,
      "grad_norm": 4.156296253204346,
      "learning_rate": 4.6449175824175825e-05,
      "loss": 0.7053,
      "step": 361900
    },
    {
      "epoch": 5.6828885400313975,
      "grad_norm": 4.316360950469971,
      "learning_rate": 4.6448194662480376e-05,
      "loss": 0.6765,
      "step": 362000
    },
    {
      "epoch": 5.684458398744113,
      "grad_norm": 3.217301368713379,
      "learning_rate": 4.6447213500784934e-05,
      "loss": 0.697,
      "step": 362100
    },
    {
      "epoch": 5.686028257456829,
      "grad_norm": 4.141224384307861,
      "learning_rate": 4.6446232339089485e-05,
      "loss": 0.6828,
      "step": 362200
    },
    {
      "epoch": 5.687598116169545,
      "grad_norm": 4.510013580322266,
      "learning_rate": 4.644525117739404e-05,
      "loss": 0.6783,
      "step": 362300
    },
    {
      "epoch": 5.689167974882261,
      "grad_norm": 4.732990741729736,
      "learning_rate": 4.6444270015698587e-05,
      "loss": 0.6408,
      "step": 362400
    },
    {
      "epoch": 5.6907378335949765,
      "grad_norm": 4.0954742431640625,
      "learning_rate": 4.6443288854003144e-05,
      "loss": 0.6577,
      "step": 362500
    },
    {
      "epoch": 5.6923076923076925,
      "grad_norm": 5.0318684577941895,
      "learning_rate": 4.6442307692307695e-05,
      "loss": 0.6832,
      "step": 362600
    },
    {
      "epoch": 5.6938775510204085,
      "grad_norm": 4.017497539520264,
      "learning_rate": 4.6441326530612246e-05,
      "loss": 0.6767,
      "step": 362700
    },
    {
      "epoch": 5.695447409733124,
      "grad_norm": 3.382269859313965,
      "learning_rate": 4.64403453689168e-05,
      "loss": 0.6101,
      "step": 362800
    },
    {
      "epoch": 5.6970172684458396,
      "grad_norm": 3.4398720264434814,
      "learning_rate": 4.6439364207221355e-05,
      "loss": 0.7079,
      "step": 362900
    },
    {
      "epoch": 5.6985871271585555,
      "grad_norm": 3.2434353828430176,
      "learning_rate": 4.6438383045525906e-05,
      "loss": 0.6755,
      "step": 363000
    },
    {
      "epoch": 5.7001569858712715,
      "grad_norm": 4.193785667419434,
      "learning_rate": 4.643740188383046e-05,
      "loss": 0.6769,
      "step": 363100
    },
    {
      "epoch": 5.7017268445839875,
      "grad_norm": 4.217918872833252,
      "learning_rate": 4.643642072213501e-05,
      "loss": 0.661,
      "step": 363200
    },
    {
      "epoch": 5.7032967032967035,
      "grad_norm": 4.657800197601318,
      "learning_rate": 4.6435439560439566e-05,
      "loss": 0.6938,
      "step": 363300
    },
    {
      "epoch": 5.704866562009419,
      "grad_norm": 4.209283351898193,
      "learning_rate": 4.643445839874411e-05,
      "loss": 0.6764,
      "step": 363400
    },
    {
      "epoch": 5.7064364207221345,
      "grad_norm": 3.634992837905884,
      "learning_rate": 4.643347723704867e-05,
      "loss": 0.6119,
      "step": 363500
    },
    {
      "epoch": 5.7080062794348505,
      "grad_norm": 3.3028316497802734,
      "learning_rate": 4.643249607535322e-05,
      "loss": 0.6515,
      "step": 363600
    },
    {
      "epoch": 5.7095761381475665,
      "grad_norm": 4.169280052185059,
      "learning_rate": 4.6431514913657776e-05,
      "loss": 0.6367,
      "step": 363700
    },
    {
      "epoch": 5.7111459968602825,
      "grad_norm": 4.181911468505859,
      "learning_rate": 4.643053375196233e-05,
      "loss": 0.7122,
      "step": 363800
    },
    {
      "epoch": 5.712715855572998,
      "grad_norm": 4.058631420135498,
      "learning_rate": 4.642955259026688e-05,
      "loss": 0.6501,
      "step": 363900
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 4.3515472412109375,
      "learning_rate": 4.642857142857143e-05,
      "loss": 0.6714,
      "step": 364000
    },
    {
      "epoch": 5.71585557299843,
      "grad_norm": 3.176666736602783,
      "learning_rate": 4.642759026687598e-05,
      "loss": 0.6509,
      "step": 364100
    },
    {
      "epoch": 5.717425431711146,
      "grad_norm": 5.43344783782959,
      "learning_rate": 4.642660910518054e-05,
      "loss": 0.6636,
      "step": 364200
    },
    {
      "epoch": 5.718995290423862,
      "grad_norm": 3.279088020324707,
      "learning_rate": 4.642562794348509e-05,
      "loss": 0.6529,
      "step": 364300
    },
    {
      "epoch": 5.720565149136577,
      "grad_norm": 4.393679618835449,
      "learning_rate": 4.642464678178965e-05,
      "loss": 0.672,
      "step": 364400
    },
    {
      "epoch": 5.722135007849293,
      "grad_norm": 3.6130967140197754,
      "learning_rate": 4.642366562009419e-05,
      "loss": 0.6961,
      "step": 364500
    },
    {
      "epoch": 5.723704866562009,
      "grad_norm": 3.9686193466186523,
      "learning_rate": 4.642268445839875e-05,
      "loss": 0.6439,
      "step": 364600
    },
    {
      "epoch": 5.725274725274725,
      "grad_norm": 3.906390905380249,
      "learning_rate": 4.64217032967033e-05,
      "loss": 0.6894,
      "step": 364700
    },
    {
      "epoch": 5.726844583987441,
      "grad_norm": 3.154242992401123,
      "learning_rate": 4.642072213500785e-05,
      "loss": 0.6739,
      "step": 364800
    },
    {
      "epoch": 5.728414442700157,
      "grad_norm": 4.021996021270752,
      "learning_rate": 4.64197409733124e-05,
      "loss": 0.6702,
      "step": 364900
    },
    {
      "epoch": 5.729984301412873,
      "grad_norm": 4.014223098754883,
      "learning_rate": 4.641875981161696e-05,
      "loss": 0.6575,
      "step": 365000
    },
    {
      "epoch": 5.731554160125588,
      "grad_norm": 5.138747692108154,
      "learning_rate": 4.641777864992151e-05,
      "loss": 0.6909,
      "step": 365100
    },
    {
      "epoch": 5.733124018838304,
      "grad_norm": 3.1557281017303467,
      "learning_rate": 4.641679748822606e-05,
      "loss": 0.6569,
      "step": 365200
    },
    {
      "epoch": 5.73469387755102,
      "grad_norm": 4.078875541687012,
      "learning_rate": 4.641581632653061e-05,
      "loss": 0.6164,
      "step": 365300
    },
    {
      "epoch": 5.736263736263736,
      "grad_norm": 4.242930889129639,
      "learning_rate": 4.641483516483517e-05,
      "loss": 0.6247,
      "step": 365400
    },
    {
      "epoch": 5.737833594976452,
      "grad_norm": 3.568352222442627,
      "learning_rate": 4.6413854003139714e-05,
      "loss": 0.6735,
      "step": 365500
    },
    {
      "epoch": 5.739403453689168,
      "grad_norm": 3.853691816329956,
      "learning_rate": 4.641287284144427e-05,
      "loss": 0.6382,
      "step": 365600
    },
    {
      "epoch": 5.740973312401884,
      "grad_norm": 3.9484522342681885,
      "learning_rate": 4.641189167974882e-05,
      "loss": 0.6733,
      "step": 365700
    },
    {
      "epoch": 5.742543171114599,
      "grad_norm": 3.989123821258545,
      "learning_rate": 4.641091051805338e-05,
      "loss": 0.6375,
      "step": 365800
    },
    {
      "epoch": 5.744113029827315,
      "grad_norm": 4.888178825378418,
      "learning_rate": 4.640992935635793e-05,
      "loss": 0.6536,
      "step": 365900
    },
    {
      "epoch": 5.745682888540031,
      "grad_norm": 4.254112720489502,
      "learning_rate": 4.640894819466248e-05,
      "loss": 0.6479,
      "step": 366000
    },
    {
      "epoch": 5.747252747252747,
      "grad_norm": 2.6566765308380127,
      "learning_rate": 4.6407967032967034e-05,
      "loss": 0.6578,
      "step": 366100
    },
    {
      "epoch": 5.748822605965463,
      "grad_norm": 4.491400718688965,
      "learning_rate": 4.6406985871271585e-05,
      "loss": 0.704,
      "step": 366200
    },
    {
      "epoch": 5.750392464678179,
      "grad_norm": 4.131196022033691,
      "learning_rate": 4.640600470957614e-05,
      "loss": 0.6226,
      "step": 366300
    },
    {
      "epoch": 5.751962323390895,
      "grad_norm": 4.100867748260498,
      "learning_rate": 4.6405023547880694e-05,
      "loss": 0.685,
      "step": 366400
    },
    {
      "epoch": 5.75353218210361,
      "grad_norm": 3.828582286834717,
      "learning_rate": 4.640404238618525e-05,
      "loss": 0.6379,
      "step": 366500
    },
    {
      "epoch": 5.755102040816326,
      "grad_norm": 4.057051181793213,
      "learning_rate": 4.6403061224489796e-05,
      "loss": 0.645,
      "step": 366600
    },
    {
      "epoch": 5.756671899529042,
      "grad_norm": 3.4375345706939697,
      "learning_rate": 4.640208006279435e-05,
      "loss": 0.6627,
      "step": 366700
    },
    {
      "epoch": 5.758241758241758,
      "grad_norm": 4.193061351776123,
      "learning_rate": 4.6401098901098904e-05,
      "loss": 0.7035,
      "step": 366800
    },
    {
      "epoch": 5.759811616954474,
      "grad_norm": 3.2510769367218018,
      "learning_rate": 4.6400117739403455e-05,
      "loss": 0.6827,
      "step": 366900
    },
    {
      "epoch": 5.76138147566719,
      "grad_norm": 3.318105697631836,
      "learning_rate": 4.6399136577708006e-05,
      "loss": 0.6731,
      "step": 367000
    },
    {
      "epoch": 5.762951334379906,
      "grad_norm": 4.810370922088623,
      "learning_rate": 4.6398155416012564e-05,
      "loss": 0.6216,
      "step": 367100
    },
    {
      "epoch": 5.764521193092621,
      "grad_norm": 4.325926303863525,
      "learning_rate": 4.6397174254317115e-05,
      "loss": 0.6855,
      "step": 367200
    },
    {
      "epoch": 5.766091051805337,
      "grad_norm": 4.205986022949219,
      "learning_rate": 4.6396193092621666e-05,
      "loss": 0.6793,
      "step": 367300
    },
    {
      "epoch": 5.767660910518053,
      "grad_norm": 3.0998828411102295,
      "learning_rate": 4.639521193092622e-05,
      "loss": 0.6819,
      "step": 367400
    },
    {
      "epoch": 5.769230769230769,
      "grad_norm": 3.8857743740081787,
      "learning_rate": 4.6394230769230775e-05,
      "loss": 0.6684,
      "step": 367500
    },
    {
      "epoch": 5.770800627943485,
      "grad_norm": 3.0873019695281982,
      "learning_rate": 4.639324960753532e-05,
      "loss": 0.6463,
      "step": 367600
    },
    {
      "epoch": 5.772370486656201,
      "grad_norm": 4.441108226776123,
      "learning_rate": 4.639226844583988e-05,
      "loss": 0.6501,
      "step": 367700
    },
    {
      "epoch": 5.773940345368917,
      "grad_norm": 4.037863731384277,
      "learning_rate": 4.639128728414443e-05,
      "loss": 0.6469,
      "step": 367800
    },
    {
      "epoch": 5.775510204081632,
      "grad_norm": 4.091755390167236,
      "learning_rate": 4.6390306122448985e-05,
      "loss": 0.6929,
      "step": 367900
    },
    {
      "epoch": 5.777080062794348,
      "grad_norm": 4.314333438873291,
      "learning_rate": 4.638932496075353e-05,
      "loss": 0.6521,
      "step": 368000
    },
    {
      "epoch": 5.778649921507064,
      "grad_norm": 3.9465959072113037,
      "learning_rate": 4.638834379905809e-05,
      "loss": 0.6553,
      "step": 368100
    },
    {
      "epoch": 5.78021978021978,
      "grad_norm": 3.6041767597198486,
      "learning_rate": 4.638736263736264e-05,
      "loss": 0.6933,
      "step": 368200
    },
    {
      "epoch": 5.781789638932496,
      "grad_norm": 3.943971872329712,
      "learning_rate": 4.638638147566719e-05,
      "loss": 0.6513,
      "step": 368300
    },
    {
      "epoch": 5.783359497645212,
      "grad_norm": 3.2259621620178223,
      "learning_rate": 4.638540031397175e-05,
      "loss": 0.6379,
      "step": 368400
    },
    {
      "epoch": 5.784929356357928,
      "grad_norm": 4.530580520629883,
      "learning_rate": 4.63844191522763e-05,
      "loss": 0.6669,
      "step": 368500
    },
    {
      "epoch": 5.786499215070644,
      "grad_norm": 4.051909446716309,
      "learning_rate": 4.6383437990580856e-05,
      "loss": 0.6904,
      "step": 368600
    },
    {
      "epoch": 5.78806907378336,
      "grad_norm": 4.44315767288208,
      "learning_rate": 4.63824568288854e-05,
      "loss": 0.6637,
      "step": 368700
    },
    {
      "epoch": 5.789638932496075,
      "grad_norm": 3.9387099742889404,
      "learning_rate": 4.638147566718996e-05,
      "loss": 0.6721,
      "step": 368800
    },
    {
      "epoch": 5.791208791208791,
      "grad_norm": 3.411068916320801,
      "learning_rate": 4.638049450549451e-05,
      "loss": 0.7132,
      "step": 368900
    },
    {
      "epoch": 5.792778649921507,
      "grad_norm": 3.596254825592041,
      "learning_rate": 4.637951334379906e-05,
      "loss": 0.652,
      "step": 369000
    },
    {
      "epoch": 5.794348508634223,
      "grad_norm": 3.533430337905884,
      "learning_rate": 4.637853218210361e-05,
      "loss": 0.681,
      "step": 369100
    },
    {
      "epoch": 5.795918367346939,
      "grad_norm": 4.097634315490723,
      "learning_rate": 4.637755102040817e-05,
      "loss": 0.6691,
      "step": 369200
    },
    {
      "epoch": 5.797488226059655,
      "grad_norm": 3.677363395690918,
      "learning_rate": 4.637656985871272e-05,
      "loss": 0.6403,
      "step": 369300
    },
    {
      "epoch": 5.799058084772371,
      "grad_norm": 4.296504497528076,
      "learning_rate": 4.637558869701727e-05,
      "loss": 0.673,
      "step": 369400
    },
    {
      "epoch": 5.800627943485086,
      "grad_norm": 3.5437674522399902,
      "learning_rate": 4.637460753532182e-05,
      "loss": 0.677,
      "step": 369500
    },
    {
      "epoch": 5.802197802197802,
      "grad_norm": 3.2903525829315186,
      "learning_rate": 4.637362637362638e-05,
      "loss": 0.6664,
      "step": 369600
    },
    {
      "epoch": 5.803767660910518,
      "grad_norm": 2.6349081993103027,
      "learning_rate": 4.637264521193092e-05,
      "loss": 0.6235,
      "step": 369700
    },
    {
      "epoch": 5.805337519623234,
      "grad_norm": 3.9885849952697754,
      "learning_rate": 4.637166405023548e-05,
      "loss": 0.6608,
      "step": 369800
    },
    {
      "epoch": 5.80690737833595,
      "grad_norm": 4.024028778076172,
      "learning_rate": 4.637068288854003e-05,
      "loss": 0.6941,
      "step": 369900
    },
    {
      "epoch": 5.808477237048666,
      "grad_norm": 3.8917651176452637,
      "learning_rate": 4.636970172684459e-05,
      "loss": 0.6355,
      "step": 370000
    },
    {
      "epoch": 5.810047095761382,
      "grad_norm": 4.049811840057373,
      "learning_rate": 4.6368720565149134e-05,
      "loss": 0.674,
      "step": 370100
    },
    {
      "epoch": 5.811616954474097,
      "grad_norm": 3.4535574913024902,
      "learning_rate": 4.636773940345369e-05,
      "loss": 0.6616,
      "step": 370200
    },
    {
      "epoch": 5.813186813186813,
      "grad_norm": 3.867518424987793,
      "learning_rate": 4.636675824175824e-05,
      "loss": 0.6481,
      "step": 370300
    },
    {
      "epoch": 5.814756671899529,
      "grad_norm": 4.313273906707764,
      "learning_rate": 4.6365777080062794e-05,
      "loss": 0.676,
      "step": 370400
    },
    {
      "epoch": 5.816326530612245,
      "grad_norm": 5.203489780426025,
      "learning_rate": 4.636479591836735e-05,
      "loss": 0.6555,
      "step": 370500
    },
    {
      "epoch": 5.817896389324961,
      "grad_norm": 4.493654251098633,
      "learning_rate": 4.63638147566719e-05,
      "loss": 0.6463,
      "step": 370600
    },
    {
      "epoch": 5.819466248037677,
      "grad_norm": 3.4300990104675293,
      "learning_rate": 4.636283359497646e-05,
      "loss": 0.6421,
      "step": 370700
    },
    {
      "epoch": 5.821036106750393,
      "grad_norm": 4.633777141571045,
      "learning_rate": 4.6361852433281004e-05,
      "loss": 0.6549,
      "step": 370800
    },
    {
      "epoch": 5.822605965463108,
      "grad_norm": 4.6293487548828125,
      "learning_rate": 4.636087127158556e-05,
      "loss": 0.6647,
      "step": 370900
    },
    {
      "epoch": 5.824175824175824,
      "grad_norm": 3.6131374835968018,
      "learning_rate": 4.635989010989011e-05,
      "loss": 0.725,
      "step": 371000
    },
    {
      "epoch": 5.82574568288854,
      "grad_norm": 4.225136756896973,
      "learning_rate": 4.6358908948194664e-05,
      "loss": 0.6586,
      "step": 371100
    },
    {
      "epoch": 5.827315541601256,
      "grad_norm": 4.235537528991699,
      "learning_rate": 4.6357927786499215e-05,
      "loss": 0.6146,
      "step": 371200
    },
    {
      "epoch": 5.828885400313972,
      "grad_norm": 4.61490535736084,
      "learning_rate": 4.635694662480377e-05,
      "loss": 0.6764,
      "step": 371300
    },
    {
      "epoch": 5.830455259026688,
      "grad_norm": 2.3045108318328857,
      "learning_rate": 4.6355965463108324e-05,
      "loss": 0.6115,
      "step": 371400
    },
    {
      "epoch": 5.832025117739404,
      "grad_norm": 3.1196324825286865,
      "learning_rate": 4.6354984301412875e-05,
      "loss": 0.654,
      "step": 371500
    },
    {
      "epoch": 5.833594976452119,
      "grad_norm": 3.4111692905426025,
      "learning_rate": 4.6354003139717426e-05,
      "loss": 0.7101,
      "step": 371600
    },
    {
      "epoch": 5.835164835164835,
      "grad_norm": 4.449904918670654,
      "learning_rate": 4.6353021978021984e-05,
      "loss": 0.6485,
      "step": 371700
    },
    {
      "epoch": 5.836734693877551,
      "grad_norm": 3.559354543685913,
      "learning_rate": 4.635204081632653e-05,
      "loss": 0.6352,
      "step": 371800
    },
    {
      "epoch": 5.838304552590267,
      "grad_norm": 2.7259361743927,
      "learning_rate": 4.6351059654631086e-05,
      "loss": 0.6621,
      "step": 371900
    },
    {
      "epoch": 5.839874411302983,
      "grad_norm": 3.6533596515655518,
      "learning_rate": 4.6350078492935637e-05,
      "loss": 0.6925,
      "step": 372000
    },
    {
      "epoch": 5.841444270015699,
      "grad_norm": 3.4568326473236084,
      "learning_rate": 4.6349097331240194e-05,
      "loss": 0.6424,
      "step": 372100
    },
    {
      "epoch": 5.843014128728415,
      "grad_norm": 4.288630485534668,
      "learning_rate": 4.634811616954474e-05,
      "loss": 0.6932,
      "step": 372200
    },
    {
      "epoch": 5.84458398744113,
      "grad_norm": 2.3082873821258545,
      "learning_rate": 4.6347135007849296e-05,
      "loss": 0.6402,
      "step": 372300
    },
    {
      "epoch": 5.846153846153846,
      "grad_norm": 4.566142559051514,
      "learning_rate": 4.634615384615385e-05,
      "loss": 0.6668,
      "step": 372400
    },
    {
      "epoch": 5.847723704866562,
      "grad_norm": 3.401327133178711,
      "learning_rate": 4.63451726844584e-05,
      "loss": 0.6951,
      "step": 372500
    },
    {
      "epoch": 5.849293563579278,
      "grad_norm": 3.724867343902588,
      "learning_rate": 4.6344191522762956e-05,
      "loss": 0.6597,
      "step": 372600
    },
    {
      "epoch": 5.850863422291994,
      "grad_norm": 4.210358619689941,
      "learning_rate": 4.634321036106751e-05,
      "loss": 0.6708,
      "step": 372700
    },
    {
      "epoch": 5.85243328100471,
      "grad_norm": 4.9038825035095215,
      "learning_rate": 4.6342229199372065e-05,
      "loss": 0.6633,
      "step": 372800
    },
    {
      "epoch": 5.854003139717426,
      "grad_norm": 4.345468044281006,
      "learning_rate": 4.634124803767661e-05,
      "loss": 0.6999,
      "step": 372900
    },
    {
      "epoch": 5.855572998430142,
      "grad_norm": 4.249322891235352,
      "learning_rate": 4.634026687598117e-05,
      "loss": 0.7294,
      "step": 373000
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 4.5452799797058105,
      "learning_rate": 4.633928571428572e-05,
      "loss": 0.6268,
      "step": 373100
    },
    {
      "epoch": 5.858712715855573,
      "grad_norm": 4.246339797973633,
      "learning_rate": 4.633830455259027e-05,
      "loss": 0.6649,
      "step": 373200
    },
    {
      "epoch": 5.860282574568289,
      "grad_norm": 3.556497812271118,
      "learning_rate": 4.633732339089482e-05,
      "loss": 0.6986,
      "step": 373300
    },
    {
      "epoch": 5.861852433281005,
      "grad_norm": 4.259708404541016,
      "learning_rate": 4.633634222919938e-05,
      "loss": 0.679,
      "step": 373400
    },
    {
      "epoch": 5.863422291993721,
      "grad_norm": 4.747313499450684,
      "learning_rate": 4.633536106750393e-05,
      "loss": 0.6472,
      "step": 373500
    },
    {
      "epoch": 5.864992150706437,
      "grad_norm": 5.503922939300537,
      "learning_rate": 4.633437990580848e-05,
      "loss": 0.6773,
      "step": 373600
    },
    {
      "epoch": 5.866562009419153,
      "grad_norm": 4.0377631187438965,
      "learning_rate": 4.633339874411303e-05,
      "loss": 0.6316,
      "step": 373700
    },
    {
      "epoch": 5.868131868131869,
      "grad_norm": 3.6532769203186035,
      "learning_rate": 4.633241758241759e-05,
      "loss": 0.6041,
      "step": 373800
    },
    {
      "epoch": 5.869701726844584,
      "grad_norm": 4.702557563781738,
      "learning_rate": 4.633143642072213e-05,
      "loss": 0.6659,
      "step": 373900
    },
    {
      "epoch": 5.8712715855573,
      "grad_norm": 3.841949224472046,
      "learning_rate": 4.633045525902669e-05,
      "loss": 0.6888,
      "step": 374000
    },
    {
      "epoch": 5.872841444270016,
      "grad_norm": 3.5866479873657227,
      "learning_rate": 4.632947409733124e-05,
      "loss": 0.6785,
      "step": 374100
    },
    {
      "epoch": 5.874411302982732,
      "grad_norm": 5.221521377563477,
      "learning_rate": 4.63284929356358e-05,
      "loss": 0.6472,
      "step": 374200
    },
    {
      "epoch": 5.875981161695448,
      "grad_norm": 5.299700736999512,
      "learning_rate": 4.632751177394034e-05,
      "loss": 0.6695,
      "step": 374300
    },
    {
      "epoch": 5.877551020408164,
      "grad_norm": 3.1743874549865723,
      "learning_rate": 4.63265306122449e-05,
      "loss": 0.6609,
      "step": 374400
    },
    {
      "epoch": 5.8791208791208796,
      "grad_norm": 3.6879920959472656,
      "learning_rate": 4.632554945054945e-05,
      "loss": 0.6736,
      "step": 374500
    },
    {
      "epoch": 5.880690737833595,
      "grad_norm": 3.464432954788208,
      "learning_rate": 4.6324568288854e-05,
      "loss": 0.6872,
      "step": 374600
    },
    {
      "epoch": 5.882260596546311,
      "grad_norm": 4.3694634437561035,
      "learning_rate": 4.632358712715856e-05,
      "loss": 0.6414,
      "step": 374700
    },
    {
      "epoch": 5.883830455259027,
      "grad_norm": 3.963237762451172,
      "learning_rate": 4.632260596546311e-05,
      "loss": 0.6814,
      "step": 374800
    },
    {
      "epoch": 5.885400313971743,
      "grad_norm": 7.250030517578125,
      "learning_rate": 4.632162480376767e-05,
      "loss": 0.6523,
      "step": 374900
    },
    {
      "epoch": 5.8869701726844585,
      "grad_norm": 3.488459587097168,
      "learning_rate": 4.6320643642072213e-05,
      "loss": 0.6865,
      "step": 375000
    },
    {
      "epoch": 5.8885400313971745,
      "grad_norm": 4.693628311157227,
      "learning_rate": 4.631966248037677e-05,
      "loss": 0.6997,
      "step": 375100
    },
    {
      "epoch": 5.8901098901098905,
      "grad_norm": 3.9027915000915527,
      "learning_rate": 4.631868131868132e-05,
      "loss": 0.6968,
      "step": 375200
    },
    {
      "epoch": 5.891679748822606,
      "grad_norm": 3.4118261337280273,
      "learning_rate": 4.631770015698587e-05,
      "loss": 0.6633,
      "step": 375300
    },
    {
      "epoch": 5.893249607535322,
      "grad_norm": 3.0108890533447266,
      "learning_rate": 4.6316718995290424e-05,
      "loss": 0.6464,
      "step": 375400
    },
    {
      "epoch": 5.8948194662480375,
      "grad_norm": 3.9634969234466553,
      "learning_rate": 4.631573783359498e-05,
      "loss": 0.6844,
      "step": 375500
    },
    {
      "epoch": 5.8963893249607535,
      "grad_norm": 3.6455769538879395,
      "learning_rate": 4.6314756671899526e-05,
      "loss": 0.6778,
      "step": 375600
    },
    {
      "epoch": 5.8979591836734695,
      "grad_norm": 4.754942893981934,
      "learning_rate": 4.6313775510204084e-05,
      "loss": 0.6599,
      "step": 375700
    },
    {
      "epoch": 5.8995290423861855,
      "grad_norm": 3.285341501235962,
      "learning_rate": 4.6312794348508635e-05,
      "loss": 0.6797,
      "step": 375800
    },
    {
      "epoch": 5.9010989010989015,
      "grad_norm": 4.112185001373291,
      "learning_rate": 4.631181318681319e-05,
      "loss": 0.6504,
      "step": 375900
    },
    {
      "epoch": 5.9026687598116165,
      "grad_norm": 4.051919937133789,
      "learning_rate": 4.631083202511774e-05,
      "loss": 0.6965,
      "step": 376000
    },
    {
      "epoch": 5.9042386185243325,
      "grad_norm": 4.371918678283691,
      "learning_rate": 4.6309850863422295e-05,
      "loss": 0.7046,
      "step": 376100
    },
    {
      "epoch": 5.9058084772370485,
      "grad_norm": 4.057939052581787,
      "learning_rate": 4.6308869701726846e-05,
      "loss": 0.6658,
      "step": 376200
    },
    {
      "epoch": 5.9073783359497645,
      "grad_norm": 4.234270095825195,
      "learning_rate": 4.6307888540031397e-05,
      "loss": 0.638,
      "step": 376300
    },
    {
      "epoch": 5.9089481946624804,
      "grad_norm": 4.362351417541504,
      "learning_rate": 4.630690737833595e-05,
      "loss": 0.6661,
      "step": 376400
    },
    {
      "epoch": 5.910518053375196,
      "grad_norm": 4.309915065765381,
      "learning_rate": 4.6305926216640505e-05,
      "loss": 0.7178,
      "step": 376500
    },
    {
      "epoch": 5.912087912087912,
      "grad_norm": 4.0230231285095215,
      "learning_rate": 4.6304945054945056e-05,
      "loss": 0.6338,
      "step": 376600
    },
    {
      "epoch": 5.9136577708006275,
      "grad_norm": 2.9753143787384033,
      "learning_rate": 4.630396389324961e-05,
      "loss": 0.6735,
      "step": 376700
    },
    {
      "epoch": 5.9152276295133435,
      "grad_norm": 3.457864761352539,
      "learning_rate": 4.6302982731554165e-05,
      "loss": 0.6402,
      "step": 376800
    },
    {
      "epoch": 5.916797488226059,
      "grad_norm": 3.2931642532348633,
      "learning_rate": 4.6302001569858716e-05,
      "loss": 0.6532,
      "step": 376900
    },
    {
      "epoch": 5.918367346938775,
      "grad_norm": 4.739506244659424,
      "learning_rate": 4.630102040816327e-05,
      "loss": 0.6463,
      "step": 377000
    },
    {
      "epoch": 5.919937205651491,
      "grad_norm": 2.9534997940063477,
      "learning_rate": 4.630003924646782e-05,
      "loss": 0.6243,
      "step": 377100
    },
    {
      "epoch": 5.921507064364207,
      "grad_norm": 4.144380569458008,
      "learning_rate": 4.6299058084772376e-05,
      "loss": 0.6368,
      "step": 377200
    },
    {
      "epoch": 5.923076923076923,
      "grad_norm": 3.6312615871429443,
      "learning_rate": 4.629807692307693e-05,
      "loss": 0.662,
      "step": 377300
    },
    {
      "epoch": 5.924646781789638,
      "grad_norm": 4.390826225280762,
      "learning_rate": 4.629709576138148e-05,
      "loss": 0.6609,
      "step": 377400
    },
    {
      "epoch": 5.926216640502354,
      "grad_norm": 4.177272796630859,
      "learning_rate": 4.629611459968603e-05,
      "loss": 0.6611,
      "step": 377500
    },
    {
      "epoch": 5.92778649921507,
      "grad_norm": 3.6739606857299805,
      "learning_rate": 4.6295133437990586e-05,
      "loss": 0.664,
      "step": 377600
    },
    {
      "epoch": 5.929356357927786,
      "grad_norm": 3.4477550983428955,
      "learning_rate": 4.629415227629513e-05,
      "loss": 0.6734,
      "step": 377700
    },
    {
      "epoch": 5.930926216640502,
      "grad_norm": 4.571457386016846,
      "learning_rate": 4.629317111459969e-05,
      "loss": 0.6347,
      "step": 377800
    },
    {
      "epoch": 5.932496075353218,
      "grad_norm": 3.491025924682617,
      "learning_rate": 4.629218995290424e-05,
      "loss": 0.6453,
      "step": 377900
    },
    {
      "epoch": 5.934065934065934,
      "grad_norm": 4.31247615814209,
      "learning_rate": 4.62912087912088e-05,
      "loss": 0.6275,
      "step": 378000
    },
    {
      "epoch": 5.93563579277865,
      "grad_norm": 4.095800876617432,
      "learning_rate": 4.629022762951334e-05,
      "loss": 0.6912,
      "step": 378100
    },
    {
      "epoch": 5.937205651491366,
      "grad_norm": 4.288998126983643,
      "learning_rate": 4.62892464678179e-05,
      "loss": 0.6299,
      "step": 378200
    },
    {
      "epoch": 5.938775510204081,
      "grad_norm": 3.5911662578582764,
      "learning_rate": 4.628826530612245e-05,
      "loss": 0.6407,
      "step": 378300
    },
    {
      "epoch": 5.940345368916797,
      "grad_norm": 4.331345558166504,
      "learning_rate": 4.6287284144427e-05,
      "loss": 0.6788,
      "step": 378400
    },
    {
      "epoch": 5.941915227629513,
      "grad_norm": 3.382161855697632,
      "learning_rate": 4.628630298273155e-05,
      "loss": 0.6101,
      "step": 378500
    },
    {
      "epoch": 5.943485086342229,
      "grad_norm": 4.6485819816589355,
      "learning_rate": 4.628532182103611e-05,
      "loss": 0.6537,
      "step": 378600
    },
    {
      "epoch": 5.945054945054945,
      "grad_norm": 2.74769926071167,
      "learning_rate": 4.628434065934066e-05,
      "loss": 0.6649,
      "step": 378700
    },
    {
      "epoch": 5.946624803767661,
      "grad_norm": 4.445611476898193,
      "learning_rate": 4.628335949764521e-05,
      "loss": 0.6511,
      "step": 378800
    },
    {
      "epoch": 5.948194662480377,
      "grad_norm": 3.267566442489624,
      "learning_rate": 4.628237833594977e-05,
      "loss": 0.6408,
      "step": 378900
    },
    {
      "epoch": 5.949764521193092,
      "grad_norm": 3.5087602138519287,
      "learning_rate": 4.628139717425432e-05,
      "loss": 0.6459,
      "step": 379000
    },
    {
      "epoch": 5.951334379905808,
      "grad_norm": 3.086792230606079,
      "learning_rate": 4.628041601255887e-05,
      "loss": 0.6743,
      "step": 379100
    },
    {
      "epoch": 5.952904238618524,
      "grad_norm": 3.8401358127593994,
      "learning_rate": 4.627943485086342e-05,
      "loss": 0.7153,
      "step": 379200
    },
    {
      "epoch": 5.95447409733124,
      "grad_norm": 5.0729780197143555,
      "learning_rate": 4.627845368916798e-05,
      "loss": 0.6797,
      "step": 379300
    },
    {
      "epoch": 5.956043956043956,
      "grad_norm": 3.624539613723755,
      "learning_rate": 4.627747252747253e-05,
      "loss": 0.7207,
      "step": 379400
    },
    {
      "epoch": 5.957613814756672,
      "grad_norm": 3.570833683013916,
      "learning_rate": 4.627649136577708e-05,
      "loss": 0.6625,
      "step": 379500
    },
    {
      "epoch": 5.959183673469388,
      "grad_norm": 3.964197874069214,
      "learning_rate": 4.627551020408163e-05,
      "loss": 0.6814,
      "step": 379600
    },
    {
      "epoch": 5.960753532182103,
      "grad_norm": 4.244136810302734,
      "learning_rate": 4.627452904238619e-05,
      "loss": 0.6816,
      "step": 379700
    },
    {
      "epoch": 5.962323390894819,
      "grad_norm": 4.867712497711182,
      "learning_rate": 4.6273547880690735e-05,
      "loss": 0.6705,
      "step": 379800
    },
    {
      "epoch": 5.963893249607535,
      "grad_norm": 3.880558729171753,
      "learning_rate": 4.627256671899529e-05,
      "loss": 0.6625,
      "step": 379900
    },
    {
      "epoch": 5.965463108320251,
      "grad_norm": 4.600466728210449,
      "learning_rate": 4.6271585557299844e-05,
      "loss": 0.6897,
      "step": 380000
    },
    {
      "epoch": 5.967032967032967,
      "grad_norm": 2.9872663021087646,
      "learning_rate": 4.62706043956044e-05,
      "loss": 0.6747,
      "step": 380100
    },
    {
      "epoch": 5.968602825745683,
      "grad_norm": 4.3771071434021,
      "learning_rate": 4.6269623233908946e-05,
      "loss": 0.6765,
      "step": 380200
    },
    {
      "epoch": 5.970172684458399,
      "grad_norm": 4.434260845184326,
      "learning_rate": 4.6268642072213504e-05,
      "loss": 0.6424,
      "step": 380300
    },
    {
      "epoch": 5.971742543171114,
      "grad_norm": 3.605756998062134,
      "learning_rate": 4.6267660910518055e-05,
      "loss": 0.6348,
      "step": 380400
    },
    {
      "epoch": 5.97331240188383,
      "grad_norm": 3.3112335205078125,
      "learning_rate": 4.6266679748822606e-05,
      "loss": 0.6744,
      "step": 380500
    },
    {
      "epoch": 5.974882260596546,
      "grad_norm": 4.3072638511657715,
      "learning_rate": 4.6265698587127157e-05,
      "loss": 0.6866,
      "step": 380600
    },
    {
      "epoch": 5.976452119309262,
      "grad_norm": 3.880683422088623,
      "learning_rate": 4.6264717425431714e-05,
      "loss": 0.6473,
      "step": 380700
    },
    {
      "epoch": 5.978021978021978,
      "grad_norm": 4.781772136688232,
      "learning_rate": 4.6263736263736265e-05,
      "loss": 0.6265,
      "step": 380800
    },
    {
      "epoch": 5.979591836734694,
      "grad_norm": 3.387913227081299,
      "learning_rate": 4.6262755102040816e-05,
      "loss": 0.6652,
      "step": 380900
    },
    {
      "epoch": 5.98116169544741,
      "grad_norm": 4.042553901672363,
      "learning_rate": 4.6261773940345374e-05,
      "loss": 0.655,
      "step": 381000
    },
    {
      "epoch": 5.982731554160125,
      "grad_norm": 3.7834274768829346,
      "learning_rate": 4.6260792778649925e-05,
      "loss": 0.6849,
      "step": 381100
    },
    {
      "epoch": 5.984301412872841,
      "grad_norm": 3.7385804653167725,
      "learning_rate": 4.6259811616954476e-05,
      "loss": 0.6867,
      "step": 381200
    },
    {
      "epoch": 5.985871271585557,
      "grad_norm": 4.341507911682129,
      "learning_rate": 4.625883045525903e-05,
      "loss": 0.6678,
      "step": 381300
    },
    {
      "epoch": 5.987441130298273,
      "grad_norm": 3.9179437160491943,
      "learning_rate": 4.6257849293563585e-05,
      "loss": 0.6636,
      "step": 381400
    },
    {
      "epoch": 5.989010989010989,
      "grad_norm": 2.9884016513824463,
      "learning_rate": 4.6256868131868136e-05,
      "loss": 0.6666,
      "step": 381500
    },
    {
      "epoch": 5.990580847723705,
      "grad_norm": 3.2087485790252686,
      "learning_rate": 4.625588697017269e-05,
      "loss": 0.6869,
      "step": 381600
    },
    {
      "epoch": 5.992150706436421,
      "grad_norm": 3.5302371978759766,
      "learning_rate": 4.625490580847724e-05,
      "loss": 0.6101,
      "step": 381700
    },
    {
      "epoch": 5.993720565149136,
      "grad_norm": 4.310197830200195,
      "learning_rate": 4.6253924646781795e-05,
      "loss": 0.6878,
      "step": 381800
    },
    {
      "epoch": 5.995290423861852,
      "grad_norm": 4.031547546386719,
      "learning_rate": 4.625294348508634e-05,
      "loss": 0.6889,
      "step": 381900
    },
    {
      "epoch": 5.996860282574568,
      "grad_norm": 3.9591472148895264,
      "learning_rate": 4.62519623233909e-05,
      "loss": 0.6761,
      "step": 382000
    },
    {
      "epoch": 5.998430141287284,
      "grad_norm": 3.003173589706421,
      "learning_rate": 4.625098116169545e-05,
      "loss": 0.637,
      "step": 382100
    },
    {
      "epoch": 6.0,
      "grad_norm": 3.7646963596343994,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.6962,
      "step": 382200
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.017717957496643,
      "eval_runtime": 14.9552,
      "eval_samples_per_second": 224.203,
      "eval_steps_per_second": 224.203,
      "step": 382200
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5163915157318115,
      "eval_runtime": 265.9211,
      "eval_samples_per_second": 239.545,
      "eval_steps_per_second": 239.545,
      "step": 382200
    },
    {
      "epoch": 6.001569858712716,
      "grad_norm": 3.7179269790649414,
      "learning_rate": 4.624901883830455e-05,
      "loss": 0.6704,
      "step": 382300
    },
    {
      "epoch": 6.003139717425432,
      "grad_norm": 4.952663898468018,
      "learning_rate": 4.624803767660911e-05,
      "loss": 0.6484,
      "step": 382400
    },
    {
      "epoch": 6.004709576138148,
      "grad_norm": 3.4903290271759033,
      "learning_rate": 4.624705651491366e-05,
      "loss": 0.6508,
      "step": 382500
    },
    {
      "epoch": 6.006279434850863,
      "grad_norm": 4.754391193389893,
      "learning_rate": 4.624607535321821e-05,
      "loss": 0.6746,
      "step": 382600
    },
    {
      "epoch": 6.007849293563579,
      "grad_norm": 3.389535665512085,
      "learning_rate": 4.624509419152276e-05,
      "loss": 0.6531,
      "step": 382700
    },
    {
      "epoch": 6.009419152276295,
      "grad_norm": 3.973822593688965,
      "learning_rate": 4.624411302982732e-05,
      "loss": 0.7116,
      "step": 382800
    },
    {
      "epoch": 6.010989010989011,
      "grad_norm": 4.427331924438477,
      "learning_rate": 4.624313186813187e-05,
      "loss": 0.6445,
      "step": 382900
    },
    {
      "epoch": 6.012558869701727,
      "grad_norm": 2.409346103668213,
      "learning_rate": 4.624215070643642e-05,
      "loss": 0.6546,
      "step": 383000
    },
    {
      "epoch": 6.014128728414443,
      "grad_norm": 3.4141550064086914,
      "learning_rate": 4.624116954474098e-05,
      "loss": 0.6469,
      "step": 383100
    },
    {
      "epoch": 6.015698587127159,
      "grad_norm": 4.0068464279174805,
      "learning_rate": 4.624018838304553e-05,
      "loss": 0.6317,
      "step": 383200
    },
    {
      "epoch": 6.017268445839874,
      "grad_norm": 3.163846492767334,
      "learning_rate": 4.623920722135008e-05,
      "loss": 0.6315,
      "step": 383300
    },
    {
      "epoch": 6.01883830455259,
      "grad_norm": 4.305241107940674,
      "learning_rate": 4.623822605965463e-05,
      "loss": 0.613,
      "step": 383400
    },
    {
      "epoch": 6.020408163265306,
      "grad_norm": 4.453533172607422,
      "learning_rate": 4.623724489795919e-05,
      "loss": 0.6509,
      "step": 383500
    },
    {
      "epoch": 6.021978021978022,
      "grad_norm": 3.1226980686187744,
      "learning_rate": 4.623626373626374e-05,
      "loss": 0.6475,
      "step": 383600
    },
    {
      "epoch": 6.023547880690738,
      "grad_norm": 4.919028282165527,
      "learning_rate": 4.623528257456829e-05,
      "loss": 0.6861,
      "step": 383700
    },
    {
      "epoch": 6.025117739403454,
      "grad_norm": 3.7719614505767822,
      "learning_rate": 4.623430141287284e-05,
      "loss": 0.6912,
      "step": 383800
    },
    {
      "epoch": 6.02668759811617,
      "grad_norm": 4.5295538902282715,
      "learning_rate": 4.62333202511774e-05,
      "loss": 0.6161,
      "step": 383900
    },
    {
      "epoch": 6.028257456828886,
      "grad_norm": 4.925210952758789,
      "learning_rate": 4.6232339089481944e-05,
      "loss": 0.6641,
      "step": 384000
    },
    {
      "epoch": 6.029827315541601,
      "grad_norm": 4.005380153656006,
      "learning_rate": 4.62313579277865e-05,
      "loss": 0.666,
      "step": 384100
    },
    {
      "epoch": 6.031397174254317,
      "grad_norm": 4.103209495544434,
      "learning_rate": 4.623037676609105e-05,
      "loss": 0.6329,
      "step": 384200
    },
    {
      "epoch": 6.032967032967033,
      "grad_norm": 4.347714900970459,
      "learning_rate": 4.622939560439561e-05,
      "loss": 0.6418,
      "step": 384300
    },
    {
      "epoch": 6.034536891679749,
      "grad_norm": 2.7830474376678467,
      "learning_rate": 4.6228414442700155e-05,
      "loss": 0.6495,
      "step": 384400
    },
    {
      "epoch": 6.036106750392465,
      "grad_norm": 4.087255001068115,
      "learning_rate": 4.622743328100471e-05,
      "loss": 0.6409,
      "step": 384500
    },
    {
      "epoch": 6.037676609105181,
      "grad_norm": 4.205899715423584,
      "learning_rate": 4.6226452119309264e-05,
      "loss": 0.6507,
      "step": 384600
    },
    {
      "epoch": 6.039246467817897,
      "grad_norm": 4.573139667510986,
      "learning_rate": 4.6225470957613815e-05,
      "loss": 0.7173,
      "step": 384700
    },
    {
      "epoch": 6.040816326530612,
      "grad_norm": 4.48893404006958,
      "learning_rate": 4.6224489795918366e-05,
      "loss": 0.6481,
      "step": 384800
    },
    {
      "epoch": 6.042386185243328,
      "grad_norm": 4.0609235763549805,
      "learning_rate": 4.622350863422292e-05,
      "loss": 0.6465,
      "step": 384900
    },
    {
      "epoch": 6.043956043956044,
      "grad_norm": 3.5424561500549316,
      "learning_rate": 4.6222527472527474e-05,
      "loss": 0.6826,
      "step": 385000
    },
    {
      "epoch": 6.04552590266876,
      "grad_norm": 3.560377836227417,
      "learning_rate": 4.6221546310832025e-05,
      "loss": 0.6548,
      "step": 385100
    },
    {
      "epoch": 6.047095761381476,
      "grad_norm": 4.104085922241211,
      "learning_rate": 4.622056514913658e-05,
      "loss": 0.6522,
      "step": 385200
    },
    {
      "epoch": 6.048665620094192,
      "grad_norm": 3.7108168601989746,
      "learning_rate": 4.6219583987441134e-05,
      "loss": 0.6877,
      "step": 385300
    },
    {
      "epoch": 6.050235478806908,
      "grad_norm": 2.6529319286346436,
      "learning_rate": 4.6218602825745685e-05,
      "loss": 0.6926,
      "step": 385400
    },
    {
      "epoch": 6.051805337519623,
      "grad_norm": 4.005117893218994,
      "learning_rate": 4.6217621664050236e-05,
      "loss": 0.666,
      "step": 385500
    },
    {
      "epoch": 6.053375196232339,
      "grad_norm": 3.3702213764190674,
      "learning_rate": 4.6216640502354794e-05,
      "loss": 0.6606,
      "step": 385600
    },
    {
      "epoch": 6.054945054945055,
      "grad_norm": 4.54873514175415,
      "learning_rate": 4.6215659340659345e-05,
      "loss": 0.6734,
      "step": 385700
    },
    {
      "epoch": 6.056514913657771,
      "grad_norm": 1.6776834726333618,
      "learning_rate": 4.6214678178963896e-05,
      "loss": 0.6549,
      "step": 385800
    },
    {
      "epoch": 6.058084772370487,
      "grad_norm": 4.066808700561523,
      "learning_rate": 4.621369701726845e-05,
      "loss": 0.6422,
      "step": 385900
    },
    {
      "epoch": 6.059654631083203,
      "grad_norm": 3.7277724742889404,
      "learning_rate": 4.6212715855573004e-05,
      "loss": 0.6517,
      "step": 386000
    },
    {
      "epoch": 6.061224489795919,
      "grad_norm": 3.0438520908355713,
      "learning_rate": 4.621173469387755e-05,
      "loss": 0.7132,
      "step": 386100
    },
    {
      "epoch": 6.062794348508635,
      "grad_norm": 3.3025009632110596,
      "learning_rate": 4.6210753532182106e-05,
      "loss": 0.6136,
      "step": 386200
    },
    {
      "epoch": 6.06436420722135,
      "grad_norm": 3.4012274742126465,
      "learning_rate": 4.620977237048666e-05,
      "loss": 0.6402,
      "step": 386300
    },
    {
      "epoch": 6.065934065934066,
      "grad_norm": 3.5316455364227295,
      "learning_rate": 4.6208791208791215e-05,
      "loss": 0.6222,
      "step": 386400
    },
    {
      "epoch": 6.067503924646782,
      "grad_norm": 3.9949822425842285,
      "learning_rate": 4.620781004709576e-05,
      "loss": 0.6489,
      "step": 386500
    },
    {
      "epoch": 6.069073783359498,
      "grad_norm": 4.166678428649902,
      "learning_rate": 4.620682888540032e-05,
      "loss": 0.6549,
      "step": 386600
    },
    {
      "epoch": 6.070643642072214,
      "grad_norm": 4.5390400886535645,
      "learning_rate": 4.620584772370487e-05,
      "loss": 0.6525,
      "step": 386700
    },
    {
      "epoch": 6.07221350078493,
      "grad_norm": 4.795370101928711,
      "learning_rate": 4.620486656200942e-05,
      "loss": 0.6588,
      "step": 386800
    },
    {
      "epoch": 6.073783359497646,
      "grad_norm": 4.757525444030762,
      "learning_rate": 4.620388540031397e-05,
      "loss": 0.6473,
      "step": 386900
    },
    {
      "epoch": 6.075353218210361,
      "grad_norm": 4.0784711837768555,
      "learning_rate": 4.620290423861853e-05,
      "loss": 0.6852,
      "step": 387000
    },
    {
      "epoch": 6.076923076923077,
      "grad_norm": 5.13023042678833,
      "learning_rate": 4.620192307692308e-05,
      "loss": 0.6525,
      "step": 387100
    },
    {
      "epoch": 6.078492935635793,
      "grad_norm": 4.087092399597168,
      "learning_rate": 4.620094191522763e-05,
      "loss": 0.6551,
      "step": 387200
    },
    {
      "epoch": 6.080062794348509,
      "grad_norm": 4.496434211730957,
      "learning_rate": 4.619996075353219e-05,
      "loss": 0.6742,
      "step": 387300
    },
    {
      "epoch": 6.081632653061225,
      "grad_norm": 2.2731575965881348,
      "learning_rate": 4.619897959183674e-05,
      "loss": 0.6544,
      "step": 387400
    },
    {
      "epoch": 6.083202511773941,
      "grad_norm": 3.756549835205078,
      "learning_rate": 4.619799843014129e-05,
      "loss": 0.6866,
      "step": 387500
    },
    {
      "epoch": 6.0847723704866565,
      "grad_norm": 4.095126152038574,
      "learning_rate": 4.619701726844584e-05,
      "loss": 0.6567,
      "step": 387600
    },
    {
      "epoch": 6.086342229199372,
      "grad_norm": 2.427556276321411,
      "learning_rate": 4.61960361067504e-05,
      "loss": 0.6336,
      "step": 387700
    },
    {
      "epoch": 6.087912087912088,
      "grad_norm": 4.060569763183594,
      "learning_rate": 4.619505494505495e-05,
      "loss": 0.64,
      "step": 387800
    },
    {
      "epoch": 6.089481946624804,
      "grad_norm": 3.849698543548584,
      "learning_rate": 4.61940737833595e-05,
      "loss": 0.6189,
      "step": 387900
    },
    {
      "epoch": 6.0910518053375196,
      "grad_norm": 3.714087963104248,
      "learning_rate": 4.619309262166405e-05,
      "loss": 0.6116,
      "step": 388000
    },
    {
      "epoch": 6.0926216640502355,
      "grad_norm": 2.8632028102874756,
      "learning_rate": 4.619211145996861e-05,
      "loss": 0.5928,
      "step": 388100
    },
    {
      "epoch": 6.0941915227629515,
      "grad_norm": 6.0088701248168945,
      "learning_rate": 4.619113029827315e-05,
      "loss": 0.6468,
      "step": 388200
    },
    {
      "epoch": 6.0957613814756675,
      "grad_norm": 3.610157012939453,
      "learning_rate": 4.619014913657771e-05,
      "loss": 0.6247,
      "step": 388300
    },
    {
      "epoch": 6.0973312401883835,
      "grad_norm": 3.730862855911255,
      "learning_rate": 4.618916797488226e-05,
      "loss": 0.6509,
      "step": 388400
    },
    {
      "epoch": 6.0989010989010985,
      "grad_norm": 4.066860198974609,
      "learning_rate": 4.618818681318682e-05,
      "loss": 0.663,
      "step": 388500
    },
    {
      "epoch": 6.1004709576138145,
      "grad_norm": 4.113926887512207,
      "learning_rate": 4.6187205651491364e-05,
      "loss": 0.6652,
      "step": 388600
    },
    {
      "epoch": 6.1020408163265305,
      "grad_norm": 2.4166042804718018,
      "learning_rate": 4.618622448979592e-05,
      "loss": 0.6446,
      "step": 388700
    },
    {
      "epoch": 6.1036106750392465,
      "grad_norm": 4.323604583740234,
      "learning_rate": 4.618524332810047e-05,
      "loss": 0.6877,
      "step": 388800
    },
    {
      "epoch": 6.1051805337519625,
      "grad_norm": 2.657805919647217,
      "learning_rate": 4.6184262166405024e-05,
      "loss": 0.6784,
      "step": 388900
    },
    {
      "epoch": 6.106750392464678,
      "grad_norm": 4.3692946434021,
      "learning_rate": 4.6183281004709574e-05,
      "loss": 0.6676,
      "step": 389000
    },
    {
      "epoch": 6.108320251177394,
      "grad_norm": 3.04266095161438,
      "learning_rate": 4.618229984301413e-05,
      "loss": 0.6378,
      "step": 389100
    },
    {
      "epoch": 6.1098901098901095,
      "grad_norm": 4.294496536254883,
      "learning_rate": 4.618131868131868e-05,
      "loss": 0.6719,
      "step": 389200
    },
    {
      "epoch": 6.1114599686028255,
      "grad_norm": 4.471674919128418,
      "learning_rate": 4.6180337519623234e-05,
      "loss": 0.6255,
      "step": 389300
    },
    {
      "epoch": 6.1130298273155415,
      "grad_norm": 3.787050485610962,
      "learning_rate": 4.617935635792779e-05,
      "loss": 0.6699,
      "step": 389400
    },
    {
      "epoch": 6.114599686028257,
      "grad_norm": 4.69165563583374,
      "learning_rate": 4.617837519623234e-05,
      "loss": 0.6353,
      "step": 389500
    },
    {
      "epoch": 6.116169544740973,
      "grad_norm": 2.9589686393737793,
      "learning_rate": 4.6177394034536894e-05,
      "loss": 0.6922,
      "step": 389600
    },
    {
      "epoch": 6.117739403453689,
      "grad_norm": 3.57450795173645,
      "learning_rate": 4.6176412872841445e-05,
      "loss": 0.6335,
      "step": 389700
    },
    {
      "epoch": 6.119309262166405,
      "grad_norm": 4.073467254638672,
      "learning_rate": 4.6175431711146e-05,
      "loss": 0.6648,
      "step": 389800
    },
    {
      "epoch": 6.1208791208791204,
      "grad_norm": 4.166979789733887,
      "learning_rate": 4.6174450549450554e-05,
      "loss": 0.6641,
      "step": 389900
    },
    {
      "epoch": 6.122448979591836,
      "grad_norm": 3.428457498550415,
      "learning_rate": 4.6173469387755105e-05,
      "loss": 0.641,
      "step": 390000
    },
    {
      "epoch": 6.124018838304552,
      "grad_norm": 3.6632397174835205,
      "learning_rate": 4.6172488226059656e-05,
      "loss": 0.6044,
      "step": 390100
    },
    {
      "epoch": 6.125588697017268,
      "grad_norm": 3.8194515705108643,
      "learning_rate": 4.617150706436421e-05,
      "loss": 0.6363,
      "step": 390200
    },
    {
      "epoch": 6.127158555729984,
      "grad_norm": 4.063122272491455,
      "learning_rate": 4.617052590266876e-05,
      "loss": 0.6583,
      "step": 390300
    },
    {
      "epoch": 6.1287284144427,
      "grad_norm": 4.0648417472839355,
      "learning_rate": 4.6169544740973315e-05,
      "loss": 0.6295,
      "step": 390400
    },
    {
      "epoch": 6.130298273155416,
      "grad_norm": 5.902928829193115,
      "learning_rate": 4.6168563579277866e-05,
      "loss": 0.7077,
      "step": 390500
    },
    {
      "epoch": 6.131868131868132,
      "grad_norm": 4.036954402923584,
      "learning_rate": 4.6167582417582424e-05,
      "loss": 0.6371,
      "step": 390600
    },
    {
      "epoch": 6.133437990580847,
      "grad_norm": 3.850349187850952,
      "learning_rate": 4.616660125588697e-05,
      "loss": 0.6522,
      "step": 390700
    },
    {
      "epoch": 6.135007849293563,
      "grad_norm": 4.401029586791992,
      "learning_rate": 4.6165620094191526e-05,
      "loss": 0.6237,
      "step": 390800
    },
    {
      "epoch": 6.136577708006279,
      "grad_norm": 4.423183441162109,
      "learning_rate": 4.616463893249608e-05,
      "loss": 0.671,
      "step": 390900
    },
    {
      "epoch": 6.138147566718995,
      "grad_norm": 4.40330171585083,
      "learning_rate": 4.616365777080063e-05,
      "loss": 0.6764,
      "step": 391000
    },
    {
      "epoch": 6.139717425431711,
      "grad_norm": 4.7165703773498535,
      "learning_rate": 4.616267660910518e-05,
      "loss": 0.6837,
      "step": 391100
    },
    {
      "epoch": 6.141287284144427,
      "grad_norm": 5.296135425567627,
      "learning_rate": 4.616169544740974e-05,
      "loss": 0.6936,
      "step": 391200
    },
    {
      "epoch": 6.142857142857143,
      "grad_norm": 4.62662935256958,
      "learning_rate": 4.616071428571429e-05,
      "loss": 0.6571,
      "step": 391300
    },
    {
      "epoch": 6.144427001569858,
      "grad_norm": 3.8662898540496826,
      "learning_rate": 4.615973312401884e-05,
      "loss": 0.6858,
      "step": 391400
    },
    {
      "epoch": 6.145996860282574,
      "grad_norm": 4.247626781463623,
      "learning_rate": 4.6158751962323396e-05,
      "loss": 0.6676,
      "step": 391500
    },
    {
      "epoch": 6.14756671899529,
      "grad_norm": 3.6978189945220947,
      "learning_rate": 4.615777080062795e-05,
      "loss": 0.6554,
      "step": 391600
    },
    {
      "epoch": 6.149136577708006,
      "grad_norm": 4.772575855255127,
      "learning_rate": 4.61567896389325e-05,
      "loss": 0.6812,
      "step": 391700
    },
    {
      "epoch": 6.150706436420722,
      "grad_norm": 3.0643975734710693,
      "learning_rate": 4.615580847723705e-05,
      "loss": 0.7136,
      "step": 391800
    },
    {
      "epoch": 6.152276295133438,
      "grad_norm": 4.3955841064453125,
      "learning_rate": 4.615482731554161e-05,
      "loss": 0.642,
      "step": 391900
    },
    {
      "epoch": 6.153846153846154,
      "grad_norm": 3.90242075920105,
      "learning_rate": 4.615384615384616e-05,
      "loss": 0.6963,
      "step": 392000
    },
    {
      "epoch": 6.155416012558869,
      "grad_norm": 4.307951927185059,
      "learning_rate": 4.615286499215071e-05,
      "loss": 0.6872,
      "step": 392100
    },
    {
      "epoch": 6.156985871271585,
      "grad_norm": 3.2686803340911865,
      "learning_rate": 4.615188383045526e-05,
      "loss": 0.6849,
      "step": 392200
    },
    {
      "epoch": 6.158555729984301,
      "grad_norm": 3.8355867862701416,
      "learning_rate": 4.615090266875982e-05,
      "loss": 0.6716,
      "step": 392300
    },
    {
      "epoch": 6.160125588697017,
      "grad_norm": 3.5767250061035156,
      "learning_rate": 4.614992150706436e-05,
      "loss": 0.6425,
      "step": 392400
    },
    {
      "epoch": 6.161695447409733,
      "grad_norm": 2.6582272052764893,
      "learning_rate": 4.614894034536892e-05,
      "loss": 0.6572,
      "step": 392500
    },
    {
      "epoch": 6.163265306122449,
      "grad_norm": 3.590242862701416,
      "learning_rate": 4.614795918367347e-05,
      "loss": 0.6361,
      "step": 392600
    },
    {
      "epoch": 6.164835164835165,
      "grad_norm": 3.915847063064575,
      "learning_rate": 4.614697802197803e-05,
      "loss": 0.6362,
      "step": 392700
    },
    {
      "epoch": 6.166405023547881,
      "grad_norm": 4.058078765869141,
      "learning_rate": 4.614599686028257e-05,
      "loss": 0.6613,
      "step": 392800
    },
    {
      "epoch": 6.167974882260596,
      "grad_norm": 3.6854915618896484,
      "learning_rate": 4.614501569858713e-05,
      "loss": 0.6422,
      "step": 392900
    },
    {
      "epoch": 6.169544740973312,
      "grad_norm": 3.6956498622894287,
      "learning_rate": 4.614403453689168e-05,
      "loss": 0.6596,
      "step": 393000
    },
    {
      "epoch": 6.171114599686028,
      "grad_norm": 4.3857502937316895,
      "learning_rate": 4.614305337519623e-05,
      "loss": 0.658,
      "step": 393100
    },
    {
      "epoch": 6.172684458398744,
      "grad_norm": 4.115122318267822,
      "learning_rate": 4.6142072213500783e-05,
      "loss": 0.6885,
      "step": 393200
    },
    {
      "epoch": 6.17425431711146,
      "grad_norm": 4.0746941566467285,
      "learning_rate": 4.614109105180534e-05,
      "loss": 0.6648,
      "step": 393300
    },
    {
      "epoch": 6.175824175824176,
      "grad_norm": 3.49263596534729,
      "learning_rate": 4.614010989010989e-05,
      "loss": 0.6728,
      "step": 393400
    },
    {
      "epoch": 6.177394034536892,
      "grad_norm": 3.2455365657806396,
      "learning_rate": 4.613912872841444e-05,
      "loss": 0.691,
      "step": 393500
    },
    {
      "epoch": 6.178963893249607,
      "grad_norm": 5.512495040893555,
      "learning_rate": 4.6138147566719e-05,
      "loss": 0.6662,
      "step": 393600
    },
    {
      "epoch": 6.180533751962323,
      "grad_norm": 4.8145527839660645,
      "learning_rate": 4.613716640502355e-05,
      "loss": 0.6465,
      "step": 393700
    },
    {
      "epoch": 6.182103610675039,
      "grad_norm": 4.158360481262207,
      "learning_rate": 4.61361852433281e-05,
      "loss": 0.6551,
      "step": 393800
    },
    {
      "epoch": 6.183673469387755,
      "grad_norm": 2.9003264904022217,
      "learning_rate": 4.6135204081632654e-05,
      "loss": 0.6946,
      "step": 393900
    },
    {
      "epoch": 6.185243328100471,
      "grad_norm": 3.6347200870513916,
      "learning_rate": 4.613422291993721e-05,
      "loss": 0.6611,
      "step": 394000
    },
    {
      "epoch": 6.186813186813187,
      "grad_norm": 4.981205463409424,
      "learning_rate": 4.613324175824176e-05,
      "loss": 0.6386,
      "step": 394100
    },
    {
      "epoch": 6.188383045525903,
      "grad_norm": 3.830956220626831,
      "learning_rate": 4.6132260596546314e-05,
      "loss": 0.6546,
      "step": 394200
    },
    {
      "epoch": 6.189952904238618,
      "grad_norm": 5.169563293457031,
      "learning_rate": 4.6131279434850865e-05,
      "loss": 0.669,
      "step": 394300
    },
    {
      "epoch": 6.191522762951334,
      "grad_norm": 3.8800466060638428,
      "learning_rate": 4.613029827315542e-05,
      "loss": 0.6554,
      "step": 394400
    },
    {
      "epoch": 6.19309262166405,
      "grad_norm": 3.5204355716705322,
      "learning_rate": 4.6129317111459967e-05,
      "loss": 0.6408,
      "step": 394500
    },
    {
      "epoch": 6.194662480376766,
      "grad_norm": 3.747825860977173,
      "learning_rate": 4.6128335949764524e-05,
      "loss": 0.6726,
      "step": 394600
    },
    {
      "epoch": 6.196232339089482,
      "grad_norm": 3.647535562515259,
      "learning_rate": 4.6127354788069075e-05,
      "loss": 0.657,
      "step": 394700
    },
    {
      "epoch": 6.197802197802198,
      "grad_norm": 4.29422664642334,
      "learning_rate": 4.612637362637363e-05,
      "loss": 0.6955,
      "step": 394800
    },
    {
      "epoch": 6.199372056514914,
      "grad_norm": 3.175557851791382,
      "learning_rate": 4.612539246467818e-05,
      "loss": 0.6379,
      "step": 394900
    },
    {
      "epoch": 6.20094191522763,
      "grad_norm": 4.320684432983398,
      "learning_rate": 4.6124411302982735e-05,
      "loss": 0.6596,
      "step": 395000
    },
    {
      "epoch": 6.202511773940345,
      "grad_norm": 3.91825270652771,
      "learning_rate": 4.6123430141287286e-05,
      "loss": 0.6707,
      "step": 395100
    },
    {
      "epoch": 6.204081632653061,
      "grad_norm": 4.197769641876221,
      "learning_rate": 4.612244897959184e-05,
      "loss": 0.6233,
      "step": 395200
    },
    {
      "epoch": 6.205651491365777,
      "grad_norm": 3.495575428009033,
      "learning_rate": 4.612146781789639e-05,
      "loss": 0.6637,
      "step": 395300
    },
    {
      "epoch": 6.207221350078493,
      "grad_norm": 2.845810890197754,
      "learning_rate": 4.6120486656200946e-05,
      "loss": 0.5899,
      "step": 395400
    },
    {
      "epoch": 6.208791208791209,
      "grad_norm": 3.8696489334106445,
      "learning_rate": 4.61195054945055e-05,
      "loss": 0.6561,
      "step": 395500
    },
    {
      "epoch": 6.210361067503925,
      "grad_norm": 3.52046537399292,
      "learning_rate": 4.611852433281005e-05,
      "loss": 0.6675,
      "step": 395600
    },
    {
      "epoch": 6.211930926216641,
      "grad_norm": 3.7354180812835693,
      "learning_rate": 4.6117543171114605e-05,
      "loss": 0.6182,
      "step": 395700
    },
    {
      "epoch": 6.213500784929356,
      "grad_norm": 3.168269395828247,
      "learning_rate": 4.6116562009419156e-05,
      "loss": 0.6239,
      "step": 395800
    },
    {
      "epoch": 6.215070643642072,
      "grad_norm": 3.126723051071167,
      "learning_rate": 4.611558084772371e-05,
      "loss": 0.6982,
      "step": 395900
    },
    {
      "epoch": 6.216640502354788,
      "grad_norm": 4.032322883605957,
      "learning_rate": 4.611459968602826e-05,
      "loss": 0.6736,
      "step": 396000
    },
    {
      "epoch": 6.218210361067504,
      "grad_norm": 4.670474052429199,
      "learning_rate": 4.6113618524332816e-05,
      "loss": 0.6719,
      "step": 396100
    },
    {
      "epoch": 6.21978021978022,
      "grad_norm": 4.292887210845947,
      "learning_rate": 4.611263736263737e-05,
      "loss": 0.6804,
      "step": 396200
    },
    {
      "epoch": 6.221350078492936,
      "grad_norm": 4.895878791809082,
      "learning_rate": 4.611165620094192e-05,
      "loss": 0.5932,
      "step": 396300
    },
    {
      "epoch": 6.222919937205652,
      "grad_norm": 5.1397013664245605,
      "learning_rate": 4.611067503924647e-05,
      "loss": 0.6829,
      "step": 396400
    },
    {
      "epoch": 6.224489795918367,
      "grad_norm": 3.7655820846557617,
      "learning_rate": 4.610969387755103e-05,
      "loss": 0.6514,
      "step": 396500
    },
    {
      "epoch": 6.226059654631083,
      "grad_norm": 3.5593180656433105,
      "learning_rate": 4.610871271585557e-05,
      "loss": 0.704,
      "step": 396600
    },
    {
      "epoch": 6.227629513343799,
      "grad_norm": 3.44907808303833,
      "learning_rate": 4.610773155416013e-05,
      "loss": 0.6073,
      "step": 396700
    },
    {
      "epoch": 6.229199372056515,
      "grad_norm": 4.882983207702637,
      "learning_rate": 4.610675039246468e-05,
      "loss": 0.6961,
      "step": 396800
    },
    {
      "epoch": 6.230769230769231,
      "grad_norm": 3.7932333946228027,
      "learning_rate": 4.610576923076924e-05,
      "loss": 0.6407,
      "step": 396900
    },
    {
      "epoch": 6.232339089481947,
      "grad_norm": 3.4371018409729004,
      "learning_rate": 4.610478806907378e-05,
      "loss": 0.6633,
      "step": 397000
    },
    {
      "epoch": 6.233908948194663,
      "grad_norm": 4.0678815841674805,
      "learning_rate": 4.610380690737834e-05,
      "loss": 0.655,
      "step": 397100
    },
    {
      "epoch": 6.235478806907379,
      "grad_norm": 4.31052827835083,
      "learning_rate": 4.610282574568289e-05,
      "loss": 0.6825,
      "step": 397200
    },
    {
      "epoch": 6.237048665620094,
      "grad_norm": 3.1597886085510254,
      "learning_rate": 4.610184458398744e-05,
      "loss": 0.6836,
      "step": 397300
    },
    {
      "epoch": 6.23861852433281,
      "grad_norm": 3.67272686958313,
      "learning_rate": 4.610086342229199e-05,
      "loss": 0.6287,
      "step": 397400
    },
    {
      "epoch": 6.240188383045526,
      "grad_norm": 3.6125333309173584,
      "learning_rate": 4.609988226059655e-05,
      "loss": 0.647,
      "step": 397500
    },
    {
      "epoch": 6.241758241758242,
      "grad_norm": 2.799238681793213,
      "learning_rate": 4.60989010989011e-05,
      "loss": 0.6301,
      "step": 397600
    },
    {
      "epoch": 6.243328100470958,
      "grad_norm": 3.629086494445801,
      "learning_rate": 4.609791993720565e-05,
      "loss": 0.6737,
      "step": 397700
    },
    {
      "epoch": 6.244897959183674,
      "grad_norm": 2.840510368347168,
      "learning_rate": 4.609693877551021e-05,
      "loss": 0.6612,
      "step": 397800
    },
    {
      "epoch": 6.24646781789639,
      "grad_norm": 3.395369052886963,
      "learning_rate": 4.609595761381476e-05,
      "loss": 0.6777,
      "step": 397900
    },
    {
      "epoch": 6.248037676609105,
      "grad_norm": 3.3162007331848145,
      "learning_rate": 4.609497645211931e-05,
      "loss": 0.6624,
      "step": 398000
    },
    {
      "epoch": 6.249607535321821,
      "grad_norm": 4.117309093475342,
      "learning_rate": 4.609399529042386e-05,
      "loss": 0.6465,
      "step": 398100
    },
    {
      "epoch": 6.251177394034537,
      "grad_norm": 4.531508922576904,
      "learning_rate": 4.609301412872842e-05,
      "loss": 0.6616,
      "step": 398200
    },
    {
      "epoch": 6.252747252747253,
      "grad_norm": 4.070562839508057,
      "learning_rate": 4.6092032967032965e-05,
      "loss": 0.6533,
      "step": 398300
    },
    {
      "epoch": 6.254317111459969,
      "grad_norm": 4.60153865814209,
      "learning_rate": 4.609105180533752e-05,
      "loss": 0.6338,
      "step": 398400
    },
    {
      "epoch": 6.255886970172685,
      "grad_norm": 3.409501552581787,
      "learning_rate": 4.6090070643642074e-05,
      "loss": 0.6618,
      "step": 398500
    },
    {
      "epoch": 6.257456828885401,
      "grad_norm": 4.827802658081055,
      "learning_rate": 4.608908948194663e-05,
      "loss": 0.6954,
      "step": 398600
    },
    {
      "epoch": 6.259026687598116,
      "grad_norm": 3.504678249359131,
      "learning_rate": 4.6088108320251176e-05,
      "loss": 0.6566,
      "step": 398700
    },
    {
      "epoch": 6.260596546310832,
      "grad_norm": 4.299414157867432,
      "learning_rate": 4.608712715855573e-05,
      "loss": 0.6821,
      "step": 398800
    },
    {
      "epoch": 6.262166405023548,
      "grad_norm": 3.6230015754699707,
      "learning_rate": 4.6086145996860284e-05,
      "loss": 0.6635,
      "step": 398900
    },
    {
      "epoch": 6.263736263736264,
      "grad_norm": 3.9049971103668213,
      "learning_rate": 4.6085164835164835e-05,
      "loss": 0.6718,
      "step": 399000
    },
    {
      "epoch": 6.26530612244898,
      "grad_norm": 4.8763861656188965,
      "learning_rate": 4.6084183673469386e-05,
      "loss": 0.6992,
      "step": 399100
    },
    {
      "epoch": 6.266875981161696,
      "grad_norm": 3.7426908016204834,
      "learning_rate": 4.6083202511773944e-05,
      "loss": 0.6654,
      "step": 399200
    },
    {
      "epoch": 6.268445839874412,
      "grad_norm": 4.4401936531066895,
      "learning_rate": 4.6082221350078495e-05,
      "loss": 0.6402,
      "step": 399300
    },
    {
      "epoch": 6.270015698587127,
      "grad_norm": 4.300060749053955,
      "learning_rate": 4.6081240188383046e-05,
      "loss": 0.645,
      "step": 399400
    },
    {
      "epoch": 6.271585557299843,
      "grad_norm": 4.123828887939453,
      "learning_rate": 4.60802590266876e-05,
      "loss": 0.6679,
      "step": 399500
    },
    {
      "epoch": 6.273155416012559,
      "grad_norm": 2.2068662643432617,
      "learning_rate": 4.6079277864992155e-05,
      "loss": 0.5964,
      "step": 399600
    },
    {
      "epoch": 6.274725274725275,
      "grad_norm": 3.78558611869812,
      "learning_rate": 4.6078296703296706e-05,
      "loss": 0.6619,
      "step": 399700
    },
    {
      "epoch": 6.276295133437991,
      "grad_norm": 2.972139358520508,
      "learning_rate": 4.607731554160126e-05,
      "loss": 0.6433,
      "step": 399800
    },
    {
      "epoch": 6.277864992150707,
      "grad_norm": 2.867070436477661,
      "learning_rate": 4.6076334379905814e-05,
      "loss": 0.6729,
      "step": 399900
    },
    {
      "epoch": 6.279434850863423,
      "grad_norm": 4.032867431640625,
      "learning_rate": 4.6075353218210365e-05,
      "loss": 0.6274,
      "step": 400000
    },
    {
      "epoch": 6.2810047095761385,
      "grad_norm": 3.6343114376068115,
      "learning_rate": 4.6074372056514916e-05,
      "loss": 0.6696,
      "step": 400100
    },
    {
      "epoch": 6.282574568288854,
      "grad_norm": 3.8901734352111816,
      "learning_rate": 4.607339089481947e-05,
      "loss": 0.6628,
      "step": 400200
    },
    {
      "epoch": 6.28414442700157,
      "grad_norm": 4.122868537902832,
      "learning_rate": 4.6072409733124025e-05,
      "loss": 0.6402,
      "step": 400300
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 4.396737575531006,
      "learning_rate": 4.607142857142857e-05,
      "loss": 0.634,
      "step": 400400
    },
    {
      "epoch": 6.287284144427002,
      "grad_norm": 4.031399250030518,
      "learning_rate": 4.607044740973313e-05,
      "loss": 0.6529,
      "step": 400500
    },
    {
      "epoch": 6.2888540031397175,
      "grad_norm": 3.7941553592681885,
      "learning_rate": 4.606946624803768e-05,
      "loss": 0.6577,
      "step": 400600
    },
    {
      "epoch": 6.2904238618524335,
      "grad_norm": 3.343022108078003,
      "learning_rate": 4.6068485086342236e-05,
      "loss": 0.674,
      "step": 400700
    },
    {
      "epoch": 6.2919937205651495,
      "grad_norm": 3.0518078804016113,
      "learning_rate": 4.606750392464678e-05,
      "loss": 0.6597,
      "step": 400800
    },
    {
      "epoch": 6.293563579277865,
      "grad_norm": 4.8988165855407715,
      "learning_rate": 4.606652276295134e-05,
      "loss": 0.6384,
      "step": 400900
    },
    {
      "epoch": 6.295133437990581,
      "grad_norm": 3.2389581203460693,
      "learning_rate": 4.606554160125589e-05,
      "loss": 0.6764,
      "step": 401000
    },
    {
      "epoch": 6.2967032967032965,
      "grad_norm": 3.668760061264038,
      "learning_rate": 4.606456043956044e-05,
      "loss": 0.6684,
      "step": 401100
    },
    {
      "epoch": 6.2982731554160125,
      "grad_norm": 3.988178014755249,
      "learning_rate": 4.606357927786499e-05,
      "loss": 0.6672,
      "step": 401200
    },
    {
      "epoch": 6.2998430141287285,
      "grad_norm": 4.420367240905762,
      "learning_rate": 4.606259811616955e-05,
      "loss": 0.6254,
      "step": 401300
    },
    {
      "epoch": 6.3014128728414445,
      "grad_norm": 5.635040283203125,
      "learning_rate": 4.60616169544741e-05,
      "loss": 0.6404,
      "step": 401400
    },
    {
      "epoch": 6.3029827315541604,
      "grad_norm": 3.858966112136841,
      "learning_rate": 4.606063579277865e-05,
      "loss": 0.6919,
      "step": 401500
    },
    {
      "epoch": 6.304552590266876,
      "grad_norm": 4.219160556793213,
      "learning_rate": 4.60596546310832e-05,
      "loss": 0.6693,
      "step": 401600
    },
    {
      "epoch": 6.3061224489795915,
      "grad_norm": 3.816847801208496,
      "learning_rate": 4.605867346938776e-05,
      "loss": 0.6811,
      "step": 401700
    },
    {
      "epoch": 6.3076923076923075,
      "grad_norm": 3.5989973545074463,
      "learning_rate": 4.605769230769231e-05,
      "loss": 0.6759,
      "step": 401800
    },
    {
      "epoch": 6.3092621664050235,
      "grad_norm": 3.7597851753234863,
      "learning_rate": 4.605671114599686e-05,
      "loss": 0.6281,
      "step": 401900
    },
    {
      "epoch": 6.310832025117739,
      "grad_norm": 3.1625800132751465,
      "learning_rate": 4.605572998430142e-05,
      "loss": 0.6423,
      "step": 402000
    },
    {
      "epoch": 6.312401883830455,
      "grad_norm": 4.231344699859619,
      "learning_rate": 4.605474882260597e-05,
      "loss": 0.6581,
      "step": 402100
    },
    {
      "epoch": 6.313971742543171,
      "grad_norm": 4.451351642608643,
      "learning_rate": 4.605376766091052e-05,
      "loss": 0.6736,
      "step": 402200
    },
    {
      "epoch": 6.315541601255887,
      "grad_norm": 3.671314001083374,
      "learning_rate": 4.605278649921507e-05,
      "loss": 0.6663,
      "step": 402300
    },
    {
      "epoch": 6.3171114599686025,
      "grad_norm": 3.0850892066955566,
      "learning_rate": 4.605180533751963e-05,
      "loss": 0.665,
      "step": 402400
    },
    {
      "epoch": 6.318681318681318,
      "grad_norm": 3.3119473457336426,
      "learning_rate": 4.6050824175824174e-05,
      "loss": 0.6262,
      "step": 402500
    },
    {
      "epoch": 6.320251177394034,
      "grad_norm": 3.8985564708709717,
      "learning_rate": 4.604984301412873e-05,
      "loss": 0.6456,
      "step": 402600
    },
    {
      "epoch": 6.32182103610675,
      "grad_norm": 3.931881904602051,
      "learning_rate": 4.604886185243328e-05,
      "loss": 0.6456,
      "step": 402700
    },
    {
      "epoch": 6.323390894819466,
      "grad_norm": 3.037724256515503,
      "learning_rate": 4.604788069073784e-05,
      "loss": 0.6186,
      "step": 402800
    },
    {
      "epoch": 6.324960753532182,
      "grad_norm": 4.845249176025391,
      "learning_rate": 4.6046899529042385e-05,
      "loss": 0.6674,
      "step": 402900
    },
    {
      "epoch": 6.326530612244898,
      "grad_norm": 3.765716791152954,
      "learning_rate": 4.604591836734694e-05,
      "loss": 0.6564,
      "step": 403000
    },
    {
      "epoch": 6.328100470957613,
      "grad_norm": 4.77306604385376,
      "learning_rate": 4.604493720565149e-05,
      "loss": 0.6339,
      "step": 403100
    },
    {
      "epoch": 6.329670329670329,
      "grad_norm": 3.925431728363037,
      "learning_rate": 4.6043956043956044e-05,
      "loss": 0.6744,
      "step": 403200
    },
    {
      "epoch": 6.331240188383045,
      "grad_norm": 3.8704535961151123,
      "learning_rate": 4.6042974882260595e-05,
      "loss": 0.6699,
      "step": 403300
    },
    {
      "epoch": 6.332810047095761,
      "grad_norm": 4.026076793670654,
      "learning_rate": 4.604199372056515e-05,
      "loss": 0.6491,
      "step": 403400
    },
    {
      "epoch": 6.334379905808477,
      "grad_norm": 4.2379841804504395,
      "learning_rate": 4.6041012558869704e-05,
      "loss": 0.6702,
      "step": 403500
    },
    {
      "epoch": 6.335949764521193,
      "grad_norm": 4.062709808349609,
      "learning_rate": 4.6040031397174255e-05,
      "loss": 0.6707,
      "step": 403600
    },
    {
      "epoch": 6.337519623233909,
      "grad_norm": 4.092155933380127,
      "learning_rate": 4.6039050235478806e-05,
      "loss": 0.6487,
      "step": 403700
    },
    {
      "epoch": 6.339089481946624,
      "grad_norm": 3.6414074897766113,
      "learning_rate": 4.6038069073783364e-05,
      "loss": 0.6915,
      "step": 403800
    },
    {
      "epoch": 6.34065934065934,
      "grad_norm": 4.107612133026123,
      "learning_rate": 4.6037087912087915e-05,
      "loss": 0.6973,
      "step": 403900
    },
    {
      "epoch": 6.342229199372056,
      "grad_norm": 4.771397113800049,
      "learning_rate": 4.6036106750392466e-05,
      "loss": 0.6803,
      "step": 404000
    },
    {
      "epoch": 6.343799058084772,
      "grad_norm": 1.9372316598892212,
      "learning_rate": 4.6035125588697023e-05,
      "loss": 0.6867,
      "step": 404100
    },
    {
      "epoch": 6.345368916797488,
      "grad_norm": 3.6564412117004395,
      "learning_rate": 4.6034144427001574e-05,
      "loss": 0.6047,
      "step": 404200
    },
    {
      "epoch": 6.346938775510204,
      "grad_norm": 2.77217960357666,
      "learning_rate": 4.6033163265306125e-05,
      "loss": 0.6651,
      "step": 404300
    },
    {
      "epoch": 6.34850863422292,
      "grad_norm": 4.977011680603027,
      "learning_rate": 4.6032182103610676e-05,
      "loss": 0.6489,
      "step": 404400
    },
    {
      "epoch": 6.350078492935636,
      "grad_norm": 3.8944027423858643,
      "learning_rate": 4.6031200941915234e-05,
      "loss": 0.6554,
      "step": 404500
    },
    {
      "epoch": 6.351648351648351,
      "grad_norm": 4.140940189361572,
      "learning_rate": 4.603021978021978e-05,
      "loss": 0.6691,
      "step": 404600
    },
    {
      "epoch": 6.353218210361067,
      "grad_norm": 3.7053704261779785,
      "learning_rate": 4.6029238618524336e-05,
      "loss": 0.6628,
      "step": 404700
    },
    {
      "epoch": 6.354788069073783,
      "grad_norm": 3.931229829788208,
      "learning_rate": 4.602825745682889e-05,
      "loss": 0.6692,
      "step": 404800
    },
    {
      "epoch": 6.356357927786499,
      "grad_norm": 4.072385311126709,
      "learning_rate": 4.6027276295133445e-05,
      "loss": 0.6624,
      "step": 404900
    },
    {
      "epoch": 6.357927786499215,
      "grad_norm": 4.165884494781494,
      "learning_rate": 4.602629513343799e-05,
      "loss": 0.6663,
      "step": 405000
    },
    {
      "epoch": 6.359497645211931,
      "grad_norm": 3.3730785846710205,
      "learning_rate": 4.602531397174255e-05,
      "loss": 0.6828,
      "step": 405100
    },
    {
      "epoch": 6.361067503924647,
      "grad_norm": 4.128364562988281,
      "learning_rate": 4.60243328100471e-05,
      "loss": 0.6708,
      "step": 405200
    },
    {
      "epoch": 6.362637362637362,
      "grad_norm": 3.6233744621276855,
      "learning_rate": 4.602335164835165e-05,
      "loss": 0.6343,
      "step": 405300
    },
    {
      "epoch": 6.364207221350078,
      "grad_norm": 3.23775577545166,
      "learning_rate": 4.60223704866562e-05,
      "loss": 0.6858,
      "step": 405400
    },
    {
      "epoch": 6.365777080062794,
      "grad_norm": 3.181443691253662,
      "learning_rate": 4.602138932496076e-05,
      "loss": 0.6659,
      "step": 405500
    },
    {
      "epoch": 6.36734693877551,
      "grad_norm": 3.9684760570526123,
      "learning_rate": 4.602040816326531e-05,
      "loss": 0.6689,
      "step": 405600
    },
    {
      "epoch": 6.368916797488226,
      "grad_norm": 5.733455657958984,
      "learning_rate": 4.601942700156986e-05,
      "loss": 0.6478,
      "step": 405700
    },
    {
      "epoch": 6.370486656200942,
      "grad_norm": 3.023944854736328,
      "learning_rate": 4.601844583987441e-05,
      "loss": 0.634,
      "step": 405800
    },
    {
      "epoch": 6.372056514913658,
      "grad_norm": 4.085328102111816,
      "learning_rate": 4.601746467817897e-05,
      "loss": 0.6664,
      "step": 405900
    },
    {
      "epoch": 6.373626373626374,
      "grad_norm": 5.41039514541626,
      "learning_rate": 4.601648351648352e-05,
      "loss": 0.6588,
      "step": 406000
    },
    {
      "epoch": 6.375196232339089,
      "grad_norm": 4.524205684661865,
      "learning_rate": 4.601550235478807e-05,
      "loss": 0.6547,
      "step": 406100
    },
    {
      "epoch": 6.376766091051805,
      "grad_norm": 3.3255224227905273,
      "learning_rate": 4.601452119309263e-05,
      "loss": 0.6714,
      "step": 406200
    },
    {
      "epoch": 6.378335949764521,
      "grad_norm": 4.22290563583374,
      "learning_rate": 4.601354003139718e-05,
      "loss": 0.6552,
      "step": 406300
    },
    {
      "epoch": 6.379905808477237,
      "grad_norm": 3.4978854656219482,
      "learning_rate": 4.601255886970173e-05,
      "loss": 0.6873,
      "step": 406400
    },
    {
      "epoch": 6.381475667189953,
      "grad_norm": 5.09364128112793,
      "learning_rate": 4.601157770800628e-05,
      "loss": 0.6709,
      "step": 406500
    },
    {
      "epoch": 6.383045525902669,
      "grad_norm": 4.772282123565674,
      "learning_rate": 4.601059654631084e-05,
      "loss": 0.6934,
      "step": 406600
    },
    {
      "epoch": 6.384615384615385,
      "grad_norm": 4.364409446716309,
      "learning_rate": 4.600961538461538e-05,
      "loss": 0.6573,
      "step": 406700
    },
    {
      "epoch": 6.3861852433281,
      "grad_norm": 4.080367088317871,
      "learning_rate": 4.600863422291994e-05,
      "loss": 0.6967,
      "step": 406800
    },
    {
      "epoch": 6.387755102040816,
      "grad_norm": 3.9387903213500977,
      "learning_rate": 4.600765306122449e-05,
      "loss": 0.687,
      "step": 406900
    },
    {
      "epoch": 6.389324960753532,
      "grad_norm": 4.315220832824707,
      "learning_rate": 4.600667189952905e-05,
      "loss": 0.6589,
      "step": 407000
    },
    {
      "epoch": 6.390894819466248,
      "grad_norm": 4.1642866134643555,
      "learning_rate": 4.6005690737833594e-05,
      "loss": 0.6724,
      "step": 407100
    },
    {
      "epoch": 6.392464678178964,
      "grad_norm": 4.871920108795166,
      "learning_rate": 4.600470957613815e-05,
      "loss": 0.6715,
      "step": 407200
    },
    {
      "epoch": 6.39403453689168,
      "grad_norm": 3.5306291580200195,
      "learning_rate": 4.60037284144427e-05,
      "loss": 0.688,
      "step": 407300
    },
    {
      "epoch": 6.395604395604396,
      "grad_norm": 3.9560906887054443,
      "learning_rate": 4.600274725274725e-05,
      "loss": 0.6414,
      "step": 407400
    },
    {
      "epoch": 6.397174254317111,
      "grad_norm": 4.532093048095703,
      "learning_rate": 4.6001766091051804e-05,
      "loss": 0.67,
      "step": 407500
    },
    {
      "epoch": 6.398744113029827,
      "grad_norm": 3.9816601276397705,
      "learning_rate": 4.600078492935636e-05,
      "loss": 0.654,
      "step": 407600
    },
    {
      "epoch": 6.400313971742543,
      "grad_norm": 4.671164512634277,
      "learning_rate": 4.599980376766091e-05,
      "loss": 0.6509,
      "step": 407700
    },
    {
      "epoch": 6.401883830455259,
      "grad_norm": 4.850506782531738,
      "learning_rate": 4.5998822605965464e-05,
      "loss": 0.6495,
      "step": 407800
    },
    {
      "epoch": 6.403453689167975,
      "grad_norm": 3.6252591609954834,
      "learning_rate": 4.5997841444270015e-05,
      "loss": 0.6726,
      "step": 407900
    },
    {
      "epoch": 6.405023547880691,
      "grad_norm": 4.689214706420898,
      "learning_rate": 4.599686028257457e-05,
      "loss": 0.6814,
      "step": 408000
    },
    {
      "epoch": 6.406593406593407,
      "grad_norm": 3.2930495738983154,
      "learning_rate": 4.5995879120879124e-05,
      "loss": 0.6973,
      "step": 408100
    },
    {
      "epoch": 6.408163265306122,
      "grad_norm": 3.2934858798980713,
      "learning_rate": 4.5994897959183675e-05,
      "loss": 0.7058,
      "step": 408200
    },
    {
      "epoch": 6.409733124018838,
      "grad_norm": 3.556320905685425,
      "learning_rate": 4.599391679748823e-05,
      "loss": 0.6563,
      "step": 408300
    },
    {
      "epoch": 6.411302982731554,
      "grad_norm": 4.291059494018555,
      "learning_rate": 4.599293563579278e-05,
      "loss": 0.6815,
      "step": 408400
    },
    {
      "epoch": 6.41287284144427,
      "grad_norm": 3.6324727535247803,
      "learning_rate": 4.5991954474097334e-05,
      "loss": 0.6511,
      "step": 408500
    },
    {
      "epoch": 6.414442700156986,
      "grad_norm": 4.378828048706055,
      "learning_rate": 4.5990973312401885e-05,
      "loss": 0.6359,
      "step": 408600
    },
    {
      "epoch": 6.416012558869702,
      "grad_norm": 3.9320549964904785,
      "learning_rate": 4.598999215070644e-05,
      "loss": 0.6417,
      "step": 408700
    },
    {
      "epoch": 6.417582417582418,
      "grad_norm": 3.596087694168091,
      "learning_rate": 4.598901098901099e-05,
      "loss": 0.6255,
      "step": 408800
    },
    {
      "epoch": 6.419152276295134,
      "grad_norm": 2.889838695526123,
      "learning_rate": 4.5988029827315545e-05,
      "loss": 0.6538,
      "step": 408900
    },
    {
      "epoch": 6.420722135007849,
      "grad_norm": 4.277511119842529,
      "learning_rate": 4.5987048665620096e-05,
      "loss": 0.6274,
      "step": 409000
    },
    {
      "epoch": 6.422291993720565,
      "grad_norm": 3.961832046508789,
      "learning_rate": 4.5986067503924654e-05,
      "loss": 0.6813,
      "step": 409100
    },
    {
      "epoch": 6.423861852433281,
      "grad_norm": 3.1675727367401123,
      "learning_rate": 4.59850863422292e-05,
      "loss": 0.6765,
      "step": 409200
    },
    {
      "epoch": 6.425431711145997,
      "grad_norm": 4.106078624725342,
      "learning_rate": 4.5984105180533756e-05,
      "loss": 0.6855,
      "step": 409300
    },
    {
      "epoch": 6.427001569858713,
      "grad_norm": 4.15324068069458,
      "learning_rate": 4.598312401883831e-05,
      "loss": 0.6757,
      "step": 409400
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 3.387913227081299,
      "learning_rate": 4.598214285714286e-05,
      "loss": 0.656,
      "step": 409500
    },
    {
      "epoch": 6.430141287284145,
      "grad_norm": 2.3119330406188965,
      "learning_rate": 4.598116169544741e-05,
      "loss": 0.6765,
      "step": 409600
    },
    {
      "epoch": 6.43171114599686,
      "grad_norm": 4.327392101287842,
      "learning_rate": 4.5980180533751966e-05,
      "loss": 0.6262,
      "step": 409700
    },
    {
      "epoch": 6.433281004709576,
      "grad_norm": 4.693124771118164,
      "learning_rate": 4.597919937205652e-05,
      "loss": 0.6529,
      "step": 409800
    },
    {
      "epoch": 6.434850863422292,
      "grad_norm": 3.595773696899414,
      "learning_rate": 4.597821821036107e-05,
      "loss": 0.6486,
      "step": 409900
    },
    {
      "epoch": 6.436420722135008,
      "grad_norm": 3.5381920337677,
      "learning_rate": 4.597723704866562e-05,
      "loss": 0.6627,
      "step": 410000
    },
    {
      "epoch": 6.437990580847724,
      "grad_norm": 3.298837661743164,
      "learning_rate": 4.597625588697018e-05,
      "loss": 0.6481,
      "step": 410100
    },
    {
      "epoch": 6.43956043956044,
      "grad_norm": 4.825660228729248,
      "learning_rate": 4.597527472527473e-05,
      "loss": 0.6504,
      "step": 410200
    },
    {
      "epoch": 6.441130298273156,
      "grad_norm": 4.411442279815674,
      "learning_rate": 4.597429356357928e-05,
      "loss": 0.6884,
      "step": 410300
    },
    {
      "epoch": 6.442700156985872,
      "grad_norm": 3.102088689804077,
      "learning_rate": 4.597331240188383e-05,
      "loss": 0.6639,
      "step": 410400
    },
    {
      "epoch": 6.444270015698587,
      "grad_norm": 3.426974296569824,
      "learning_rate": 4.597233124018839e-05,
      "loss": 0.6409,
      "step": 410500
    },
    {
      "epoch": 6.445839874411303,
      "grad_norm": 3.218946695327759,
      "learning_rate": 4.597135007849294e-05,
      "loss": 0.6464,
      "step": 410600
    },
    {
      "epoch": 6.447409733124019,
      "grad_norm": 6.024935245513916,
      "learning_rate": 4.597036891679749e-05,
      "loss": 0.6506,
      "step": 410700
    },
    {
      "epoch": 6.448979591836735,
      "grad_norm": 3.168421745300293,
      "learning_rate": 4.596938775510205e-05,
      "loss": 0.6878,
      "step": 410800
    },
    {
      "epoch": 6.450549450549451,
      "grad_norm": 4.106710433959961,
      "learning_rate": 4.596840659340659e-05,
      "loss": 0.6677,
      "step": 410900
    },
    {
      "epoch": 6.452119309262167,
      "grad_norm": 3.6787781715393066,
      "learning_rate": 4.596742543171115e-05,
      "loss": 0.6146,
      "step": 411000
    },
    {
      "epoch": 6.453689167974883,
      "grad_norm": 3.331718921661377,
      "learning_rate": 4.59664442700157e-05,
      "loss": 0.6711,
      "step": 411100
    },
    {
      "epoch": 6.455259026687598,
      "grad_norm": 3.76495099067688,
      "learning_rate": 4.596546310832026e-05,
      "loss": 0.6961,
      "step": 411200
    },
    {
      "epoch": 6.456828885400314,
      "grad_norm": 3.25457763671875,
      "learning_rate": 4.59644819466248e-05,
      "loss": 0.6502,
      "step": 411300
    },
    {
      "epoch": 6.45839874411303,
      "grad_norm": 4.9871063232421875,
      "learning_rate": 4.596350078492936e-05,
      "loss": 0.664,
      "step": 411400
    },
    {
      "epoch": 6.459968602825746,
      "grad_norm": 3.8176393508911133,
      "learning_rate": 4.596251962323391e-05,
      "loss": 0.6627,
      "step": 411500
    },
    {
      "epoch": 6.461538461538462,
      "grad_norm": 4.756746292114258,
      "learning_rate": 4.596153846153846e-05,
      "loss": 0.641,
      "step": 411600
    },
    {
      "epoch": 6.463108320251178,
      "grad_norm": 4.198232173919678,
      "learning_rate": 4.596055729984301e-05,
      "loss": 0.6736,
      "step": 411700
    },
    {
      "epoch": 6.464678178963894,
      "grad_norm": 4.733141899108887,
      "learning_rate": 4.595957613814757e-05,
      "loss": 0.6457,
      "step": 411800
    },
    {
      "epoch": 6.466248037676609,
      "grad_norm": 3.103459119796753,
      "learning_rate": 4.595859497645212e-05,
      "loss": 0.6381,
      "step": 411900
    },
    {
      "epoch": 6.467817896389325,
      "grad_norm": 3.1152758598327637,
      "learning_rate": 4.595761381475667e-05,
      "loss": 0.6394,
      "step": 412000
    },
    {
      "epoch": 6.469387755102041,
      "grad_norm": 4.9845194816589355,
      "learning_rate": 4.5956632653061224e-05,
      "loss": 0.6746,
      "step": 412100
    },
    {
      "epoch": 6.470957613814757,
      "grad_norm": 3.9237170219421387,
      "learning_rate": 4.595565149136578e-05,
      "loss": 0.695,
      "step": 412200
    },
    {
      "epoch": 6.472527472527473,
      "grad_norm": 4.477606773376465,
      "learning_rate": 4.595467032967033e-05,
      "loss": 0.6351,
      "step": 412300
    },
    {
      "epoch": 6.474097331240189,
      "grad_norm": 3.1128180027008057,
      "learning_rate": 4.5953689167974884e-05,
      "loss": 0.6439,
      "step": 412400
    },
    {
      "epoch": 6.475667189952905,
      "grad_norm": 4.069555282592773,
      "learning_rate": 4.5952708006279435e-05,
      "loss": 0.6751,
      "step": 412500
    },
    {
      "epoch": 6.47723704866562,
      "grad_norm": 3.776172399520874,
      "learning_rate": 4.595172684458399e-05,
      "loss": 0.6474,
      "step": 412600
    },
    {
      "epoch": 6.478806907378336,
      "grad_norm": 3.507148504257202,
      "learning_rate": 4.595074568288854e-05,
      "loss": 0.6463,
      "step": 412700
    },
    {
      "epoch": 6.480376766091052,
      "grad_norm": 3.026341199874878,
      "learning_rate": 4.5949764521193094e-05,
      "loss": 0.6414,
      "step": 412800
    },
    {
      "epoch": 6.481946624803768,
      "grad_norm": 3.1654083728790283,
      "learning_rate": 4.594878335949765e-05,
      "loss": 0.6382,
      "step": 412900
    },
    {
      "epoch": 6.483516483516484,
      "grad_norm": 4.6379780769348145,
      "learning_rate": 4.5947802197802196e-05,
      "loss": 0.6864,
      "step": 413000
    },
    {
      "epoch": 6.4850863422291996,
      "grad_norm": 4.487007141113281,
      "learning_rate": 4.5946821036106754e-05,
      "loss": 0.6305,
      "step": 413100
    },
    {
      "epoch": 6.4866562009419155,
      "grad_norm": 4.560346603393555,
      "learning_rate": 4.5945839874411305e-05,
      "loss": 0.6166,
      "step": 413200
    },
    {
      "epoch": 6.488226059654631,
      "grad_norm": 4.876959323883057,
      "learning_rate": 4.594485871271586e-05,
      "loss": 0.6786,
      "step": 413300
    },
    {
      "epoch": 6.489795918367347,
      "grad_norm": 4.436014652252197,
      "learning_rate": 4.594387755102041e-05,
      "loss": 0.6438,
      "step": 413400
    },
    {
      "epoch": 6.491365777080063,
      "grad_norm": 3.295520305633545,
      "learning_rate": 4.5942896389324965e-05,
      "loss": 0.6884,
      "step": 413500
    },
    {
      "epoch": 6.4929356357927785,
      "grad_norm": 3.3996286392211914,
      "learning_rate": 4.5941915227629516e-05,
      "loss": 0.6844,
      "step": 413600
    },
    {
      "epoch": 6.4945054945054945,
      "grad_norm": 3.568183183670044,
      "learning_rate": 4.594093406593407e-05,
      "loss": 0.594,
      "step": 413700
    },
    {
      "epoch": 6.4960753532182105,
      "grad_norm": 3.962428569793701,
      "learning_rate": 4.593995290423862e-05,
      "loss": 0.6973,
      "step": 413800
    },
    {
      "epoch": 6.4976452119309265,
      "grad_norm": 3.861335515975952,
      "learning_rate": 4.5938971742543175e-05,
      "loss": 0.6652,
      "step": 413900
    },
    {
      "epoch": 6.4992150706436425,
      "grad_norm": 4.007878303527832,
      "learning_rate": 4.5937990580847726e-05,
      "loss": 0.6339,
      "step": 414000
    },
    {
      "epoch": 6.5007849293563575,
      "grad_norm": 4.3895440101623535,
      "learning_rate": 4.593700941915228e-05,
      "loss": 0.6743,
      "step": 414100
    },
    {
      "epoch": 6.5023547880690735,
      "grad_norm": 4.199695587158203,
      "learning_rate": 4.593602825745683e-05,
      "loss": 0.6696,
      "step": 414200
    },
    {
      "epoch": 6.5039246467817895,
      "grad_norm": 3.367414712905884,
      "learning_rate": 4.5935047095761386e-05,
      "loss": 0.6214,
      "step": 414300
    },
    {
      "epoch": 6.5054945054945055,
      "grad_norm": 2.0535728931427,
      "learning_rate": 4.593406593406594e-05,
      "loss": 0.6606,
      "step": 414400
    },
    {
      "epoch": 6.5070643642072215,
      "grad_norm": 3.680453062057495,
      "learning_rate": 4.593308477237049e-05,
      "loss": 0.6855,
      "step": 414500
    },
    {
      "epoch": 6.508634222919937,
      "grad_norm": 4.249226093292236,
      "learning_rate": 4.593210361067504e-05,
      "loss": 0.6764,
      "step": 414600
    },
    {
      "epoch": 6.510204081632653,
      "grad_norm": 3.3060827255249023,
      "learning_rate": 4.59311224489796e-05,
      "loss": 0.6271,
      "step": 414700
    },
    {
      "epoch": 6.511773940345369,
      "grad_norm": 4.757967948913574,
      "learning_rate": 4.593014128728415e-05,
      "loss": 0.6298,
      "step": 414800
    },
    {
      "epoch": 6.5133437990580845,
      "grad_norm": 2.5183629989624023,
      "learning_rate": 4.59291601255887e-05,
      "loss": 0.6842,
      "step": 414900
    },
    {
      "epoch": 6.5149136577708004,
      "grad_norm": 3.4924886226654053,
      "learning_rate": 4.5928178963893257e-05,
      "loss": 0.6348,
      "step": 415000
    },
    {
      "epoch": 6.516483516483516,
      "grad_norm": 5.277151107788086,
      "learning_rate": 4.59271978021978e-05,
      "loss": 0.6884,
      "step": 415100
    },
    {
      "epoch": 6.518053375196232,
      "grad_norm": 3.903306722640991,
      "learning_rate": 4.592621664050236e-05,
      "loss": 0.64,
      "step": 415200
    },
    {
      "epoch": 6.519623233908948,
      "grad_norm": 3.5218987464904785,
      "learning_rate": 4.592523547880691e-05,
      "loss": 0.6759,
      "step": 415300
    },
    {
      "epoch": 6.521193092621664,
      "grad_norm": 4.750838756561279,
      "learning_rate": 4.592425431711147e-05,
      "loss": 0.6545,
      "step": 415400
    },
    {
      "epoch": 6.52276295133438,
      "grad_norm": 3.003324031829834,
      "learning_rate": 4.592327315541601e-05,
      "loss": 0.616,
      "step": 415500
    },
    {
      "epoch": 6.524332810047095,
      "grad_norm": 3.945167064666748,
      "learning_rate": 4.592229199372057e-05,
      "loss": 0.6552,
      "step": 415600
    },
    {
      "epoch": 6.525902668759811,
      "grad_norm": 3.3588783740997314,
      "learning_rate": 4.592131083202512e-05,
      "loss": 0.6753,
      "step": 415700
    },
    {
      "epoch": 6.527472527472527,
      "grad_norm": 4.515867233276367,
      "learning_rate": 4.592032967032967e-05,
      "loss": 0.6389,
      "step": 415800
    },
    {
      "epoch": 6.529042386185243,
      "grad_norm": 4.63041353225708,
      "learning_rate": 4.591934850863422e-05,
      "loss": 0.6872,
      "step": 415900
    },
    {
      "epoch": 6.530612244897959,
      "grad_norm": 4.3187408447265625,
      "learning_rate": 4.591836734693878e-05,
      "loss": 0.6498,
      "step": 416000
    },
    {
      "epoch": 6.532182103610675,
      "grad_norm": 4.074862480163574,
      "learning_rate": 4.591738618524333e-05,
      "loss": 0.6309,
      "step": 416100
    },
    {
      "epoch": 6.533751962323391,
      "grad_norm": 3.983651638031006,
      "learning_rate": 4.591640502354788e-05,
      "loss": 0.6392,
      "step": 416200
    },
    {
      "epoch": 6.535321821036106,
      "grad_norm": 4.597512722015381,
      "learning_rate": 4.591542386185243e-05,
      "loss": 0.7072,
      "step": 416300
    },
    {
      "epoch": 6.536891679748822,
      "grad_norm": 3.3223965167999268,
      "learning_rate": 4.591444270015699e-05,
      "loss": 0.6505,
      "step": 416400
    },
    {
      "epoch": 6.538461538461538,
      "grad_norm": 4.807534217834473,
      "learning_rate": 4.591346153846154e-05,
      "loss": 0.6515,
      "step": 416500
    },
    {
      "epoch": 6.540031397174254,
      "grad_norm": 3.340238332748413,
      "learning_rate": 4.591248037676609e-05,
      "loss": 0.6505,
      "step": 416600
    },
    {
      "epoch": 6.54160125588697,
      "grad_norm": 1.9629251956939697,
      "learning_rate": 4.5911499215070644e-05,
      "loss": 0.6106,
      "step": 416700
    },
    {
      "epoch": 6.543171114599686,
      "grad_norm": 4.2187418937683105,
      "learning_rate": 4.59105180533752e-05,
      "loss": 0.6429,
      "step": 416800
    },
    {
      "epoch": 6.544740973312402,
      "grad_norm": 3.4776408672332764,
      "learning_rate": 4.590953689167975e-05,
      "loss": 0.6569,
      "step": 416900
    },
    {
      "epoch": 6.546310832025117,
      "grad_norm": 2.9861671924591064,
      "learning_rate": 4.59085557299843e-05,
      "loss": 0.6309,
      "step": 417000
    },
    {
      "epoch": 6.547880690737833,
      "grad_norm": 2.0190045833587646,
      "learning_rate": 4.590757456828886e-05,
      "loss": 0.6615,
      "step": 417100
    },
    {
      "epoch": 6.549450549450549,
      "grad_norm": 3.419851541519165,
      "learning_rate": 4.5906593406593405e-05,
      "loss": 0.6288,
      "step": 417200
    },
    {
      "epoch": 6.551020408163265,
      "grad_norm": 4.096134662628174,
      "learning_rate": 4.590561224489796e-05,
      "loss": 0.6418,
      "step": 417300
    },
    {
      "epoch": 6.552590266875981,
      "grad_norm": 4.719537734985352,
      "learning_rate": 4.5904631083202514e-05,
      "loss": 0.6897,
      "step": 417400
    },
    {
      "epoch": 6.554160125588697,
      "grad_norm": 4.69447660446167,
      "learning_rate": 4.590364992150707e-05,
      "loss": 0.6675,
      "step": 417500
    },
    {
      "epoch": 6.555729984301413,
      "grad_norm": 4.773644924163818,
      "learning_rate": 4.5902668759811616e-05,
      "loss": 0.66,
      "step": 417600
    },
    {
      "epoch": 6.557299843014128,
      "grad_norm": 5.126326084136963,
      "learning_rate": 4.5901687598116174e-05,
      "loss": 0.7022,
      "step": 417700
    },
    {
      "epoch": 6.558869701726844,
      "grad_norm": 3.49926495552063,
      "learning_rate": 4.5900706436420725e-05,
      "loss": 0.6995,
      "step": 417800
    },
    {
      "epoch": 6.56043956043956,
      "grad_norm": 4.891801357269287,
      "learning_rate": 4.5899725274725276e-05,
      "loss": 0.635,
      "step": 417900
    },
    {
      "epoch": 6.562009419152276,
      "grad_norm": 3.204181671142578,
      "learning_rate": 4.589874411302983e-05,
      "loss": 0.634,
      "step": 418000
    },
    {
      "epoch": 6.563579277864992,
      "grad_norm": 3.1010265350341797,
      "learning_rate": 4.5897762951334384e-05,
      "loss": 0.6285,
      "step": 418100
    },
    {
      "epoch": 6.565149136577708,
      "grad_norm": 4.024437427520752,
      "learning_rate": 4.5896781789638935e-05,
      "loss": 0.655,
      "step": 418200
    },
    {
      "epoch": 6.566718995290424,
      "grad_norm": 4.98419189453125,
      "learning_rate": 4.5895800627943486e-05,
      "loss": 0.7013,
      "step": 418300
    },
    {
      "epoch": 6.568288854003139,
      "grad_norm": 3.555830240249634,
      "learning_rate": 4.589481946624804e-05,
      "loss": 0.6861,
      "step": 418400
    },
    {
      "epoch": 6.569858712715855,
      "grad_norm": 3.6048831939697266,
      "learning_rate": 4.5893838304552595e-05,
      "loss": 0.6472,
      "step": 418500
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 4.980076789855957,
      "learning_rate": 4.5892857142857146e-05,
      "loss": 0.6256,
      "step": 418600
    },
    {
      "epoch": 6.572998430141287,
      "grad_norm": 4.167980194091797,
      "learning_rate": 4.58918759811617e-05,
      "loss": 0.6583,
      "step": 418700
    },
    {
      "epoch": 6.574568288854003,
      "grad_norm": 3.813037157058716,
      "learning_rate": 4.589089481946625e-05,
      "loss": 0.6625,
      "step": 418800
    },
    {
      "epoch": 6.576138147566719,
      "grad_norm": 4.161264896392822,
      "learning_rate": 4.5889913657770806e-05,
      "loss": 0.6321,
      "step": 418900
    },
    {
      "epoch": 6.577708006279435,
      "grad_norm": 3.6531565189361572,
      "learning_rate": 4.588893249607536e-05,
      "loss": 0.6602,
      "step": 419000
    },
    {
      "epoch": 6.579277864992151,
      "grad_norm": 4.448050498962402,
      "learning_rate": 4.588795133437991e-05,
      "loss": 0.6982,
      "step": 419100
    },
    {
      "epoch": 6.580847723704867,
      "grad_norm": 2.6888890266418457,
      "learning_rate": 4.5886970172684466e-05,
      "loss": 0.6384,
      "step": 419200
    },
    {
      "epoch": 6.582417582417582,
      "grad_norm": 3.5435540676116943,
      "learning_rate": 4.588598901098901e-05,
      "loss": 0.6844,
      "step": 419300
    },
    {
      "epoch": 6.583987441130298,
      "grad_norm": 4.437620639801025,
      "learning_rate": 4.588500784929357e-05,
      "loss": 0.6663,
      "step": 419400
    },
    {
      "epoch": 6.585557299843014,
      "grad_norm": 3.6947429180145264,
      "learning_rate": 4.588402668759812e-05,
      "loss": 0.6792,
      "step": 419500
    },
    {
      "epoch": 6.58712715855573,
      "grad_norm": 4.414155960083008,
      "learning_rate": 4.5883045525902676e-05,
      "loss": 0.6662,
      "step": 419600
    },
    {
      "epoch": 6.588697017268446,
      "grad_norm": 4.632822036743164,
      "learning_rate": 4.588206436420722e-05,
      "loss": 0.6869,
      "step": 419700
    },
    {
      "epoch": 6.590266875981162,
      "grad_norm": 4.941387176513672,
      "learning_rate": 4.588108320251178e-05,
      "loss": 0.6462,
      "step": 419800
    },
    {
      "epoch": 6.591836734693878,
      "grad_norm": 4.128376007080078,
      "learning_rate": 4.588010204081633e-05,
      "loss": 0.6438,
      "step": 419900
    },
    {
      "epoch": 6.593406593406593,
      "grad_norm": 4.330506324768066,
      "learning_rate": 4.587912087912088e-05,
      "loss": 0.7086,
      "step": 420000
    },
    {
      "epoch": 6.594976452119309,
      "grad_norm": 3.6380932331085205,
      "learning_rate": 4.587813971742543e-05,
      "loss": 0.6378,
      "step": 420100
    },
    {
      "epoch": 6.596546310832025,
      "grad_norm": 2.215407371520996,
      "learning_rate": 4.587715855572999e-05,
      "loss": 0.6471,
      "step": 420200
    },
    {
      "epoch": 6.598116169544741,
      "grad_norm": 4.045429229736328,
      "learning_rate": 4.587617739403454e-05,
      "loss": 0.6643,
      "step": 420300
    },
    {
      "epoch": 6.599686028257457,
      "grad_norm": 3.878676652908325,
      "learning_rate": 4.587519623233909e-05,
      "loss": 0.6531,
      "step": 420400
    },
    {
      "epoch": 6.601255886970173,
      "grad_norm": 4.28791618347168,
      "learning_rate": 4.587421507064364e-05,
      "loss": 0.6929,
      "step": 420500
    },
    {
      "epoch": 6.602825745682889,
      "grad_norm": 4.032438278198242,
      "learning_rate": 4.58732339089482e-05,
      "loss": 0.6935,
      "step": 420600
    },
    {
      "epoch": 6.604395604395604,
      "grad_norm": 3.92838716506958,
      "learning_rate": 4.587225274725275e-05,
      "loss": 0.6638,
      "step": 420700
    },
    {
      "epoch": 6.60596546310832,
      "grad_norm": 4.5888671875,
      "learning_rate": 4.58712715855573e-05,
      "loss": 0.6743,
      "step": 420800
    },
    {
      "epoch": 6.607535321821036,
      "grad_norm": 4.326857566833496,
      "learning_rate": 4.587029042386185e-05,
      "loss": 0.6656,
      "step": 420900
    },
    {
      "epoch": 6.609105180533752,
      "grad_norm": 4.291805267333984,
      "learning_rate": 4.5869309262166404e-05,
      "loss": 0.6178,
      "step": 421000
    },
    {
      "epoch": 6.610675039246468,
      "grad_norm": 3.4836206436157227,
      "learning_rate": 4.586832810047096e-05,
      "loss": 0.6832,
      "step": 421100
    },
    {
      "epoch": 6.612244897959184,
      "grad_norm": 3.1322548389434814,
      "learning_rate": 4.586734693877551e-05,
      "loss": 0.6534,
      "step": 421200
    },
    {
      "epoch": 6.6138147566719,
      "grad_norm": 3.563323497772217,
      "learning_rate": 4.586636577708007e-05,
      "loss": 0.665,
      "step": 421300
    },
    {
      "epoch": 6.615384615384615,
      "grad_norm": 4.335646629333496,
      "learning_rate": 4.5865384615384614e-05,
      "loss": 0.6825,
      "step": 421400
    },
    {
      "epoch": 6.616954474097331,
      "grad_norm": 4.087550163269043,
      "learning_rate": 4.586440345368917e-05,
      "loss": 0.681,
      "step": 421500
    },
    {
      "epoch": 6.618524332810047,
      "grad_norm": 4.439944744110107,
      "learning_rate": 4.586342229199372e-05,
      "loss": 0.6488,
      "step": 421600
    },
    {
      "epoch": 6.620094191522763,
      "grad_norm": 3.452848196029663,
      "learning_rate": 4.5862441130298274e-05,
      "loss": 0.6733,
      "step": 421700
    },
    {
      "epoch": 6.621664050235479,
      "grad_norm": 4.168079853057861,
      "learning_rate": 4.5861459968602825e-05,
      "loss": 0.6581,
      "step": 421800
    },
    {
      "epoch": 6.623233908948195,
      "grad_norm": 5.123873710632324,
      "learning_rate": 4.586047880690738e-05,
      "loss": 0.6494,
      "step": 421900
    },
    {
      "epoch": 6.624803767660911,
      "grad_norm": 4.797772407531738,
      "learning_rate": 4.5859497645211934e-05,
      "loss": 0.653,
      "step": 422000
    },
    {
      "epoch": 6.626373626373626,
      "grad_norm": 3.767425060272217,
      "learning_rate": 4.5858516483516485e-05,
      "loss": 0.6672,
      "step": 422100
    },
    {
      "epoch": 6.627943485086342,
      "grad_norm": 3.510530471801758,
      "learning_rate": 4.5857535321821036e-05,
      "loss": 0.6521,
      "step": 422200
    },
    {
      "epoch": 6.629513343799058,
      "grad_norm": 3.823760747909546,
      "learning_rate": 4.5856554160125593e-05,
      "loss": 0.6086,
      "step": 422300
    },
    {
      "epoch": 6.631083202511774,
      "grad_norm": 3.0282325744628906,
      "learning_rate": 4.585557299843014e-05,
      "loss": 0.6451,
      "step": 422400
    },
    {
      "epoch": 6.63265306122449,
      "grad_norm": 5.055635929107666,
      "learning_rate": 4.5854591836734695e-05,
      "loss": 0.6656,
      "step": 422500
    },
    {
      "epoch": 6.634222919937206,
      "grad_norm": 3.741554021835327,
      "learning_rate": 4.5853610675039246e-05,
      "loss": 0.6501,
      "step": 422600
    },
    {
      "epoch": 6.635792778649922,
      "grad_norm": 3.335679292678833,
      "learning_rate": 4.5852629513343804e-05,
      "loss": 0.6621,
      "step": 422700
    },
    {
      "epoch": 6.637362637362637,
      "grad_norm": 3.3105015754699707,
      "learning_rate": 4.5851648351648355e-05,
      "loss": 0.6541,
      "step": 422800
    },
    {
      "epoch": 6.638932496075353,
      "grad_norm": 4.522146701812744,
      "learning_rate": 4.5850667189952906e-05,
      "loss": 0.6373,
      "step": 422900
    },
    {
      "epoch": 6.640502354788069,
      "grad_norm": 3.880875825881958,
      "learning_rate": 4.584968602825746e-05,
      "loss": 0.6554,
      "step": 423000
    },
    {
      "epoch": 6.642072213500785,
      "grad_norm": 4.012777805328369,
      "learning_rate": 4.584870486656201e-05,
      "loss": 0.6574,
      "step": 423100
    },
    {
      "epoch": 6.643642072213501,
      "grad_norm": 4.33594274520874,
      "learning_rate": 4.5847723704866566e-05,
      "loss": 0.6552,
      "step": 423200
    },
    {
      "epoch": 6.645211930926217,
      "grad_norm": 4.109043121337891,
      "learning_rate": 4.584674254317112e-05,
      "loss": 0.6459,
      "step": 423300
    },
    {
      "epoch": 6.646781789638933,
      "grad_norm": 3.6981091499328613,
      "learning_rate": 4.5845761381475675e-05,
      "loss": 0.6527,
      "step": 423400
    },
    {
      "epoch": 6.648351648351649,
      "grad_norm": 2.8393442630767822,
      "learning_rate": 4.584478021978022e-05,
      "loss": 0.6533,
      "step": 423500
    },
    {
      "epoch": 6.649921507064365,
      "grad_norm": 4.11692476272583,
      "learning_rate": 4.5843799058084776e-05,
      "loss": 0.646,
      "step": 423600
    },
    {
      "epoch": 6.65149136577708,
      "grad_norm": 4.896356105804443,
      "learning_rate": 4.584281789638933e-05,
      "loss": 0.6601,
      "step": 423700
    },
    {
      "epoch": 6.653061224489796,
      "grad_norm": 4.2451348304748535,
      "learning_rate": 4.584183673469388e-05,
      "loss": 0.6282,
      "step": 423800
    },
    {
      "epoch": 6.654631083202512,
      "grad_norm": 2.9617629051208496,
      "learning_rate": 4.584085557299843e-05,
      "loss": 0.6762,
      "step": 423900
    },
    {
      "epoch": 6.656200941915228,
      "grad_norm": 4.152562141418457,
      "learning_rate": 4.583987441130299e-05,
      "loss": 0.6663,
      "step": 424000
    },
    {
      "epoch": 6.657770800627944,
      "grad_norm": 3.903202533721924,
      "learning_rate": 4.583889324960754e-05,
      "loss": 0.658,
      "step": 424100
    },
    {
      "epoch": 6.65934065934066,
      "grad_norm": 3.8606700897216797,
      "learning_rate": 4.583791208791209e-05,
      "loss": 0.6723,
      "step": 424200
    },
    {
      "epoch": 6.660910518053376,
      "grad_norm": 4.6639580726623535,
      "learning_rate": 4.583693092621664e-05,
      "loss": 0.6419,
      "step": 424300
    },
    {
      "epoch": 6.662480376766091,
      "grad_norm": 3.5335583686828613,
      "learning_rate": 4.58359497645212e-05,
      "loss": 0.675,
      "step": 424400
    },
    {
      "epoch": 6.664050235478807,
      "grad_norm": 3.5940918922424316,
      "learning_rate": 4.583496860282574e-05,
      "loss": 0.6257,
      "step": 424500
    },
    {
      "epoch": 6.665620094191523,
      "grad_norm": 3.3623459339141846,
      "learning_rate": 4.58339874411303e-05,
      "loss": 0.6557,
      "step": 424600
    },
    {
      "epoch": 6.667189952904239,
      "grad_norm": 4.651628494262695,
      "learning_rate": 4.583300627943485e-05,
      "loss": 0.6425,
      "step": 424700
    },
    {
      "epoch": 6.668759811616955,
      "grad_norm": 7.007755279541016,
      "learning_rate": 4.583202511773941e-05,
      "loss": 0.6703,
      "step": 424800
    },
    {
      "epoch": 6.670329670329671,
      "grad_norm": 3.973419189453125,
      "learning_rate": 4.583104395604396e-05,
      "loss": 0.6912,
      "step": 424900
    },
    {
      "epoch": 6.671899529042387,
      "grad_norm": 5.108785629272461,
      "learning_rate": 4.583006279434851e-05,
      "loss": 0.667,
      "step": 425000
    },
    {
      "epoch": 6.673469387755102,
      "grad_norm": 4.459264755249023,
      "learning_rate": 4.582908163265306e-05,
      "loss": 0.6756,
      "step": 425100
    },
    {
      "epoch": 6.675039246467818,
      "grad_norm": 3.676427125930786,
      "learning_rate": 4.582810047095761e-05,
      "loss": 0.6494,
      "step": 425200
    },
    {
      "epoch": 6.676609105180534,
      "grad_norm": 2.5297088623046875,
      "learning_rate": 4.582711930926217e-05,
      "loss": 0.6285,
      "step": 425300
    },
    {
      "epoch": 6.67817896389325,
      "grad_norm": 4.20247745513916,
      "learning_rate": 4.582613814756672e-05,
      "loss": 0.6374,
      "step": 425400
    },
    {
      "epoch": 6.679748822605966,
      "grad_norm": 4.2741875648498535,
      "learning_rate": 4.582515698587128e-05,
      "loss": 0.6713,
      "step": 425500
    },
    {
      "epoch": 6.681318681318682,
      "grad_norm": 3.078528642654419,
      "learning_rate": 4.582417582417582e-05,
      "loss": 0.6586,
      "step": 425600
    },
    {
      "epoch": 6.6828885400313975,
      "grad_norm": 3.7387678623199463,
      "learning_rate": 4.582319466248038e-05,
      "loss": 0.6357,
      "step": 425700
    },
    {
      "epoch": 6.684458398744113,
      "grad_norm": 4.37501335144043,
      "learning_rate": 4.582221350078493e-05,
      "loss": 0.6244,
      "step": 425800
    },
    {
      "epoch": 6.686028257456829,
      "grad_norm": 3.8193628787994385,
      "learning_rate": 4.582123233908948e-05,
      "loss": 0.6473,
      "step": 425900
    },
    {
      "epoch": 6.687598116169545,
      "grad_norm": 4.281713962554932,
      "learning_rate": 4.5820251177394034e-05,
      "loss": 0.7034,
      "step": 426000
    },
    {
      "epoch": 6.689167974882261,
      "grad_norm": 4.0328192710876465,
      "learning_rate": 4.581927001569859e-05,
      "loss": 0.6899,
      "step": 426100
    },
    {
      "epoch": 6.6907378335949765,
      "grad_norm": 4.77145528793335,
      "learning_rate": 4.581828885400314e-05,
      "loss": 0.6437,
      "step": 426200
    },
    {
      "epoch": 6.6923076923076925,
      "grad_norm": 4.2229485511779785,
      "learning_rate": 4.5817307692307694e-05,
      "loss": 0.6657,
      "step": 426300
    },
    {
      "epoch": 6.6938775510204085,
      "grad_norm": 2.807551860809326,
      "learning_rate": 4.5816326530612245e-05,
      "loss": 0.7034,
      "step": 426400
    },
    {
      "epoch": 6.695447409733124,
      "grad_norm": 3.396970748901367,
      "learning_rate": 4.58153453689168e-05,
      "loss": 0.6596,
      "step": 426500
    },
    {
      "epoch": 6.6970172684458396,
      "grad_norm": 4.136072635650635,
      "learning_rate": 4.5814364207221347e-05,
      "loss": 0.6734,
      "step": 426600
    },
    {
      "epoch": 6.6985871271585555,
      "grad_norm": 4.392918586730957,
      "learning_rate": 4.5813383045525904e-05,
      "loss": 0.6913,
      "step": 426700
    },
    {
      "epoch": 6.7001569858712715,
      "grad_norm": 5.790802001953125,
      "learning_rate": 4.5812401883830455e-05,
      "loss": 0.6301,
      "step": 426800
    },
    {
      "epoch": 6.7017268445839875,
      "grad_norm": 3.874716281890869,
      "learning_rate": 4.581142072213501e-05,
      "loss": 0.6607,
      "step": 426900
    },
    {
      "epoch": 6.7032967032967035,
      "grad_norm": 4.395610332489014,
      "learning_rate": 4.5810439560439564e-05,
      "loss": 0.6691,
      "step": 427000
    },
    {
      "epoch": 6.704866562009419,
      "grad_norm": 3.387570858001709,
      "learning_rate": 4.5809458398744115e-05,
      "loss": 0.6282,
      "step": 427100
    },
    {
      "epoch": 6.7064364207221345,
      "grad_norm": 3.767056703567505,
      "learning_rate": 4.5808477237048666e-05,
      "loss": 0.713,
      "step": 427200
    },
    {
      "epoch": 6.7080062794348505,
      "grad_norm": 4.390620231628418,
      "learning_rate": 4.580749607535322e-05,
      "loss": 0.6369,
      "step": 427300
    },
    {
      "epoch": 6.7095761381475665,
      "grad_norm": 3.692119598388672,
      "learning_rate": 4.5806514913657775e-05,
      "loss": 0.6837,
      "step": 427400
    },
    {
      "epoch": 6.7111459968602825,
      "grad_norm": 4.337389945983887,
      "learning_rate": 4.5805533751962326e-05,
      "loss": 0.6362,
      "step": 427500
    },
    {
      "epoch": 6.712715855572998,
      "grad_norm": 3.284839391708374,
      "learning_rate": 4.5804552590266883e-05,
      "loss": 0.6517,
      "step": 427600
    },
    {
      "epoch": 6.714285714285714,
      "grad_norm": 3.8923492431640625,
      "learning_rate": 4.580357142857143e-05,
      "loss": 0.6605,
      "step": 427700
    },
    {
      "epoch": 6.71585557299843,
      "grad_norm": 3.559241533279419,
      "learning_rate": 4.5802590266875985e-05,
      "loss": 0.6372,
      "step": 427800
    },
    {
      "epoch": 6.717425431711146,
      "grad_norm": 4.244718551635742,
      "learning_rate": 4.5801609105180536e-05,
      "loss": 0.6618,
      "step": 427900
    },
    {
      "epoch": 6.718995290423862,
      "grad_norm": 4.218554496765137,
      "learning_rate": 4.580062794348509e-05,
      "loss": 0.676,
      "step": 428000
    },
    {
      "epoch": 6.720565149136577,
      "grad_norm": 4.280318737030029,
      "learning_rate": 4.579964678178964e-05,
      "loss": 0.6621,
      "step": 428100
    },
    {
      "epoch": 6.722135007849293,
      "grad_norm": 4.462216377258301,
      "learning_rate": 4.5798665620094196e-05,
      "loss": 0.6728,
      "step": 428200
    },
    {
      "epoch": 6.723704866562009,
      "grad_norm": 3.9067952632904053,
      "learning_rate": 4.579768445839875e-05,
      "loss": 0.6731,
      "step": 428300
    },
    {
      "epoch": 6.725274725274725,
      "grad_norm": 3.880420446395874,
      "learning_rate": 4.57967032967033e-05,
      "loss": 0.6565,
      "step": 428400
    },
    {
      "epoch": 6.726844583987441,
      "grad_norm": 4.482823371887207,
      "learning_rate": 4.579572213500785e-05,
      "loss": 0.6583,
      "step": 428500
    },
    {
      "epoch": 6.728414442700157,
      "grad_norm": 4.281001091003418,
      "learning_rate": 4.579474097331241e-05,
      "loss": 0.6334,
      "step": 428600
    },
    {
      "epoch": 6.729984301412873,
      "grad_norm": 2.972917318344116,
      "learning_rate": 4.579375981161695e-05,
      "loss": 0.6311,
      "step": 428700
    },
    {
      "epoch": 6.731554160125588,
      "grad_norm": 4.214217662811279,
      "learning_rate": 4.579277864992151e-05,
      "loss": 0.6351,
      "step": 428800
    },
    {
      "epoch": 6.733124018838304,
      "grad_norm": 5.009715557098389,
      "learning_rate": 4.579179748822606e-05,
      "loss": 0.6873,
      "step": 428900
    },
    {
      "epoch": 6.73469387755102,
      "grad_norm": 3.6489620208740234,
      "learning_rate": 4.579081632653062e-05,
      "loss": 0.6444,
      "step": 429000
    },
    {
      "epoch": 6.736263736263736,
      "grad_norm": 3.779873847961426,
      "learning_rate": 4.578983516483517e-05,
      "loss": 0.6229,
      "step": 429100
    },
    {
      "epoch": 6.737833594976452,
      "grad_norm": 4.247011184692383,
      "learning_rate": 4.578885400313972e-05,
      "loss": 0.6378,
      "step": 429200
    },
    {
      "epoch": 6.739403453689168,
      "grad_norm": 4.552279949188232,
      "learning_rate": 4.578787284144427e-05,
      "loss": 0.7053,
      "step": 429300
    },
    {
      "epoch": 6.740973312401884,
      "grad_norm": 4.827187538146973,
      "learning_rate": 4.578689167974882e-05,
      "loss": 0.6572,
      "step": 429400
    },
    {
      "epoch": 6.742543171114599,
      "grad_norm": 4.240299224853516,
      "learning_rate": 4.578591051805338e-05,
      "loss": 0.659,
      "step": 429500
    },
    {
      "epoch": 6.744113029827315,
      "grad_norm": 3.70780086517334,
      "learning_rate": 4.578492935635793e-05,
      "loss": 0.6653,
      "step": 429600
    },
    {
      "epoch": 6.745682888540031,
      "grad_norm": 4.4580912590026855,
      "learning_rate": 4.578394819466249e-05,
      "loss": 0.6633,
      "step": 429700
    },
    {
      "epoch": 6.747252747252747,
      "grad_norm": 4.233243942260742,
      "learning_rate": 4.578296703296703e-05,
      "loss": 0.649,
      "step": 429800
    },
    {
      "epoch": 6.748822605965463,
      "grad_norm": 4.81566047668457,
      "learning_rate": 4.578198587127159e-05,
      "loss": 0.6725,
      "step": 429900
    },
    {
      "epoch": 6.750392464678179,
      "grad_norm": 4.6708149909973145,
      "learning_rate": 4.578100470957614e-05,
      "loss": 0.6415,
      "step": 430000
    },
    {
      "epoch": 6.751962323390895,
      "grad_norm": 2.7481982707977295,
      "learning_rate": 4.578002354788069e-05,
      "loss": 0.6553,
      "step": 430100
    },
    {
      "epoch": 6.75353218210361,
      "grad_norm": 3.882148027420044,
      "learning_rate": 4.577904238618524e-05,
      "loss": 0.6383,
      "step": 430200
    },
    {
      "epoch": 6.755102040816326,
      "grad_norm": 4.112165927886963,
      "learning_rate": 4.57780612244898e-05,
      "loss": 0.6391,
      "step": 430300
    },
    {
      "epoch": 6.756671899529042,
      "grad_norm": 4.00404691696167,
      "learning_rate": 4.577708006279435e-05,
      "loss": 0.648,
      "step": 430400
    },
    {
      "epoch": 6.758241758241758,
      "grad_norm": 3.5009355545043945,
      "learning_rate": 4.57760989010989e-05,
      "loss": 0.6802,
      "step": 430500
    },
    {
      "epoch": 6.759811616954474,
      "grad_norm": 4.200953483581543,
      "learning_rate": 4.5775117739403454e-05,
      "loss": 0.6829,
      "step": 430600
    },
    {
      "epoch": 6.76138147566719,
      "grad_norm": 3.9032814502716064,
      "learning_rate": 4.577413657770801e-05,
      "loss": 0.649,
      "step": 430700
    },
    {
      "epoch": 6.762951334379906,
      "grad_norm": 3.642066240310669,
      "learning_rate": 4.5773155416012556e-05,
      "loss": 0.6723,
      "step": 430800
    },
    {
      "epoch": 6.764521193092621,
      "grad_norm": 3.618288516998291,
      "learning_rate": 4.577217425431711e-05,
      "loss": 0.6479,
      "step": 430900
    },
    {
      "epoch": 6.766091051805337,
      "grad_norm": 3.5966498851776123,
      "learning_rate": 4.5771193092621664e-05,
      "loss": 0.6114,
      "step": 431000
    },
    {
      "epoch": 6.767660910518053,
      "grad_norm": 3.9885480403900146,
      "learning_rate": 4.577021193092622e-05,
      "loss": 0.6362,
      "step": 431100
    },
    {
      "epoch": 6.769230769230769,
      "grad_norm": 4.490698337554932,
      "learning_rate": 4.576923076923077e-05,
      "loss": 0.6384,
      "step": 431200
    },
    {
      "epoch": 6.770800627943485,
      "grad_norm": 4.133393287658691,
      "learning_rate": 4.5768249607535324e-05,
      "loss": 0.6178,
      "step": 431300
    },
    {
      "epoch": 6.772370486656201,
      "grad_norm": 4.011916637420654,
      "learning_rate": 4.5767268445839875e-05,
      "loss": 0.6766,
      "step": 431400
    },
    {
      "epoch": 6.773940345368917,
      "grad_norm": 4.588395118713379,
      "learning_rate": 4.5766287284144426e-05,
      "loss": 0.6669,
      "step": 431500
    },
    {
      "epoch": 6.775510204081632,
      "grad_norm": 4.767653942108154,
      "learning_rate": 4.5765306122448984e-05,
      "loss": 0.6669,
      "step": 431600
    },
    {
      "epoch": 6.777080062794348,
      "grad_norm": 4.314510345458984,
      "learning_rate": 4.5764324960753535e-05,
      "loss": 0.6514,
      "step": 431700
    },
    {
      "epoch": 6.778649921507064,
      "grad_norm": 5.804250717163086,
      "learning_rate": 4.576334379905809e-05,
      "loss": 0.6511,
      "step": 431800
    },
    {
      "epoch": 6.78021978021978,
      "grad_norm": 3.3836562633514404,
      "learning_rate": 4.576236263736264e-05,
      "loss": 0.6669,
      "step": 431900
    },
    {
      "epoch": 6.781789638932496,
      "grad_norm": 3.5371930599212646,
      "learning_rate": 4.5761381475667194e-05,
      "loss": 0.6509,
      "step": 432000
    },
    {
      "epoch": 6.783359497645212,
      "grad_norm": 4.107825756072998,
      "learning_rate": 4.5760400313971745e-05,
      "loss": 0.6447,
      "step": 432100
    },
    {
      "epoch": 6.784929356357928,
      "grad_norm": 3.326155424118042,
      "learning_rate": 4.5759419152276296e-05,
      "loss": 0.605,
      "step": 432200
    },
    {
      "epoch": 6.786499215070644,
      "grad_norm": 3.370508909225464,
      "learning_rate": 4.575843799058085e-05,
      "loss": 0.6214,
      "step": 432300
    },
    {
      "epoch": 6.78806907378336,
      "grad_norm": 4.3597092628479,
      "learning_rate": 4.5757456828885405e-05,
      "loss": 0.6149,
      "step": 432400
    },
    {
      "epoch": 6.789638932496075,
      "grad_norm": 4.254698753356934,
      "learning_rate": 4.5756475667189956e-05,
      "loss": 0.6534,
      "step": 432500
    },
    {
      "epoch": 6.791208791208791,
      "grad_norm": 4.078469276428223,
      "learning_rate": 4.575549450549451e-05,
      "loss": 0.6842,
      "step": 432600
    },
    {
      "epoch": 6.792778649921507,
      "grad_norm": 4.405307769775391,
      "learning_rate": 4.575451334379906e-05,
      "loss": 0.6538,
      "step": 432700
    },
    {
      "epoch": 6.794348508634223,
      "grad_norm": 3.670112133026123,
      "learning_rate": 4.5753532182103616e-05,
      "loss": 0.6452,
      "step": 432800
    },
    {
      "epoch": 6.795918367346939,
      "grad_norm": 3.6952414512634277,
      "learning_rate": 4.575255102040816e-05,
      "loss": 0.6391,
      "step": 432900
    },
    {
      "epoch": 6.797488226059655,
      "grad_norm": 3.733358860015869,
      "learning_rate": 4.575156985871272e-05,
      "loss": 0.646,
      "step": 433000
    },
    {
      "epoch": 6.799058084772371,
      "grad_norm": 3.8675379753112793,
      "learning_rate": 4.575058869701727e-05,
      "loss": 0.628,
      "step": 433100
    },
    {
      "epoch": 6.800627943485086,
      "grad_norm": 3.778604745864868,
      "learning_rate": 4.5749607535321827e-05,
      "loss": 0.6433,
      "step": 433200
    },
    {
      "epoch": 6.802197802197802,
      "grad_norm": 4.621773719787598,
      "learning_rate": 4.574862637362638e-05,
      "loss": 0.6697,
      "step": 433300
    },
    {
      "epoch": 6.803767660910518,
      "grad_norm": 3.2460124492645264,
      "learning_rate": 4.574764521193093e-05,
      "loss": 0.6233,
      "step": 433400
    },
    {
      "epoch": 6.805337519623234,
      "grad_norm": 4.025027275085449,
      "learning_rate": 4.574666405023548e-05,
      "loss": 0.6617,
      "step": 433500
    },
    {
      "epoch": 6.80690737833595,
      "grad_norm": 3.1558876037597656,
      "learning_rate": 4.574568288854003e-05,
      "loss": 0.6098,
      "step": 433600
    },
    {
      "epoch": 6.808477237048666,
      "grad_norm": 3.818434715270996,
      "learning_rate": 4.574470172684459e-05,
      "loss": 0.635,
      "step": 433700
    },
    {
      "epoch": 6.810047095761382,
      "grad_norm": 4.230484485626221,
      "learning_rate": 4.574372056514914e-05,
      "loss": 0.6619,
      "step": 433800
    },
    {
      "epoch": 6.811616954474097,
      "grad_norm": 4.639458179473877,
      "learning_rate": 4.57427394034537e-05,
      "loss": 0.6252,
      "step": 433900
    },
    {
      "epoch": 6.813186813186813,
      "grad_norm": 3.927868127822876,
      "learning_rate": 4.574175824175824e-05,
      "loss": 0.6518,
      "step": 434000
    },
    {
      "epoch": 6.814756671899529,
      "grad_norm": 3.9438536167144775,
      "learning_rate": 4.57407770800628e-05,
      "loss": 0.6614,
      "step": 434100
    },
    {
      "epoch": 6.816326530612245,
      "grad_norm": 3.8913848400115967,
      "learning_rate": 4.573979591836735e-05,
      "loss": 0.677,
      "step": 434200
    },
    {
      "epoch": 6.817896389324961,
      "grad_norm": 3.881385087966919,
      "learning_rate": 4.57388147566719e-05,
      "loss": 0.6657,
      "step": 434300
    },
    {
      "epoch": 6.819466248037677,
      "grad_norm": 3.900059938430786,
      "learning_rate": 4.573783359497645e-05,
      "loss": 0.6016,
      "step": 434400
    },
    {
      "epoch": 6.821036106750393,
      "grad_norm": 4.1443915367126465,
      "learning_rate": 4.573685243328101e-05,
      "loss": 0.6776,
      "step": 434500
    },
    {
      "epoch": 6.822605965463108,
      "grad_norm": 3.6655824184417725,
      "learning_rate": 4.573587127158556e-05,
      "loss": 0.6375,
      "step": 434600
    },
    {
      "epoch": 6.824175824175824,
      "grad_norm": 3.664362907409668,
      "learning_rate": 4.573489010989011e-05,
      "loss": 0.6876,
      "step": 434700
    },
    {
      "epoch": 6.82574568288854,
      "grad_norm": 4.444950580596924,
      "learning_rate": 4.573390894819466e-05,
      "loss": 0.6816,
      "step": 434800
    },
    {
      "epoch": 6.827315541601256,
      "grad_norm": 4.488329887390137,
      "learning_rate": 4.573292778649922e-05,
      "loss": 0.6779,
      "step": 434900
    },
    {
      "epoch": 6.828885400313972,
      "grad_norm": 4.0432257652282715,
      "learning_rate": 4.5731946624803765e-05,
      "loss": 0.629,
      "step": 435000
    },
    {
      "epoch": 6.830455259026688,
      "grad_norm": 3.772587776184082,
      "learning_rate": 4.573096546310832e-05,
      "loss": 0.6133,
      "step": 435100
    },
    {
      "epoch": 6.832025117739404,
      "grad_norm": 3.7648396492004395,
      "learning_rate": 4.572998430141287e-05,
      "loss": 0.659,
      "step": 435200
    },
    {
      "epoch": 6.833594976452119,
      "grad_norm": 2.8326163291931152,
      "learning_rate": 4.572900313971743e-05,
      "loss": 0.6218,
      "step": 435300
    },
    {
      "epoch": 6.835164835164835,
      "grad_norm": 2.2693214416503906,
      "learning_rate": 4.572802197802198e-05,
      "loss": 0.6734,
      "step": 435400
    },
    {
      "epoch": 6.836734693877551,
      "grad_norm": 4.715219497680664,
      "learning_rate": 4.572704081632653e-05,
      "loss": 0.6448,
      "step": 435500
    },
    {
      "epoch": 6.838304552590267,
      "grad_norm": 3.176703453063965,
      "learning_rate": 4.5726059654631084e-05,
      "loss": 0.685,
      "step": 435600
    },
    {
      "epoch": 6.839874411302983,
      "grad_norm": 4.0978803634643555,
      "learning_rate": 4.5725078492935635e-05,
      "loss": 0.6433,
      "step": 435700
    },
    {
      "epoch": 6.841444270015699,
      "grad_norm": 3.7719240188598633,
      "learning_rate": 4.572409733124019e-05,
      "loss": 0.6597,
      "step": 435800
    },
    {
      "epoch": 6.843014128728415,
      "grad_norm": 3.508165121078491,
      "learning_rate": 4.5723116169544744e-05,
      "loss": 0.6352,
      "step": 435900
    },
    {
      "epoch": 6.84458398744113,
      "grad_norm": 3.145791530609131,
      "learning_rate": 4.57221350078493e-05,
      "loss": 0.6503,
      "step": 436000
    },
    {
      "epoch": 6.846153846153846,
      "grad_norm": 4.26174259185791,
      "learning_rate": 4.5721153846153846e-05,
      "loss": 0.6572,
      "step": 436100
    },
    {
      "epoch": 6.847723704866562,
      "grad_norm": 4.168955326080322,
      "learning_rate": 4.5720172684458403e-05,
      "loss": 0.6845,
      "step": 436200
    },
    {
      "epoch": 6.849293563579278,
      "grad_norm": 4.155153751373291,
      "learning_rate": 4.5719191522762954e-05,
      "loss": 0.6415,
      "step": 436300
    },
    {
      "epoch": 6.850863422291994,
      "grad_norm": 3.127309560775757,
      "learning_rate": 4.5718210361067505e-05,
      "loss": 0.6592,
      "step": 436400
    },
    {
      "epoch": 6.85243328100471,
      "grad_norm": 3.8592991828918457,
      "learning_rate": 4.5717229199372056e-05,
      "loss": 0.6628,
      "step": 436500
    },
    {
      "epoch": 6.854003139717426,
      "grad_norm": 3.7779603004455566,
      "learning_rate": 4.5716248037676614e-05,
      "loss": 0.7005,
      "step": 436600
    },
    {
      "epoch": 6.855572998430142,
      "grad_norm": 4.193482875823975,
      "learning_rate": 4.5715266875981165e-05,
      "loss": 0.7105,
      "step": 436700
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 4.125256538391113,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.6419,
      "step": 436800
    },
    {
      "epoch": 6.858712715855573,
      "grad_norm": 3.3390161991119385,
      "learning_rate": 4.571330455259027e-05,
      "loss": 0.6721,
      "step": 436900
    },
    {
      "epoch": 6.860282574568289,
      "grad_norm": 3.6562445163726807,
      "learning_rate": 4.5712323390894825e-05,
      "loss": 0.6075,
      "step": 437000
    },
    {
      "epoch": 6.861852433281005,
      "grad_norm": 4.802356243133545,
      "learning_rate": 4.571134222919937e-05,
      "loss": 0.6604,
      "step": 437100
    },
    {
      "epoch": 6.863422291993721,
      "grad_norm": 5.1092939376831055,
      "learning_rate": 4.571036106750393e-05,
      "loss": 0.6428,
      "step": 437200
    },
    {
      "epoch": 6.864992150706437,
      "grad_norm": 4.144022464752197,
      "learning_rate": 4.570937990580848e-05,
      "loss": 0.6464,
      "step": 437300
    },
    {
      "epoch": 6.866562009419153,
      "grad_norm": 4.088846206665039,
      "learning_rate": 4.5708398744113036e-05,
      "loss": 0.645,
      "step": 437400
    },
    {
      "epoch": 6.868131868131869,
      "grad_norm": 4.071243762969971,
      "learning_rate": 4.5707417582417587e-05,
      "loss": 0.6097,
      "step": 437500
    },
    {
      "epoch": 6.869701726844584,
      "grad_norm": 3.68660306930542,
      "learning_rate": 4.570643642072214e-05,
      "loss": 0.6901,
      "step": 437600
    },
    {
      "epoch": 6.8712715855573,
      "grad_norm": 4.038558483123779,
      "learning_rate": 4.570545525902669e-05,
      "loss": 0.6946,
      "step": 437700
    },
    {
      "epoch": 6.872841444270016,
      "grad_norm": 3.4169440269470215,
      "learning_rate": 4.570447409733124e-05,
      "loss": 0.6615,
      "step": 437800
    },
    {
      "epoch": 6.874411302982732,
      "grad_norm": 3.809004068374634,
      "learning_rate": 4.57034929356358e-05,
      "loss": 0.6651,
      "step": 437900
    },
    {
      "epoch": 6.875981161695448,
      "grad_norm": 3.7975666522979736,
      "learning_rate": 4.570251177394035e-05,
      "loss": 0.6477,
      "step": 438000
    },
    {
      "epoch": 6.877551020408164,
      "grad_norm": 2.993135452270508,
      "learning_rate": 4.5701530612244906e-05,
      "loss": 0.681,
      "step": 438100
    },
    {
      "epoch": 6.8791208791208796,
      "grad_norm": 3.2484428882598877,
      "learning_rate": 4.570054945054945e-05,
      "loss": 0.6755,
      "step": 438200
    },
    {
      "epoch": 6.880690737833595,
      "grad_norm": 4.178186416625977,
      "learning_rate": 4.569956828885401e-05,
      "loss": 0.6576,
      "step": 438300
    },
    {
      "epoch": 6.882260596546311,
      "grad_norm": 3.485551357269287,
      "learning_rate": 4.569858712715856e-05,
      "loss": 0.6409,
      "step": 438400
    },
    {
      "epoch": 6.883830455259027,
      "grad_norm": 4.192556381225586,
      "learning_rate": 4.569760596546311e-05,
      "loss": 0.6364,
      "step": 438500
    },
    {
      "epoch": 6.885400313971743,
      "grad_norm": 4.419401168823242,
      "learning_rate": 4.569662480376766e-05,
      "loss": 0.6978,
      "step": 438600
    },
    {
      "epoch": 6.8869701726844585,
      "grad_norm": 3.366205930709839,
      "learning_rate": 4.569564364207222e-05,
      "loss": 0.6146,
      "step": 438700
    },
    {
      "epoch": 6.8885400313971745,
      "grad_norm": 4.611764430999756,
      "learning_rate": 4.569466248037677e-05,
      "loss": 0.6384,
      "step": 438800
    },
    {
      "epoch": 6.8901098901098905,
      "grad_norm": 3.458566665649414,
      "learning_rate": 4.569368131868132e-05,
      "loss": 0.6092,
      "step": 438900
    },
    {
      "epoch": 6.891679748822606,
      "grad_norm": 3.3793044090270996,
      "learning_rate": 4.569270015698587e-05,
      "loss": 0.6594,
      "step": 439000
    },
    {
      "epoch": 6.893249607535322,
      "grad_norm": 3.071533441543579,
      "learning_rate": 4.569171899529043e-05,
      "loss": 0.6987,
      "step": 439100
    },
    {
      "epoch": 6.8948194662480375,
      "grad_norm": 4.055788516998291,
      "learning_rate": 4.5690737833594974e-05,
      "loss": 0.6434,
      "step": 439200
    },
    {
      "epoch": 6.8963893249607535,
      "grad_norm": 4.594134330749512,
      "learning_rate": 4.568975667189953e-05,
      "loss": 0.7011,
      "step": 439300
    },
    {
      "epoch": 6.8979591836734695,
      "grad_norm": 3.9044289588928223,
      "learning_rate": 4.568877551020408e-05,
      "loss": 0.6481,
      "step": 439400
    },
    {
      "epoch": 6.8995290423861855,
      "grad_norm": 3.741888999938965,
      "learning_rate": 4.568779434850864e-05,
      "loss": 0.5915,
      "step": 439500
    },
    {
      "epoch": 6.9010989010989015,
      "grad_norm": 4.198981761932373,
      "learning_rate": 4.568681318681319e-05,
      "loss": 0.6699,
      "step": 439600
    },
    {
      "epoch": 6.9026687598116165,
      "grad_norm": 2.9552738666534424,
      "learning_rate": 4.568583202511774e-05,
      "loss": 0.6301,
      "step": 439700
    },
    {
      "epoch": 6.9042386185243325,
      "grad_norm": 3.496197462081909,
      "learning_rate": 4.568485086342229e-05,
      "loss": 0.6149,
      "step": 439800
    },
    {
      "epoch": 6.9058084772370485,
      "grad_norm": 4.054678916931152,
      "learning_rate": 4.5683869701726844e-05,
      "loss": 0.6765,
      "step": 439900
    },
    {
      "epoch": 6.9073783359497645,
      "grad_norm": 4.607807636260986,
      "learning_rate": 4.56828885400314e-05,
      "loss": 0.6397,
      "step": 440000
    },
    {
      "epoch": 6.9089481946624804,
      "grad_norm": 3.396005630493164,
      "learning_rate": 4.568190737833595e-05,
      "loss": 0.6662,
      "step": 440100
    },
    {
      "epoch": 6.910518053375196,
      "grad_norm": 3.507615566253662,
      "learning_rate": 4.568092621664051e-05,
      "loss": 0.6519,
      "step": 440200
    },
    {
      "epoch": 6.912087912087912,
      "grad_norm": 4.660302639007568,
      "learning_rate": 4.5679945054945055e-05,
      "loss": 0.6686,
      "step": 440300
    },
    {
      "epoch": 6.9136577708006275,
      "grad_norm": 4.174223899841309,
      "learning_rate": 4.567896389324961e-05,
      "loss": 0.6535,
      "step": 440400
    },
    {
      "epoch": 6.9152276295133435,
      "grad_norm": 4.847259521484375,
      "learning_rate": 4.567798273155416e-05,
      "loss": 0.6689,
      "step": 440500
    },
    {
      "epoch": 6.916797488226059,
      "grad_norm": 3.7339024543762207,
      "learning_rate": 4.5677001569858714e-05,
      "loss": 0.6731,
      "step": 440600
    },
    {
      "epoch": 6.918367346938775,
      "grad_norm": 4.93514347076416,
      "learning_rate": 4.5676020408163265e-05,
      "loss": 0.6057,
      "step": 440700
    },
    {
      "epoch": 6.919937205651491,
      "grad_norm": 4.376393795013428,
      "learning_rate": 4.567503924646782e-05,
      "loss": 0.6253,
      "step": 440800
    },
    {
      "epoch": 6.921507064364207,
      "grad_norm": 3.21909761428833,
      "learning_rate": 4.5674058084772374e-05,
      "loss": 0.6455,
      "step": 440900
    },
    {
      "epoch": 6.923076923076923,
      "grad_norm": 4.241886138916016,
      "learning_rate": 4.5673076923076925e-05,
      "loss": 0.6736,
      "step": 441000
    },
    {
      "epoch": 6.924646781789638,
      "grad_norm": 4.486710548400879,
      "learning_rate": 4.5672095761381476e-05,
      "loss": 0.6515,
      "step": 441100
    },
    {
      "epoch": 6.926216640502354,
      "grad_norm": 3.533912420272827,
      "learning_rate": 4.5671114599686034e-05,
      "loss": 0.6573,
      "step": 441200
    },
    {
      "epoch": 6.92778649921507,
      "grad_norm": 4.5491557121276855,
      "learning_rate": 4.567013343799058e-05,
      "loss": 0.669,
      "step": 441300
    },
    {
      "epoch": 6.929356357927786,
      "grad_norm": 4.582521915435791,
      "learning_rate": 4.5669152276295136e-05,
      "loss": 0.6491,
      "step": 441400
    },
    {
      "epoch": 6.930926216640502,
      "grad_norm": 4.593319416046143,
      "learning_rate": 4.566817111459969e-05,
      "loss": 0.6534,
      "step": 441500
    },
    {
      "epoch": 6.932496075353218,
      "grad_norm": 4.321818828582764,
      "learning_rate": 4.5667189952904245e-05,
      "loss": 0.631,
      "step": 441600
    },
    {
      "epoch": 6.934065934065934,
      "grad_norm": 4.582895278930664,
      "learning_rate": 4.5666208791208795e-05,
      "loss": 0.6055,
      "step": 441700
    },
    {
      "epoch": 6.93563579277865,
      "grad_norm": 3.833357810974121,
      "learning_rate": 4.5665227629513346e-05,
      "loss": 0.671,
      "step": 441800
    },
    {
      "epoch": 6.937205651491366,
      "grad_norm": 4.236477851867676,
      "learning_rate": 4.56642464678179e-05,
      "loss": 0.6612,
      "step": 441900
    },
    {
      "epoch": 6.938775510204081,
      "grad_norm": 4.045938968658447,
      "learning_rate": 4.566326530612245e-05,
      "loss": 0.6617,
      "step": 442000
    },
    {
      "epoch": 6.940345368916797,
      "grad_norm": 5.2313456535339355,
      "learning_rate": 4.5662284144427006e-05,
      "loss": 0.6449,
      "step": 442100
    },
    {
      "epoch": 6.941915227629513,
      "grad_norm": 3.7538533210754395,
      "learning_rate": 4.566130298273156e-05,
      "loss": 0.618,
      "step": 442200
    },
    {
      "epoch": 6.943485086342229,
      "grad_norm": 4.1116743087768555,
      "learning_rate": 4.5660321821036115e-05,
      "loss": 0.6554,
      "step": 442300
    },
    {
      "epoch": 6.945054945054945,
      "grad_norm": 4.796196937561035,
      "learning_rate": 4.565934065934066e-05,
      "loss": 0.6554,
      "step": 442400
    },
    {
      "epoch": 6.946624803767661,
      "grad_norm": 4.226678848266602,
      "learning_rate": 4.565835949764522e-05,
      "loss": 0.6613,
      "step": 442500
    },
    {
      "epoch": 6.948194662480377,
      "grad_norm": 2.9684948921203613,
      "learning_rate": 4.565737833594977e-05,
      "loss": 0.6486,
      "step": 442600
    },
    {
      "epoch": 6.949764521193092,
      "grad_norm": 4.131485939025879,
      "learning_rate": 4.565639717425432e-05,
      "loss": 0.6338,
      "step": 442700
    },
    {
      "epoch": 6.951334379905808,
      "grad_norm": 3.94693922996521,
      "learning_rate": 4.565541601255887e-05,
      "loss": 0.6747,
      "step": 442800
    },
    {
      "epoch": 6.952904238618524,
      "grad_norm": 4.4829230308532715,
      "learning_rate": 4.565443485086343e-05,
      "loss": 0.6601,
      "step": 442900
    },
    {
      "epoch": 6.95447409733124,
      "grad_norm": 4.411757469177246,
      "learning_rate": 4.565345368916798e-05,
      "loss": 0.6661,
      "step": 443000
    },
    {
      "epoch": 6.956043956043956,
      "grad_norm": 3.9463860988616943,
      "learning_rate": 4.565247252747253e-05,
      "loss": 0.6267,
      "step": 443100
    },
    {
      "epoch": 6.957613814756672,
      "grad_norm": 3.9472568035125732,
      "learning_rate": 4.565149136577708e-05,
      "loss": 0.6362,
      "step": 443200
    },
    {
      "epoch": 6.959183673469388,
      "grad_norm": 3.568089246749878,
      "learning_rate": 4.565051020408164e-05,
      "loss": 0.6625,
      "step": 443300
    },
    {
      "epoch": 6.960753532182103,
      "grad_norm": 4.146876811981201,
      "learning_rate": 4.564952904238618e-05,
      "loss": 0.6545,
      "step": 443400
    },
    {
      "epoch": 6.962323390894819,
      "grad_norm": 4.471206188201904,
      "learning_rate": 4.564854788069074e-05,
      "loss": 0.6348,
      "step": 443500
    },
    {
      "epoch": 6.963893249607535,
      "grad_norm": 3.952955484390259,
      "learning_rate": 4.564756671899529e-05,
      "loss": 0.6258,
      "step": 443600
    },
    {
      "epoch": 6.965463108320251,
      "grad_norm": 4.1167144775390625,
      "learning_rate": 4.564658555729984e-05,
      "loss": 0.6477,
      "step": 443700
    },
    {
      "epoch": 6.967032967032967,
      "grad_norm": 3.9961342811584473,
      "learning_rate": 4.56456043956044e-05,
      "loss": 0.6302,
      "step": 443800
    },
    {
      "epoch": 6.968602825745683,
      "grad_norm": 3.9446825981140137,
      "learning_rate": 4.564462323390895e-05,
      "loss": 0.6273,
      "step": 443900
    },
    {
      "epoch": 6.970172684458399,
      "grad_norm": 4.291285991668701,
      "learning_rate": 4.56436420722135e-05,
      "loss": 0.6358,
      "step": 444000
    },
    {
      "epoch": 6.971742543171114,
      "grad_norm": 4.3148016929626465,
      "learning_rate": 4.564266091051805e-05,
      "loss": 0.676,
      "step": 444100
    },
    {
      "epoch": 6.97331240188383,
      "grad_norm": 3.8088858127593994,
      "learning_rate": 4.564167974882261e-05,
      "loss": 0.6854,
      "step": 444200
    },
    {
      "epoch": 6.974882260596546,
      "grad_norm": 4.160046100616455,
      "learning_rate": 4.564069858712716e-05,
      "loss": 0.6462,
      "step": 444300
    },
    {
      "epoch": 6.976452119309262,
      "grad_norm": 4.076075553894043,
      "learning_rate": 4.563971742543171e-05,
      "loss": 0.6428,
      "step": 444400
    },
    {
      "epoch": 6.978021978021978,
      "grad_norm": 4.511480808258057,
      "learning_rate": 4.5638736263736264e-05,
      "loss": 0.6681,
      "step": 444500
    },
    {
      "epoch": 6.979591836734694,
      "grad_norm": 4.225461483001709,
      "learning_rate": 4.563775510204082e-05,
      "loss": 0.6397,
      "step": 444600
    },
    {
      "epoch": 6.98116169544741,
      "grad_norm": 3.9902071952819824,
      "learning_rate": 4.563677394034537e-05,
      "loss": 0.6626,
      "step": 444700
    },
    {
      "epoch": 6.982731554160125,
      "grad_norm": 4.340320587158203,
      "learning_rate": 4.563579277864992e-05,
      "loss": 0.6585,
      "step": 444800
    },
    {
      "epoch": 6.984301412872841,
      "grad_norm": 4.411925315856934,
      "learning_rate": 4.5634811616954474e-05,
      "loss": 0.6571,
      "step": 444900
    },
    {
      "epoch": 6.985871271585557,
      "grad_norm": 4.4411163330078125,
      "learning_rate": 4.563383045525903e-05,
      "loss": 0.6301,
      "step": 445000
    },
    {
      "epoch": 6.987441130298273,
      "grad_norm": 4.145135402679443,
      "learning_rate": 4.5632849293563576e-05,
      "loss": 0.6421,
      "step": 445100
    },
    {
      "epoch": 6.989010989010989,
      "grad_norm": 4.247194766998291,
      "learning_rate": 4.5631868131868134e-05,
      "loss": 0.6572,
      "step": 445200
    },
    {
      "epoch": 6.990580847723705,
      "grad_norm": 4.656375408172607,
      "learning_rate": 4.5630886970172685e-05,
      "loss": 0.6422,
      "step": 445300
    },
    {
      "epoch": 6.992150706436421,
      "grad_norm": 4.644025802612305,
      "learning_rate": 4.562990580847724e-05,
      "loss": 0.6571,
      "step": 445400
    },
    {
      "epoch": 6.993720565149136,
      "grad_norm": 3.1176624298095703,
      "learning_rate": 4.562892464678179e-05,
      "loss": 0.6642,
      "step": 445500
    },
    {
      "epoch": 6.995290423861852,
      "grad_norm": 4.346960544586182,
      "learning_rate": 4.5627943485086345e-05,
      "loss": 0.6859,
      "step": 445600
    },
    {
      "epoch": 6.996860282574568,
      "grad_norm": 3.7239112854003906,
      "learning_rate": 4.5626962323390896e-05,
      "loss": 0.6485,
      "step": 445700
    },
    {
      "epoch": 6.998430141287284,
      "grad_norm": 5.153891086578369,
      "learning_rate": 4.562598116169545e-05,
      "loss": 0.6672,
      "step": 445800
    },
    {
      "epoch": 7.0,
      "grad_norm": 3.1124415397644043,
      "learning_rate": 4.5625e-05,
      "loss": 0.6548,
      "step": 445900
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.032477617263794,
      "eval_runtime": 15.2417,
      "eval_samples_per_second": 219.989,
      "eval_steps_per_second": 219.989,
      "step": 445900
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.5122681856155396,
      "eval_runtime": 281.4912,
      "eval_samples_per_second": 226.295,
      "eval_steps_per_second": 226.295,
      "step": 445900
    },
    {
      "epoch": 7.001569858712716,
      "grad_norm": 3.783388614654541,
      "learning_rate": 4.5624018838304555e-05,
      "loss": 0.682,
      "step": 446000
    },
    {
      "epoch": 7.003139717425432,
      "grad_norm": 2.8153388500213623,
      "learning_rate": 4.5623037676609106e-05,
      "loss": 0.6481,
      "step": 446100
    },
    {
      "epoch": 7.004709576138148,
      "grad_norm": 3.6206343173980713,
      "learning_rate": 4.562205651491366e-05,
      "loss": 0.6389,
      "step": 446200
    },
    {
      "epoch": 7.006279434850863,
      "grad_norm": 4.037407398223877,
      "learning_rate": 4.5621075353218215e-05,
      "loss": 0.6258,
      "step": 446300
    },
    {
      "epoch": 7.007849293563579,
      "grad_norm": 4.097532272338867,
      "learning_rate": 4.5620094191522766e-05,
      "loss": 0.635,
      "step": 446400
    },
    {
      "epoch": 7.009419152276295,
      "grad_norm": 4.634883403778076,
      "learning_rate": 4.561911302982732e-05,
      "loss": 0.6835,
      "step": 446500
    },
    {
      "epoch": 7.010989010989011,
      "grad_norm": 4.377307415008545,
      "learning_rate": 4.561813186813187e-05,
      "loss": 0.6347,
      "step": 446600
    },
    {
      "epoch": 7.012558869701727,
      "grad_norm": 3.480782985687256,
      "learning_rate": 4.5617150706436426e-05,
      "loss": 0.6917,
      "step": 446700
    },
    {
      "epoch": 7.014128728414443,
      "grad_norm": 3.6614880561828613,
      "learning_rate": 4.561616954474098e-05,
      "loss": 0.6335,
      "step": 446800
    },
    {
      "epoch": 7.015698587127159,
      "grad_norm": 4.266921520233154,
      "learning_rate": 4.561518838304553e-05,
      "loss": 0.6342,
      "step": 446900
    },
    {
      "epoch": 7.017268445839874,
      "grad_norm": 3.910398244857788,
      "learning_rate": 4.561420722135008e-05,
      "loss": 0.6201,
      "step": 447000
    },
    {
      "epoch": 7.01883830455259,
      "grad_norm": 4.898576736450195,
      "learning_rate": 4.5613226059654637e-05,
      "loss": 0.6818,
      "step": 447100
    },
    {
      "epoch": 7.020408163265306,
      "grad_norm": 3.980654001235962,
      "learning_rate": 4.561224489795918e-05,
      "loss": 0.6453,
      "step": 447200
    },
    {
      "epoch": 7.021978021978022,
      "grad_norm": 4.148647785186768,
      "learning_rate": 4.561126373626374e-05,
      "loss": 0.6118,
      "step": 447300
    },
    {
      "epoch": 7.023547880690738,
      "grad_norm": 4.684995174407959,
      "learning_rate": 4.561028257456829e-05,
      "loss": 0.6246,
      "step": 447400
    },
    {
      "epoch": 7.025117739403454,
      "grad_norm": 3.809290647506714,
      "learning_rate": 4.560930141287285e-05,
      "loss": 0.6372,
      "step": 447500
    },
    {
      "epoch": 7.02668759811617,
      "grad_norm": 4.507391929626465,
      "learning_rate": 4.560832025117739e-05,
      "loss": 0.6407,
      "step": 447600
    },
    {
      "epoch": 7.028257456828886,
      "grad_norm": 3.974778652191162,
      "learning_rate": 4.560733908948195e-05,
      "loss": 0.6176,
      "step": 447700
    },
    {
      "epoch": 7.029827315541601,
      "grad_norm": 3.4328134059906006,
      "learning_rate": 4.56063579277865e-05,
      "loss": 0.6474,
      "step": 447800
    },
    {
      "epoch": 7.031397174254317,
      "grad_norm": 3.684251546859741,
      "learning_rate": 4.560537676609105e-05,
      "loss": 0.6226,
      "step": 447900
    },
    {
      "epoch": 7.032967032967033,
      "grad_norm": 3.6001839637756348,
      "learning_rate": 4.56043956043956e-05,
      "loss": 0.5817,
      "step": 448000
    },
    {
      "epoch": 7.034536891679749,
      "grad_norm": 4.2871623039245605,
      "learning_rate": 4.560341444270016e-05,
      "loss": 0.6605,
      "step": 448100
    },
    {
      "epoch": 7.036106750392465,
      "grad_norm": 3.787943124771118,
      "learning_rate": 4.560243328100471e-05,
      "loss": 0.6894,
      "step": 448200
    },
    {
      "epoch": 7.037676609105181,
      "grad_norm": 3.056819438934326,
      "learning_rate": 4.560145211930926e-05,
      "loss": 0.6511,
      "step": 448300
    },
    {
      "epoch": 7.039246467817897,
      "grad_norm": 3.7058939933776855,
      "learning_rate": 4.560047095761382e-05,
      "loss": 0.6799,
      "step": 448400
    },
    {
      "epoch": 7.040816326530612,
      "grad_norm": 4.442514419555664,
      "learning_rate": 4.559948979591837e-05,
      "loss": 0.6589,
      "step": 448500
    },
    {
      "epoch": 7.042386185243328,
      "grad_norm": 4.401740550994873,
      "learning_rate": 4.559850863422292e-05,
      "loss": 0.6615,
      "step": 448600
    },
    {
      "epoch": 7.043956043956044,
      "grad_norm": 3.6890974044799805,
      "learning_rate": 4.559752747252747e-05,
      "loss": 0.7075,
      "step": 448700
    },
    {
      "epoch": 7.04552590266876,
      "grad_norm": 4.103818416595459,
      "learning_rate": 4.559654631083203e-05,
      "loss": 0.6331,
      "step": 448800
    },
    {
      "epoch": 7.047095761381476,
      "grad_norm": 3.8877649307250977,
      "learning_rate": 4.559556514913658e-05,
      "loss": 0.6334,
      "step": 448900
    },
    {
      "epoch": 7.048665620094192,
      "grad_norm": 4.039058208465576,
      "learning_rate": 4.559458398744113e-05,
      "loss": 0.6524,
      "step": 449000
    },
    {
      "epoch": 7.050235478806908,
      "grad_norm": 3.592456102371216,
      "learning_rate": 4.559360282574568e-05,
      "loss": 0.617,
      "step": 449100
    },
    {
      "epoch": 7.051805337519623,
      "grad_norm": 4.332650184631348,
      "learning_rate": 4.559262166405024e-05,
      "loss": 0.6576,
      "step": 449200
    },
    {
      "epoch": 7.053375196232339,
      "grad_norm": 4.932799339294434,
      "learning_rate": 4.5591640502354785e-05,
      "loss": 0.6663,
      "step": 449300
    },
    {
      "epoch": 7.054945054945055,
      "grad_norm": 3.992286205291748,
      "learning_rate": 4.559065934065934e-05,
      "loss": 0.6347,
      "step": 449400
    },
    {
      "epoch": 7.056514913657771,
      "grad_norm": 3.7790536880493164,
      "learning_rate": 4.5589678178963894e-05,
      "loss": 0.6089,
      "step": 449500
    },
    {
      "epoch": 7.058084772370487,
      "grad_norm": 3.7350618839263916,
      "learning_rate": 4.558869701726845e-05,
      "loss": 0.6407,
      "step": 449600
    },
    {
      "epoch": 7.059654631083203,
      "grad_norm": 3.5086886882781982,
      "learning_rate": 4.5587715855572996e-05,
      "loss": 0.668,
      "step": 449700
    },
    {
      "epoch": 7.061224489795919,
      "grad_norm": 5.327637195587158,
      "learning_rate": 4.5586734693877554e-05,
      "loss": 0.695,
      "step": 449800
    },
    {
      "epoch": 7.062794348508635,
      "grad_norm": 4.867947101593018,
      "learning_rate": 4.5585753532182105e-05,
      "loss": 0.5825,
      "step": 449900
    },
    {
      "epoch": 7.06436420722135,
      "grad_norm": 4.154505729675293,
      "learning_rate": 4.5584772370486656e-05,
      "loss": 0.692,
      "step": 450000
    },
    {
      "epoch": 7.065934065934066,
      "grad_norm": 3.6984362602233887,
      "learning_rate": 4.558379120879121e-05,
      "loss": 0.6395,
      "step": 450100
    },
    {
      "epoch": 7.067503924646782,
      "grad_norm": 4.075893878936768,
      "learning_rate": 4.5582810047095764e-05,
      "loss": 0.6575,
      "step": 450200
    },
    {
      "epoch": 7.069073783359498,
      "grad_norm": 5.353878021240234,
      "learning_rate": 4.5581828885400315e-05,
      "loss": 0.6323,
      "step": 450300
    },
    {
      "epoch": 7.070643642072214,
      "grad_norm": 5.016134738922119,
      "learning_rate": 4.5580847723704866e-05,
      "loss": 0.6364,
      "step": 450400
    },
    {
      "epoch": 7.07221350078493,
      "grad_norm": 4.601731300354004,
      "learning_rate": 4.5579866562009424e-05,
      "loss": 0.6073,
      "step": 450500
    },
    {
      "epoch": 7.073783359497646,
      "grad_norm": 2.831864595413208,
      "learning_rate": 4.5578885400313975e-05,
      "loss": 0.6411,
      "step": 450600
    },
    {
      "epoch": 7.075353218210361,
      "grad_norm": 3.1437957286834717,
      "learning_rate": 4.5577904238618526e-05,
      "loss": 0.6512,
      "step": 450700
    },
    {
      "epoch": 7.076923076923077,
      "grad_norm": 4.868432521820068,
      "learning_rate": 4.557692307692308e-05,
      "loss": 0.6459,
      "step": 450800
    },
    {
      "epoch": 7.078492935635793,
      "grad_norm": 4.3444623947143555,
      "learning_rate": 4.5575941915227635e-05,
      "loss": 0.654,
      "step": 450900
    },
    {
      "epoch": 7.080062794348509,
      "grad_norm": 4.620891094207764,
      "learning_rate": 4.5574960753532186e-05,
      "loss": 0.6338,
      "step": 451000
    },
    {
      "epoch": 7.081632653061225,
      "grad_norm": 3.6541121006011963,
      "learning_rate": 4.557397959183674e-05,
      "loss": 0.6238,
      "step": 451100
    },
    {
      "epoch": 7.083202511773941,
      "grad_norm": 3.839050054550171,
      "learning_rate": 4.557299843014129e-05,
      "loss": 0.6514,
      "step": 451200
    },
    {
      "epoch": 7.0847723704866565,
      "grad_norm": 3.94492769241333,
      "learning_rate": 4.5572017268445846e-05,
      "loss": 0.6614,
      "step": 451300
    },
    {
      "epoch": 7.086342229199372,
      "grad_norm": 3.5937318801879883,
      "learning_rate": 4.557103610675039e-05,
      "loss": 0.6699,
      "step": 451400
    },
    {
      "epoch": 7.087912087912088,
      "grad_norm": 4.4147114753723145,
      "learning_rate": 4.557005494505495e-05,
      "loss": 0.6625,
      "step": 451500
    },
    {
      "epoch": 7.089481946624804,
      "grad_norm": 4.566666126251221,
      "learning_rate": 4.55690737833595e-05,
      "loss": 0.6425,
      "step": 451600
    },
    {
      "epoch": 7.0910518053375196,
      "grad_norm": 4.342640399932861,
      "learning_rate": 4.5568092621664056e-05,
      "loss": 0.6527,
      "step": 451700
    },
    {
      "epoch": 7.0926216640502355,
      "grad_norm": 4.137521266937256,
      "learning_rate": 4.55671114599686e-05,
      "loss": 0.6576,
      "step": 451800
    },
    {
      "epoch": 7.0941915227629515,
      "grad_norm": 2.638197422027588,
      "learning_rate": 4.556613029827316e-05,
      "loss": 0.6562,
      "step": 451900
    },
    {
      "epoch": 7.0957613814756675,
      "grad_norm": 3.155062198638916,
      "learning_rate": 4.556514913657771e-05,
      "loss": 0.6263,
      "step": 452000
    },
    {
      "epoch": 7.0973312401883835,
      "grad_norm": 2.8020145893096924,
      "learning_rate": 4.556416797488226e-05,
      "loss": 0.6395,
      "step": 452100
    },
    {
      "epoch": 7.0989010989010985,
      "grad_norm": 3.7867136001586914,
      "learning_rate": 4.556318681318681e-05,
      "loss": 0.6042,
      "step": 452200
    },
    {
      "epoch": 7.1004709576138145,
      "grad_norm": 4.144199848175049,
      "learning_rate": 4.556220565149137e-05,
      "loss": 0.6753,
      "step": 452300
    },
    {
      "epoch": 7.1020408163265305,
      "grad_norm": 3.893301248550415,
      "learning_rate": 4.556122448979592e-05,
      "loss": 0.6554,
      "step": 452400
    },
    {
      "epoch": 7.1036106750392465,
      "grad_norm": 5.076624393463135,
      "learning_rate": 4.556024332810047e-05,
      "loss": 0.6562,
      "step": 452500
    },
    {
      "epoch": 7.1051805337519625,
      "grad_norm": 4.4472527503967285,
      "learning_rate": 4.555926216640503e-05,
      "loss": 0.6412,
      "step": 452600
    },
    {
      "epoch": 7.106750392464678,
      "grad_norm": 3.6717658042907715,
      "learning_rate": 4.555828100470958e-05,
      "loss": 0.6466,
      "step": 452700
    },
    {
      "epoch": 7.108320251177394,
      "grad_norm": 4.390102386474609,
      "learning_rate": 4.555729984301413e-05,
      "loss": 0.6722,
      "step": 452800
    },
    {
      "epoch": 7.1098901098901095,
      "grad_norm": 4.303506851196289,
      "learning_rate": 4.555631868131868e-05,
      "loss": 0.648,
      "step": 452900
    },
    {
      "epoch": 7.1114599686028255,
      "grad_norm": 4.223278999328613,
      "learning_rate": 4.555533751962324e-05,
      "loss": 0.6173,
      "step": 453000
    },
    {
      "epoch": 7.1130298273155415,
      "grad_norm": 4.585992813110352,
      "learning_rate": 4.555435635792779e-05,
      "loss": 0.6882,
      "step": 453100
    },
    {
      "epoch": 7.114599686028257,
      "grad_norm": 4.6158671379089355,
      "learning_rate": 4.555337519623234e-05,
      "loss": 0.6561,
      "step": 453200
    },
    {
      "epoch": 7.116169544740973,
      "grad_norm": 3.796340227127075,
      "learning_rate": 4.555239403453689e-05,
      "loss": 0.6655,
      "step": 453300
    },
    {
      "epoch": 7.117739403453689,
      "grad_norm": 4.5097784996032715,
      "learning_rate": 4.555141287284145e-05,
      "loss": 0.636,
      "step": 453400
    },
    {
      "epoch": 7.119309262166405,
      "grad_norm": 4.579285144805908,
      "learning_rate": 4.5550431711145994e-05,
      "loss": 0.6654,
      "step": 453500
    },
    {
      "epoch": 7.1208791208791204,
      "grad_norm": 3.6261069774627686,
      "learning_rate": 4.554945054945055e-05,
      "loss": 0.6947,
      "step": 453600
    },
    {
      "epoch": 7.122448979591836,
      "grad_norm": 4.214444160461426,
      "learning_rate": 4.55484693877551e-05,
      "loss": 0.6528,
      "step": 453700
    },
    {
      "epoch": 7.124018838304552,
      "grad_norm": 2.657478094100952,
      "learning_rate": 4.554748822605966e-05,
      "loss": 0.6758,
      "step": 453800
    },
    {
      "epoch": 7.125588697017268,
      "grad_norm": 2.0179569721221924,
      "learning_rate": 4.5546507064364205e-05,
      "loss": 0.6699,
      "step": 453900
    },
    {
      "epoch": 7.127158555729984,
      "grad_norm": 2.871246576309204,
      "learning_rate": 4.554552590266876e-05,
      "loss": 0.6322,
      "step": 454000
    },
    {
      "epoch": 7.1287284144427,
      "grad_norm": 3.4957590103149414,
      "learning_rate": 4.5544544740973314e-05,
      "loss": 0.66,
      "step": 454100
    },
    {
      "epoch": 7.130298273155416,
      "grad_norm": 5.723478317260742,
      "learning_rate": 4.5543563579277865e-05,
      "loss": 0.7091,
      "step": 454200
    },
    {
      "epoch": 7.131868131868132,
      "grad_norm": 4.352814674377441,
      "learning_rate": 4.5542582417582416e-05,
      "loss": 0.6533,
      "step": 454300
    },
    {
      "epoch": 7.133437990580847,
      "grad_norm": 3.700563669204712,
      "learning_rate": 4.5541601255886973e-05,
      "loss": 0.6628,
      "step": 454400
    },
    {
      "epoch": 7.135007849293563,
      "grad_norm": 4.111011505126953,
      "learning_rate": 4.5540620094191524e-05,
      "loss": 0.6635,
      "step": 454500
    },
    {
      "epoch": 7.136577708006279,
      "grad_norm": 4.412389755249023,
      "learning_rate": 4.5539638932496075e-05,
      "loss": 0.6569,
      "step": 454600
    },
    {
      "epoch": 7.138147566718995,
      "grad_norm": 4.69755220413208,
      "learning_rate": 4.553865777080063e-05,
      "loss": 0.649,
      "step": 454700
    },
    {
      "epoch": 7.139717425431711,
      "grad_norm": 4.983943939208984,
      "learning_rate": 4.5537676609105184e-05,
      "loss": 0.6771,
      "step": 454800
    },
    {
      "epoch": 7.141287284144427,
      "grad_norm": 4.464947700500488,
      "learning_rate": 4.5536695447409735e-05,
      "loss": 0.672,
      "step": 454900
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 3.4336624145507812,
      "learning_rate": 4.5535714285714286e-05,
      "loss": 0.626,
      "step": 455000
    },
    {
      "epoch": 7.144427001569858,
      "grad_norm": 3.8764636516571045,
      "learning_rate": 4.5534733124018844e-05,
      "loss": 0.6509,
      "step": 455100
    },
    {
      "epoch": 7.145996860282574,
      "grad_norm": 3.530121326446533,
      "learning_rate": 4.5533751962323395e-05,
      "loss": 0.6664,
      "step": 455200
    },
    {
      "epoch": 7.14756671899529,
      "grad_norm": 4.1399827003479,
      "learning_rate": 4.5532770800627946e-05,
      "loss": 0.6553,
      "step": 455300
    },
    {
      "epoch": 7.149136577708006,
      "grad_norm": 2.6951076984405518,
      "learning_rate": 4.55317896389325e-05,
      "loss": 0.6121,
      "step": 455400
    },
    {
      "epoch": 7.150706436420722,
      "grad_norm": 4.4756317138671875,
      "learning_rate": 4.5530808477237055e-05,
      "loss": 0.6954,
      "step": 455500
    },
    {
      "epoch": 7.152276295133438,
      "grad_norm": 3.922863483428955,
      "learning_rate": 4.55298273155416e-05,
      "loss": 0.6694,
      "step": 455600
    },
    {
      "epoch": 7.153846153846154,
      "grad_norm": 3.9857354164123535,
      "learning_rate": 4.5528846153846157e-05,
      "loss": 0.6189,
      "step": 455700
    },
    {
      "epoch": 7.155416012558869,
      "grad_norm": 3.4237496852874756,
      "learning_rate": 4.552786499215071e-05,
      "loss": 0.6342,
      "step": 455800
    },
    {
      "epoch": 7.156985871271585,
      "grad_norm": 3.473259449005127,
      "learning_rate": 4.5526883830455265e-05,
      "loss": 0.6133,
      "step": 455900
    },
    {
      "epoch": 7.158555729984301,
      "grad_norm": 2.467921018600464,
      "learning_rate": 4.552590266875981e-05,
      "loss": 0.6348,
      "step": 456000
    },
    {
      "epoch": 7.160125588697017,
      "grad_norm": 3.2813143730163574,
      "learning_rate": 4.552492150706437e-05,
      "loss": 0.6099,
      "step": 456100
    },
    {
      "epoch": 7.161695447409733,
      "grad_norm": 3.8727526664733887,
      "learning_rate": 4.552394034536892e-05,
      "loss": 0.6592,
      "step": 456200
    },
    {
      "epoch": 7.163265306122449,
      "grad_norm": 3.044179916381836,
      "learning_rate": 4.552295918367347e-05,
      "loss": 0.6538,
      "step": 456300
    },
    {
      "epoch": 7.164835164835165,
      "grad_norm": 3.7352852821350098,
      "learning_rate": 4.552197802197802e-05,
      "loss": 0.6705,
      "step": 456400
    },
    {
      "epoch": 7.166405023547881,
      "grad_norm": 4.0863823890686035,
      "learning_rate": 4.552099686028258e-05,
      "loss": 0.6262,
      "step": 456500
    },
    {
      "epoch": 7.167974882260596,
      "grad_norm": 3.249206781387329,
      "learning_rate": 4.552001569858713e-05,
      "loss": 0.6463,
      "step": 456600
    },
    {
      "epoch": 7.169544740973312,
      "grad_norm": 4.296398639678955,
      "learning_rate": 4.551903453689168e-05,
      "loss": 0.6203,
      "step": 456700
    },
    {
      "epoch": 7.171114599686028,
      "grad_norm": 4.366339206695557,
      "learning_rate": 4.551805337519624e-05,
      "loss": 0.6387,
      "step": 456800
    },
    {
      "epoch": 7.172684458398744,
      "grad_norm": 3.6365606784820557,
      "learning_rate": 4.551707221350079e-05,
      "loss": 0.6403,
      "step": 456900
    },
    {
      "epoch": 7.17425431711146,
      "grad_norm": 4.02263069152832,
      "learning_rate": 4.551609105180534e-05,
      "loss": 0.6614,
      "step": 457000
    },
    {
      "epoch": 7.175824175824176,
      "grad_norm": 3.576890468597412,
      "learning_rate": 4.551510989010989e-05,
      "loss": 0.6496,
      "step": 457100
    },
    {
      "epoch": 7.177394034536892,
      "grad_norm": 4.5338664054870605,
      "learning_rate": 4.551412872841445e-05,
      "loss": 0.6211,
      "step": 457200
    },
    {
      "epoch": 7.178963893249607,
      "grad_norm": 2.9071195125579834,
      "learning_rate": 4.5513147566719e-05,
      "loss": 0.6478,
      "step": 457300
    },
    {
      "epoch": 7.180533751962323,
      "grad_norm": 4.878489971160889,
      "learning_rate": 4.551216640502355e-05,
      "loss": 0.6426,
      "step": 457400
    },
    {
      "epoch": 7.182103610675039,
      "grad_norm": 4.731039524078369,
      "learning_rate": 4.55111852433281e-05,
      "loss": 0.6365,
      "step": 457500
    },
    {
      "epoch": 7.183673469387755,
      "grad_norm": 3.954954147338867,
      "learning_rate": 4.551020408163266e-05,
      "loss": 0.6729,
      "step": 457600
    },
    {
      "epoch": 7.185243328100471,
      "grad_norm": 4.050992012023926,
      "learning_rate": 4.55092229199372e-05,
      "loss": 0.6423,
      "step": 457700
    },
    {
      "epoch": 7.186813186813187,
      "grad_norm": 4.051702499389648,
      "learning_rate": 4.550824175824176e-05,
      "loss": 0.6299,
      "step": 457800
    },
    {
      "epoch": 7.188383045525903,
      "grad_norm": 3.257516622543335,
      "learning_rate": 4.550726059654631e-05,
      "loss": 0.7027,
      "step": 457900
    },
    {
      "epoch": 7.189952904238618,
      "grad_norm": 2.8242087364196777,
      "learning_rate": 4.550627943485087e-05,
      "loss": 0.6272,
      "step": 458000
    },
    {
      "epoch": 7.191522762951334,
      "grad_norm": 4.642301559448242,
      "learning_rate": 4.5505298273155414e-05,
      "loss": 0.6754,
      "step": 458100
    },
    {
      "epoch": 7.19309262166405,
      "grad_norm": 3.6992502212524414,
      "learning_rate": 4.550431711145997e-05,
      "loss": 0.6292,
      "step": 458200
    },
    {
      "epoch": 7.194662480376766,
      "grad_norm": 3.3661961555480957,
      "learning_rate": 4.550333594976452e-05,
      "loss": 0.625,
      "step": 458300
    },
    {
      "epoch": 7.196232339089482,
      "grad_norm": 3.424487590789795,
      "learning_rate": 4.5502354788069074e-05,
      "loss": 0.6506,
      "step": 458400
    },
    {
      "epoch": 7.197802197802198,
      "grad_norm": 4.2632155418396,
      "learning_rate": 4.5501373626373625e-05,
      "loss": 0.6601,
      "step": 458500
    },
    {
      "epoch": 7.199372056514914,
      "grad_norm": 3.0186636447906494,
      "learning_rate": 4.550039246467818e-05,
      "loss": 0.6076,
      "step": 458600
    },
    {
      "epoch": 7.20094191522763,
      "grad_norm": 4.644141674041748,
      "learning_rate": 4.549941130298273e-05,
      "loss": 0.6539,
      "step": 458700
    },
    {
      "epoch": 7.202511773940345,
      "grad_norm": 4.827728271484375,
      "learning_rate": 4.5498430141287284e-05,
      "loss": 0.67,
      "step": 458800
    },
    {
      "epoch": 7.204081632653061,
      "grad_norm": 4.727623462677002,
      "learning_rate": 4.549744897959184e-05,
      "loss": 0.6532,
      "step": 458900
    },
    {
      "epoch": 7.205651491365777,
      "grad_norm": 3.4934921264648438,
      "learning_rate": 4.549646781789639e-05,
      "loss": 0.6537,
      "step": 459000
    },
    {
      "epoch": 7.207221350078493,
      "grad_norm": 2.5027995109558105,
      "learning_rate": 4.5495486656200944e-05,
      "loss": 0.6649,
      "step": 459100
    },
    {
      "epoch": 7.208791208791209,
      "grad_norm": 4.366940975189209,
      "learning_rate": 4.5494505494505495e-05,
      "loss": 0.638,
      "step": 459200
    },
    {
      "epoch": 7.210361067503925,
      "grad_norm": 4.606823444366455,
      "learning_rate": 4.549352433281005e-05,
      "loss": 0.6763,
      "step": 459300
    },
    {
      "epoch": 7.211930926216641,
      "grad_norm": 4.167636871337891,
      "learning_rate": 4.5492543171114604e-05,
      "loss": 0.6645,
      "step": 459400
    },
    {
      "epoch": 7.213500784929356,
      "grad_norm": 3.5406711101531982,
      "learning_rate": 4.5491562009419155e-05,
      "loss": 0.6598,
      "step": 459500
    },
    {
      "epoch": 7.215070643642072,
      "grad_norm": 3.342681884765625,
      "learning_rate": 4.5490580847723706e-05,
      "loss": 0.6666,
      "step": 459600
    },
    {
      "epoch": 7.216640502354788,
      "grad_norm": 4.7269392013549805,
      "learning_rate": 4.5489599686028264e-05,
      "loss": 0.6414,
      "step": 459700
    },
    {
      "epoch": 7.218210361067504,
      "grad_norm": 4.263563632965088,
      "learning_rate": 4.548861852433281e-05,
      "loss": 0.654,
      "step": 459800
    },
    {
      "epoch": 7.21978021978022,
      "grad_norm": 4.498409748077393,
      "learning_rate": 4.5487637362637365e-05,
      "loss": 0.7082,
      "step": 459900
    },
    {
      "epoch": 7.221350078492936,
      "grad_norm": 4.60196590423584,
      "learning_rate": 4.5486656200941916e-05,
      "loss": 0.6457,
      "step": 460000
    },
    {
      "epoch": 7.222919937205652,
      "grad_norm": 2.922403335571289,
      "learning_rate": 4.5485675039246474e-05,
      "loss": 0.6698,
      "step": 460100
    },
    {
      "epoch": 7.224489795918367,
      "grad_norm": 3.84700608253479,
      "learning_rate": 4.548469387755102e-05,
      "loss": 0.6452,
      "step": 460200
    },
    {
      "epoch": 7.226059654631083,
      "grad_norm": 3.9126453399658203,
      "learning_rate": 4.5483712715855576e-05,
      "loss": 0.6721,
      "step": 460300
    },
    {
      "epoch": 7.227629513343799,
      "grad_norm": 4.218962669372559,
      "learning_rate": 4.548273155416013e-05,
      "loss": 0.6065,
      "step": 460400
    },
    {
      "epoch": 7.229199372056515,
      "grad_norm": 3.10908579826355,
      "learning_rate": 4.548175039246468e-05,
      "loss": 0.6614,
      "step": 460500
    },
    {
      "epoch": 7.230769230769231,
      "grad_norm": 4.590944766998291,
      "learning_rate": 4.548076923076923e-05,
      "loss": 0.5926,
      "step": 460600
    },
    {
      "epoch": 7.232339089481947,
      "grad_norm": 4.204300880432129,
      "learning_rate": 4.547978806907379e-05,
      "loss": 0.705,
      "step": 460700
    },
    {
      "epoch": 7.233908948194663,
      "grad_norm": 4.49310302734375,
      "learning_rate": 4.547880690737834e-05,
      "loss": 0.6286,
      "step": 460800
    },
    {
      "epoch": 7.235478806907379,
      "grad_norm": 4.148470401763916,
      "learning_rate": 4.547782574568289e-05,
      "loss": 0.7093,
      "step": 460900
    },
    {
      "epoch": 7.237048665620094,
      "grad_norm": 4.510499000549316,
      "learning_rate": 4.5476844583987447e-05,
      "loss": 0.7037,
      "step": 461000
    },
    {
      "epoch": 7.23861852433281,
      "grad_norm": 3.500936985015869,
      "learning_rate": 4.5475863422292e-05,
      "loss": 0.665,
      "step": 461100
    },
    {
      "epoch": 7.240188383045526,
      "grad_norm": 4.545742034912109,
      "learning_rate": 4.547488226059655e-05,
      "loss": 0.582,
      "step": 461200
    },
    {
      "epoch": 7.241758241758242,
      "grad_norm": 4.989836692810059,
      "learning_rate": 4.54739010989011e-05,
      "loss": 0.6984,
      "step": 461300
    },
    {
      "epoch": 7.243328100470958,
      "grad_norm": 3.6931962966918945,
      "learning_rate": 4.547291993720566e-05,
      "loss": 0.6588,
      "step": 461400
    },
    {
      "epoch": 7.244897959183674,
      "grad_norm": 3.5564870834350586,
      "learning_rate": 4.547193877551021e-05,
      "loss": 0.6076,
      "step": 461500
    },
    {
      "epoch": 7.24646781789639,
      "grad_norm": 3.5642995834350586,
      "learning_rate": 4.547095761381476e-05,
      "loss": 0.6751,
      "step": 461600
    },
    {
      "epoch": 7.248037676609105,
      "grad_norm": 4.239251613616943,
      "learning_rate": 4.546997645211931e-05,
      "loss": 0.6456,
      "step": 461700
    },
    {
      "epoch": 7.249607535321821,
      "grad_norm": 4.786087512969971,
      "learning_rate": 4.546899529042387e-05,
      "loss": 0.6246,
      "step": 461800
    },
    {
      "epoch": 7.251177394034537,
      "grad_norm": 3.7197821140289307,
      "learning_rate": 4.546801412872841e-05,
      "loss": 0.6724,
      "step": 461900
    },
    {
      "epoch": 7.252747252747253,
      "grad_norm": 3.5612001419067383,
      "learning_rate": 4.546703296703297e-05,
      "loss": 0.6735,
      "step": 462000
    },
    {
      "epoch": 7.254317111459969,
      "grad_norm": 4.019781112670898,
      "learning_rate": 4.546605180533752e-05,
      "loss": 0.6506,
      "step": 462100
    },
    {
      "epoch": 7.255886970172685,
      "grad_norm": 3.6256351470947266,
      "learning_rate": 4.546507064364208e-05,
      "loss": 0.5972,
      "step": 462200
    },
    {
      "epoch": 7.257456828885401,
      "grad_norm": 3.789149045944214,
      "learning_rate": 4.546408948194662e-05,
      "loss": 0.6421,
      "step": 462300
    },
    {
      "epoch": 7.259026687598116,
      "grad_norm": 2.6792960166931152,
      "learning_rate": 4.546310832025118e-05,
      "loss": 0.6595,
      "step": 462400
    },
    {
      "epoch": 7.260596546310832,
      "grad_norm": 3.761608839035034,
      "learning_rate": 4.546212715855573e-05,
      "loss": 0.6129,
      "step": 462500
    },
    {
      "epoch": 7.262166405023548,
      "grad_norm": 4.249887943267822,
      "learning_rate": 4.546114599686028e-05,
      "loss": 0.6553,
      "step": 462600
    },
    {
      "epoch": 7.263736263736264,
      "grad_norm": 4.004291534423828,
      "learning_rate": 4.5460164835164834e-05,
      "loss": 0.6157,
      "step": 462700
    },
    {
      "epoch": 7.26530612244898,
      "grad_norm": 3.6143836975097656,
      "learning_rate": 4.545918367346939e-05,
      "loss": 0.621,
      "step": 462800
    },
    {
      "epoch": 7.266875981161696,
      "grad_norm": 3.013557195663452,
      "learning_rate": 4.545820251177394e-05,
      "loss": 0.6572,
      "step": 462900
    },
    {
      "epoch": 7.268445839874412,
      "grad_norm": 3.878002405166626,
      "learning_rate": 4.545722135007849e-05,
      "loss": 0.6542,
      "step": 463000
    },
    {
      "epoch": 7.270015698587127,
      "grad_norm": 3.3143861293792725,
      "learning_rate": 4.545624018838305e-05,
      "loss": 0.674,
      "step": 463100
    },
    {
      "epoch": 7.271585557299843,
      "grad_norm": 3.548609733581543,
      "learning_rate": 4.54552590266876e-05,
      "loss": 0.6887,
      "step": 463200
    },
    {
      "epoch": 7.273155416012559,
      "grad_norm": 3.960719585418701,
      "learning_rate": 4.545427786499215e-05,
      "loss": 0.641,
      "step": 463300
    },
    {
      "epoch": 7.274725274725275,
      "grad_norm": 3.7682788372039795,
      "learning_rate": 4.5453296703296704e-05,
      "loss": 0.6848,
      "step": 463400
    },
    {
      "epoch": 7.276295133437991,
      "grad_norm": 3.889812469482422,
      "learning_rate": 4.545231554160126e-05,
      "loss": 0.6744,
      "step": 463500
    },
    {
      "epoch": 7.277864992150707,
      "grad_norm": 3.151843547821045,
      "learning_rate": 4.545133437990581e-05,
      "loss": 0.6422,
      "step": 463600
    },
    {
      "epoch": 7.279434850863423,
      "grad_norm": 4.18883752822876,
      "learning_rate": 4.5450353218210364e-05,
      "loss": 0.633,
      "step": 463700
    },
    {
      "epoch": 7.2810047095761385,
      "grad_norm": 4.392208576202393,
      "learning_rate": 4.5449372056514915e-05,
      "loss": 0.6596,
      "step": 463800
    },
    {
      "epoch": 7.282574568288854,
      "grad_norm": 3.9663162231445312,
      "learning_rate": 4.544839089481947e-05,
      "loss": 0.636,
      "step": 463900
    },
    {
      "epoch": 7.28414442700157,
      "grad_norm": 4.422399044036865,
      "learning_rate": 4.544740973312402e-05,
      "loss": 0.6358,
      "step": 464000
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 3.892031192779541,
      "learning_rate": 4.5446428571428574e-05,
      "loss": 0.6315,
      "step": 464100
    },
    {
      "epoch": 7.287284144427002,
      "grad_norm": 4.221410751342773,
      "learning_rate": 4.5445447409733125e-05,
      "loss": 0.6544,
      "step": 464200
    },
    {
      "epoch": 7.2888540031397175,
      "grad_norm": 3.7553303241729736,
      "learning_rate": 4.544446624803768e-05,
      "loss": 0.6281,
      "step": 464300
    },
    {
      "epoch": 7.2904238618524335,
      "grad_norm": 4.323093891143799,
      "learning_rate": 4.544348508634223e-05,
      "loss": 0.621,
      "step": 464400
    },
    {
      "epoch": 7.2919937205651495,
      "grad_norm": 4.13469934463501,
      "learning_rate": 4.5442503924646785e-05,
      "loss": 0.6377,
      "step": 464500
    },
    {
      "epoch": 7.293563579277865,
      "grad_norm": 3.4296984672546387,
      "learning_rate": 4.5441522762951336e-05,
      "loss": 0.6741,
      "step": 464600
    },
    {
      "epoch": 7.295133437990581,
      "grad_norm": 3.8404088020324707,
      "learning_rate": 4.544054160125589e-05,
      "loss": 0.6377,
      "step": 464700
    },
    {
      "epoch": 7.2967032967032965,
      "grad_norm": 4.8893232345581055,
      "learning_rate": 4.543956043956044e-05,
      "loss": 0.6608,
      "step": 464800
    },
    {
      "epoch": 7.2982731554160125,
      "grad_norm": 4.361140251159668,
      "learning_rate": 4.5438579277864996e-05,
      "loss": 0.6504,
      "step": 464900
    },
    {
      "epoch": 7.2998430141287285,
      "grad_norm": 4.358987808227539,
      "learning_rate": 4.543759811616955e-05,
      "loss": 0.7222,
      "step": 465000
    },
    {
      "epoch": 7.3014128728414445,
      "grad_norm": 4.108227252960205,
      "learning_rate": 4.54366169544741e-05,
      "loss": 0.6737,
      "step": 465100
    },
    {
      "epoch": 7.3029827315541604,
      "grad_norm": 4.080066204071045,
      "learning_rate": 4.5435635792778656e-05,
      "loss": 0.6235,
      "step": 465200
    },
    {
      "epoch": 7.304552590266876,
      "grad_norm": 4.1577863693237305,
      "learning_rate": 4.5434654631083207e-05,
      "loss": 0.6552,
      "step": 465300
    },
    {
      "epoch": 7.3061224489795915,
      "grad_norm": 3.547325611114502,
      "learning_rate": 4.543367346938776e-05,
      "loss": 0.681,
      "step": 465400
    },
    {
      "epoch": 7.3076923076923075,
      "grad_norm": 3.1527349948883057,
      "learning_rate": 4.543269230769231e-05,
      "loss": 0.5925,
      "step": 465500
    },
    {
      "epoch": 7.3092621664050235,
      "grad_norm": 5.386803150177002,
      "learning_rate": 4.5431711145996866e-05,
      "loss": 0.5923,
      "step": 465600
    },
    {
      "epoch": 7.310832025117739,
      "grad_norm": 2.535693407058716,
      "learning_rate": 4.543072998430142e-05,
      "loss": 0.6548,
      "step": 465700
    },
    {
      "epoch": 7.312401883830455,
      "grad_norm": 4.085943222045898,
      "learning_rate": 4.542974882260597e-05,
      "loss": 0.6395,
      "step": 465800
    },
    {
      "epoch": 7.313971742543171,
      "grad_norm": 4.914498329162598,
      "learning_rate": 4.542876766091052e-05,
      "loss": 0.6373,
      "step": 465900
    },
    {
      "epoch": 7.315541601255887,
      "grad_norm": 3.9976444244384766,
      "learning_rate": 4.542778649921508e-05,
      "loss": 0.6616,
      "step": 466000
    },
    {
      "epoch": 7.3171114599686025,
      "grad_norm": 4.047825336456299,
      "learning_rate": 4.542680533751962e-05,
      "loss": 0.6535,
      "step": 466100
    },
    {
      "epoch": 7.318681318681318,
      "grad_norm": 3.3626389503479004,
      "learning_rate": 4.542582417582418e-05,
      "loss": 0.691,
      "step": 466200
    },
    {
      "epoch": 7.320251177394034,
      "grad_norm": 4.007037162780762,
      "learning_rate": 4.542484301412873e-05,
      "loss": 0.6263,
      "step": 466300
    },
    {
      "epoch": 7.32182103610675,
      "grad_norm": 2.6462738513946533,
      "learning_rate": 4.542386185243328e-05,
      "loss": 0.6329,
      "step": 466400
    },
    {
      "epoch": 7.323390894819466,
      "grad_norm": 2.9676918983459473,
      "learning_rate": 4.542288069073783e-05,
      "loss": 0.6414,
      "step": 466500
    },
    {
      "epoch": 7.324960753532182,
      "grad_norm": 4.113656520843506,
      "learning_rate": 4.542189952904239e-05,
      "loss": 0.7108,
      "step": 466600
    },
    {
      "epoch": 7.326530612244898,
      "grad_norm": 4.742531776428223,
      "learning_rate": 4.542091836734694e-05,
      "loss": 0.6225,
      "step": 466700
    },
    {
      "epoch": 7.328100470957613,
      "grad_norm": 2.807241439819336,
      "learning_rate": 4.541993720565149e-05,
      "loss": 0.61,
      "step": 466800
    },
    {
      "epoch": 7.329670329670329,
      "grad_norm": 3.8619470596313477,
      "learning_rate": 4.541895604395604e-05,
      "loss": 0.6592,
      "step": 466900
    },
    {
      "epoch": 7.331240188383045,
      "grad_norm": 3.636404037475586,
      "learning_rate": 4.54179748822606e-05,
      "loss": 0.6899,
      "step": 467000
    },
    {
      "epoch": 7.332810047095761,
      "grad_norm": 5.231335639953613,
      "learning_rate": 4.541699372056515e-05,
      "loss": 0.6688,
      "step": 467100
    },
    {
      "epoch": 7.334379905808477,
      "grad_norm": 3.812676429748535,
      "learning_rate": 4.54160125588697e-05,
      "loss": 0.6719,
      "step": 467200
    },
    {
      "epoch": 7.335949764521193,
      "grad_norm": 3.7361204624176025,
      "learning_rate": 4.541503139717426e-05,
      "loss": 0.6392,
      "step": 467300
    },
    {
      "epoch": 7.337519623233909,
      "grad_norm": 4.252860069274902,
      "learning_rate": 4.541405023547881e-05,
      "loss": 0.6319,
      "step": 467400
    },
    {
      "epoch": 7.339089481946624,
      "grad_norm": 4.239567279815674,
      "learning_rate": 4.541306907378336e-05,
      "loss": 0.6355,
      "step": 467500
    },
    {
      "epoch": 7.34065934065934,
      "grad_norm": 3.9657375812530518,
      "learning_rate": 4.541208791208791e-05,
      "loss": 0.6686,
      "step": 467600
    },
    {
      "epoch": 7.342229199372056,
      "grad_norm": 3.305171012878418,
      "learning_rate": 4.541110675039247e-05,
      "loss": 0.6655,
      "step": 467700
    },
    {
      "epoch": 7.343799058084772,
      "grad_norm": 3.5907716751098633,
      "learning_rate": 4.5410125588697015e-05,
      "loss": 0.6445,
      "step": 467800
    },
    {
      "epoch": 7.345368916797488,
      "grad_norm": 3.8187410831451416,
      "learning_rate": 4.540914442700157e-05,
      "loss": 0.6559,
      "step": 467900
    },
    {
      "epoch": 7.346938775510204,
      "grad_norm": 3.0889599323272705,
      "learning_rate": 4.5408163265306124e-05,
      "loss": 0.6587,
      "step": 468000
    },
    {
      "epoch": 7.34850863422292,
      "grad_norm": 4.093754291534424,
      "learning_rate": 4.540718210361068e-05,
      "loss": 0.6849,
      "step": 468100
    },
    {
      "epoch": 7.350078492935636,
      "grad_norm": 3.516540765762329,
      "learning_rate": 4.5406200941915226e-05,
      "loss": 0.6639,
      "step": 468200
    },
    {
      "epoch": 7.351648351648351,
      "grad_norm": 4.2896409034729,
      "learning_rate": 4.5405219780219783e-05,
      "loss": 0.6547,
      "step": 468300
    },
    {
      "epoch": 7.353218210361067,
      "grad_norm": 4.003179550170898,
      "learning_rate": 4.5404238618524334e-05,
      "loss": 0.6748,
      "step": 468400
    },
    {
      "epoch": 7.354788069073783,
      "grad_norm": 3.905810832977295,
      "learning_rate": 4.5403257456828885e-05,
      "loss": 0.6538,
      "step": 468500
    },
    {
      "epoch": 7.356357927786499,
      "grad_norm": 4.693512439727783,
      "learning_rate": 4.5402276295133436e-05,
      "loss": 0.6188,
      "step": 468600
    },
    {
      "epoch": 7.357927786499215,
      "grad_norm": 4.339682102203369,
      "learning_rate": 4.5401295133437994e-05,
      "loss": 0.6542,
      "step": 468700
    },
    {
      "epoch": 7.359497645211931,
      "grad_norm": 3.9683094024658203,
      "learning_rate": 4.5400313971742545e-05,
      "loss": 0.6376,
      "step": 468800
    },
    {
      "epoch": 7.361067503924647,
      "grad_norm": 4.3864006996154785,
      "learning_rate": 4.5399332810047096e-05,
      "loss": 0.6182,
      "step": 468900
    },
    {
      "epoch": 7.362637362637362,
      "grad_norm": 3.741731882095337,
      "learning_rate": 4.539835164835165e-05,
      "loss": 0.6449,
      "step": 469000
    },
    {
      "epoch": 7.364207221350078,
      "grad_norm": 3.519243001937866,
      "learning_rate": 4.5397370486656205e-05,
      "loss": 0.6634,
      "step": 469100
    },
    {
      "epoch": 7.365777080062794,
      "grad_norm": 3.913429021835327,
      "learning_rate": 4.5396389324960756e-05,
      "loss": 0.6524,
      "step": 469200
    },
    {
      "epoch": 7.36734693877551,
      "grad_norm": 3.0331814289093018,
      "learning_rate": 4.539540816326531e-05,
      "loss": 0.6491,
      "step": 469300
    },
    {
      "epoch": 7.368916797488226,
      "grad_norm": 4.39706563949585,
      "learning_rate": 4.5394427001569865e-05,
      "loss": 0.6922,
      "step": 469400
    },
    {
      "epoch": 7.370486656200942,
      "grad_norm": 2.9792239665985107,
      "learning_rate": 4.5393445839874416e-05,
      "loss": 0.6293,
      "step": 469500
    },
    {
      "epoch": 7.372056514913658,
      "grad_norm": 4.42266845703125,
      "learning_rate": 4.5392464678178967e-05,
      "loss": 0.6609,
      "step": 469600
    },
    {
      "epoch": 7.373626373626374,
      "grad_norm": 6.428913116455078,
      "learning_rate": 4.539148351648352e-05,
      "loss": 0.629,
      "step": 469700
    },
    {
      "epoch": 7.375196232339089,
      "grad_norm": 3.935105800628662,
      "learning_rate": 4.5390502354788075e-05,
      "loss": 0.635,
      "step": 469800
    },
    {
      "epoch": 7.376766091051805,
      "grad_norm": 4.162189960479736,
      "learning_rate": 4.538952119309262e-05,
      "loss": 0.6289,
      "step": 469900
    },
    {
      "epoch": 7.378335949764521,
      "grad_norm": 3.3749852180480957,
      "learning_rate": 4.538854003139718e-05,
      "loss": 0.6517,
      "step": 470000
    },
    {
      "epoch": 7.379905808477237,
      "grad_norm": 4.207951068878174,
      "learning_rate": 4.538755886970173e-05,
      "loss": 0.6559,
      "step": 470100
    },
    {
      "epoch": 7.381475667189953,
      "grad_norm": 4.971065044403076,
      "learning_rate": 4.5386577708006286e-05,
      "loss": 0.6424,
      "step": 470200
    },
    {
      "epoch": 7.383045525902669,
      "grad_norm": 4.1807026863098145,
      "learning_rate": 4.538559654631083e-05,
      "loss": 0.655,
      "step": 470300
    },
    {
      "epoch": 7.384615384615385,
      "grad_norm": 4.1094536781311035,
      "learning_rate": 4.538461538461539e-05,
      "loss": 0.6594,
      "step": 470400
    },
    {
      "epoch": 7.3861852433281,
      "grad_norm": 3.58874249458313,
      "learning_rate": 4.538363422291994e-05,
      "loss": 0.6246,
      "step": 470500
    },
    {
      "epoch": 7.387755102040816,
      "grad_norm": 4.665841102600098,
      "learning_rate": 4.538265306122449e-05,
      "loss": 0.6667,
      "step": 470600
    },
    {
      "epoch": 7.389324960753532,
      "grad_norm": 3.103484869003296,
      "learning_rate": 4.538167189952904e-05,
      "loss": 0.6363,
      "step": 470700
    },
    {
      "epoch": 7.390894819466248,
      "grad_norm": 3.5496017932891846,
      "learning_rate": 4.53806907378336e-05,
      "loss": 0.6777,
      "step": 470800
    },
    {
      "epoch": 7.392464678178964,
      "grad_norm": 3.9693915843963623,
      "learning_rate": 4.537970957613815e-05,
      "loss": 0.662,
      "step": 470900
    },
    {
      "epoch": 7.39403453689168,
      "grad_norm": 3.8371026515960693,
      "learning_rate": 4.53787284144427e-05,
      "loss": 0.6709,
      "step": 471000
    },
    {
      "epoch": 7.395604395604396,
      "grad_norm": 4.26023530960083,
      "learning_rate": 4.537774725274725e-05,
      "loss": 0.6662,
      "step": 471100
    },
    {
      "epoch": 7.397174254317111,
      "grad_norm": 3.346737861633301,
      "learning_rate": 4.537676609105181e-05,
      "loss": 0.6339,
      "step": 471200
    },
    {
      "epoch": 7.398744113029827,
      "grad_norm": 4.2112932205200195,
      "learning_rate": 4.537578492935636e-05,
      "loss": 0.6047,
      "step": 471300
    },
    {
      "epoch": 7.400313971742543,
      "grad_norm": 3.8139383792877197,
      "learning_rate": 4.537480376766091e-05,
      "loss": 0.6165,
      "step": 471400
    },
    {
      "epoch": 7.401883830455259,
      "grad_norm": 4.257140636444092,
      "learning_rate": 4.537382260596547e-05,
      "loss": 0.6504,
      "step": 471500
    },
    {
      "epoch": 7.403453689167975,
      "grad_norm": 4.199419021606445,
      "learning_rate": 4.537284144427002e-05,
      "loss": 0.663,
      "step": 471600
    },
    {
      "epoch": 7.405023547880691,
      "grad_norm": 4.478989601135254,
      "learning_rate": 4.537186028257457e-05,
      "loss": 0.6685,
      "step": 471700
    },
    {
      "epoch": 7.406593406593407,
      "grad_norm": 3.802480697631836,
      "learning_rate": 4.537087912087912e-05,
      "loss": 0.6094,
      "step": 471800
    },
    {
      "epoch": 7.408163265306122,
      "grad_norm": 3.9003472328186035,
      "learning_rate": 4.536989795918368e-05,
      "loss": 0.6396,
      "step": 471900
    },
    {
      "epoch": 7.409733124018838,
      "grad_norm": 4.06011962890625,
      "learning_rate": 4.5368916797488224e-05,
      "loss": 0.644,
      "step": 472000
    },
    {
      "epoch": 7.411302982731554,
      "grad_norm": 5.070458889007568,
      "learning_rate": 4.536793563579278e-05,
      "loss": 0.617,
      "step": 472100
    },
    {
      "epoch": 7.41287284144427,
      "grad_norm": 4.825700283050537,
      "learning_rate": 4.536695447409733e-05,
      "loss": 0.6792,
      "step": 472200
    },
    {
      "epoch": 7.414442700156986,
      "grad_norm": 3.6212563514709473,
      "learning_rate": 4.536597331240189e-05,
      "loss": 0.6626,
      "step": 472300
    },
    {
      "epoch": 7.416012558869702,
      "grad_norm": 4.982387065887451,
      "learning_rate": 4.5364992150706435e-05,
      "loss": 0.6262,
      "step": 472400
    },
    {
      "epoch": 7.417582417582418,
      "grad_norm": 4.491528034210205,
      "learning_rate": 4.536401098901099e-05,
      "loss": 0.6266,
      "step": 472500
    },
    {
      "epoch": 7.419152276295134,
      "grad_norm": 4.205935955047607,
      "learning_rate": 4.5363029827315543e-05,
      "loss": 0.6468,
      "step": 472600
    },
    {
      "epoch": 7.420722135007849,
      "grad_norm": 4.068990230560303,
      "learning_rate": 4.5362048665620094e-05,
      "loss": 0.6761,
      "step": 472700
    },
    {
      "epoch": 7.422291993720565,
      "grad_norm": 4.216148376464844,
      "learning_rate": 4.5361067503924645e-05,
      "loss": 0.6474,
      "step": 472800
    },
    {
      "epoch": 7.423861852433281,
      "grad_norm": 3.833446741104126,
      "learning_rate": 4.53600863422292e-05,
      "loss": 0.6186,
      "step": 472900
    },
    {
      "epoch": 7.425431711145997,
      "grad_norm": 3.546818256378174,
      "learning_rate": 4.5359105180533754e-05,
      "loss": 0.6595,
      "step": 473000
    },
    {
      "epoch": 7.427001569858713,
      "grad_norm": 2.915801763534546,
      "learning_rate": 4.5358124018838305e-05,
      "loss": 0.6426,
      "step": 473100
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 4.022762298583984,
      "learning_rate": 4.5357142857142856e-05,
      "loss": 0.6633,
      "step": 473200
    },
    {
      "epoch": 7.430141287284145,
      "grad_norm": 3.8851354122161865,
      "learning_rate": 4.5356161695447414e-05,
      "loss": 0.6246,
      "step": 473300
    },
    {
      "epoch": 7.43171114599686,
      "grad_norm": 4.38795280456543,
      "learning_rate": 4.5355180533751965e-05,
      "loss": 0.6424,
      "step": 473400
    },
    {
      "epoch": 7.433281004709576,
      "grad_norm": 2.7769322395324707,
      "learning_rate": 4.5354199372056516e-05,
      "loss": 0.6039,
      "step": 473500
    },
    {
      "epoch": 7.434850863422292,
      "grad_norm": 4.201911926269531,
      "learning_rate": 4.5353218210361074e-05,
      "loss": 0.678,
      "step": 473600
    },
    {
      "epoch": 7.436420722135008,
      "grad_norm": 4.0484466552734375,
      "learning_rate": 4.5352237048665625e-05,
      "loss": 0.6607,
      "step": 473700
    },
    {
      "epoch": 7.437990580847724,
      "grad_norm": 3.8108224868774414,
      "learning_rate": 4.5351255886970176e-05,
      "loss": 0.6349,
      "step": 473800
    },
    {
      "epoch": 7.43956043956044,
      "grad_norm": 4.396463394165039,
      "learning_rate": 4.5350274725274726e-05,
      "loss": 0.6759,
      "step": 473900
    },
    {
      "epoch": 7.441130298273156,
      "grad_norm": 4.635012149810791,
      "learning_rate": 4.5349293563579284e-05,
      "loss": 0.6774,
      "step": 474000
    },
    {
      "epoch": 7.442700156985872,
      "grad_norm": 5.11631441116333,
      "learning_rate": 4.534831240188383e-05,
      "loss": 0.6327,
      "step": 474100
    },
    {
      "epoch": 7.444270015698587,
      "grad_norm": 4.356932640075684,
      "learning_rate": 4.5347331240188386e-05,
      "loss": 0.6095,
      "step": 474200
    },
    {
      "epoch": 7.445839874411303,
      "grad_norm": 3.729050397872925,
      "learning_rate": 4.534635007849294e-05,
      "loss": 0.6345,
      "step": 474300
    },
    {
      "epoch": 7.447409733124019,
      "grad_norm": 4.683655738830566,
      "learning_rate": 4.5345368916797495e-05,
      "loss": 0.6304,
      "step": 474400
    },
    {
      "epoch": 7.448979591836735,
      "grad_norm": 4.921781063079834,
      "learning_rate": 4.534438775510204e-05,
      "loss": 0.598,
      "step": 474500
    },
    {
      "epoch": 7.450549450549451,
      "grad_norm": 3.8415212631225586,
      "learning_rate": 4.53434065934066e-05,
      "loss": 0.6083,
      "step": 474600
    },
    {
      "epoch": 7.452119309262167,
      "grad_norm": 2.6411654949188232,
      "learning_rate": 4.534242543171115e-05,
      "loss": 0.6763,
      "step": 474700
    },
    {
      "epoch": 7.453689167974883,
      "grad_norm": 3.4228641986846924,
      "learning_rate": 4.53414442700157e-05,
      "loss": 0.6687,
      "step": 474800
    },
    {
      "epoch": 7.455259026687598,
      "grad_norm": 3.529489278793335,
      "learning_rate": 4.534046310832025e-05,
      "loss": 0.5951,
      "step": 474900
    },
    {
      "epoch": 7.456828885400314,
      "grad_norm": 4.369470119476318,
      "learning_rate": 4.533948194662481e-05,
      "loss": 0.6607,
      "step": 475000
    },
    {
      "epoch": 7.45839874411303,
      "grad_norm": 2.572601795196533,
      "learning_rate": 4.533850078492936e-05,
      "loss": 0.6828,
      "step": 475100
    },
    {
      "epoch": 7.459968602825746,
      "grad_norm": 3.871875047683716,
      "learning_rate": 4.533751962323391e-05,
      "loss": 0.6836,
      "step": 475200
    },
    {
      "epoch": 7.461538461538462,
      "grad_norm": 3.322723865509033,
      "learning_rate": 4.533653846153846e-05,
      "loss": 0.6538,
      "step": 475300
    },
    {
      "epoch": 7.463108320251178,
      "grad_norm": 5.060608863830566,
      "learning_rate": 4.533555729984302e-05,
      "loss": 0.6618,
      "step": 475400
    },
    {
      "epoch": 7.464678178963894,
      "grad_norm": 3.6595566272735596,
      "learning_rate": 4.533457613814757e-05,
      "loss": 0.6797,
      "step": 475500
    },
    {
      "epoch": 7.466248037676609,
      "grad_norm": 2.43814754486084,
      "learning_rate": 4.533359497645212e-05,
      "loss": 0.6206,
      "step": 475600
    },
    {
      "epoch": 7.467817896389325,
      "grad_norm": 4.165511131286621,
      "learning_rate": 4.533261381475668e-05,
      "loss": 0.6151,
      "step": 475700
    },
    {
      "epoch": 7.469387755102041,
      "grad_norm": 3.7338080406188965,
      "learning_rate": 4.533163265306123e-05,
      "loss": 0.6698,
      "step": 475800
    },
    {
      "epoch": 7.470957613814757,
      "grad_norm": 4.448345184326172,
      "learning_rate": 4.533065149136578e-05,
      "loss": 0.6281,
      "step": 475900
    },
    {
      "epoch": 7.472527472527473,
      "grad_norm": 3.8401389122009277,
      "learning_rate": 4.532967032967033e-05,
      "loss": 0.6353,
      "step": 476000
    },
    {
      "epoch": 7.474097331240189,
      "grad_norm": 4.633065223693848,
      "learning_rate": 4.532868916797489e-05,
      "loss": 0.6569,
      "step": 476100
    },
    {
      "epoch": 7.475667189952905,
      "grad_norm": 4.629982948303223,
      "learning_rate": 4.532770800627943e-05,
      "loss": 0.6648,
      "step": 476200
    },
    {
      "epoch": 7.47723704866562,
      "grad_norm": 3.733835458755493,
      "learning_rate": 4.532672684458399e-05,
      "loss": 0.6434,
      "step": 476300
    },
    {
      "epoch": 7.478806907378336,
      "grad_norm": 3.3399040699005127,
      "learning_rate": 4.532574568288854e-05,
      "loss": 0.6001,
      "step": 476400
    },
    {
      "epoch": 7.480376766091052,
      "grad_norm": 3.80926251411438,
      "learning_rate": 4.53247645211931e-05,
      "loss": 0.6255,
      "step": 476500
    },
    {
      "epoch": 7.481946624803768,
      "grad_norm": 3.938115119934082,
      "learning_rate": 4.5323783359497644e-05,
      "loss": 0.6493,
      "step": 476600
    },
    {
      "epoch": 7.483516483516484,
      "grad_norm": 4.623208999633789,
      "learning_rate": 4.53228021978022e-05,
      "loss": 0.6729,
      "step": 476700
    },
    {
      "epoch": 7.4850863422291996,
      "grad_norm": 4.608475208282471,
      "learning_rate": 4.532182103610675e-05,
      "loss": 0.6334,
      "step": 476800
    },
    {
      "epoch": 7.4866562009419155,
      "grad_norm": 4.20109224319458,
      "learning_rate": 4.53208398744113e-05,
      "loss": 0.6164,
      "step": 476900
    },
    {
      "epoch": 7.488226059654631,
      "grad_norm": 3.6985058784484863,
      "learning_rate": 4.5319858712715854e-05,
      "loss": 0.6624,
      "step": 477000
    },
    {
      "epoch": 7.489795918367347,
      "grad_norm": 3.416900634765625,
      "learning_rate": 4.531887755102041e-05,
      "loss": 0.6317,
      "step": 477100
    },
    {
      "epoch": 7.491365777080063,
      "grad_norm": 4.3965020179748535,
      "learning_rate": 4.531789638932496e-05,
      "loss": 0.6418,
      "step": 477200
    },
    {
      "epoch": 7.4929356357927785,
      "grad_norm": 2.89514422416687,
      "learning_rate": 4.5316915227629514e-05,
      "loss": 0.6977,
      "step": 477300
    },
    {
      "epoch": 7.4945054945054945,
      "grad_norm": 3.7151083946228027,
      "learning_rate": 4.5315934065934065e-05,
      "loss": 0.6625,
      "step": 477400
    },
    {
      "epoch": 7.4960753532182105,
      "grad_norm": 3.0576725006103516,
      "learning_rate": 4.531495290423862e-05,
      "loss": 0.6273,
      "step": 477500
    },
    {
      "epoch": 7.4976452119309265,
      "grad_norm": 2.7338545322418213,
      "learning_rate": 4.5313971742543174e-05,
      "loss": 0.6822,
      "step": 477600
    },
    {
      "epoch": 7.4992150706436425,
      "grad_norm": 5.618361949920654,
      "learning_rate": 4.5312990580847725e-05,
      "loss": 0.6424,
      "step": 477700
    },
    {
      "epoch": 7.5007849293563575,
      "grad_norm": 3.0387072563171387,
      "learning_rate": 4.531200941915228e-05,
      "loss": 0.6341,
      "step": 477800
    },
    {
      "epoch": 7.5023547880690735,
      "grad_norm": 3.5874173641204834,
      "learning_rate": 4.5311028257456834e-05,
      "loss": 0.6704,
      "step": 477900
    },
    {
      "epoch": 7.5039246467817895,
      "grad_norm": 2.6424500942230225,
      "learning_rate": 4.5310047095761384e-05,
      "loss": 0.6446,
      "step": 478000
    },
    {
      "epoch": 7.5054945054945055,
      "grad_norm": 4.102578639984131,
      "learning_rate": 4.5309065934065935e-05,
      "loss": 0.6687,
      "step": 478100
    },
    {
      "epoch": 7.5070643642072215,
      "grad_norm": 3.676912307739258,
      "learning_rate": 4.530808477237049e-05,
      "loss": 0.6343,
      "step": 478200
    },
    {
      "epoch": 7.508634222919937,
      "grad_norm": 5.124135971069336,
      "learning_rate": 4.530710361067504e-05,
      "loss": 0.6644,
      "step": 478300
    },
    {
      "epoch": 7.510204081632653,
      "grad_norm": 3.7105367183685303,
      "learning_rate": 4.5306122448979595e-05,
      "loss": 0.6405,
      "step": 478400
    },
    {
      "epoch": 7.511773940345369,
      "grad_norm": 3.6658217906951904,
      "learning_rate": 4.5305141287284146e-05,
      "loss": 0.6187,
      "step": 478500
    },
    {
      "epoch": 7.5133437990580845,
      "grad_norm": 3.7219855785369873,
      "learning_rate": 4.5304160125588704e-05,
      "loss": 0.6609,
      "step": 478600
    },
    {
      "epoch": 7.5149136577708004,
      "grad_norm": 3.47794508934021,
      "learning_rate": 4.530317896389325e-05,
      "loss": 0.6308,
      "step": 478700
    },
    {
      "epoch": 7.516483516483516,
      "grad_norm": 4.265655994415283,
      "learning_rate": 4.5302197802197806e-05,
      "loss": 0.6376,
      "step": 478800
    },
    {
      "epoch": 7.518053375196232,
      "grad_norm": 3.2142043113708496,
      "learning_rate": 4.530121664050236e-05,
      "loss": 0.6448,
      "step": 478900
    },
    {
      "epoch": 7.519623233908948,
      "grad_norm": 2.881437063217163,
      "learning_rate": 4.530023547880691e-05,
      "loss": 0.6141,
      "step": 479000
    },
    {
      "epoch": 7.521193092621664,
      "grad_norm": 4.316728115081787,
      "learning_rate": 4.529925431711146e-05,
      "loss": 0.6543,
      "step": 479100
    },
    {
      "epoch": 7.52276295133438,
      "grad_norm": 3.070465564727783,
      "learning_rate": 4.5298273155416017e-05,
      "loss": 0.6398,
      "step": 479200
    },
    {
      "epoch": 7.524332810047095,
      "grad_norm": 3.083333730697632,
      "learning_rate": 4.529729199372057e-05,
      "loss": 0.6372,
      "step": 479300
    },
    {
      "epoch": 7.525902668759811,
      "grad_norm": 3.795051097869873,
      "learning_rate": 4.529631083202512e-05,
      "loss": 0.665,
      "step": 479400
    },
    {
      "epoch": 7.527472527472527,
      "grad_norm": 5.0198140144348145,
      "learning_rate": 4.529532967032967e-05,
      "loss": 0.6191,
      "step": 479500
    },
    {
      "epoch": 7.529042386185243,
      "grad_norm": 4.670448303222656,
      "learning_rate": 4.529434850863423e-05,
      "loss": 0.6865,
      "step": 479600
    },
    {
      "epoch": 7.530612244897959,
      "grad_norm": 4.3407511711120605,
      "learning_rate": 4.529336734693878e-05,
      "loss": 0.6661,
      "step": 479700
    },
    {
      "epoch": 7.532182103610675,
      "grad_norm": 4.204563617706299,
      "learning_rate": 4.529238618524333e-05,
      "loss": 0.6319,
      "step": 479800
    },
    {
      "epoch": 7.533751962323391,
      "grad_norm": 4.108593940734863,
      "learning_rate": 4.529140502354789e-05,
      "loss": 0.6523,
      "step": 479900
    },
    {
      "epoch": 7.535321821036106,
      "grad_norm": 2.339252233505249,
      "learning_rate": 4.529042386185244e-05,
      "loss": 0.6214,
      "step": 480000
    },
    {
      "epoch": 7.536891679748822,
      "grad_norm": 4.225778102874756,
      "learning_rate": 4.528944270015699e-05,
      "loss": 0.6779,
      "step": 480100
    },
    {
      "epoch": 7.538461538461538,
      "grad_norm": 4.514763355255127,
      "learning_rate": 4.528846153846154e-05,
      "loss": 0.6373,
      "step": 480200
    },
    {
      "epoch": 7.540031397174254,
      "grad_norm": 4.224471092224121,
      "learning_rate": 4.52874803767661e-05,
      "loss": 0.6318,
      "step": 480300
    },
    {
      "epoch": 7.54160125588697,
      "grad_norm": 3.9412248134613037,
      "learning_rate": 4.528649921507064e-05,
      "loss": 0.6592,
      "step": 480400
    },
    {
      "epoch": 7.543171114599686,
      "grad_norm": 4.244016170501709,
      "learning_rate": 4.52855180533752e-05,
      "loss": 0.6292,
      "step": 480500
    },
    {
      "epoch": 7.544740973312402,
      "grad_norm": 2.9770209789276123,
      "learning_rate": 4.528453689167975e-05,
      "loss": 0.6645,
      "step": 480600
    },
    {
      "epoch": 7.546310832025117,
      "grad_norm": 3.9249093532562256,
      "learning_rate": 4.528355572998431e-05,
      "loss": 0.6376,
      "step": 480700
    },
    {
      "epoch": 7.547880690737833,
      "grad_norm": 5.3057332038879395,
      "learning_rate": 4.528257456828885e-05,
      "loss": 0.6636,
      "step": 480800
    },
    {
      "epoch": 7.549450549450549,
      "grad_norm": 3.2127623558044434,
      "learning_rate": 4.528159340659341e-05,
      "loss": 0.6204,
      "step": 480900
    },
    {
      "epoch": 7.551020408163265,
      "grad_norm": 3.991739511489868,
      "learning_rate": 4.528061224489796e-05,
      "loss": 0.6538,
      "step": 481000
    },
    {
      "epoch": 7.552590266875981,
      "grad_norm": 4.497997760772705,
      "learning_rate": 4.527963108320251e-05,
      "loss": 0.6467,
      "step": 481100
    },
    {
      "epoch": 7.554160125588697,
      "grad_norm": 3.805983066558838,
      "learning_rate": 4.527864992150706e-05,
      "loss": 0.6888,
      "step": 481200
    },
    {
      "epoch": 7.555729984301413,
      "grad_norm": 4.852888584136963,
      "learning_rate": 4.527766875981162e-05,
      "loss": 0.6733,
      "step": 481300
    },
    {
      "epoch": 7.557299843014128,
      "grad_norm": 2.6775424480438232,
      "learning_rate": 4.527668759811617e-05,
      "loss": 0.6562,
      "step": 481400
    },
    {
      "epoch": 7.558869701726844,
      "grad_norm": 4.206821441650391,
      "learning_rate": 4.527570643642072e-05,
      "loss": 0.6048,
      "step": 481500
    },
    {
      "epoch": 7.56043956043956,
      "grad_norm": 2.8423948287963867,
      "learning_rate": 4.5274725274725274e-05,
      "loss": 0.6319,
      "step": 481600
    },
    {
      "epoch": 7.562009419152276,
      "grad_norm": 3.9187142848968506,
      "learning_rate": 4.527374411302983e-05,
      "loss": 0.6476,
      "step": 481700
    },
    {
      "epoch": 7.563579277864992,
      "grad_norm": 3.5776753425598145,
      "learning_rate": 4.527276295133438e-05,
      "loss": 0.6409,
      "step": 481800
    },
    {
      "epoch": 7.565149136577708,
      "grad_norm": 4.332414150238037,
      "learning_rate": 4.5271781789638934e-05,
      "loss": 0.6468,
      "step": 481900
    },
    {
      "epoch": 7.566718995290424,
      "grad_norm": 4.393723964691162,
      "learning_rate": 4.527080062794349e-05,
      "loss": 0.6601,
      "step": 482000
    },
    {
      "epoch": 7.568288854003139,
      "grad_norm": 3.813499689102173,
      "learning_rate": 4.526981946624804e-05,
      "loss": 0.6654,
      "step": 482100
    },
    {
      "epoch": 7.569858712715855,
      "grad_norm": 2.9579925537109375,
      "learning_rate": 4.5268838304552593e-05,
      "loss": 0.6457,
      "step": 482200
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 3.898155450820923,
      "learning_rate": 4.5267857142857144e-05,
      "loss": 0.6154,
      "step": 482300
    },
    {
      "epoch": 7.572998430141287,
      "grad_norm": 3.828228235244751,
      "learning_rate": 4.52668759811617e-05,
      "loss": 0.6436,
      "step": 482400
    },
    {
      "epoch": 7.574568288854003,
      "grad_norm": 3.8732028007507324,
      "learning_rate": 4.5265894819466246e-05,
      "loss": 0.6188,
      "step": 482500
    },
    {
      "epoch": 7.576138147566719,
      "grad_norm": 4.896740913391113,
      "learning_rate": 4.5264913657770804e-05,
      "loss": 0.628,
      "step": 482600
    },
    {
      "epoch": 7.577708006279435,
      "grad_norm": 3.720137119293213,
      "learning_rate": 4.5263932496075355e-05,
      "loss": 0.6381,
      "step": 482700
    },
    {
      "epoch": 7.579277864992151,
      "grad_norm": 3.0683085918426514,
      "learning_rate": 4.526295133437991e-05,
      "loss": 0.6197,
      "step": 482800
    },
    {
      "epoch": 7.580847723704867,
      "grad_norm": 4.0870585441589355,
      "learning_rate": 4.526197017268446e-05,
      "loss": 0.6644,
      "step": 482900
    },
    {
      "epoch": 7.582417582417582,
      "grad_norm": 3.719151496887207,
      "learning_rate": 4.5260989010989015e-05,
      "loss": 0.6454,
      "step": 483000
    },
    {
      "epoch": 7.583987441130298,
      "grad_norm": 3.7578811645507812,
      "learning_rate": 4.5260007849293566e-05,
      "loss": 0.6348,
      "step": 483100
    },
    {
      "epoch": 7.585557299843014,
      "grad_norm": 4.585264682769775,
      "learning_rate": 4.525902668759812e-05,
      "loss": 0.5852,
      "step": 483200
    },
    {
      "epoch": 7.58712715855573,
      "grad_norm": 3.391922950744629,
      "learning_rate": 4.525804552590267e-05,
      "loss": 0.6312,
      "step": 483300
    },
    {
      "epoch": 7.588697017268446,
      "grad_norm": 4.920281410217285,
      "learning_rate": 4.5257064364207226e-05,
      "loss": 0.6812,
      "step": 483400
    },
    {
      "epoch": 7.590266875981162,
      "grad_norm": 4.021872043609619,
      "learning_rate": 4.5256083202511777e-05,
      "loss": 0.6455,
      "step": 483500
    },
    {
      "epoch": 7.591836734693878,
      "grad_norm": 4.286561012268066,
      "learning_rate": 4.525510204081633e-05,
      "loss": 0.6393,
      "step": 483600
    },
    {
      "epoch": 7.593406593406593,
      "grad_norm": 3.3233375549316406,
      "learning_rate": 4.525412087912088e-05,
      "loss": 0.6443,
      "step": 483700
    },
    {
      "epoch": 7.594976452119309,
      "grad_norm": 4.5823493003845215,
      "learning_rate": 4.5253139717425436e-05,
      "loss": 0.6385,
      "step": 483800
    },
    {
      "epoch": 7.596546310832025,
      "grad_norm": 4.647805213928223,
      "learning_rate": 4.525215855572999e-05,
      "loss": 0.6398,
      "step": 483900
    },
    {
      "epoch": 7.598116169544741,
      "grad_norm": 3.1560637950897217,
      "learning_rate": 4.525117739403454e-05,
      "loss": 0.6024,
      "step": 484000
    },
    {
      "epoch": 7.599686028257457,
      "grad_norm": 4.422451496124268,
      "learning_rate": 4.5250196232339096e-05,
      "loss": 0.6487,
      "step": 484100
    },
    {
      "epoch": 7.601255886970173,
      "grad_norm": 5.033768653869629,
      "learning_rate": 4.524921507064365e-05,
      "loss": 0.6643,
      "step": 484200
    },
    {
      "epoch": 7.602825745682889,
      "grad_norm": 2.9654648303985596,
      "learning_rate": 4.52482339089482e-05,
      "loss": 0.6731,
      "step": 484300
    },
    {
      "epoch": 7.604395604395604,
      "grad_norm": 3.7007224559783936,
      "learning_rate": 4.524725274725275e-05,
      "loss": 0.6463,
      "step": 484400
    },
    {
      "epoch": 7.60596546310832,
      "grad_norm": 3.0168607234954834,
      "learning_rate": 4.524627158555731e-05,
      "loss": 0.6504,
      "step": 484500
    },
    {
      "epoch": 7.607535321821036,
      "grad_norm": 4.358815670013428,
      "learning_rate": 4.524529042386185e-05,
      "loss": 0.6658,
      "step": 484600
    },
    {
      "epoch": 7.609105180533752,
      "grad_norm": 3.18831729888916,
      "learning_rate": 4.524430926216641e-05,
      "loss": 0.6742,
      "step": 484700
    },
    {
      "epoch": 7.610675039246468,
      "grad_norm": 3.9556946754455566,
      "learning_rate": 4.524332810047096e-05,
      "loss": 0.6635,
      "step": 484800
    },
    {
      "epoch": 7.612244897959184,
      "grad_norm": 4.132087230682373,
      "learning_rate": 4.524234693877552e-05,
      "loss": 0.6633,
      "step": 484900
    },
    {
      "epoch": 7.6138147566719,
      "grad_norm": 3.827343702316284,
      "learning_rate": 4.524136577708006e-05,
      "loss": 0.6534,
      "step": 485000
    },
    {
      "epoch": 7.615384615384615,
      "grad_norm": 4.599282741546631,
      "learning_rate": 4.524038461538462e-05,
      "loss": 0.6301,
      "step": 485100
    },
    {
      "epoch": 7.616954474097331,
      "grad_norm": 3.6659562587738037,
      "learning_rate": 4.523940345368917e-05,
      "loss": 0.6362,
      "step": 485200
    },
    {
      "epoch": 7.618524332810047,
      "grad_norm": 3.1517107486724854,
      "learning_rate": 4.523842229199372e-05,
      "loss": 0.6474,
      "step": 485300
    },
    {
      "epoch": 7.620094191522763,
      "grad_norm": 6.32534646987915,
      "learning_rate": 4.523744113029827e-05,
      "loss": 0.6268,
      "step": 485400
    },
    {
      "epoch": 7.621664050235479,
      "grad_norm": 4.287709712982178,
      "learning_rate": 4.523645996860283e-05,
      "loss": 0.6923,
      "step": 485500
    },
    {
      "epoch": 7.623233908948195,
      "grad_norm": 2.979832649230957,
      "learning_rate": 4.523547880690738e-05,
      "loss": 0.6583,
      "step": 485600
    },
    {
      "epoch": 7.624803767660911,
      "grad_norm": 3.716931104660034,
      "learning_rate": 4.523449764521193e-05,
      "loss": 0.6379,
      "step": 485700
    },
    {
      "epoch": 7.626373626373626,
      "grad_norm": 3.106661319732666,
      "learning_rate": 4.523351648351648e-05,
      "loss": 0.6635,
      "step": 485800
    },
    {
      "epoch": 7.627943485086342,
      "grad_norm": 4.073364734649658,
      "learning_rate": 4.523253532182104e-05,
      "loss": 0.653,
      "step": 485900
    },
    {
      "epoch": 7.629513343799058,
      "grad_norm": 4.437788009643555,
      "learning_rate": 4.523155416012559e-05,
      "loss": 0.6539,
      "step": 486000
    },
    {
      "epoch": 7.631083202511774,
      "grad_norm": 3.559818983078003,
      "learning_rate": 4.523057299843014e-05,
      "loss": 0.6375,
      "step": 486100
    },
    {
      "epoch": 7.63265306122449,
      "grad_norm": 4.6764302253723145,
      "learning_rate": 4.52295918367347e-05,
      "loss": 0.6443,
      "step": 486200
    },
    {
      "epoch": 7.634222919937206,
      "grad_norm": 4.011380672454834,
      "learning_rate": 4.522861067503925e-05,
      "loss": 0.6269,
      "step": 486300
    },
    {
      "epoch": 7.635792778649922,
      "grad_norm": 5.289903163909912,
      "learning_rate": 4.52276295133438e-05,
      "loss": 0.7057,
      "step": 486400
    },
    {
      "epoch": 7.637362637362637,
      "grad_norm": 4.100209712982178,
      "learning_rate": 4.5226648351648353e-05,
      "loss": 0.6511,
      "step": 486500
    },
    {
      "epoch": 7.638932496075353,
      "grad_norm": 3.881650924682617,
      "learning_rate": 4.522566718995291e-05,
      "loss": 0.6343,
      "step": 486600
    },
    {
      "epoch": 7.640502354788069,
      "grad_norm": 4.032605171203613,
      "learning_rate": 4.5224686028257455e-05,
      "loss": 0.6844,
      "step": 486700
    },
    {
      "epoch": 7.642072213500785,
      "grad_norm": 4.452290058135986,
      "learning_rate": 4.522370486656201e-05,
      "loss": 0.6418,
      "step": 486800
    },
    {
      "epoch": 7.643642072213501,
      "grad_norm": 2.576986312866211,
      "learning_rate": 4.5222723704866564e-05,
      "loss": 0.6364,
      "step": 486900
    },
    {
      "epoch": 7.645211930926217,
      "grad_norm": 4.293253421783447,
      "learning_rate": 4.522174254317112e-05,
      "loss": 0.6366,
      "step": 487000
    },
    {
      "epoch": 7.646781789638933,
      "grad_norm": 3.900942087173462,
      "learning_rate": 4.5220761381475666e-05,
      "loss": 0.6913,
      "step": 487100
    },
    {
      "epoch": 7.648351648351649,
      "grad_norm": 3.5812935829162598,
      "learning_rate": 4.5219780219780224e-05,
      "loss": 0.6403,
      "step": 487200
    },
    {
      "epoch": 7.649921507064365,
      "grad_norm": 4.115777492523193,
      "learning_rate": 4.5218799058084775e-05,
      "loss": 0.6327,
      "step": 487300
    },
    {
      "epoch": 7.65149136577708,
      "grad_norm": 3.3026397228240967,
      "learning_rate": 4.5217817896389326e-05,
      "loss": 0.6246,
      "step": 487400
    },
    {
      "epoch": 7.653061224489796,
      "grad_norm": 3.210282564163208,
      "learning_rate": 4.521683673469388e-05,
      "loss": 0.6673,
      "step": 487500
    },
    {
      "epoch": 7.654631083202512,
      "grad_norm": 3.9890334606170654,
      "learning_rate": 4.5215855572998435e-05,
      "loss": 0.6469,
      "step": 487600
    },
    {
      "epoch": 7.656200941915228,
      "grad_norm": 4.049375057220459,
      "learning_rate": 4.5214874411302986e-05,
      "loss": 0.6441,
      "step": 487700
    },
    {
      "epoch": 7.657770800627944,
      "grad_norm": 3.087475299835205,
      "learning_rate": 4.5213893249607537e-05,
      "loss": 0.6544,
      "step": 487800
    },
    {
      "epoch": 7.65934065934066,
      "grad_norm": 3.4424030780792236,
      "learning_rate": 4.521291208791209e-05,
      "loss": 0.6712,
      "step": 487900
    },
    {
      "epoch": 7.660910518053376,
      "grad_norm": 3.5721869468688965,
      "learning_rate": 4.5211930926216645e-05,
      "loss": 0.6346,
      "step": 488000
    },
    {
      "epoch": 7.662480376766091,
      "grad_norm": 3.8322794437408447,
      "learning_rate": 4.5210949764521196e-05,
      "loss": 0.6529,
      "step": 488100
    },
    {
      "epoch": 7.664050235478807,
      "grad_norm": 4.395904064178467,
      "learning_rate": 4.520996860282575e-05,
      "loss": 0.6451,
      "step": 488200
    },
    {
      "epoch": 7.665620094191523,
      "grad_norm": 4.610713005065918,
      "learning_rate": 4.5208987441130305e-05,
      "loss": 0.7039,
      "step": 488300
    },
    {
      "epoch": 7.667189952904239,
      "grad_norm": 4.875594615936279,
      "learning_rate": 4.5208006279434856e-05,
      "loss": 0.6267,
      "step": 488400
    },
    {
      "epoch": 7.668759811616955,
      "grad_norm": 3.628938674926758,
      "learning_rate": 4.520702511773941e-05,
      "loss": 0.6656,
      "step": 488500
    },
    {
      "epoch": 7.670329670329671,
      "grad_norm": 3.9174702167510986,
      "learning_rate": 4.520604395604396e-05,
      "loss": 0.5915,
      "step": 488600
    },
    {
      "epoch": 7.671899529042387,
      "grad_norm": 4.382264614105225,
      "learning_rate": 4.5205062794348516e-05,
      "loss": 0.6571,
      "step": 488700
    },
    {
      "epoch": 7.673469387755102,
      "grad_norm": 4.775045871734619,
      "learning_rate": 4.520408163265306e-05,
      "loss": 0.6552,
      "step": 488800
    },
    {
      "epoch": 7.675039246467818,
      "grad_norm": 3.385932683944702,
      "learning_rate": 4.520310047095762e-05,
      "loss": 0.5971,
      "step": 488900
    },
    {
      "epoch": 7.676609105180534,
      "grad_norm": 3.543081760406494,
      "learning_rate": 4.520211930926217e-05,
      "loss": 0.6703,
      "step": 489000
    },
    {
      "epoch": 7.67817896389325,
      "grad_norm": 2.739438772201538,
      "learning_rate": 4.5201138147566726e-05,
      "loss": 0.6883,
      "step": 489100
    },
    {
      "epoch": 7.679748822605966,
      "grad_norm": 3.0259385108947754,
      "learning_rate": 4.520015698587127e-05,
      "loss": 0.6263,
      "step": 489200
    },
    {
      "epoch": 7.681318681318682,
      "grad_norm": 3.596712112426758,
      "learning_rate": 4.519917582417583e-05,
      "loss": 0.6832,
      "step": 489300
    },
    {
      "epoch": 7.6828885400313975,
      "grad_norm": 4.197406768798828,
      "learning_rate": 4.519819466248038e-05,
      "loss": 0.6314,
      "step": 489400
    },
    {
      "epoch": 7.684458398744113,
      "grad_norm": 3.139620065689087,
      "learning_rate": 4.519721350078493e-05,
      "loss": 0.655,
      "step": 489500
    },
    {
      "epoch": 7.686028257456829,
      "grad_norm": 3.566957473754883,
      "learning_rate": 4.519623233908948e-05,
      "loss": 0.6539,
      "step": 489600
    },
    {
      "epoch": 7.687598116169545,
      "grad_norm": 3.811854600906372,
      "learning_rate": 4.519525117739404e-05,
      "loss": 0.6559,
      "step": 489700
    },
    {
      "epoch": 7.689167974882261,
      "grad_norm": 3.8980348110198975,
      "learning_rate": 4.519427001569858e-05,
      "loss": 0.6026,
      "step": 489800
    },
    {
      "epoch": 7.6907378335949765,
      "grad_norm": 3.160677909851074,
      "learning_rate": 4.519328885400314e-05,
      "loss": 0.6405,
      "step": 489900
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 3.758246421813965,
      "learning_rate": 4.519230769230769e-05,
      "loss": 0.6598,
      "step": 490000
    },
    {
      "epoch": 7.6938775510204085,
      "grad_norm": 3.993847608566284,
      "learning_rate": 4.519132653061225e-05,
      "loss": 0.6803,
      "step": 490100
    },
    {
      "epoch": 7.695447409733124,
      "grad_norm": 3.514887571334839,
      "learning_rate": 4.51903453689168e-05,
      "loss": 0.636,
      "step": 490200
    },
    {
      "epoch": 7.6970172684458396,
      "grad_norm": 3.704043388366699,
      "learning_rate": 4.518936420722135e-05,
      "loss": 0.674,
      "step": 490300
    },
    {
      "epoch": 7.6985871271585555,
      "grad_norm": 3.615494966506958,
      "learning_rate": 4.51883830455259e-05,
      "loss": 0.6473,
      "step": 490400
    },
    {
      "epoch": 7.7001569858712715,
      "grad_norm": 3.904005527496338,
      "learning_rate": 4.5187401883830454e-05,
      "loss": 0.6553,
      "step": 490500
    },
    {
      "epoch": 7.7017268445839875,
      "grad_norm": 4.4550957679748535,
      "learning_rate": 4.518642072213501e-05,
      "loss": 0.684,
      "step": 490600
    },
    {
      "epoch": 7.7032967032967035,
      "grad_norm": 4.611151695251465,
      "learning_rate": 4.518543956043956e-05,
      "loss": 0.6213,
      "step": 490700
    },
    {
      "epoch": 7.704866562009419,
      "grad_norm": 4.397189140319824,
      "learning_rate": 4.518445839874412e-05,
      "loss": 0.678,
      "step": 490800
    },
    {
      "epoch": 7.7064364207221345,
      "grad_norm": 3.070841073989868,
      "learning_rate": 4.5183477237048664e-05,
      "loss": 0.6551,
      "step": 490900
    },
    {
      "epoch": 7.7080062794348505,
      "grad_norm": 3.8983030319213867,
      "learning_rate": 4.518249607535322e-05,
      "loss": 0.6475,
      "step": 491000
    },
    {
      "epoch": 7.7095761381475665,
      "grad_norm": 4.3922014236450195,
      "learning_rate": 4.518151491365777e-05,
      "loss": 0.6302,
      "step": 491100
    },
    {
      "epoch": 7.7111459968602825,
      "grad_norm": 4.3992919921875,
      "learning_rate": 4.5180533751962324e-05,
      "loss": 0.6593,
      "step": 491200
    },
    {
      "epoch": 7.712715855572998,
      "grad_norm": 3.5287513732910156,
      "learning_rate": 4.5179552590266875e-05,
      "loss": 0.6635,
      "step": 491300
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 3.863615036010742,
      "learning_rate": 4.517857142857143e-05,
      "loss": 0.7054,
      "step": 491400
    },
    {
      "epoch": 7.71585557299843,
      "grad_norm": 3.3553619384765625,
      "learning_rate": 4.5177590266875984e-05,
      "loss": 0.647,
      "step": 491500
    },
    {
      "epoch": 7.717425431711146,
      "grad_norm": 4.247029781341553,
      "learning_rate": 4.5176609105180535e-05,
      "loss": 0.6427,
      "step": 491600
    },
    {
      "epoch": 7.718995290423862,
      "grad_norm": 3.856109142303467,
      "learning_rate": 4.5175627943485086e-05,
      "loss": 0.6306,
      "step": 491700
    },
    {
      "epoch": 7.720565149136577,
      "grad_norm": 3.2999625205993652,
      "learning_rate": 4.5174646781789644e-05,
      "loss": 0.6279,
      "step": 491800
    },
    {
      "epoch": 7.722135007849293,
      "grad_norm": 2.7190284729003906,
      "learning_rate": 4.517366562009419e-05,
      "loss": 0.6374,
      "step": 491900
    },
    {
      "epoch": 7.723704866562009,
      "grad_norm": 3.879354476928711,
      "learning_rate": 4.5172684458398746e-05,
      "loss": 0.646,
      "step": 492000
    },
    {
      "epoch": 7.725274725274725,
      "grad_norm": 3.0086593627929688,
      "learning_rate": 4.5171703296703296e-05,
      "loss": 0.6389,
      "step": 492100
    },
    {
      "epoch": 7.726844583987441,
      "grad_norm": 3.303194522857666,
      "learning_rate": 4.5170722135007854e-05,
      "loss": 0.6585,
      "step": 492200
    },
    {
      "epoch": 7.728414442700157,
      "grad_norm": 2.7390236854553223,
      "learning_rate": 4.5169740973312405e-05,
      "loss": 0.6264,
      "step": 492300
    },
    {
      "epoch": 7.729984301412873,
      "grad_norm": 3.4625802040100098,
      "learning_rate": 4.5168759811616956e-05,
      "loss": 0.6403,
      "step": 492400
    },
    {
      "epoch": 7.731554160125588,
      "grad_norm": 4.1185102462768555,
      "learning_rate": 4.516777864992151e-05,
      "loss": 0.6744,
      "step": 492500
    },
    {
      "epoch": 7.733124018838304,
      "grad_norm": 4.528750896453857,
      "learning_rate": 4.516679748822606e-05,
      "loss": 0.6269,
      "step": 492600
    },
    {
      "epoch": 7.73469387755102,
      "grad_norm": 4.254639625549316,
      "learning_rate": 4.5165816326530616e-05,
      "loss": 0.6448,
      "step": 492700
    },
    {
      "epoch": 7.736263736263736,
      "grad_norm": 3.6124820709228516,
      "learning_rate": 4.516483516483517e-05,
      "loss": 0.6403,
      "step": 492800
    },
    {
      "epoch": 7.737833594976452,
      "grad_norm": 4.512972831726074,
      "learning_rate": 4.5163854003139725e-05,
      "loss": 0.6598,
      "step": 492900
    },
    {
      "epoch": 7.739403453689168,
      "grad_norm": 5.512923717498779,
      "learning_rate": 4.516287284144427e-05,
      "loss": 0.6675,
      "step": 493000
    },
    {
      "epoch": 7.740973312401884,
      "grad_norm": 4.2947492599487305,
      "learning_rate": 4.516189167974883e-05,
      "loss": 0.6318,
      "step": 493100
    },
    {
      "epoch": 7.742543171114599,
      "grad_norm": 4.016552925109863,
      "learning_rate": 4.516091051805338e-05,
      "loss": 0.6445,
      "step": 493200
    },
    {
      "epoch": 7.744113029827315,
      "grad_norm": 3.2326908111572266,
      "learning_rate": 4.515992935635793e-05,
      "loss": 0.6232,
      "step": 493300
    },
    {
      "epoch": 7.745682888540031,
      "grad_norm": 2.3772716522216797,
      "learning_rate": 4.515894819466248e-05,
      "loss": 0.7066,
      "step": 493400
    },
    {
      "epoch": 7.747252747252747,
      "grad_norm": 3.798745632171631,
      "learning_rate": 4.515796703296704e-05,
      "loss": 0.6227,
      "step": 493500
    },
    {
      "epoch": 7.748822605965463,
      "grad_norm": 3.903792381286621,
      "learning_rate": 4.515698587127159e-05,
      "loss": 0.6528,
      "step": 493600
    },
    {
      "epoch": 7.750392464678179,
      "grad_norm": 3.661841630935669,
      "learning_rate": 4.515600470957614e-05,
      "loss": 0.6445,
      "step": 493700
    },
    {
      "epoch": 7.751962323390895,
      "grad_norm": 3.279142379760742,
      "learning_rate": 4.515502354788069e-05,
      "loss": 0.6169,
      "step": 493800
    },
    {
      "epoch": 7.75353218210361,
      "grad_norm": 3.4273793697357178,
      "learning_rate": 4.515404238618525e-05,
      "loss": 0.6716,
      "step": 493900
    },
    {
      "epoch": 7.755102040816326,
      "grad_norm": 3.372530460357666,
      "learning_rate": 4.515306122448979e-05,
      "loss": 0.6203,
      "step": 494000
    },
    {
      "epoch": 7.756671899529042,
      "grad_norm": 3.24513840675354,
      "learning_rate": 4.515208006279435e-05,
      "loss": 0.6721,
      "step": 494100
    },
    {
      "epoch": 7.758241758241758,
      "grad_norm": 4.123809337615967,
      "learning_rate": 4.51510989010989e-05,
      "loss": 0.65,
      "step": 494200
    },
    {
      "epoch": 7.759811616954474,
      "grad_norm": 5.030951023101807,
      "learning_rate": 4.515011773940346e-05,
      "loss": 0.6566,
      "step": 494300
    },
    {
      "epoch": 7.76138147566719,
      "grad_norm": 6.134461402893066,
      "learning_rate": 4.514913657770801e-05,
      "loss": 0.6418,
      "step": 494400
    },
    {
      "epoch": 7.762951334379906,
      "grad_norm": 4.459980010986328,
      "learning_rate": 4.514815541601256e-05,
      "loss": 0.6667,
      "step": 494500
    },
    {
      "epoch": 7.764521193092621,
      "grad_norm": 4.092518329620361,
      "learning_rate": 4.514717425431711e-05,
      "loss": 0.6633,
      "step": 494600
    },
    {
      "epoch": 7.766091051805337,
      "grad_norm": 4.584765434265137,
      "learning_rate": 4.514619309262166e-05,
      "loss": 0.658,
      "step": 494700
    },
    {
      "epoch": 7.767660910518053,
      "grad_norm": 3.2781903743743896,
      "learning_rate": 4.514521193092622e-05,
      "loss": 0.683,
      "step": 494800
    },
    {
      "epoch": 7.769230769230769,
      "grad_norm": 4.643771648406982,
      "learning_rate": 4.514423076923077e-05,
      "loss": 0.6231,
      "step": 494900
    },
    {
      "epoch": 7.770800627943485,
      "grad_norm": 3.8664679527282715,
      "learning_rate": 4.514324960753533e-05,
      "loss": 0.641,
      "step": 495000
    },
    {
      "epoch": 7.772370486656201,
      "grad_norm": 4.497760772705078,
      "learning_rate": 4.514226844583987e-05,
      "loss": 0.6191,
      "step": 495100
    },
    {
      "epoch": 7.773940345368917,
      "grad_norm": 3.632519483566284,
      "learning_rate": 4.514128728414443e-05,
      "loss": 0.6829,
      "step": 495200
    },
    {
      "epoch": 7.775510204081632,
      "grad_norm": 4.326026916503906,
      "learning_rate": 4.514030612244898e-05,
      "loss": 0.6441,
      "step": 495300
    },
    {
      "epoch": 7.777080062794348,
      "grad_norm": 3.771115779876709,
      "learning_rate": 4.513932496075353e-05,
      "loss": 0.6541,
      "step": 495400
    },
    {
      "epoch": 7.778649921507064,
      "grad_norm": 2.613415241241455,
      "learning_rate": 4.5138343799058084e-05,
      "loss": 0.5952,
      "step": 495500
    },
    {
      "epoch": 7.78021978021978,
      "grad_norm": 4.37483549118042,
      "learning_rate": 4.513736263736264e-05,
      "loss": 0.6479,
      "step": 495600
    },
    {
      "epoch": 7.781789638932496,
      "grad_norm": 3.46317982673645,
      "learning_rate": 4.513638147566719e-05,
      "loss": 0.657,
      "step": 495700
    },
    {
      "epoch": 7.783359497645212,
      "grad_norm": 3.7182979583740234,
      "learning_rate": 4.5135400313971744e-05,
      "loss": 0.7027,
      "step": 495800
    },
    {
      "epoch": 7.784929356357928,
      "grad_norm": 3.474538803100586,
      "learning_rate": 4.5134419152276295e-05,
      "loss": 0.6175,
      "step": 495900
    },
    {
      "epoch": 7.786499215070644,
      "grad_norm": 3.5245325565338135,
      "learning_rate": 4.513343799058085e-05,
      "loss": 0.6846,
      "step": 496000
    },
    {
      "epoch": 7.78806907378336,
      "grad_norm": 3.4690563678741455,
      "learning_rate": 4.51324568288854e-05,
      "loss": 0.6102,
      "step": 496100
    },
    {
      "epoch": 7.789638932496075,
      "grad_norm": 4.439887046813965,
      "learning_rate": 4.5131475667189954e-05,
      "loss": 0.6671,
      "step": 496200
    },
    {
      "epoch": 7.791208791208791,
      "grad_norm": 3.9803969860076904,
      "learning_rate": 4.5130494505494505e-05,
      "loss": 0.6542,
      "step": 496300
    },
    {
      "epoch": 7.792778649921507,
      "grad_norm": 3.8018267154693604,
      "learning_rate": 4.512951334379906e-05,
      "loss": 0.6503,
      "step": 496400
    },
    {
      "epoch": 7.794348508634223,
      "grad_norm": 3.153228282928467,
      "learning_rate": 4.5128532182103614e-05,
      "loss": 0.6327,
      "step": 496500
    },
    {
      "epoch": 7.795918367346939,
      "grad_norm": 3.659777879714966,
      "learning_rate": 4.5127551020408165e-05,
      "loss": 0.6114,
      "step": 496600
    },
    {
      "epoch": 7.797488226059655,
      "grad_norm": 3.3969638347625732,
      "learning_rate": 4.5126569858712716e-05,
      "loss": 0.687,
      "step": 496700
    },
    {
      "epoch": 7.799058084772371,
      "grad_norm": 2.690903425216675,
      "learning_rate": 4.512558869701727e-05,
      "loss": 0.65,
      "step": 496800
    },
    {
      "epoch": 7.800627943485086,
      "grad_norm": 3.899172306060791,
      "learning_rate": 4.5124607535321825e-05,
      "loss": 0.6656,
      "step": 496900
    },
    {
      "epoch": 7.802197802197802,
      "grad_norm": 4.124828815460205,
      "learning_rate": 4.5123626373626376e-05,
      "loss": 0.6419,
      "step": 497000
    },
    {
      "epoch": 7.803767660910518,
      "grad_norm": 3.9246811866760254,
      "learning_rate": 4.5122645211930934e-05,
      "loss": 0.6666,
      "step": 497100
    },
    {
      "epoch": 7.805337519623234,
      "grad_norm": 3.873521089553833,
      "learning_rate": 4.512166405023548e-05,
      "loss": 0.6809,
      "step": 497200
    },
    {
      "epoch": 7.80690737833595,
      "grad_norm": 3.1626956462860107,
      "learning_rate": 4.5120682888540036e-05,
      "loss": 0.6984,
      "step": 497300
    },
    {
      "epoch": 7.808477237048666,
      "grad_norm": 4.8493971824646,
      "learning_rate": 4.5119701726844587e-05,
      "loss": 0.6229,
      "step": 497400
    },
    {
      "epoch": 7.810047095761382,
      "grad_norm": 3.454341173171997,
      "learning_rate": 4.511872056514914e-05,
      "loss": 0.6961,
      "step": 497500
    },
    {
      "epoch": 7.811616954474097,
      "grad_norm": 4.050588130950928,
      "learning_rate": 4.511773940345369e-05,
      "loss": 0.645,
      "step": 497600
    },
    {
      "epoch": 7.813186813186813,
      "grad_norm": 3.4380204677581787,
      "learning_rate": 4.5116758241758246e-05,
      "loss": 0.6194,
      "step": 497700
    },
    {
      "epoch": 7.814756671899529,
      "grad_norm": 4.6230669021606445,
      "learning_rate": 4.51157770800628e-05,
      "loss": 0.6577,
      "step": 497800
    },
    {
      "epoch": 7.816326530612245,
      "grad_norm": 3.745710611343384,
      "learning_rate": 4.511479591836735e-05,
      "loss": 0.6316,
      "step": 497900
    },
    {
      "epoch": 7.817896389324961,
      "grad_norm": 4.148224830627441,
      "learning_rate": 4.51138147566719e-05,
      "loss": 0.7044,
      "step": 498000
    },
    {
      "epoch": 7.819466248037677,
      "grad_norm": 3.056553602218628,
      "learning_rate": 4.511283359497646e-05,
      "loss": 0.7156,
      "step": 498100
    },
    {
      "epoch": 7.821036106750393,
      "grad_norm": 3.608576774597168,
      "learning_rate": 4.5111852433281e-05,
      "loss": 0.6241,
      "step": 498200
    },
    {
      "epoch": 7.822605965463108,
      "grad_norm": 4.570631980895996,
      "learning_rate": 4.511087127158556e-05,
      "loss": 0.6462,
      "step": 498300
    },
    {
      "epoch": 7.824175824175824,
      "grad_norm": 3.42940092086792,
      "learning_rate": 4.510989010989011e-05,
      "loss": 0.661,
      "step": 498400
    },
    {
      "epoch": 7.82574568288854,
      "grad_norm": 3.495018482208252,
      "learning_rate": 4.510890894819467e-05,
      "loss": 0.6574,
      "step": 498500
    },
    {
      "epoch": 7.827315541601256,
      "grad_norm": 3.7217495441436768,
      "learning_rate": 4.510792778649922e-05,
      "loss": 0.655,
      "step": 498600
    },
    {
      "epoch": 7.828885400313972,
      "grad_norm": 2.0728330612182617,
      "learning_rate": 4.510694662480377e-05,
      "loss": 0.6816,
      "step": 498700
    },
    {
      "epoch": 7.830455259026688,
      "grad_norm": 3.2653515338897705,
      "learning_rate": 4.510596546310832e-05,
      "loss": 0.6313,
      "step": 498800
    },
    {
      "epoch": 7.832025117739404,
      "grad_norm": 2.9023234844207764,
      "learning_rate": 4.510498430141287e-05,
      "loss": 0.6039,
      "step": 498900
    },
    {
      "epoch": 7.833594976452119,
      "grad_norm": 4.355992794036865,
      "learning_rate": 4.510400313971743e-05,
      "loss": 0.6468,
      "step": 499000
    },
    {
      "epoch": 7.835164835164835,
      "grad_norm": 3.9053378105163574,
      "learning_rate": 4.510302197802198e-05,
      "loss": 0.6584,
      "step": 499100
    },
    {
      "epoch": 7.836734693877551,
      "grad_norm": 3.540404796600342,
      "learning_rate": 4.510204081632654e-05,
      "loss": 0.6388,
      "step": 499200
    },
    {
      "epoch": 7.838304552590267,
      "grad_norm": 4.537827014923096,
      "learning_rate": 4.510105965463108e-05,
      "loss": 0.6246,
      "step": 499300
    },
    {
      "epoch": 7.839874411302983,
      "grad_norm": 4.080810546875,
      "learning_rate": 4.510007849293564e-05,
      "loss": 0.6732,
      "step": 499400
    },
    {
      "epoch": 7.841444270015699,
      "grad_norm": 4.169857501983643,
      "learning_rate": 4.509909733124019e-05,
      "loss": 0.7045,
      "step": 499500
    },
    {
      "epoch": 7.843014128728415,
      "grad_norm": 3.372661828994751,
      "learning_rate": 4.509811616954474e-05,
      "loss": 0.6504,
      "step": 499600
    },
    {
      "epoch": 7.84458398744113,
      "grad_norm": 3.423595905303955,
      "learning_rate": 4.509713500784929e-05,
      "loss": 0.6629,
      "step": 499700
    },
    {
      "epoch": 7.846153846153846,
      "grad_norm": 3.8775250911712646,
      "learning_rate": 4.509615384615385e-05,
      "loss": 0.6458,
      "step": 499800
    },
    {
      "epoch": 7.847723704866562,
      "grad_norm": 4.140602111816406,
      "learning_rate": 4.50951726844584e-05,
      "loss": 0.6715,
      "step": 499900
    },
    {
      "epoch": 7.849293563579278,
      "grad_norm": 3.307495594024658,
      "learning_rate": 4.509419152276295e-05,
      "loss": 0.6522,
      "step": 500000
    },
    {
      "epoch": 7.850863422291994,
      "grad_norm": 3.7717180252075195,
      "learning_rate": 4.5093210361067504e-05,
      "loss": 0.6582,
      "step": 500100
    },
    {
      "epoch": 7.85243328100471,
      "grad_norm": 4.564573287963867,
      "learning_rate": 4.509222919937206e-05,
      "loss": 0.6891,
      "step": 500200
    },
    {
      "epoch": 7.854003139717426,
      "grad_norm": 3.3202037811279297,
      "learning_rate": 4.5091248037676606e-05,
      "loss": 0.6453,
      "step": 500300
    },
    {
      "epoch": 7.855572998430142,
      "grad_norm": 3.911801338195801,
      "learning_rate": 4.5090266875981163e-05,
      "loss": 0.6675,
      "step": 500400
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 3.8103926181793213,
      "learning_rate": 4.5089285714285714e-05,
      "loss": 0.7015,
      "step": 500500
    },
    {
      "epoch": 7.858712715855573,
      "grad_norm": 3.981905698776245,
      "learning_rate": 4.508830455259027e-05,
      "loss": 0.6528,
      "step": 500600
    },
    {
      "epoch": 7.860282574568289,
      "grad_norm": 4.569162845611572,
      "learning_rate": 4.508732339089482e-05,
      "loss": 0.675,
      "step": 500700
    },
    {
      "epoch": 7.861852433281005,
      "grad_norm": 4.310275077819824,
      "learning_rate": 4.5086342229199374e-05,
      "loss": 0.6574,
      "step": 500800
    },
    {
      "epoch": 7.863422291993721,
      "grad_norm": 4.476008415222168,
      "learning_rate": 4.5085361067503925e-05,
      "loss": 0.6582,
      "step": 500900
    },
    {
      "epoch": 7.864992150706437,
      "grad_norm": 3.148810863494873,
      "learning_rate": 4.5084379905808476e-05,
      "loss": 0.6346,
      "step": 501000
    },
    {
      "epoch": 7.866562009419153,
      "grad_norm": 3.6708905696868896,
      "learning_rate": 4.5083398744113034e-05,
      "loss": 0.6429,
      "step": 501100
    },
    {
      "epoch": 7.868131868131869,
      "grad_norm": 4.34429931640625,
      "learning_rate": 4.5082417582417585e-05,
      "loss": 0.6529,
      "step": 501200
    },
    {
      "epoch": 7.869701726844584,
      "grad_norm": 4.110969543457031,
      "learning_rate": 4.508143642072214e-05,
      "loss": 0.6587,
      "step": 501300
    },
    {
      "epoch": 7.8712715855573,
      "grad_norm": 3.481945276260376,
      "learning_rate": 4.508045525902669e-05,
      "loss": 0.6443,
      "step": 501400
    },
    {
      "epoch": 7.872841444270016,
      "grad_norm": 3.495173454284668,
      "learning_rate": 4.5079474097331245e-05,
      "loss": 0.5925,
      "step": 501500
    },
    {
      "epoch": 7.874411302982732,
      "grad_norm": 3.2681474685668945,
      "learning_rate": 4.5078492935635796e-05,
      "loss": 0.6695,
      "step": 501600
    },
    {
      "epoch": 7.875981161695448,
      "grad_norm": 3.7052133083343506,
      "learning_rate": 4.5077511773940347e-05,
      "loss": 0.597,
      "step": 501700
    },
    {
      "epoch": 7.877551020408164,
      "grad_norm": 3.5982632637023926,
      "learning_rate": 4.50765306122449e-05,
      "loss": 0.6794,
      "step": 501800
    },
    {
      "epoch": 7.8791208791208796,
      "grad_norm": 2.7845208644866943,
      "learning_rate": 4.5075549450549455e-05,
      "loss": 0.6283,
      "step": 501900
    },
    {
      "epoch": 7.880690737833595,
      "grad_norm": 4.24960470199585,
      "learning_rate": 4.5074568288854006e-05,
      "loss": 0.6275,
      "step": 502000
    },
    {
      "epoch": 7.882260596546311,
      "grad_norm": 5.035439968109131,
      "learning_rate": 4.507358712715856e-05,
      "loss": 0.6433,
      "step": 502100
    },
    {
      "epoch": 7.883830455259027,
      "grad_norm": 4.581474304199219,
      "learning_rate": 4.507260596546311e-05,
      "loss": 0.6633,
      "step": 502200
    },
    {
      "epoch": 7.885400313971743,
      "grad_norm": 3.891791343688965,
      "learning_rate": 4.5071624803767666e-05,
      "loss": 0.5988,
      "step": 502300
    },
    {
      "epoch": 7.8869701726844585,
      "grad_norm": 3.828943967819214,
      "learning_rate": 4.507064364207221e-05,
      "loss": 0.6526,
      "step": 502400
    },
    {
      "epoch": 7.8885400313971745,
      "grad_norm": 3.830289602279663,
      "learning_rate": 4.506966248037677e-05,
      "loss": 0.6386,
      "step": 502500
    },
    {
      "epoch": 7.8901098901098905,
      "grad_norm": 3.3754849433898926,
      "learning_rate": 4.506868131868132e-05,
      "loss": 0.6791,
      "step": 502600
    },
    {
      "epoch": 7.891679748822606,
      "grad_norm": 4.275303840637207,
      "learning_rate": 4.506770015698588e-05,
      "loss": 0.68,
      "step": 502700
    },
    {
      "epoch": 7.893249607535322,
      "grad_norm": 4.712838649749756,
      "learning_rate": 4.506671899529043e-05,
      "loss": 0.6311,
      "step": 502800
    },
    {
      "epoch": 7.8948194662480375,
      "grad_norm": 4.0013041496276855,
      "learning_rate": 4.506573783359498e-05,
      "loss": 0.629,
      "step": 502900
    },
    {
      "epoch": 7.8963893249607535,
      "grad_norm": 3.2868521213531494,
      "learning_rate": 4.506475667189953e-05,
      "loss": 0.6322,
      "step": 503000
    },
    {
      "epoch": 7.8979591836734695,
      "grad_norm": 3.6476149559020996,
      "learning_rate": 4.506377551020408e-05,
      "loss": 0.6603,
      "step": 503100
    },
    {
      "epoch": 7.8995290423861855,
      "grad_norm": 4.630504131317139,
      "learning_rate": 4.506279434850864e-05,
      "loss": 0.6837,
      "step": 503200
    },
    {
      "epoch": 7.9010989010989015,
      "grad_norm": 4.476953506469727,
      "learning_rate": 4.506181318681319e-05,
      "loss": 0.6106,
      "step": 503300
    },
    {
      "epoch": 7.9026687598116165,
      "grad_norm": 4.450602054595947,
      "learning_rate": 4.506083202511775e-05,
      "loss": 0.6382,
      "step": 503400
    },
    {
      "epoch": 7.9042386185243325,
      "grad_norm": 3.8026809692382812,
      "learning_rate": 4.505985086342229e-05,
      "loss": 0.6335,
      "step": 503500
    },
    {
      "epoch": 7.9058084772370485,
      "grad_norm": 3.8058063983917236,
      "learning_rate": 4.505886970172685e-05,
      "loss": 0.6252,
      "step": 503600
    },
    {
      "epoch": 7.9073783359497645,
      "grad_norm": 3.4022252559661865,
      "learning_rate": 4.50578885400314e-05,
      "loss": 0.6685,
      "step": 503700
    },
    {
      "epoch": 7.9089481946624804,
      "grad_norm": 4.300821304321289,
      "learning_rate": 4.505690737833595e-05,
      "loss": 0.6633,
      "step": 503800
    },
    {
      "epoch": 7.910518053375196,
      "grad_norm": 4.458932876586914,
      "learning_rate": 4.50559262166405e-05,
      "loss": 0.6325,
      "step": 503900
    },
    {
      "epoch": 7.912087912087912,
      "grad_norm": 2.6457297801971436,
      "learning_rate": 4.505494505494506e-05,
      "loss": 0.6742,
      "step": 504000
    },
    {
      "epoch": 7.9136577708006275,
      "grad_norm": 3.510307788848877,
      "learning_rate": 4.505396389324961e-05,
      "loss": 0.5876,
      "step": 504100
    },
    {
      "epoch": 7.9152276295133435,
      "grad_norm": 3.951763868331909,
      "learning_rate": 4.505298273155416e-05,
      "loss": 0.6693,
      "step": 504200
    },
    {
      "epoch": 7.916797488226059,
      "grad_norm": 4.243913173675537,
      "learning_rate": 4.505200156985871e-05,
      "loss": 0.6399,
      "step": 504300
    },
    {
      "epoch": 7.918367346938775,
      "grad_norm": 4.2362165451049805,
      "learning_rate": 4.505102040816327e-05,
      "loss": 0.5961,
      "step": 504400
    },
    {
      "epoch": 7.919937205651491,
      "grad_norm": 4.7934064865112305,
      "learning_rate": 4.5050039246467815e-05,
      "loss": 0.6275,
      "step": 504500
    },
    {
      "epoch": 7.921507064364207,
      "grad_norm": 4.754087924957275,
      "learning_rate": 4.504905808477237e-05,
      "loss": 0.6266,
      "step": 504600
    },
    {
      "epoch": 7.923076923076923,
      "grad_norm": 4.069921493530273,
      "learning_rate": 4.5048076923076923e-05,
      "loss": 0.6624,
      "step": 504700
    },
    {
      "epoch": 7.924646781789638,
      "grad_norm": 4.083944320678711,
      "learning_rate": 4.504709576138148e-05,
      "loss": 0.6468,
      "step": 504800
    },
    {
      "epoch": 7.926216640502354,
      "grad_norm": 4.148378372192383,
      "learning_rate": 4.504611459968603e-05,
      "loss": 0.6312,
      "step": 504900
    },
    {
      "epoch": 7.92778649921507,
      "grad_norm": 3.638946056365967,
      "learning_rate": 4.504513343799058e-05,
      "loss": 0.6808,
      "step": 505000
    },
    {
      "epoch": 7.929356357927786,
      "grad_norm": 4.673056602478027,
      "learning_rate": 4.5044152276295134e-05,
      "loss": 0.6835,
      "step": 505100
    },
    {
      "epoch": 7.930926216640502,
      "grad_norm": 4.0167059898376465,
      "learning_rate": 4.5043171114599685e-05,
      "loss": 0.628,
      "step": 505200
    },
    {
      "epoch": 7.932496075353218,
      "grad_norm": 5.0015106201171875,
      "learning_rate": 4.504218995290424e-05,
      "loss": 0.66,
      "step": 505300
    },
    {
      "epoch": 7.934065934065934,
      "grad_norm": 1.7976704835891724,
      "learning_rate": 4.5041208791208794e-05,
      "loss": 0.6293,
      "step": 505400
    },
    {
      "epoch": 7.93563579277865,
      "grad_norm": 4.616443157196045,
      "learning_rate": 4.504022762951335e-05,
      "loss": 0.6738,
      "step": 505500
    },
    {
      "epoch": 7.937205651491366,
      "grad_norm": 4.087133407592773,
      "learning_rate": 4.5039246467817896e-05,
      "loss": 0.6733,
      "step": 505600
    },
    {
      "epoch": 7.938775510204081,
      "grad_norm": 4.6771135330200195,
      "learning_rate": 4.5038265306122454e-05,
      "loss": 0.6675,
      "step": 505700
    },
    {
      "epoch": 7.940345368916797,
      "grad_norm": 4.244614124298096,
      "learning_rate": 4.5037284144427005e-05,
      "loss": 0.67,
      "step": 505800
    },
    {
      "epoch": 7.941915227629513,
      "grad_norm": 4.325164318084717,
      "learning_rate": 4.5036302982731556e-05,
      "loss": 0.6637,
      "step": 505900
    },
    {
      "epoch": 7.943485086342229,
      "grad_norm": 3.8340282440185547,
      "learning_rate": 4.5035321821036107e-05,
      "loss": 0.6383,
      "step": 506000
    },
    {
      "epoch": 7.945054945054945,
      "grad_norm": 4.646467208862305,
      "learning_rate": 4.5034340659340664e-05,
      "loss": 0.6812,
      "step": 506100
    },
    {
      "epoch": 7.946624803767661,
      "grad_norm": 4.929853439331055,
      "learning_rate": 4.5033359497645215e-05,
      "loss": 0.6705,
      "step": 506200
    },
    {
      "epoch": 7.948194662480377,
      "grad_norm": 4.444495677947998,
      "learning_rate": 4.5032378335949766e-05,
      "loss": 0.6569,
      "step": 506300
    },
    {
      "epoch": 7.949764521193092,
      "grad_norm": 4.9557719230651855,
      "learning_rate": 4.503139717425432e-05,
      "loss": 0.6269,
      "step": 506400
    },
    {
      "epoch": 7.951334379905808,
      "grad_norm": 3.809955596923828,
      "learning_rate": 4.5030416012558875e-05,
      "loss": 0.6292,
      "step": 506500
    },
    {
      "epoch": 7.952904238618524,
      "grad_norm": 4.157133102416992,
      "learning_rate": 4.502943485086342e-05,
      "loss": 0.68,
      "step": 506600
    },
    {
      "epoch": 7.95447409733124,
      "grad_norm": 4.580371379852295,
      "learning_rate": 4.502845368916798e-05,
      "loss": 0.6186,
      "step": 506700
    },
    {
      "epoch": 7.956043956043956,
      "grad_norm": 3.044100284576416,
      "learning_rate": 4.502747252747253e-05,
      "loss": 0.6272,
      "step": 506800
    },
    {
      "epoch": 7.957613814756672,
      "grad_norm": 4.395911693572998,
      "learning_rate": 4.5026491365777086e-05,
      "loss": 0.6636,
      "step": 506900
    },
    {
      "epoch": 7.959183673469388,
      "grad_norm": 4.179964542388916,
      "learning_rate": 4.502551020408164e-05,
      "loss": 0.6659,
      "step": 507000
    },
    {
      "epoch": 7.960753532182103,
      "grad_norm": 5.020493030548096,
      "learning_rate": 4.502452904238619e-05,
      "loss": 0.6548,
      "step": 507100
    },
    {
      "epoch": 7.962323390894819,
      "grad_norm": 3.6140213012695312,
      "learning_rate": 4.502354788069074e-05,
      "loss": 0.6086,
      "step": 507200
    },
    {
      "epoch": 7.963893249607535,
      "grad_norm": 3.8901419639587402,
      "learning_rate": 4.502256671899529e-05,
      "loss": 0.6728,
      "step": 507300
    },
    {
      "epoch": 7.965463108320251,
      "grad_norm": 4.819034576416016,
      "learning_rate": 4.502158555729985e-05,
      "loss": 0.6441,
      "step": 507400
    },
    {
      "epoch": 7.967032967032967,
      "grad_norm": 4.808409214019775,
      "learning_rate": 4.50206043956044e-05,
      "loss": 0.6507,
      "step": 507500
    },
    {
      "epoch": 7.968602825745683,
      "grad_norm": 3.0239076614379883,
      "learning_rate": 4.5019623233908956e-05,
      "loss": 0.6674,
      "step": 507600
    },
    {
      "epoch": 7.970172684458399,
      "grad_norm": 3.6864066123962402,
      "learning_rate": 4.50186420722135e-05,
      "loss": 0.651,
      "step": 507700
    },
    {
      "epoch": 7.971742543171114,
      "grad_norm": 4.5799031257629395,
      "learning_rate": 4.501766091051806e-05,
      "loss": 0.6632,
      "step": 507800
    },
    {
      "epoch": 7.97331240188383,
      "grad_norm": 4.177787780761719,
      "learning_rate": 4.501667974882261e-05,
      "loss": 0.7212,
      "step": 507900
    },
    {
      "epoch": 7.974882260596546,
      "grad_norm": 4.4919819831848145,
      "learning_rate": 4.501569858712716e-05,
      "loss": 0.7088,
      "step": 508000
    },
    {
      "epoch": 7.976452119309262,
      "grad_norm": 3.782589912414551,
      "learning_rate": 4.501471742543171e-05,
      "loss": 0.6762,
      "step": 508100
    },
    {
      "epoch": 7.978021978021978,
      "grad_norm": 3.002924680709839,
      "learning_rate": 4.501373626373627e-05,
      "loss": 0.6709,
      "step": 508200
    },
    {
      "epoch": 7.979591836734694,
      "grad_norm": 5.395791530609131,
      "learning_rate": 4.501275510204082e-05,
      "loss": 0.678,
      "step": 508300
    },
    {
      "epoch": 7.98116169544741,
      "grad_norm": 4.12830114364624,
      "learning_rate": 4.501177394034537e-05,
      "loss": 0.6165,
      "step": 508400
    },
    {
      "epoch": 7.982731554160125,
      "grad_norm": 4.0082526206970215,
      "learning_rate": 4.501079277864992e-05,
      "loss": 0.6955,
      "step": 508500
    },
    {
      "epoch": 7.984301412872841,
      "grad_norm": 3.291598320007324,
      "learning_rate": 4.500981161695448e-05,
      "loss": 0.617,
      "step": 508600
    },
    {
      "epoch": 7.985871271585557,
      "grad_norm": 3.7360451221466064,
      "learning_rate": 4.5008830455259024e-05,
      "loss": 0.6762,
      "step": 508700
    },
    {
      "epoch": 7.987441130298273,
      "grad_norm": 3.8412890434265137,
      "learning_rate": 4.500784929356358e-05,
      "loss": 0.6828,
      "step": 508800
    },
    {
      "epoch": 7.989010989010989,
      "grad_norm": 3.790454626083374,
      "learning_rate": 4.500686813186813e-05,
      "loss": 0.6192,
      "step": 508900
    },
    {
      "epoch": 7.990580847723705,
      "grad_norm": 3.627631425857544,
      "learning_rate": 4.500588697017269e-05,
      "loss": 0.6777,
      "step": 509000
    },
    {
      "epoch": 7.992150706436421,
      "grad_norm": 4.733345031738281,
      "learning_rate": 4.500490580847724e-05,
      "loss": 0.6195,
      "step": 509100
    },
    {
      "epoch": 7.993720565149136,
      "grad_norm": 3.6838812828063965,
      "learning_rate": 4.500392464678179e-05,
      "loss": 0.6741,
      "step": 509200
    },
    {
      "epoch": 7.995290423861852,
      "grad_norm": 3.438567638397217,
      "learning_rate": 4.500294348508634e-05,
      "loss": 0.6219,
      "step": 509300
    },
    {
      "epoch": 7.996860282574568,
      "grad_norm": 3.4952609539031982,
      "learning_rate": 4.5001962323390894e-05,
      "loss": 0.6045,
      "step": 509400
    },
    {
      "epoch": 7.998430141287284,
      "grad_norm": 3.27622127532959,
      "learning_rate": 4.500098116169545e-05,
      "loss": 0.6807,
      "step": 509500
    },
    {
      "epoch": 8.0,
      "grad_norm": 4.0394368171691895,
      "learning_rate": 4.5e-05,
      "loss": 0.6528,
      "step": 509600
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.018864631652832,
      "eval_runtime": 14.8325,
      "eval_samples_per_second": 226.058,
      "eval_steps_per_second": 226.058,
      "step": 509600
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.503132164478302,
      "eval_runtime": 280.0518,
      "eval_samples_per_second": 227.458,
      "eval_steps_per_second": 227.458,
      "step": 509600
    },
    {
      "epoch": 8.001569858712715,
      "grad_norm": 4.242687702178955,
      "learning_rate": 4.499901883830456e-05,
      "loss": 0.6851,
      "step": 509700
    },
    {
      "epoch": 8.003139717425432,
      "grad_norm": 4.469395160675049,
      "learning_rate": 4.4998037676609105e-05,
      "loss": 0.6401,
      "step": 509800
    },
    {
      "epoch": 8.004709576138147,
      "grad_norm": 2.743016242980957,
      "learning_rate": 4.499705651491366e-05,
      "loss": 0.6175,
      "step": 509900
    },
    {
      "epoch": 8.006279434850864,
      "grad_norm": 4.298671722412109,
      "learning_rate": 4.4996075353218214e-05,
      "loss": 0.5978,
      "step": 510000
    },
    {
      "epoch": 8.007849293563579,
      "grad_norm": 3.6611907482147217,
      "learning_rate": 4.4995094191522765e-05,
      "loss": 0.6212,
      "step": 510100
    },
    {
      "epoch": 8.009419152276296,
      "grad_norm": 4.334061622619629,
      "learning_rate": 4.4994113029827315e-05,
      "loss": 0.6567,
      "step": 510200
    },
    {
      "epoch": 8.010989010989011,
      "grad_norm": 2.7359094619750977,
      "learning_rate": 4.499313186813187e-05,
      "loss": 0.6127,
      "step": 510300
    },
    {
      "epoch": 8.012558869701726,
      "grad_norm": 3.775719165802002,
      "learning_rate": 4.4992150706436424e-05,
      "loss": 0.6439,
      "step": 510400
    },
    {
      "epoch": 8.014128728414443,
      "grad_norm": 4.516295433044434,
      "learning_rate": 4.4991169544740975e-05,
      "loss": 0.6459,
      "step": 510500
    },
    {
      "epoch": 8.015698587127158,
      "grad_norm": 4.42476749420166,
      "learning_rate": 4.4990188383045526e-05,
      "loss": 0.6537,
      "step": 510600
    },
    {
      "epoch": 8.017268445839875,
      "grad_norm": 3.7646806240081787,
      "learning_rate": 4.4989207221350084e-05,
      "loss": 0.6544,
      "step": 510700
    },
    {
      "epoch": 8.01883830455259,
      "grad_norm": 4.179827690124512,
      "learning_rate": 4.498822605965463e-05,
      "loss": 0.6366,
      "step": 510800
    },
    {
      "epoch": 8.020408163265307,
      "grad_norm": 3.758852481842041,
      "learning_rate": 4.4987244897959186e-05,
      "loss": 0.6292,
      "step": 510900
    },
    {
      "epoch": 8.021978021978022,
      "grad_norm": 4.021727085113525,
      "learning_rate": 4.498626373626374e-05,
      "loss": 0.6544,
      "step": 511000
    },
    {
      "epoch": 8.023547880690737,
      "grad_norm": 4.164616584777832,
      "learning_rate": 4.4985282574568295e-05,
      "loss": 0.6043,
      "step": 511100
    },
    {
      "epoch": 8.025117739403454,
      "grad_norm": 2.5130410194396973,
      "learning_rate": 4.4984301412872846e-05,
      "loss": 0.6344,
      "step": 511200
    },
    {
      "epoch": 8.026687598116169,
      "grad_norm": 4.409287929534912,
      "learning_rate": 4.49833202511774e-05,
      "loss": 0.6236,
      "step": 511300
    },
    {
      "epoch": 8.028257456828886,
      "grad_norm": 4.534342288970947,
      "learning_rate": 4.498233908948195e-05,
      "loss": 0.632,
      "step": 511400
    },
    {
      "epoch": 8.029827315541601,
      "grad_norm": 4.9688334465026855,
      "learning_rate": 4.49813579277865e-05,
      "loss": 0.6528,
      "step": 511500
    },
    {
      "epoch": 8.031397174254318,
      "grad_norm": 3.807053565979004,
      "learning_rate": 4.4980376766091056e-05,
      "loss": 0.598,
      "step": 511600
    },
    {
      "epoch": 8.032967032967033,
      "grad_norm": 4.0128397941589355,
      "learning_rate": 4.497939560439561e-05,
      "loss": 0.581,
      "step": 511700
    },
    {
      "epoch": 8.034536891679748,
      "grad_norm": 3.9351625442504883,
      "learning_rate": 4.4978414442700165e-05,
      "loss": 0.621,
      "step": 511800
    },
    {
      "epoch": 8.036106750392465,
      "grad_norm": 3.5377490520477295,
      "learning_rate": 4.497743328100471e-05,
      "loss": 0.66,
      "step": 511900
    },
    {
      "epoch": 8.03767660910518,
      "grad_norm": 4.654923439025879,
      "learning_rate": 4.497645211930927e-05,
      "loss": 0.6527,
      "step": 512000
    },
    {
      "epoch": 8.039246467817897,
      "grad_norm": 4.192917823791504,
      "learning_rate": 4.497547095761382e-05,
      "loss": 0.6282,
      "step": 512100
    },
    {
      "epoch": 8.040816326530612,
      "grad_norm": 3.7026119232177734,
      "learning_rate": 4.497448979591837e-05,
      "loss": 0.66,
      "step": 512200
    },
    {
      "epoch": 8.042386185243329,
      "grad_norm": 3.5766685009002686,
      "learning_rate": 4.497350863422292e-05,
      "loss": 0.6504,
      "step": 512300
    },
    {
      "epoch": 8.043956043956044,
      "grad_norm": 4.6989617347717285,
      "learning_rate": 4.497252747252748e-05,
      "loss": 0.637,
      "step": 512400
    },
    {
      "epoch": 8.04552590266876,
      "grad_norm": 4.7281928062438965,
      "learning_rate": 4.497154631083202e-05,
      "loss": 0.6589,
      "step": 512500
    },
    {
      "epoch": 8.047095761381476,
      "grad_norm": 4.341664791107178,
      "learning_rate": 4.497056514913658e-05,
      "loss": 0.6554,
      "step": 512600
    },
    {
      "epoch": 8.04866562009419,
      "grad_norm": 3.5629959106445312,
      "learning_rate": 4.496958398744113e-05,
      "loss": 0.6559,
      "step": 512700
    },
    {
      "epoch": 8.050235478806908,
      "grad_norm": 4.544864177703857,
      "learning_rate": 4.496860282574569e-05,
      "loss": 0.6357,
      "step": 512800
    },
    {
      "epoch": 8.051805337519623,
      "grad_norm": 4.56658935546875,
      "learning_rate": 4.496762166405023e-05,
      "loss": 0.6481,
      "step": 512900
    },
    {
      "epoch": 8.05337519623234,
      "grad_norm": 3.7336506843566895,
      "learning_rate": 4.496664050235479e-05,
      "loss": 0.5947,
      "step": 513000
    },
    {
      "epoch": 8.054945054945055,
      "grad_norm": 4.057068347930908,
      "learning_rate": 4.496565934065934e-05,
      "loss": 0.6181,
      "step": 513100
    },
    {
      "epoch": 8.056514913657772,
      "grad_norm": 5.137832164764404,
      "learning_rate": 4.496467817896389e-05,
      "loss": 0.6326,
      "step": 513200
    },
    {
      "epoch": 8.058084772370487,
      "grad_norm": 4.128127574920654,
      "learning_rate": 4.496369701726845e-05,
      "loss": 0.6531,
      "step": 513300
    },
    {
      "epoch": 8.059654631083202,
      "grad_norm": 3.482174873352051,
      "learning_rate": 4.4962715855573e-05,
      "loss": 0.6105,
      "step": 513400
    },
    {
      "epoch": 8.061224489795919,
      "grad_norm": 4.721871852874756,
      "learning_rate": 4.496173469387755e-05,
      "loss": 0.6507,
      "step": 513500
    },
    {
      "epoch": 8.062794348508634,
      "grad_norm": 4.155844688415527,
      "learning_rate": 4.49607535321821e-05,
      "loss": 0.6213,
      "step": 513600
    },
    {
      "epoch": 8.06436420722135,
      "grad_norm": 5.117886066436768,
      "learning_rate": 4.495977237048666e-05,
      "loss": 0.6439,
      "step": 513700
    },
    {
      "epoch": 8.065934065934066,
      "grad_norm": 4.384549140930176,
      "learning_rate": 4.495879120879121e-05,
      "loss": 0.6575,
      "step": 513800
    },
    {
      "epoch": 8.067503924646783,
      "grad_norm": 3.5843048095703125,
      "learning_rate": 4.495781004709576e-05,
      "loss": 0.6122,
      "step": 513900
    },
    {
      "epoch": 8.069073783359498,
      "grad_norm": 2.725214719772339,
      "learning_rate": 4.4956828885400314e-05,
      "loss": 0.5867,
      "step": 514000
    },
    {
      "epoch": 8.070643642072213,
      "grad_norm": 4.852629661560059,
      "learning_rate": 4.495584772370487e-05,
      "loss": 0.6461,
      "step": 514100
    },
    {
      "epoch": 8.07221350078493,
      "grad_norm": 3.5923714637756348,
      "learning_rate": 4.495486656200942e-05,
      "loss": 0.6299,
      "step": 514200
    },
    {
      "epoch": 8.073783359497645,
      "grad_norm": 4.710148334503174,
      "learning_rate": 4.4953885400313973e-05,
      "loss": 0.6392,
      "step": 514300
    },
    {
      "epoch": 8.075353218210362,
      "grad_norm": 3.8358333110809326,
      "learning_rate": 4.4952904238618524e-05,
      "loss": 0.6694,
      "step": 514400
    },
    {
      "epoch": 8.076923076923077,
      "grad_norm": 2.8247945308685303,
      "learning_rate": 4.495192307692308e-05,
      "loss": 0.6265,
      "step": 514500
    },
    {
      "epoch": 8.078492935635794,
      "grad_norm": 3.017829179763794,
      "learning_rate": 4.4950941915227626e-05,
      "loss": 0.6299,
      "step": 514600
    },
    {
      "epoch": 8.080062794348509,
      "grad_norm": 4.5231547355651855,
      "learning_rate": 4.4949960753532184e-05,
      "loss": 0.6485,
      "step": 514700
    },
    {
      "epoch": 8.081632653061224,
      "grad_norm": 4.6150031089782715,
      "learning_rate": 4.4948979591836735e-05,
      "loss": 0.6704,
      "step": 514800
    },
    {
      "epoch": 8.08320251177394,
      "grad_norm": 4.267635345458984,
      "learning_rate": 4.494799843014129e-05,
      "loss": 0.6507,
      "step": 514900
    },
    {
      "epoch": 8.084772370486656,
      "grad_norm": 3.7325215339660645,
      "learning_rate": 4.494701726844584e-05,
      "loss": 0.645,
      "step": 515000
    },
    {
      "epoch": 8.086342229199373,
      "grad_norm": 2.774656295776367,
      "learning_rate": 4.4946036106750395e-05,
      "loss": 0.6635,
      "step": 515100
    },
    {
      "epoch": 8.087912087912088,
      "grad_norm": 3.5095648765563965,
      "learning_rate": 4.4945054945054946e-05,
      "loss": 0.6473,
      "step": 515200
    },
    {
      "epoch": 8.089481946624804,
      "grad_norm": 4.7614054679870605,
      "learning_rate": 4.49440737833595e-05,
      "loss": 0.6514,
      "step": 515300
    },
    {
      "epoch": 8.09105180533752,
      "grad_norm": 3.888643264770508,
      "learning_rate": 4.4943092621664055e-05,
      "loss": 0.6286,
      "step": 515400
    },
    {
      "epoch": 8.092621664050235,
      "grad_norm": 3.1009652614593506,
      "learning_rate": 4.4942111459968606e-05,
      "loss": 0.62,
      "step": 515500
    },
    {
      "epoch": 8.094191522762952,
      "grad_norm": 5.641409873962402,
      "learning_rate": 4.4941130298273157e-05,
      "loss": 0.6456,
      "step": 515600
    },
    {
      "epoch": 8.095761381475667,
      "grad_norm": 3.9797794818878174,
      "learning_rate": 4.494014913657771e-05,
      "loss": 0.625,
      "step": 515700
    },
    {
      "epoch": 8.097331240188383,
      "grad_norm": 4.557952880859375,
      "learning_rate": 4.4939167974882265e-05,
      "loss": 0.6342,
      "step": 515800
    },
    {
      "epoch": 8.098901098901099,
      "grad_norm": 3.5000672340393066,
      "learning_rate": 4.4938186813186816e-05,
      "loss": 0.5963,
      "step": 515900
    },
    {
      "epoch": 8.100470957613815,
      "grad_norm": 4.177694797515869,
      "learning_rate": 4.493720565149137e-05,
      "loss": 0.6592,
      "step": 516000
    },
    {
      "epoch": 8.10204081632653,
      "grad_norm": 4.245131492614746,
      "learning_rate": 4.493622448979592e-05,
      "loss": 0.6538,
      "step": 516100
    },
    {
      "epoch": 8.103610675039246,
      "grad_norm": 3.2945234775543213,
      "learning_rate": 4.4935243328100476e-05,
      "loss": 0.656,
      "step": 516200
    },
    {
      "epoch": 8.105180533751962,
      "grad_norm": 4.270898342132568,
      "learning_rate": 4.493426216640503e-05,
      "loss": 0.684,
      "step": 516300
    },
    {
      "epoch": 8.106750392464678,
      "grad_norm": 4.244873523712158,
      "learning_rate": 4.493328100470958e-05,
      "loss": 0.6778,
      "step": 516400
    },
    {
      "epoch": 8.108320251177394,
      "grad_norm": 4.327705383300781,
      "learning_rate": 4.493229984301413e-05,
      "loss": 0.5945,
      "step": 516500
    },
    {
      "epoch": 8.10989010989011,
      "grad_norm": 5.757964611053467,
      "learning_rate": 4.493131868131869e-05,
      "loss": 0.6754,
      "step": 516600
    },
    {
      "epoch": 8.111459968602826,
      "grad_norm": 4.584496974945068,
      "learning_rate": 4.493033751962323e-05,
      "loss": 0.6192,
      "step": 516700
    },
    {
      "epoch": 8.113029827315541,
      "grad_norm": 4.157893180847168,
      "learning_rate": 4.492935635792779e-05,
      "loss": 0.6349,
      "step": 516800
    },
    {
      "epoch": 8.114599686028258,
      "grad_norm": 3.073273181915283,
      "learning_rate": 4.492837519623234e-05,
      "loss": 0.6335,
      "step": 516900
    },
    {
      "epoch": 8.116169544740973,
      "grad_norm": 4.044925212860107,
      "learning_rate": 4.49273940345369e-05,
      "loss": 0.6685,
      "step": 517000
    },
    {
      "epoch": 8.117739403453688,
      "grad_norm": 4.057754993438721,
      "learning_rate": 4.492641287284144e-05,
      "loss": 0.6454,
      "step": 517100
    },
    {
      "epoch": 8.119309262166405,
      "grad_norm": 3.9919161796569824,
      "learning_rate": 4.4925431711146e-05,
      "loss": 0.6427,
      "step": 517200
    },
    {
      "epoch": 8.12087912087912,
      "grad_norm": 4.286557197570801,
      "learning_rate": 4.492445054945055e-05,
      "loss": 0.6342,
      "step": 517300
    },
    {
      "epoch": 8.122448979591837,
      "grad_norm": 2.9356741905212402,
      "learning_rate": 4.49234693877551e-05,
      "loss": 0.6362,
      "step": 517400
    },
    {
      "epoch": 8.124018838304552,
      "grad_norm": 3.289193630218506,
      "learning_rate": 4.492248822605966e-05,
      "loss": 0.6637,
      "step": 517500
    },
    {
      "epoch": 8.12558869701727,
      "grad_norm": 4.266867160797119,
      "learning_rate": 4.492150706436421e-05,
      "loss": 0.664,
      "step": 517600
    },
    {
      "epoch": 8.127158555729984,
      "grad_norm": 3.8966667652130127,
      "learning_rate": 4.492052590266876e-05,
      "loss": 0.6088,
      "step": 517700
    },
    {
      "epoch": 8.1287284144427,
      "grad_norm": 4.149702548980713,
      "learning_rate": 4.491954474097331e-05,
      "loss": 0.6638,
      "step": 517800
    },
    {
      "epoch": 8.130298273155416,
      "grad_norm": 4.647461414337158,
      "learning_rate": 4.491856357927787e-05,
      "loss": 0.6318,
      "step": 517900
    },
    {
      "epoch": 8.131868131868131,
      "grad_norm": 3.812513589859009,
      "learning_rate": 4.491758241758242e-05,
      "loss": 0.6643,
      "step": 518000
    },
    {
      "epoch": 8.133437990580848,
      "grad_norm": 3.7640440464019775,
      "learning_rate": 4.491660125588697e-05,
      "loss": 0.6589,
      "step": 518100
    },
    {
      "epoch": 8.135007849293563,
      "grad_norm": 3.463369607925415,
      "learning_rate": 4.491562009419152e-05,
      "loss": 0.6466,
      "step": 518200
    },
    {
      "epoch": 8.13657770800628,
      "grad_norm": 2.7706046104431152,
      "learning_rate": 4.491463893249608e-05,
      "loss": 0.6574,
      "step": 518300
    },
    {
      "epoch": 8.138147566718995,
      "grad_norm": 3.0131306648254395,
      "learning_rate": 4.491365777080063e-05,
      "loss": 0.6856,
      "step": 518400
    },
    {
      "epoch": 8.13971742543171,
      "grad_norm": 2.199110984802246,
      "learning_rate": 4.491267660910518e-05,
      "loss": 0.6349,
      "step": 518500
    },
    {
      "epoch": 8.141287284144427,
      "grad_norm": 4.942772388458252,
      "learning_rate": 4.4911695447409733e-05,
      "loss": 0.6292,
      "step": 518600
    },
    {
      "epoch": 8.142857142857142,
      "grad_norm": 4.178452968597412,
      "learning_rate": 4.491071428571429e-05,
      "loss": 0.656,
      "step": 518700
    },
    {
      "epoch": 8.14442700156986,
      "grad_norm": 4.050422191619873,
      "learning_rate": 4.4909733124018835e-05,
      "loss": 0.6431,
      "step": 518800
    },
    {
      "epoch": 8.145996860282574,
      "grad_norm": 3.881131887435913,
      "learning_rate": 4.490875196232339e-05,
      "loss": 0.6434,
      "step": 518900
    },
    {
      "epoch": 8.147566718995291,
      "grad_norm": 4.855235576629639,
      "learning_rate": 4.4907770800627944e-05,
      "loss": 0.6933,
      "step": 519000
    },
    {
      "epoch": 8.149136577708006,
      "grad_norm": 4.351293563842773,
      "learning_rate": 4.49067896389325e-05,
      "loss": 0.6445,
      "step": 519100
    },
    {
      "epoch": 8.150706436420721,
      "grad_norm": 5.164736747741699,
      "learning_rate": 4.4905808477237046e-05,
      "loss": 0.6101,
      "step": 519200
    },
    {
      "epoch": 8.152276295133438,
      "grad_norm": 3.4527225494384766,
      "learning_rate": 4.4904827315541604e-05,
      "loss": 0.6337,
      "step": 519300
    },
    {
      "epoch": 8.153846153846153,
      "grad_norm": 2.7823009490966797,
      "learning_rate": 4.4903846153846155e-05,
      "loss": 0.6919,
      "step": 519400
    },
    {
      "epoch": 8.15541601255887,
      "grad_norm": 3.0727477073669434,
      "learning_rate": 4.4902864992150706e-05,
      "loss": 0.6625,
      "step": 519500
    },
    {
      "epoch": 8.156985871271585,
      "grad_norm": 4.085124969482422,
      "learning_rate": 4.4901883830455264e-05,
      "loss": 0.6755,
      "step": 519600
    },
    {
      "epoch": 8.158555729984302,
      "grad_norm": 3.4364430904388428,
      "learning_rate": 4.4900902668759815e-05,
      "loss": 0.6219,
      "step": 519700
    },
    {
      "epoch": 8.160125588697017,
      "grad_norm": 4.231364727020264,
      "learning_rate": 4.4899921507064366e-05,
      "loss": 0.6007,
      "step": 519800
    },
    {
      "epoch": 8.161695447409732,
      "grad_norm": 4.598134517669678,
      "learning_rate": 4.4898940345368917e-05,
      "loss": 0.614,
      "step": 519900
    },
    {
      "epoch": 8.16326530612245,
      "grad_norm": 2.8530054092407227,
      "learning_rate": 4.4897959183673474e-05,
      "loss": 0.6484,
      "step": 520000
    },
    {
      "epoch": 8.164835164835164,
      "grad_norm": 4.923865795135498,
      "learning_rate": 4.4896978021978025e-05,
      "loss": 0.6208,
      "step": 520100
    },
    {
      "epoch": 8.166405023547881,
      "grad_norm": 3.956770658493042,
      "learning_rate": 4.4895996860282576e-05,
      "loss": 0.666,
      "step": 520200
    },
    {
      "epoch": 8.167974882260596,
      "grad_norm": 3.9882707595825195,
      "learning_rate": 4.489501569858713e-05,
      "loss": 0.6596,
      "step": 520300
    },
    {
      "epoch": 8.169544740973313,
      "grad_norm": 4.533645153045654,
      "learning_rate": 4.4894034536891685e-05,
      "loss": 0.6873,
      "step": 520400
    },
    {
      "epoch": 8.171114599686028,
      "grad_norm": 4.8014397621154785,
      "learning_rate": 4.4893053375196236e-05,
      "loss": 0.6444,
      "step": 520500
    },
    {
      "epoch": 8.172684458398743,
      "grad_norm": 4.042928218841553,
      "learning_rate": 4.489207221350079e-05,
      "loss": 0.6195,
      "step": 520600
    },
    {
      "epoch": 8.17425431711146,
      "grad_norm": 3.3217368125915527,
      "learning_rate": 4.489109105180534e-05,
      "loss": 0.6538,
      "step": 520700
    },
    {
      "epoch": 8.175824175824175,
      "grad_norm": 4.036296367645264,
      "learning_rate": 4.4890109890109896e-05,
      "loss": 0.6571,
      "step": 520800
    },
    {
      "epoch": 8.177394034536892,
      "grad_norm": 4.071994781494141,
      "learning_rate": 4.488912872841444e-05,
      "loss": 0.6228,
      "step": 520900
    },
    {
      "epoch": 8.178963893249607,
      "grad_norm": 4.278880596160889,
      "learning_rate": 4.4888147566719e-05,
      "loss": 0.6448,
      "step": 521000
    },
    {
      "epoch": 8.180533751962324,
      "grad_norm": 4.2120866775512695,
      "learning_rate": 4.488716640502355e-05,
      "loss": 0.6247,
      "step": 521100
    },
    {
      "epoch": 8.182103610675039,
      "grad_norm": 4.589147567749023,
      "learning_rate": 4.4886185243328106e-05,
      "loss": 0.6608,
      "step": 521200
    },
    {
      "epoch": 8.183673469387756,
      "grad_norm": 4.529997825622559,
      "learning_rate": 4.488520408163265e-05,
      "loss": 0.6414,
      "step": 521300
    },
    {
      "epoch": 8.185243328100471,
      "grad_norm": 2.1450743675231934,
      "learning_rate": 4.488422291993721e-05,
      "loss": 0.6157,
      "step": 521400
    },
    {
      "epoch": 8.186813186813186,
      "grad_norm": 2.9252147674560547,
      "learning_rate": 4.488324175824176e-05,
      "loss": 0.6375,
      "step": 521500
    },
    {
      "epoch": 8.188383045525903,
      "grad_norm": 3.8914740085601807,
      "learning_rate": 4.488226059654631e-05,
      "loss": 0.6353,
      "step": 521600
    },
    {
      "epoch": 8.189952904238618,
      "grad_norm": 3.871157646179199,
      "learning_rate": 4.488127943485087e-05,
      "loss": 0.6177,
      "step": 521700
    },
    {
      "epoch": 8.191522762951335,
      "grad_norm": 4.113392353057861,
      "learning_rate": 4.488029827315542e-05,
      "loss": 0.6422,
      "step": 521800
    },
    {
      "epoch": 8.19309262166405,
      "grad_norm": 4.390649318695068,
      "learning_rate": 4.487931711145997e-05,
      "loss": 0.6214,
      "step": 521900
    },
    {
      "epoch": 8.194662480376767,
      "grad_norm": 4.588747978210449,
      "learning_rate": 4.487833594976452e-05,
      "loss": 0.6305,
      "step": 522000
    },
    {
      "epoch": 8.196232339089482,
      "grad_norm": 4.771842956542969,
      "learning_rate": 4.487735478806908e-05,
      "loss": 0.6437,
      "step": 522100
    },
    {
      "epoch": 8.197802197802197,
      "grad_norm": 3.5838210582733154,
      "learning_rate": 4.487637362637363e-05,
      "loss": 0.6509,
      "step": 522200
    },
    {
      "epoch": 8.199372056514914,
      "grad_norm": 2.825913429260254,
      "learning_rate": 4.487539246467818e-05,
      "loss": 0.6514,
      "step": 522300
    },
    {
      "epoch": 8.200941915227629,
      "grad_norm": 4.683938980102539,
      "learning_rate": 4.487441130298273e-05,
      "loss": 0.5911,
      "step": 522400
    },
    {
      "epoch": 8.202511773940346,
      "grad_norm": 3.7337820529937744,
      "learning_rate": 4.487343014128729e-05,
      "loss": 0.6747,
      "step": 522500
    },
    {
      "epoch": 8.204081632653061,
      "grad_norm": 4.332239151000977,
      "learning_rate": 4.487244897959184e-05,
      "loss": 0.6187,
      "step": 522600
    },
    {
      "epoch": 8.205651491365778,
      "grad_norm": 3.53818941116333,
      "learning_rate": 4.487146781789639e-05,
      "loss": 0.6224,
      "step": 522700
    },
    {
      "epoch": 8.207221350078493,
      "grad_norm": 4.207333564758301,
      "learning_rate": 4.487048665620094e-05,
      "loss": 0.6625,
      "step": 522800
    },
    {
      "epoch": 8.208791208791208,
      "grad_norm": 4.702861785888672,
      "learning_rate": 4.48695054945055e-05,
      "loss": 0.6357,
      "step": 522900
    },
    {
      "epoch": 8.210361067503925,
      "grad_norm": 4.068239688873291,
      "learning_rate": 4.4868524332810044e-05,
      "loss": 0.5862,
      "step": 523000
    },
    {
      "epoch": 8.21193092621664,
      "grad_norm": 4.369140625,
      "learning_rate": 4.48675431711146e-05,
      "loss": 0.6335,
      "step": 523100
    },
    {
      "epoch": 8.213500784929357,
      "grad_norm": 4.318854808807373,
      "learning_rate": 4.486656200941915e-05,
      "loss": 0.6496,
      "step": 523200
    },
    {
      "epoch": 8.215070643642072,
      "grad_norm": 2.984241008758545,
      "learning_rate": 4.486558084772371e-05,
      "loss": 0.6343,
      "step": 523300
    },
    {
      "epoch": 8.216640502354789,
      "grad_norm": 3.9206783771514893,
      "learning_rate": 4.4864599686028255e-05,
      "loss": 0.5594,
      "step": 523400
    },
    {
      "epoch": 8.218210361067504,
      "grad_norm": 4.410781383514404,
      "learning_rate": 4.486361852433281e-05,
      "loss": 0.6886,
      "step": 523500
    },
    {
      "epoch": 8.219780219780219,
      "grad_norm": 3.8809237480163574,
      "learning_rate": 4.4862637362637364e-05,
      "loss": 0.6499,
      "step": 523600
    },
    {
      "epoch": 8.221350078492936,
      "grad_norm": 3.4920196533203125,
      "learning_rate": 4.4861656200941915e-05,
      "loss": 0.6381,
      "step": 523700
    },
    {
      "epoch": 8.222919937205651,
      "grad_norm": 3.578723907470703,
      "learning_rate": 4.4860675039246466e-05,
      "loss": 0.6481,
      "step": 523800
    },
    {
      "epoch": 8.224489795918368,
      "grad_norm": 4.065830230712891,
      "learning_rate": 4.4859693877551024e-05,
      "loss": 0.6465,
      "step": 523900
    },
    {
      "epoch": 8.226059654631083,
      "grad_norm": 3.439362049102783,
      "learning_rate": 4.4858712715855575e-05,
      "loss": 0.6258,
      "step": 524000
    },
    {
      "epoch": 8.2276295133438,
      "grad_norm": 4.215141773223877,
      "learning_rate": 4.4857731554160126e-05,
      "loss": 0.6782,
      "step": 524100
    },
    {
      "epoch": 8.229199372056515,
      "grad_norm": 4.135444641113281,
      "learning_rate": 4.485675039246468e-05,
      "loss": 0.6553,
      "step": 524200
    },
    {
      "epoch": 8.23076923076923,
      "grad_norm": 4.58133602142334,
      "learning_rate": 4.4855769230769234e-05,
      "loss": 0.6541,
      "step": 524300
    },
    {
      "epoch": 8.232339089481947,
      "grad_norm": 4.187436580657959,
      "learning_rate": 4.4854788069073785e-05,
      "loss": 0.6233,
      "step": 524400
    },
    {
      "epoch": 8.233908948194662,
      "grad_norm": 4.380456924438477,
      "learning_rate": 4.4853806907378336e-05,
      "loss": 0.6571,
      "step": 524500
    },
    {
      "epoch": 8.235478806907379,
      "grad_norm": 3.1572515964508057,
      "learning_rate": 4.4852825745682894e-05,
      "loss": 0.6375,
      "step": 524600
    },
    {
      "epoch": 8.237048665620094,
      "grad_norm": 5.209408283233643,
      "learning_rate": 4.4851844583987445e-05,
      "loss": 0.6433,
      "step": 524700
    },
    {
      "epoch": 8.23861852433281,
      "grad_norm": 4.816285610198975,
      "learning_rate": 4.4850863422291996e-05,
      "loss": 0.6378,
      "step": 524800
    },
    {
      "epoch": 8.240188383045526,
      "grad_norm": 4.110683917999268,
      "learning_rate": 4.484988226059655e-05,
      "loss": 0.6085,
      "step": 524900
    },
    {
      "epoch": 8.241758241758241,
      "grad_norm": 4.940200328826904,
      "learning_rate": 4.4848901098901105e-05,
      "loss": 0.6746,
      "step": 525000
    },
    {
      "epoch": 8.243328100470958,
      "grad_norm": 3.3036859035491943,
      "learning_rate": 4.484791993720565e-05,
      "loss": 0.634,
      "step": 525100
    },
    {
      "epoch": 8.244897959183673,
      "grad_norm": 2.830061674118042,
      "learning_rate": 4.484693877551021e-05,
      "loss": 0.6329,
      "step": 525200
    },
    {
      "epoch": 8.24646781789639,
      "grad_norm": 3.5725252628326416,
      "learning_rate": 4.484595761381476e-05,
      "loss": 0.6331,
      "step": 525300
    },
    {
      "epoch": 8.248037676609105,
      "grad_norm": 2.9441075325012207,
      "learning_rate": 4.4844976452119315e-05,
      "loss": 0.6173,
      "step": 525400
    },
    {
      "epoch": 8.249607535321822,
      "grad_norm": 4.914233684539795,
      "learning_rate": 4.484399529042386e-05,
      "loss": 0.646,
      "step": 525500
    },
    {
      "epoch": 8.251177394034537,
      "grad_norm": 3.886608123779297,
      "learning_rate": 4.484301412872842e-05,
      "loss": 0.6372,
      "step": 525600
    },
    {
      "epoch": 8.252747252747252,
      "grad_norm": 3.7887370586395264,
      "learning_rate": 4.484203296703297e-05,
      "loss": 0.64,
      "step": 525700
    },
    {
      "epoch": 8.254317111459969,
      "grad_norm": 2.900068521499634,
      "learning_rate": 4.484105180533752e-05,
      "loss": 0.715,
      "step": 525800
    },
    {
      "epoch": 8.255886970172684,
      "grad_norm": 3.8259644508361816,
      "learning_rate": 4.484007064364207e-05,
      "loss": 0.6048,
      "step": 525900
    },
    {
      "epoch": 8.2574568288854,
      "grad_norm": 3.865527629852295,
      "learning_rate": 4.483908948194663e-05,
      "loss": 0.665,
      "step": 526000
    },
    {
      "epoch": 8.259026687598116,
      "grad_norm": 3.2556564807891846,
      "learning_rate": 4.483810832025118e-05,
      "loss": 0.597,
      "step": 526100
    },
    {
      "epoch": 8.260596546310833,
      "grad_norm": 4.722154140472412,
      "learning_rate": 4.483712715855573e-05,
      "loss": 0.6749,
      "step": 526200
    },
    {
      "epoch": 8.262166405023548,
      "grad_norm": 3.8695662021636963,
      "learning_rate": 4.483614599686029e-05,
      "loss": 0.6474,
      "step": 526300
    },
    {
      "epoch": 8.263736263736265,
      "grad_norm": 3.804967164993286,
      "learning_rate": 4.483516483516484e-05,
      "loss": 0.6591,
      "step": 526400
    },
    {
      "epoch": 8.26530612244898,
      "grad_norm": 4.0463457107543945,
      "learning_rate": 4.483418367346939e-05,
      "loss": 0.6467,
      "step": 526500
    },
    {
      "epoch": 8.266875981161695,
      "grad_norm": 3.0601298809051514,
      "learning_rate": 4.483320251177394e-05,
      "loss": 0.6021,
      "step": 526600
    },
    {
      "epoch": 8.268445839874412,
      "grad_norm": 4.206435203552246,
      "learning_rate": 4.48322213500785e-05,
      "loss": 0.6156,
      "step": 526700
    },
    {
      "epoch": 8.270015698587127,
      "grad_norm": 3.7739808559417725,
      "learning_rate": 4.483124018838305e-05,
      "loss": 0.6784,
      "step": 526800
    },
    {
      "epoch": 8.271585557299844,
      "grad_norm": 4.168003559112549,
      "learning_rate": 4.48302590266876e-05,
      "loss": 0.6613,
      "step": 526900
    },
    {
      "epoch": 8.273155416012559,
      "grad_norm": 3.521066665649414,
      "learning_rate": 4.482927786499215e-05,
      "loss": 0.6063,
      "step": 527000
    },
    {
      "epoch": 8.274725274725276,
      "grad_norm": 3.27634859085083,
      "learning_rate": 4.482829670329671e-05,
      "loss": 0.6568,
      "step": 527100
    },
    {
      "epoch": 8.27629513343799,
      "grad_norm": 4.277988910675049,
      "learning_rate": 4.482731554160125e-05,
      "loss": 0.6338,
      "step": 527200
    },
    {
      "epoch": 8.277864992150706,
      "grad_norm": 3.4101006984710693,
      "learning_rate": 4.482633437990581e-05,
      "loss": 0.6435,
      "step": 527300
    },
    {
      "epoch": 8.279434850863423,
      "grad_norm": 4.176324367523193,
      "learning_rate": 4.482535321821036e-05,
      "loss": 0.6697,
      "step": 527400
    },
    {
      "epoch": 8.281004709576138,
      "grad_norm": 3.958913803100586,
      "learning_rate": 4.482437205651492e-05,
      "loss": 0.5958,
      "step": 527500
    },
    {
      "epoch": 8.282574568288855,
      "grad_norm": 4.076059818267822,
      "learning_rate": 4.4823390894819464e-05,
      "loss": 0.6689,
      "step": 527600
    },
    {
      "epoch": 8.28414442700157,
      "grad_norm": 4.882373809814453,
      "learning_rate": 4.482240973312402e-05,
      "loss": 0.6115,
      "step": 527700
    },
    {
      "epoch": 8.285714285714286,
      "grad_norm": 4.279069423675537,
      "learning_rate": 4.482142857142857e-05,
      "loss": 0.6458,
      "step": 527800
    },
    {
      "epoch": 8.287284144427002,
      "grad_norm": 4.688267707824707,
      "learning_rate": 4.4820447409733124e-05,
      "loss": 0.6174,
      "step": 527900
    },
    {
      "epoch": 8.288854003139717,
      "grad_norm": 4.3394646644592285,
      "learning_rate": 4.4819466248037675e-05,
      "loss": 0.656,
      "step": 528000
    },
    {
      "epoch": 8.290423861852434,
      "grad_norm": 3.8685364723205566,
      "learning_rate": 4.481848508634223e-05,
      "loss": 0.6321,
      "step": 528100
    },
    {
      "epoch": 8.291993720565149,
      "grad_norm": 2.904529333114624,
      "learning_rate": 4.4817503924646784e-05,
      "loss": 0.6334,
      "step": 528200
    },
    {
      "epoch": 8.293563579277865,
      "grad_norm": 3.533555507659912,
      "learning_rate": 4.4816522762951335e-05,
      "loss": 0.6759,
      "step": 528300
    },
    {
      "epoch": 8.29513343799058,
      "grad_norm": 4.2849345207214355,
      "learning_rate": 4.481554160125589e-05,
      "loss": 0.6335,
      "step": 528400
    },
    {
      "epoch": 8.296703296703297,
      "grad_norm": 4.490040302276611,
      "learning_rate": 4.481456043956044e-05,
      "loss": 0.6682,
      "step": 528500
    },
    {
      "epoch": 8.298273155416013,
      "grad_norm": 4.175706386566162,
      "learning_rate": 4.4813579277864994e-05,
      "loss": 0.6402,
      "step": 528600
    },
    {
      "epoch": 8.299843014128728,
      "grad_norm": 4.762386798858643,
      "learning_rate": 4.4812598116169545e-05,
      "loss": 0.6653,
      "step": 528700
    },
    {
      "epoch": 8.301412872841444,
      "grad_norm": 4.110306739807129,
      "learning_rate": 4.48116169544741e-05,
      "loss": 0.678,
      "step": 528800
    },
    {
      "epoch": 8.30298273155416,
      "grad_norm": 4.122762203216553,
      "learning_rate": 4.4810635792778654e-05,
      "loss": 0.6307,
      "step": 528900
    },
    {
      "epoch": 8.304552590266876,
      "grad_norm": 3.9811949729919434,
      "learning_rate": 4.4809654631083205e-05,
      "loss": 0.6602,
      "step": 529000
    },
    {
      "epoch": 8.306122448979592,
      "grad_norm": 3.091981887817383,
      "learning_rate": 4.4808673469387756e-05,
      "loss": 0.6452,
      "step": 529100
    },
    {
      "epoch": 8.307692307692308,
      "grad_norm": 4.0108418464660645,
      "learning_rate": 4.4807692307692314e-05,
      "loss": 0.6271,
      "step": 529200
    },
    {
      "epoch": 8.309262166405023,
      "grad_norm": 4.314752578735352,
      "learning_rate": 4.480671114599686e-05,
      "loss": 0.6375,
      "step": 529300
    },
    {
      "epoch": 8.310832025117739,
      "grad_norm": 4.908102035522461,
      "learning_rate": 4.4805729984301416e-05,
      "loss": 0.643,
      "step": 529400
    },
    {
      "epoch": 8.312401883830455,
      "grad_norm": 3.6541342735290527,
      "learning_rate": 4.480474882260597e-05,
      "loss": 0.6719,
      "step": 529500
    },
    {
      "epoch": 8.31397174254317,
      "grad_norm": 4.798084735870361,
      "learning_rate": 4.4803767660910524e-05,
      "loss": 0.6492,
      "step": 529600
    },
    {
      "epoch": 8.315541601255887,
      "grad_norm": 3.7034695148468018,
      "learning_rate": 4.480278649921507e-05,
      "loss": 0.6341,
      "step": 529700
    },
    {
      "epoch": 8.317111459968602,
      "grad_norm": 4.217511177062988,
      "learning_rate": 4.4801805337519626e-05,
      "loss": 0.6686,
      "step": 529800
    },
    {
      "epoch": 8.31868131868132,
      "grad_norm": 3.8183271884918213,
      "learning_rate": 4.480082417582418e-05,
      "loss": 0.6301,
      "step": 529900
    },
    {
      "epoch": 8.320251177394034,
      "grad_norm": 2.9203028678894043,
      "learning_rate": 4.479984301412873e-05,
      "loss": 0.6524,
      "step": 530000
    },
    {
      "epoch": 8.321821036106751,
      "grad_norm": 3.706115484237671,
      "learning_rate": 4.479886185243328e-05,
      "loss": 0.6172,
      "step": 530100
    },
    {
      "epoch": 8.323390894819466,
      "grad_norm": 3.8985705375671387,
      "learning_rate": 4.479788069073784e-05,
      "loss": 0.6527,
      "step": 530200
    },
    {
      "epoch": 8.324960753532181,
      "grad_norm": 3.8274340629577637,
      "learning_rate": 4.479689952904239e-05,
      "loss": 0.6447,
      "step": 530300
    },
    {
      "epoch": 8.326530612244898,
      "grad_norm": 3.086526870727539,
      "learning_rate": 4.479591836734694e-05,
      "loss": 0.6905,
      "step": 530400
    },
    {
      "epoch": 8.328100470957613,
      "grad_norm": 4.256105422973633,
      "learning_rate": 4.47949372056515e-05,
      "loss": 0.639,
      "step": 530500
    },
    {
      "epoch": 8.32967032967033,
      "grad_norm": 3.171429395675659,
      "learning_rate": 4.479395604395605e-05,
      "loss": 0.6682,
      "step": 530600
    },
    {
      "epoch": 8.331240188383045,
      "grad_norm": 3.96547794342041,
      "learning_rate": 4.47929748822606e-05,
      "loss": 0.6183,
      "step": 530700
    },
    {
      "epoch": 8.332810047095762,
      "grad_norm": 3.4136292934417725,
      "learning_rate": 4.479199372056515e-05,
      "loss": 0.6356,
      "step": 530800
    },
    {
      "epoch": 8.334379905808477,
      "grad_norm": 4.218736171722412,
      "learning_rate": 4.479101255886971e-05,
      "loss": 0.661,
      "step": 530900
    },
    {
      "epoch": 8.335949764521192,
      "grad_norm": 4.600721836090088,
      "learning_rate": 4.479003139717426e-05,
      "loss": 0.6199,
      "step": 531000
    },
    {
      "epoch": 8.33751962323391,
      "grad_norm": 3.9654810428619385,
      "learning_rate": 4.478905023547881e-05,
      "loss": 0.6206,
      "step": 531100
    },
    {
      "epoch": 8.339089481946624,
      "grad_norm": 4.129423141479492,
      "learning_rate": 4.478806907378336e-05,
      "loss": 0.6198,
      "step": 531200
    },
    {
      "epoch": 8.340659340659341,
      "grad_norm": 3.672010898590088,
      "learning_rate": 4.478708791208792e-05,
      "loss": 0.6477,
      "step": 531300
    },
    {
      "epoch": 8.342229199372056,
      "grad_norm": 2.8386452198028564,
      "learning_rate": 4.478610675039246e-05,
      "loss": 0.6294,
      "step": 531400
    },
    {
      "epoch": 8.343799058084773,
      "grad_norm": 3.3084301948547363,
      "learning_rate": 4.478512558869702e-05,
      "loss": 0.6465,
      "step": 531500
    },
    {
      "epoch": 8.345368916797488,
      "grad_norm": 4.068749904632568,
      "learning_rate": 4.478414442700157e-05,
      "loss": 0.6144,
      "step": 531600
    },
    {
      "epoch": 8.346938775510203,
      "grad_norm": 4.46021842956543,
      "learning_rate": 4.478316326530613e-05,
      "loss": 0.6373,
      "step": 531700
    },
    {
      "epoch": 8.34850863422292,
      "grad_norm": 3.317385196685791,
      "learning_rate": 4.478218210361067e-05,
      "loss": 0.622,
      "step": 531800
    },
    {
      "epoch": 8.350078492935635,
      "grad_norm": 4.787199020385742,
      "learning_rate": 4.478120094191523e-05,
      "loss": 0.6634,
      "step": 531900
    },
    {
      "epoch": 8.351648351648352,
      "grad_norm": 3.6209139823913574,
      "learning_rate": 4.478021978021978e-05,
      "loss": 0.6735,
      "step": 532000
    },
    {
      "epoch": 8.353218210361067,
      "grad_norm": 4.523027420043945,
      "learning_rate": 4.477923861852433e-05,
      "loss": 0.6403,
      "step": 532100
    },
    {
      "epoch": 8.354788069073784,
      "grad_norm": 4.443858623504639,
      "learning_rate": 4.4778257456828884e-05,
      "loss": 0.6296,
      "step": 532200
    },
    {
      "epoch": 8.3563579277865,
      "grad_norm": 4.131259441375732,
      "learning_rate": 4.477727629513344e-05,
      "loss": 0.6031,
      "step": 532300
    },
    {
      "epoch": 8.357927786499214,
      "grad_norm": 3.9243509769439697,
      "learning_rate": 4.477629513343799e-05,
      "loss": 0.7066,
      "step": 532400
    },
    {
      "epoch": 8.359497645211931,
      "grad_norm": 3.5550169944763184,
      "learning_rate": 4.4775313971742543e-05,
      "loss": 0.6609,
      "step": 532500
    },
    {
      "epoch": 8.361067503924646,
      "grad_norm": 3.694040060043335,
      "learning_rate": 4.47743328100471e-05,
      "loss": 0.6621,
      "step": 532600
    },
    {
      "epoch": 8.362637362637363,
      "grad_norm": 4.788295269012451,
      "learning_rate": 4.477335164835165e-05,
      "loss": 0.6339,
      "step": 532700
    },
    {
      "epoch": 8.364207221350078,
      "grad_norm": 4.40284538269043,
      "learning_rate": 4.47723704866562e-05,
      "loss": 0.6477,
      "step": 532800
    },
    {
      "epoch": 8.365777080062795,
      "grad_norm": 1.9208120107650757,
      "learning_rate": 4.4771389324960754e-05,
      "loss": 0.6593,
      "step": 532900
    },
    {
      "epoch": 8.36734693877551,
      "grad_norm": 3.9230844974517822,
      "learning_rate": 4.477040816326531e-05,
      "loss": 0.6437,
      "step": 533000
    },
    {
      "epoch": 8.368916797488225,
      "grad_norm": 3.9480631351470947,
      "learning_rate": 4.476942700156986e-05,
      "loss": 0.6783,
      "step": 533100
    },
    {
      "epoch": 8.370486656200942,
      "grad_norm": 4.499701499938965,
      "learning_rate": 4.4768445839874414e-05,
      "loss": 0.6521,
      "step": 533200
    },
    {
      "epoch": 8.372056514913657,
      "grad_norm": 5.121220588684082,
      "learning_rate": 4.4767464678178965e-05,
      "loss": 0.6439,
      "step": 533300
    },
    {
      "epoch": 8.373626373626374,
      "grad_norm": 4.706462383270264,
      "learning_rate": 4.476648351648352e-05,
      "loss": 0.6363,
      "step": 533400
    },
    {
      "epoch": 8.37519623233909,
      "grad_norm": 3.1937026977539062,
      "learning_rate": 4.476550235478807e-05,
      "loss": 0.6489,
      "step": 533500
    },
    {
      "epoch": 8.376766091051806,
      "grad_norm": 4.239788055419922,
      "learning_rate": 4.4764521193092625e-05,
      "loss": 0.6211,
      "step": 533600
    },
    {
      "epoch": 8.378335949764521,
      "grad_norm": 3.5804169178009033,
      "learning_rate": 4.4763540031397176e-05,
      "loss": 0.6485,
      "step": 533700
    },
    {
      "epoch": 8.379905808477236,
      "grad_norm": 4.075547218322754,
      "learning_rate": 4.476255886970173e-05,
      "loss": 0.591,
      "step": 533800
    },
    {
      "epoch": 8.381475667189953,
      "grad_norm": 4.569753170013428,
      "learning_rate": 4.476157770800628e-05,
      "loss": 0.6533,
      "step": 533900
    },
    {
      "epoch": 8.383045525902668,
      "grad_norm": 3.635761022567749,
      "learning_rate": 4.4760596546310835e-05,
      "loss": 0.6562,
      "step": 534000
    },
    {
      "epoch": 8.384615384615385,
      "grad_norm": 3.511521816253662,
      "learning_rate": 4.4759615384615386e-05,
      "loss": 0.647,
      "step": 534100
    },
    {
      "epoch": 8.3861852433281,
      "grad_norm": 4.324837684631348,
      "learning_rate": 4.475863422291994e-05,
      "loss": 0.6512,
      "step": 534200
    },
    {
      "epoch": 8.387755102040817,
      "grad_norm": 3.3037590980529785,
      "learning_rate": 4.475765306122449e-05,
      "loss": 0.6703,
      "step": 534300
    },
    {
      "epoch": 8.389324960753532,
      "grad_norm": 4.238438606262207,
      "learning_rate": 4.4756671899529046e-05,
      "loss": 0.6642,
      "step": 534400
    },
    {
      "epoch": 8.390894819466247,
      "grad_norm": 2.998905897140503,
      "learning_rate": 4.47556907378336e-05,
      "loss": 0.6648,
      "step": 534500
    },
    {
      "epoch": 8.392464678178964,
      "grad_norm": 3.92497181892395,
      "learning_rate": 4.475470957613815e-05,
      "loss": 0.6688,
      "step": 534600
    },
    {
      "epoch": 8.394034536891679,
      "grad_norm": 3.2286624908447266,
      "learning_rate": 4.4753728414442706e-05,
      "loss": 0.6524,
      "step": 534700
    },
    {
      "epoch": 8.395604395604396,
      "grad_norm": 2.42856502532959,
      "learning_rate": 4.475274725274726e-05,
      "loss": 0.696,
      "step": 534800
    },
    {
      "epoch": 8.397174254317111,
      "grad_norm": 3.900069236755371,
      "learning_rate": 4.475176609105181e-05,
      "loss": 0.6351,
      "step": 534900
    },
    {
      "epoch": 8.398744113029828,
      "grad_norm": 4.0697832107543945,
      "learning_rate": 4.475078492935636e-05,
      "loss": 0.6311,
      "step": 535000
    },
    {
      "epoch": 8.400313971742543,
      "grad_norm": 3.9239869117736816,
      "learning_rate": 4.4749803767660916e-05,
      "loss": 0.618,
      "step": 535100
    },
    {
      "epoch": 8.40188383045526,
      "grad_norm": 3.197643280029297,
      "learning_rate": 4.474882260596546e-05,
      "loss": 0.6327,
      "step": 535200
    },
    {
      "epoch": 8.403453689167975,
      "grad_norm": 2.4098007678985596,
      "learning_rate": 4.474784144427002e-05,
      "loss": 0.6179,
      "step": 535300
    },
    {
      "epoch": 8.40502354788069,
      "grad_norm": 4.281312465667725,
      "learning_rate": 4.474686028257457e-05,
      "loss": 0.643,
      "step": 535400
    },
    {
      "epoch": 8.406593406593407,
      "grad_norm": 4.6715850830078125,
      "learning_rate": 4.474587912087913e-05,
      "loss": 0.6114,
      "step": 535500
    },
    {
      "epoch": 8.408163265306122,
      "grad_norm": 4.381338119506836,
      "learning_rate": 4.474489795918367e-05,
      "loss": 0.6258,
      "step": 535600
    },
    {
      "epoch": 8.409733124018839,
      "grad_norm": 2.615248203277588,
      "learning_rate": 4.474391679748823e-05,
      "loss": 0.6274,
      "step": 535700
    },
    {
      "epoch": 8.411302982731554,
      "grad_norm": 4.773179531097412,
      "learning_rate": 4.474293563579278e-05,
      "loss": 0.5864,
      "step": 535800
    },
    {
      "epoch": 8.41287284144427,
      "grad_norm": 3.364373207092285,
      "learning_rate": 4.474195447409733e-05,
      "loss": 0.673,
      "step": 535900
    },
    {
      "epoch": 8.414442700156986,
      "grad_norm": 2.054025650024414,
      "learning_rate": 4.474097331240188e-05,
      "loss": 0.6394,
      "step": 536000
    },
    {
      "epoch": 8.416012558869701,
      "grad_norm": 4.493728160858154,
      "learning_rate": 4.473999215070644e-05,
      "loss": 0.6604,
      "step": 536100
    },
    {
      "epoch": 8.417582417582418,
      "grad_norm": 3.3388826847076416,
      "learning_rate": 4.473901098901099e-05,
      "loss": 0.6491,
      "step": 536200
    },
    {
      "epoch": 8.419152276295133,
      "grad_norm": 3.4761271476745605,
      "learning_rate": 4.473802982731554e-05,
      "loss": 0.6388,
      "step": 536300
    },
    {
      "epoch": 8.42072213500785,
      "grad_norm": 4.8200201988220215,
      "learning_rate": 4.473704866562009e-05,
      "loss": 0.6521,
      "step": 536400
    },
    {
      "epoch": 8.422291993720565,
      "grad_norm": 4.308272361755371,
      "learning_rate": 4.473606750392465e-05,
      "loss": 0.6073,
      "step": 536500
    },
    {
      "epoch": 8.423861852433282,
      "grad_norm": 3.3884429931640625,
      "learning_rate": 4.47350863422292e-05,
      "loss": 0.6573,
      "step": 536600
    },
    {
      "epoch": 8.425431711145997,
      "grad_norm": 3.3282687664031982,
      "learning_rate": 4.473410518053375e-05,
      "loss": 0.6376,
      "step": 536700
    },
    {
      "epoch": 8.427001569858712,
      "grad_norm": 4.502938747406006,
      "learning_rate": 4.473312401883831e-05,
      "loss": 0.6473,
      "step": 536800
    },
    {
      "epoch": 8.428571428571429,
      "grad_norm": 4.364627361297607,
      "learning_rate": 4.473214285714286e-05,
      "loss": 0.6863,
      "step": 536900
    },
    {
      "epoch": 8.430141287284144,
      "grad_norm": 3.2116124629974365,
      "learning_rate": 4.473116169544741e-05,
      "loss": 0.6474,
      "step": 537000
    },
    {
      "epoch": 8.43171114599686,
      "grad_norm": 4.251326560974121,
      "learning_rate": 4.473018053375196e-05,
      "loss": 0.6657,
      "step": 537100
    },
    {
      "epoch": 8.433281004709576,
      "grad_norm": 4.091655731201172,
      "learning_rate": 4.472919937205652e-05,
      "loss": 0.628,
      "step": 537200
    },
    {
      "epoch": 8.434850863422293,
      "grad_norm": 1.8797426223754883,
      "learning_rate": 4.4728218210361065e-05,
      "loss": 0.6715,
      "step": 537300
    },
    {
      "epoch": 8.436420722135008,
      "grad_norm": 4.474593639373779,
      "learning_rate": 4.472723704866562e-05,
      "loss": 0.6292,
      "step": 537400
    },
    {
      "epoch": 8.437990580847723,
      "grad_norm": 5.1310930252075195,
      "learning_rate": 4.4726255886970174e-05,
      "loss": 0.6368,
      "step": 537500
    },
    {
      "epoch": 8.43956043956044,
      "grad_norm": 3.2669804096221924,
      "learning_rate": 4.472527472527473e-05,
      "loss": 0.6494,
      "step": 537600
    },
    {
      "epoch": 8.441130298273155,
      "grad_norm": 2.459599733352661,
      "learning_rate": 4.4724293563579276e-05,
      "loss": 0.6326,
      "step": 537700
    },
    {
      "epoch": 8.442700156985872,
      "grad_norm": 3.1536152362823486,
      "learning_rate": 4.4723312401883834e-05,
      "loss": 0.6507,
      "step": 537800
    },
    {
      "epoch": 8.444270015698587,
      "grad_norm": 3.021850109100342,
      "learning_rate": 4.4722331240188385e-05,
      "loss": 0.6678,
      "step": 537900
    },
    {
      "epoch": 8.445839874411304,
      "grad_norm": 3.9110283851623535,
      "learning_rate": 4.4721350078492936e-05,
      "loss": 0.6289,
      "step": 538000
    },
    {
      "epoch": 8.447409733124019,
      "grad_norm": 3.8693807125091553,
      "learning_rate": 4.4720368916797487e-05,
      "loss": 0.6806,
      "step": 538100
    },
    {
      "epoch": 8.448979591836734,
      "grad_norm": 3.0994656085968018,
      "learning_rate": 4.4719387755102044e-05,
      "loss": 0.6805,
      "step": 538200
    },
    {
      "epoch": 8.45054945054945,
      "grad_norm": 4.698944091796875,
      "learning_rate": 4.4718406593406595e-05,
      "loss": 0.6773,
      "step": 538300
    },
    {
      "epoch": 8.452119309262166,
      "grad_norm": 3.257948637008667,
      "learning_rate": 4.4717425431711146e-05,
      "loss": 0.6397,
      "step": 538400
    },
    {
      "epoch": 8.453689167974883,
      "grad_norm": 3.6990182399749756,
      "learning_rate": 4.47164442700157e-05,
      "loss": 0.6044,
      "step": 538500
    },
    {
      "epoch": 8.455259026687598,
      "grad_norm": 4.0761308670043945,
      "learning_rate": 4.4715463108320255e-05,
      "loss": 0.646,
      "step": 538600
    },
    {
      "epoch": 8.456828885400315,
      "grad_norm": 4.753509998321533,
      "learning_rate": 4.4714481946624806e-05,
      "loss": 0.6711,
      "step": 538700
    },
    {
      "epoch": 8.45839874411303,
      "grad_norm": 4.808549880981445,
      "learning_rate": 4.471350078492936e-05,
      "loss": 0.6508,
      "step": 538800
    },
    {
      "epoch": 8.459968602825747,
      "grad_norm": 3.9855878353118896,
      "learning_rate": 4.4712519623233915e-05,
      "loss": 0.6269,
      "step": 538900
    },
    {
      "epoch": 8.461538461538462,
      "grad_norm": 4.150574207305908,
      "learning_rate": 4.4711538461538466e-05,
      "loss": 0.669,
      "step": 539000
    },
    {
      "epoch": 8.463108320251177,
      "grad_norm": 3.8996689319610596,
      "learning_rate": 4.471055729984302e-05,
      "loss": 0.6357,
      "step": 539100
    },
    {
      "epoch": 8.464678178963894,
      "grad_norm": 3.845860242843628,
      "learning_rate": 4.470957613814757e-05,
      "loss": 0.6536,
      "step": 539200
    },
    {
      "epoch": 8.466248037676609,
      "grad_norm": 3.9259092807769775,
      "learning_rate": 4.4708594976452125e-05,
      "loss": 0.617,
      "step": 539300
    },
    {
      "epoch": 8.467817896389326,
      "grad_norm": 3.3004541397094727,
      "learning_rate": 4.470761381475667e-05,
      "loss": 0.6458,
      "step": 539400
    },
    {
      "epoch": 8.46938775510204,
      "grad_norm": 3.684607982635498,
      "learning_rate": 4.470663265306123e-05,
      "loss": 0.6424,
      "step": 539500
    },
    {
      "epoch": 8.470957613814758,
      "grad_norm": 3.714205503463745,
      "learning_rate": 4.470565149136578e-05,
      "loss": 0.6271,
      "step": 539600
    },
    {
      "epoch": 8.472527472527473,
      "grad_norm": 4.501523017883301,
      "learning_rate": 4.4704670329670336e-05,
      "loss": 0.6366,
      "step": 539700
    },
    {
      "epoch": 8.474097331240188,
      "grad_norm": 3.8780148029327393,
      "learning_rate": 4.470368916797488e-05,
      "loss": 0.6818,
      "step": 539800
    },
    {
      "epoch": 8.475667189952905,
      "grad_norm": 4.051945686340332,
      "learning_rate": 4.470270800627944e-05,
      "loss": 0.6687,
      "step": 539900
    },
    {
      "epoch": 8.47723704866562,
      "grad_norm": 4.537442207336426,
      "learning_rate": 4.470172684458399e-05,
      "loss": 0.6999,
      "step": 540000
    },
    {
      "epoch": 8.478806907378337,
      "grad_norm": 3.556654691696167,
      "learning_rate": 4.470074568288854e-05,
      "loss": 0.6187,
      "step": 540100
    },
    {
      "epoch": 8.480376766091052,
      "grad_norm": 5.403510570526123,
      "learning_rate": 4.469976452119309e-05,
      "loss": 0.68,
      "step": 540200
    },
    {
      "epoch": 8.481946624803768,
      "grad_norm": 4.717094421386719,
      "learning_rate": 4.469878335949765e-05,
      "loss": 0.6557,
      "step": 540300
    },
    {
      "epoch": 8.483516483516484,
      "grad_norm": 2.827012777328491,
      "learning_rate": 4.46978021978022e-05,
      "loss": 0.6396,
      "step": 540400
    },
    {
      "epoch": 8.485086342229199,
      "grad_norm": 4.428867340087891,
      "learning_rate": 4.469682103610675e-05,
      "loss": 0.6512,
      "step": 540500
    },
    {
      "epoch": 8.486656200941916,
      "grad_norm": 4.376957416534424,
      "learning_rate": 4.46958398744113e-05,
      "loss": 0.629,
      "step": 540600
    },
    {
      "epoch": 8.48822605965463,
      "grad_norm": 3.5685157775878906,
      "learning_rate": 4.469485871271586e-05,
      "loss": 0.6572,
      "step": 540700
    },
    {
      "epoch": 8.489795918367347,
      "grad_norm": 2.933317184448242,
      "learning_rate": 4.469387755102041e-05,
      "loss": 0.6313,
      "step": 540800
    },
    {
      "epoch": 8.491365777080063,
      "grad_norm": 3.8234193325042725,
      "learning_rate": 4.469289638932496e-05,
      "loss": 0.6305,
      "step": 540900
    },
    {
      "epoch": 8.49293563579278,
      "grad_norm": 3.352407693862915,
      "learning_rate": 4.469191522762952e-05,
      "loss": 0.6276,
      "step": 541000
    },
    {
      "epoch": 8.494505494505495,
      "grad_norm": 3.60353946685791,
      "learning_rate": 4.469093406593407e-05,
      "loss": 0.6596,
      "step": 541100
    },
    {
      "epoch": 8.49607535321821,
      "grad_norm": 3.3088064193725586,
      "learning_rate": 4.468995290423862e-05,
      "loss": 0.6504,
      "step": 541200
    },
    {
      "epoch": 8.497645211930926,
      "grad_norm": 3.5116209983825684,
      "learning_rate": 4.468897174254317e-05,
      "loss": 0.6179,
      "step": 541300
    },
    {
      "epoch": 8.499215070643642,
      "grad_norm": 4.320158004760742,
      "learning_rate": 4.468799058084773e-05,
      "loss": 0.6436,
      "step": 541400
    },
    {
      "epoch": 8.500784929356358,
      "grad_norm": 4.435278415679932,
      "learning_rate": 4.4687009419152274e-05,
      "loss": 0.6417,
      "step": 541500
    },
    {
      "epoch": 8.502354788069074,
      "grad_norm": 4.016184329986572,
      "learning_rate": 4.468602825745683e-05,
      "loss": 0.6148,
      "step": 541600
    },
    {
      "epoch": 8.50392464678179,
      "grad_norm": 5.264167308807373,
      "learning_rate": 4.468504709576138e-05,
      "loss": 0.6551,
      "step": 541700
    },
    {
      "epoch": 8.505494505494505,
      "grad_norm": 5.040525436401367,
      "learning_rate": 4.468406593406594e-05,
      "loss": 0.6235,
      "step": 541800
    },
    {
      "epoch": 8.50706436420722,
      "grad_norm": 3.767245292663574,
      "learning_rate": 4.4683084772370485e-05,
      "loss": 0.6404,
      "step": 541900
    },
    {
      "epoch": 8.508634222919937,
      "grad_norm": 3.855206251144409,
      "learning_rate": 4.468210361067504e-05,
      "loss": 0.665,
      "step": 542000
    },
    {
      "epoch": 8.510204081632653,
      "grad_norm": 3.9395158290863037,
      "learning_rate": 4.4681122448979594e-05,
      "loss": 0.663,
      "step": 542100
    },
    {
      "epoch": 8.51177394034537,
      "grad_norm": 3.565904378890991,
      "learning_rate": 4.4680141287284145e-05,
      "loss": 0.6236,
      "step": 542200
    },
    {
      "epoch": 8.513343799058084,
      "grad_norm": 3.820206880569458,
      "learning_rate": 4.4679160125588696e-05,
      "loss": 0.621,
      "step": 542300
    },
    {
      "epoch": 8.514913657770801,
      "grad_norm": 4.733306884765625,
      "learning_rate": 4.467817896389325e-05,
      "loss": 0.6176,
      "step": 542400
    },
    {
      "epoch": 8.516483516483516,
      "grad_norm": 4.109920024871826,
      "learning_rate": 4.4677197802197804e-05,
      "loss": 0.6083,
      "step": 542500
    },
    {
      "epoch": 8.518053375196232,
      "grad_norm": 2.7255892753601074,
      "learning_rate": 4.4676216640502355e-05,
      "loss": 0.6584,
      "step": 542600
    },
    {
      "epoch": 8.519623233908948,
      "grad_norm": 3.4321935176849365,
      "learning_rate": 4.4675235478806906e-05,
      "loss": 0.6466,
      "step": 542700
    },
    {
      "epoch": 8.521193092621663,
      "grad_norm": 5.287840366363525,
      "learning_rate": 4.4674254317111464e-05,
      "loss": 0.6062,
      "step": 542800
    },
    {
      "epoch": 8.52276295133438,
      "grad_norm": 4.623509883880615,
      "learning_rate": 4.4673273155416015e-05,
      "loss": 0.6338,
      "step": 542900
    },
    {
      "epoch": 8.524332810047095,
      "grad_norm": 3.1565206050872803,
      "learning_rate": 4.4672291993720566e-05,
      "loss": 0.6912,
      "step": 543000
    },
    {
      "epoch": 8.525902668759812,
      "grad_norm": 4.5904059410095215,
      "learning_rate": 4.4671310832025124e-05,
      "loss": 0.6821,
      "step": 543100
    },
    {
      "epoch": 8.527472527472527,
      "grad_norm": 3.780866861343384,
      "learning_rate": 4.4670329670329675e-05,
      "loss": 0.6542,
      "step": 543200
    },
    {
      "epoch": 8.529042386185242,
      "grad_norm": 4.102720260620117,
      "learning_rate": 4.4669348508634226e-05,
      "loss": 0.6136,
      "step": 543300
    },
    {
      "epoch": 8.53061224489796,
      "grad_norm": 4.199356555938721,
      "learning_rate": 4.466836734693878e-05,
      "loss": 0.6298,
      "step": 543400
    },
    {
      "epoch": 8.532182103610674,
      "grad_norm": 4.8231916427612305,
      "learning_rate": 4.4667386185243334e-05,
      "loss": 0.6505,
      "step": 543500
    },
    {
      "epoch": 8.533751962323391,
      "grad_norm": 3.7969090938568115,
      "learning_rate": 4.466640502354788e-05,
      "loss": 0.667,
      "step": 543600
    },
    {
      "epoch": 8.535321821036106,
      "grad_norm": 3.1607959270477295,
      "learning_rate": 4.4665423861852436e-05,
      "loss": 0.6338,
      "step": 543700
    },
    {
      "epoch": 8.536891679748823,
      "grad_norm": 3.8355040550231934,
      "learning_rate": 4.466444270015699e-05,
      "loss": 0.6286,
      "step": 543800
    },
    {
      "epoch": 8.538461538461538,
      "grad_norm": 5.527542591094971,
      "learning_rate": 4.4663461538461545e-05,
      "loss": 0.626,
      "step": 543900
    },
    {
      "epoch": 8.540031397174253,
      "grad_norm": 4.0768656730651855,
      "learning_rate": 4.466248037676609e-05,
      "loss": 0.6476,
      "step": 544000
    },
    {
      "epoch": 8.54160125588697,
      "grad_norm": 3.4996891021728516,
      "learning_rate": 4.466149921507065e-05,
      "loss": 0.6401,
      "step": 544100
    },
    {
      "epoch": 8.543171114599685,
      "grad_norm": 3.888002634048462,
      "learning_rate": 4.46605180533752e-05,
      "loss": 0.615,
      "step": 544200
    },
    {
      "epoch": 8.544740973312402,
      "grad_norm": 4.375980854034424,
      "learning_rate": 4.465953689167975e-05,
      "loss": 0.6411,
      "step": 544300
    },
    {
      "epoch": 8.546310832025117,
      "grad_norm": 4.511134624481201,
      "learning_rate": 4.46585557299843e-05,
      "loss": 0.6366,
      "step": 544400
    },
    {
      "epoch": 8.547880690737834,
      "grad_norm": 4.250370979309082,
      "learning_rate": 4.465757456828886e-05,
      "loss": 0.6499,
      "step": 544500
    },
    {
      "epoch": 8.54945054945055,
      "grad_norm": 2.863492727279663,
      "learning_rate": 4.465659340659341e-05,
      "loss": 0.6363,
      "step": 544600
    },
    {
      "epoch": 8.551020408163264,
      "grad_norm": 3.6356201171875,
      "learning_rate": 4.465561224489796e-05,
      "loss": 0.6376,
      "step": 544700
    },
    {
      "epoch": 8.552590266875981,
      "grad_norm": 4.0312819480896,
      "learning_rate": 4.465463108320251e-05,
      "loss": 0.6511,
      "step": 544800
    },
    {
      "epoch": 8.554160125588696,
      "grad_norm": 4.433473110198975,
      "learning_rate": 4.465364992150707e-05,
      "loss": 0.6488,
      "step": 544900
    },
    {
      "epoch": 8.555729984301413,
      "grad_norm": 3.255591869354248,
      "learning_rate": 4.465266875981162e-05,
      "loss": 0.6663,
      "step": 545000
    },
    {
      "epoch": 8.557299843014128,
      "grad_norm": 3.527977466583252,
      "learning_rate": 4.465168759811617e-05,
      "loss": 0.6286,
      "step": 545100
    },
    {
      "epoch": 8.558869701726845,
      "grad_norm": 2.8465285301208496,
      "learning_rate": 4.465070643642073e-05,
      "loss": 0.6153,
      "step": 545200
    },
    {
      "epoch": 8.56043956043956,
      "grad_norm": 4.005417823791504,
      "learning_rate": 4.464972527472528e-05,
      "loss": 0.6362,
      "step": 545300
    },
    {
      "epoch": 8.562009419152277,
      "grad_norm": 4.769573211669922,
      "learning_rate": 4.464874411302983e-05,
      "loss": 0.6525,
      "step": 545400
    },
    {
      "epoch": 8.563579277864992,
      "grad_norm": 3.8819329738616943,
      "learning_rate": 4.464776295133438e-05,
      "loss": 0.6343,
      "step": 545500
    },
    {
      "epoch": 8.565149136577707,
      "grad_norm": 4.2512664794921875,
      "learning_rate": 4.464678178963894e-05,
      "loss": 0.6794,
      "step": 545600
    },
    {
      "epoch": 8.566718995290424,
      "grad_norm": 5.388014316558838,
      "learning_rate": 4.464580062794348e-05,
      "loss": 0.6285,
      "step": 545700
    },
    {
      "epoch": 8.56828885400314,
      "grad_norm": 4.182073593139648,
      "learning_rate": 4.464481946624804e-05,
      "loss": 0.6435,
      "step": 545800
    },
    {
      "epoch": 8.569858712715856,
      "grad_norm": 3.7800118923187256,
      "learning_rate": 4.464383830455259e-05,
      "loss": 0.6246,
      "step": 545900
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 2.766309976577759,
      "learning_rate": 4.464285714285715e-05,
      "loss": 0.6809,
      "step": 546000
    },
    {
      "epoch": 8.572998430141288,
      "grad_norm": 4.774652004241943,
      "learning_rate": 4.4641875981161694e-05,
      "loss": 0.6506,
      "step": 546100
    },
    {
      "epoch": 8.574568288854003,
      "grad_norm": 4.470527648925781,
      "learning_rate": 4.464089481946625e-05,
      "loss": 0.6086,
      "step": 546200
    },
    {
      "epoch": 8.576138147566718,
      "grad_norm": 4.234553337097168,
      "learning_rate": 4.46399136577708e-05,
      "loss": 0.632,
      "step": 546300
    },
    {
      "epoch": 8.577708006279435,
      "grad_norm": 4.432703018188477,
      "learning_rate": 4.4638932496075354e-05,
      "loss": 0.5944,
      "step": 546400
    },
    {
      "epoch": 8.57927786499215,
      "grad_norm": 5.060828685760498,
      "learning_rate": 4.4637951334379905e-05,
      "loss": 0.6736,
      "step": 546500
    },
    {
      "epoch": 8.580847723704867,
      "grad_norm": 4.63459587097168,
      "learning_rate": 4.463697017268446e-05,
      "loss": 0.6466,
      "step": 546600
    },
    {
      "epoch": 8.582417582417582,
      "grad_norm": 3.2202625274658203,
      "learning_rate": 4.463598901098901e-05,
      "loss": 0.6217,
      "step": 546700
    },
    {
      "epoch": 8.583987441130299,
      "grad_norm": 4.257347583770752,
      "learning_rate": 4.4635007849293564e-05,
      "loss": 0.6592,
      "step": 546800
    },
    {
      "epoch": 8.585557299843014,
      "grad_norm": 3.724857807159424,
      "learning_rate": 4.4634026687598115e-05,
      "loss": 0.673,
      "step": 546900
    },
    {
      "epoch": 8.58712715855573,
      "grad_norm": 3.8960766792297363,
      "learning_rate": 4.463304552590267e-05,
      "loss": 0.6081,
      "step": 547000
    },
    {
      "epoch": 8.588697017268446,
      "grad_norm": 3.3294715881347656,
      "learning_rate": 4.4632064364207224e-05,
      "loss": 0.6466,
      "step": 547100
    },
    {
      "epoch": 8.590266875981161,
      "grad_norm": 4.33992338180542,
      "learning_rate": 4.4631083202511775e-05,
      "loss": 0.6712,
      "step": 547200
    },
    {
      "epoch": 8.591836734693878,
      "grad_norm": 4.404007434844971,
      "learning_rate": 4.463010204081633e-05,
      "loss": 0.6674,
      "step": 547300
    },
    {
      "epoch": 8.593406593406593,
      "grad_norm": 3.808558464050293,
      "learning_rate": 4.4629120879120884e-05,
      "loss": 0.6239,
      "step": 547400
    },
    {
      "epoch": 8.59497645211931,
      "grad_norm": 4.4272050857543945,
      "learning_rate": 4.4628139717425435e-05,
      "loss": 0.6468,
      "step": 547500
    },
    {
      "epoch": 8.596546310832025,
      "grad_norm": 3.196402072906494,
      "learning_rate": 4.4627158555729986e-05,
      "loss": 0.6316,
      "step": 547600
    },
    {
      "epoch": 8.598116169544742,
      "grad_norm": 4.481168746948242,
      "learning_rate": 4.462617739403454e-05,
      "loss": 0.6643,
      "step": 547700
    },
    {
      "epoch": 8.599686028257457,
      "grad_norm": 3.8344361782073975,
      "learning_rate": 4.462519623233909e-05,
      "loss": 0.612,
      "step": 547800
    },
    {
      "epoch": 8.601255886970172,
      "grad_norm": 3.7747762203216553,
      "learning_rate": 4.4624215070643645e-05,
      "loss": 0.6529,
      "step": 547900
    },
    {
      "epoch": 8.602825745682889,
      "grad_norm": 4.120547294616699,
      "learning_rate": 4.4623233908948196e-05,
      "loss": 0.64,
      "step": 548000
    },
    {
      "epoch": 8.604395604395604,
      "grad_norm": 3.2243587970733643,
      "learning_rate": 4.4622252747252754e-05,
      "loss": 0.6568,
      "step": 548100
    },
    {
      "epoch": 8.605965463108321,
      "grad_norm": 3.296102523803711,
      "learning_rate": 4.46212715855573e-05,
      "loss": 0.6687,
      "step": 548200
    },
    {
      "epoch": 8.607535321821036,
      "grad_norm": 3.4311892986297607,
      "learning_rate": 4.4620290423861856e-05,
      "loss": 0.6688,
      "step": 548300
    },
    {
      "epoch": 8.609105180533753,
      "grad_norm": 4.064205169677734,
      "learning_rate": 4.461930926216641e-05,
      "loss": 0.6488,
      "step": 548400
    },
    {
      "epoch": 8.610675039246468,
      "grad_norm": 3.5175349712371826,
      "learning_rate": 4.461832810047096e-05,
      "loss": 0.6287,
      "step": 548500
    },
    {
      "epoch": 8.612244897959183,
      "grad_norm": 4.864899635314941,
      "learning_rate": 4.461734693877551e-05,
      "loss": 0.6225,
      "step": 548600
    },
    {
      "epoch": 8.6138147566719,
      "grad_norm": 3.0411102771759033,
      "learning_rate": 4.461636577708007e-05,
      "loss": 0.612,
      "step": 548700
    },
    {
      "epoch": 8.615384615384615,
      "grad_norm": 5.207744598388672,
      "learning_rate": 4.461538461538462e-05,
      "loss": 0.635,
      "step": 548800
    },
    {
      "epoch": 8.616954474097332,
      "grad_norm": 4.767632961273193,
      "learning_rate": 4.461440345368917e-05,
      "loss": 0.6672,
      "step": 548900
    },
    {
      "epoch": 8.618524332810047,
      "grad_norm": 4.01978874206543,
      "learning_rate": 4.461342229199372e-05,
      "loss": 0.6217,
      "step": 549000
    },
    {
      "epoch": 8.620094191522764,
      "grad_norm": 3.93436861038208,
      "learning_rate": 4.461244113029828e-05,
      "loss": 0.6599,
      "step": 549100
    },
    {
      "epoch": 8.621664050235479,
      "grad_norm": 4.121201515197754,
      "learning_rate": 4.461145996860283e-05,
      "loss": 0.586,
      "step": 549200
    },
    {
      "epoch": 8.623233908948194,
      "grad_norm": 4.5815887451171875,
      "learning_rate": 4.461047880690738e-05,
      "loss": 0.6079,
      "step": 549300
    },
    {
      "epoch": 8.62480376766091,
      "grad_norm": 4.470512390136719,
      "learning_rate": 4.460949764521194e-05,
      "loss": 0.6784,
      "step": 549400
    },
    {
      "epoch": 8.626373626373626,
      "grad_norm": 2.1192734241485596,
      "learning_rate": 4.460851648351649e-05,
      "loss": 0.6376,
      "step": 549500
    },
    {
      "epoch": 8.627943485086343,
      "grad_norm": 3.729621410369873,
      "learning_rate": 4.460753532182104e-05,
      "loss": 0.6352,
      "step": 549600
    },
    {
      "epoch": 8.629513343799058,
      "grad_norm": 3.537909984588623,
      "learning_rate": 4.460655416012559e-05,
      "loss": 0.6375,
      "step": 549700
    },
    {
      "epoch": 8.631083202511775,
      "grad_norm": 4.16455078125,
      "learning_rate": 4.460557299843015e-05,
      "loss": 0.6762,
      "step": 549800
    },
    {
      "epoch": 8.63265306122449,
      "grad_norm": 3.821638584136963,
      "learning_rate": 4.460459183673469e-05,
      "loss": 0.6576,
      "step": 549900
    },
    {
      "epoch": 8.634222919937205,
      "grad_norm": 3.846587657928467,
      "learning_rate": 4.460361067503925e-05,
      "loss": 0.6117,
      "step": 550000
    },
    {
      "epoch": 8.635792778649922,
      "grad_norm": 3.9354944229125977,
      "learning_rate": 4.46026295133438e-05,
      "loss": 0.6539,
      "step": 550100
    },
    {
      "epoch": 8.637362637362637,
      "grad_norm": 4.443063259124756,
      "learning_rate": 4.460164835164836e-05,
      "loss": 0.6521,
      "step": 550200
    },
    {
      "epoch": 8.638932496075354,
      "grad_norm": 3.674755573272705,
      "learning_rate": 4.46006671899529e-05,
      "loss": 0.666,
      "step": 550300
    },
    {
      "epoch": 8.640502354788069,
      "grad_norm": 4.1220245361328125,
      "learning_rate": 4.459968602825746e-05,
      "loss": 0.626,
      "step": 550400
    },
    {
      "epoch": 8.642072213500786,
      "grad_norm": 4.283100128173828,
      "learning_rate": 4.459870486656201e-05,
      "loss": 0.6237,
      "step": 550500
    },
    {
      "epoch": 8.6436420722135,
      "grad_norm": 3.4150009155273438,
      "learning_rate": 4.459772370486656e-05,
      "loss": 0.6511,
      "step": 550600
    },
    {
      "epoch": 8.645211930926216,
      "grad_norm": 4.973848342895508,
      "learning_rate": 4.4596742543171113e-05,
      "loss": 0.6746,
      "step": 550700
    },
    {
      "epoch": 8.646781789638933,
      "grad_norm": 4.012418270111084,
      "learning_rate": 4.459576138147567e-05,
      "loss": 0.6428,
      "step": 550800
    },
    {
      "epoch": 8.648351648351648,
      "grad_norm": 3.8936188220977783,
      "learning_rate": 4.459478021978022e-05,
      "loss": 0.653,
      "step": 550900
    },
    {
      "epoch": 8.649921507064365,
      "grad_norm": 3.6226956844329834,
      "learning_rate": 4.459379905808477e-05,
      "loss": 0.6567,
      "step": 551000
    },
    {
      "epoch": 8.65149136577708,
      "grad_norm": 4.279961585998535,
      "learning_rate": 4.4592817896389324e-05,
      "loss": 0.7069,
      "step": 551100
    },
    {
      "epoch": 8.653061224489797,
      "grad_norm": 3.375014305114746,
      "learning_rate": 4.459183673469388e-05,
      "loss": 0.648,
      "step": 551200
    },
    {
      "epoch": 8.654631083202512,
      "grad_norm": 4.100111484527588,
      "learning_rate": 4.459085557299843e-05,
      "loss": 0.6527,
      "step": 551300
    },
    {
      "epoch": 8.656200941915227,
      "grad_norm": 3.1111583709716797,
      "learning_rate": 4.4589874411302984e-05,
      "loss": 0.6222,
      "step": 551400
    },
    {
      "epoch": 8.657770800627944,
      "grad_norm": 3.7138450145721436,
      "learning_rate": 4.458889324960754e-05,
      "loss": 0.5892,
      "step": 551500
    },
    {
      "epoch": 8.659340659340659,
      "grad_norm": 3.771125078201294,
      "learning_rate": 4.458791208791209e-05,
      "loss": 0.6605,
      "step": 551600
    },
    {
      "epoch": 8.660910518053376,
      "grad_norm": 3.944216728210449,
      "learning_rate": 4.4586930926216644e-05,
      "loss": 0.6147,
      "step": 551700
    },
    {
      "epoch": 8.66248037676609,
      "grad_norm": 3.929875135421753,
      "learning_rate": 4.4585949764521195e-05,
      "loss": 0.6779,
      "step": 551800
    },
    {
      "epoch": 8.664050235478808,
      "grad_norm": 3.8279013633728027,
      "learning_rate": 4.458496860282575e-05,
      "loss": 0.6668,
      "step": 551900
    },
    {
      "epoch": 8.665620094191523,
      "grad_norm": 4.511736869812012,
      "learning_rate": 4.4583987441130297e-05,
      "loss": 0.6744,
      "step": 552000
    },
    {
      "epoch": 8.667189952904238,
      "grad_norm": 2.843446731567383,
      "learning_rate": 4.4583006279434854e-05,
      "loss": 0.6352,
      "step": 552100
    },
    {
      "epoch": 8.668759811616955,
      "grad_norm": 4.571122646331787,
      "learning_rate": 4.4582025117739405e-05,
      "loss": 0.6647,
      "step": 552200
    },
    {
      "epoch": 8.67032967032967,
      "grad_norm": 5.201892375946045,
      "learning_rate": 4.458104395604396e-05,
      "loss": 0.6205,
      "step": 552300
    },
    {
      "epoch": 8.671899529042387,
      "grad_norm": 4.30837345123291,
      "learning_rate": 4.458006279434851e-05,
      "loss": 0.6388,
      "step": 552400
    },
    {
      "epoch": 8.673469387755102,
      "grad_norm": 3.4544293880462646,
      "learning_rate": 4.4579081632653065e-05,
      "loss": 0.5984,
      "step": 552500
    },
    {
      "epoch": 8.675039246467819,
      "grad_norm": 3.2559590339660645,
      "learning_rate": 4.4578100470957616e-05,
      "loss": 0.6331,
      "step": 552600
    },
    {
      "epoch": 8.676609105180534,
      "grad_norm": 3.398355484008789,
      "learning_rate": 4.457711930926217e-05,
      "loss": 0.6127,
      "step": 552700
    },
    {
      "epoch": 8.678178963893249,
      "grad_norm": 2.8165194988250732,
      "learning_rate": 4.457613814756672e-05,
      "loss": 0.623,
      "step": 552800
    },
    {
      "epoch": 8.679748822605966,
      "grad_norm": 3.741530418395996,
      "learning_rate": 4.4575156985871276e-05,
      "loss": 0.6544,
      "step": 552900
    },
    {
      "epoch": 8.68131868131868,
      "grad_norm": 4.590303421020508,
      "learning_rate": 4.457417582417583e-05,
      "loss": 0.693,
      "step": 553000
    },
    {
      "epoch": 8.682888540031398,
      "grad_norm": 4.471126079559326,
      "learning_rate": 4.457319466248038e-05,
      "loss": 0.6808,
      "step": 553100
    },
    {
      "epoch": 8.684458398744113,
      "grad_norm": 4.419036865234375,
      "learning_rate": 4.457221350078493e-05,
      "loss": 0.6218,
      "step": 553200
    },
    {
      "epoch": 8.68602825745683,
      "grad_norm": 3.5098297595977783,
      "learning_rate": 4.4571232339089486e-05,
      "loss": 0.6396,
      "step": 553300
    },
    {
      "epoch": 8.687598116169545,
      "grad_norm": 2.869407892227173,
      "learning_rate": 4.457025117739404e-05,
      "loss": 0.6246,
      "step": 553400
    },
    {
      "epoch": 8.68916797488226,
      "grad_norm": 3.36124849319458,
      "learning_rate": 4.456927001569859e-05,
      "loss": 0.6362,
      "step": 553500
    },
    {
      "epoch": 8.690737833594977,
      "grad_norm": 4.180881977081299,
      "learning_rate": 4.4568288854003146e-05,
      "loss": 0.6109,
      "step": 553600
    },
    {
      "epoch": 8.692307692307692,
      "grad_norm": 4.307601451873779,
      "learning_rate": 4.45673076923077e-05,
      "loss": 0.7003,
      "step": 553700
    },
    {
      "epoch": 8.693877551020408,
      "grad_norm": 3.42596697807312,
      "learning_rate": 4.456632653061225e-05,
      "loss": 0.6249,
      "step": 553800
    },
    {
      "epoch": 8.695447409733124,
      "grad_norm": 3.7110908031463623,
      "learning_rate": 4.45653453689168e-05,
      "loss": 0.6544,
      "step": 553900
    },
    {
      "epoch": 8.69701726844584,
      "grad_norm": 4.04210090637207,
      "learning_rate": 4.456436420722136e-05,
      "loss": 0.6268,
      "step": 554000
    },
    {
      "epoch": 8.698587127158556,
      "grad_norm": 4.530005931854248,
      "learning_rate": 4.45633830455259e-05,
      "loss": 0.6522,
      "step": 554100
    },
    {
      "epoch": 8.700156985871272,
      "grad_norm": 4.031763553619385,
      "learning_rate": 4.456240188383046e-05,
      "loss": 0.6565,
      "step": 554200
    },
    {
      "epoch": 8.701726844583987,
      "grad_norm": 4.1988139152526855,
      "learning_rate": 4.456142072213501e-05,
      "loss": 0.6268,
      "step": 554300
    },
    {
      "epoch": 8.703296703296703,
      "grad_norm": 3.078183889389038,
      "learning_rate": 4.456043956043957e-05,
      "loss": 0.6123,
      "step": 554400
    },
    {
      "epoch": 8.70486656200942,
      "grad_norm": 3.2686686515808105,
      "learning_rate": 4.455945839874411e-05,
      "loss": 0.6518,
      "step": 554500
    },
    {
      "epoch": 8.706436420722135,
      "grad_norm": 4.795871257781982,
      "learning_rate": 4.455847723704867e-05,
      "loss": 0.6399,
      "step": 554600
    },
    {
      "epoch": 8.708006279434851,
      "grad_norm": 4.001168251037598,
      "learning_rate": 4.455749607535322e-05,
      "loss": 0.6934,
      "step": 554700
    },
    {
      "epoch": 8.709576138147566,
      "grad_norm": 3.9208474159240723,
      "learning_rate": 4.455651491365777e-05,
      "loss": 0.6395,
      "step": 554800
    },
    {
      "epoch": 8.711145996860283,
      "grad_norm": 3.9280145168304443,
      "learning_rate": 4.455553375196232e-05,
      "loss": 0.6488,
      "step": 554900
    },
    {
      "epoch": 8.712715855572998,
      "grad_norm": 4.051980495452881,
      "learning_rate": 4.455455259026688e-05,
      "loss": 0.6648,
      "step": 555000
    },
    {
      "epoch": 8.714285714285714,
      "grad_norm": 3.843500852584839,
      "learning_rate": 4.455357142857143e-05,
      "loss": 0.6609,
      "step": 555100
    },
    {
      "epoch": 8.71585557299843,
      "grad_norm": 3.957681894302368,
      "learning_rate": 4.455259026687598e-05,
      "loss": 0.6753,
      "step": 555200
    },
    {
      "epoch": 8.717425431711145,
      "grad_norm": 2.9167137145996094,
      "learning_rate": 4.455160910518053e-05,
      "loss": 0.6451,
      "step": 555300
    },
    {
      "epoch": 8.718995290423862,
      "grad_norm": 3.4165329933166504,
      "learning_rate": 4.455062794348509e-05,
      "loss": 0.6349,
      "step": 555400
    },
    {
      "epoch": 8.720565149136577,
      "grad_norm": 3.6209864616394043,
      "learning_rate": 4.454964678178964e-05,
      "loss": 0.6149,
      "step": 555500
    },
    {
      "epoch": 8.722135007849294,
      "grad_norm": 3.7204530239105225,
      "learning_rate": 4.454866562009419e-05,
      "loss": 0.6396,
      "step": 555600
    },
    {
      "epoch": 8.72370486656201,
      "grad_norm": 4.470249176025391,
      "learning_rate": 4.454768445839875e-05,
      "loss": 0.6829,
      "step": 555700
    },
    {
      "epoch": 8.725274725274724,
      "grad_norm": 2.966470956802368,
      "learning_rate": 4.45467032967033e-05,
      "loss": 0.6444,
      "step": 555800
    },
    {
      "epoch": 8.726844583987441,
      "grad_norm": 3.297403335571289,
      "learning_rate": 4.454572213500785e-05,
      "loss": 0.6588,
      "step": 555900
    },
    {
      "epoch": 8.728414442700156,
      "grad_norm": 2.9590256214141846,
      "learning_rate": 4.4544740973312404e-05,
      "loss": 0.6629,
      "step": 556000
    },
    {
      "epoch": 8.729984301412873,
      "grad_norm": 3.3313941955566406,
      "learning_rate": 4.454375981161696e-05,
      "loss": 0.6177,
      "step": 556100
    },
    {
      "epoch": 8.731554160125588,
      "grad_norm": 4.161615371704102,
      "learning_rate": 4.4542778649921506e-05,
      "loss": 0.6975,
      "step": 556200
    },
    {
      "epoch": 8.733124018838305,
      "grad_norm": 3.210759401321411,
      "learning_rate": 4.454179748822606e-05,
      "loss": 0.6557,
      "step": 556300
    },
    {
      "epoch": 8.73469387755102,
      "grad_norm": 4.190515041351318,
      "learning_rate": 4.4540816326530614e-05,
      "loss": 0.65,
      "step": 556400
    },
    {
      "epoch": 8.736263736263737,
      "grad_norm": 4.531270503997803,
      "learning_rate": 4.453983516483517e-05,
      "loss": 0.6404,
      "step": 556500
    },
    {
      "epoch": 8.737833594976452,
      "grad_norm": 3.167632579803467,
      "learning_rate": 4.4538854003139716e-05,
      "loss": 0.6467,
      "step": 556600
    },
    {
      "epoch": 8.739403453689167,
      "grad_norm": 4.028703689575195,
      "learning_rate": 4.4537872841444274e-05,
      "loss": 0.6498,
      "step": 556700
    },
    {
      "epoch": 8.740973312401884,
      "grad_norm": 3.218050956726074,
      "learning_rate": 4.4536891679748825e-05,
      "loss": 0.631,
      "step": 556800
    },
    {
      "epoch": 8.7425431711146,
      "grad_norm": 4.76231575012207,
      "learning_rate": 4.4535910518053376e-05,
      "loss": 0.6234,
      "step": 556900
    },
    {
      "epoch": 8.744113029827316,
      "grad_norm": 4.300209045410156,
      "learning_rate": 4.453492935635793e-05,
      "loss": 0.6769,
      "step": 557000
    },
    {
      "epoch": 8.745682888540031,
      "grad_norm": 3.308732509613037,
      "learning_rate": 4.4533948194662485e-05,
      "loss": 0.6372,
      "step": 557100
    },
    {
      "epoch": 8.747252747252748,
      "grad_norm": 4.030964374542236,
      "learning_rate": 4.4532967032967036e-05,
      "loss": 0.6242,
      "step": 557200
    },
    {
      "epoch": 8.748822605965463,
      "grad_norm": 4.235559463500977,
      "learning_rate": 4.453198587127159e-05,
      "loss": 0.6215,
      "step": 557300
    },
    {
      "epoch": 8.750392464678178,
      "grad_norm": 4.718372344970703,
      "learning_rate": 4.453100470957614e-05,
      "loss": 0.6096,
      "step": 557400
    },
    {
      "epoch": 8.751962323390895,
      "grad_norm": 4.014254093170166,
      "learning_rate": 4.4530023547880695e-05,
      "loss": 0.6111,
      "step": 557500
    },
    {
      "epoch": 8.75353218210361,
      "grad_norm": 4.012407302856445,
      "learning_rate": 4.4529042386185246e-05,
      "loss": 0.6657,
      "step": 557600
    },
    {
      "epoch": 8.755102040816327,
      "grad_norm": 4.50803279876709,
      "learning_rate": 4.45280612244898e-05,
      "loss": 0.6503,
      "step": 557700
    },
    {
      "epoch": 8.756671899529042,
      "grad_norm": 5.019672870635986,
      "learning_rate": 4.4527080062794355e-05,
      "loss": 0.6466,
      "step": 557800
    },
    {
      "epoch": 8.758241758241759,
      "grad_norm": 3.5241639614105225,
      "learning_rate": 4.45260989010989e-05,
      "loss": 0.635,
      "step": 557900
    },
    {
      "epoch": 8.759811616954474,
      "grad_norm": 4.670170783996582,
      "learning_rate": 4.452511773940346e-05,
      "loss": 0.6351,
      "step": 558000
    },
    {
      "epoch": 8.76138147566719,
      "grad_norm": 4.257440090179443,
      "learning_rate": 4.452413657770801e-05,
      "loss": 0.6796,
      "step": 558100
    },
    {
      "epoch": 8.762951334379906,
      "grad_norm": 3.1512365341186523,
      "learning_rate": 4.4523155416012566e-05,
      "loss": 0.6722,
      "step": 558200
    },
    {
      "epoch": 8.764521193092621,
      "grad_norm": 3.655930757522583,
      "learning_rate": 4.452217425431711e-05,
      "loss": 0.6422,
      "step": 558300
    },
    {
      "epoch": 8.766091051805338,
      "grad_norm": 3.596062660217285,
      "learning_rate": 4.452119309262167e-05,
      "loss": 0.6332,
      "step": 558400
    },
    {
      "epoch": 8.767660910518053,
      "grad_norm": 3.7279787063598633,
      "learning_rate": 4.452021193092622e-05,
      "loss": 0.6387,
      "step": 558500
    },
    {
      "epoch": 8.76923076923077,
      "grad_norm": 4.023801803588867,
      "learning_rate": 4.451923076923077e-05,
      "loss": 0.6765,
      "step": 558600
    },
    {
      "epoch": 8.770800627943485,
      "grad_norm": 3.8874495029449463,
      "learning_rate": 4.451824960753532e-05,
      "loss": 0.585,
      "step": 558700
    },
    {
      "epoch": 8.7723704866562,
      "grad_norm": 3.361320734024048,
      "learning_rate": 4.451726844583988e-05,
      "loss": 0.6502,
      "step": 558800
    },
    {
      "epoch": 8.773940345368917,
      "grad_norm": 3.6477816104888916,
      "learning_rate": 4.451628728414443e-05,
      "loss": 0.6826,
      "step": 558900
    },
    {
      "epoch": 8.775510204081632,
      "grad_norm": 3.9823107719421387,
      "learning_rate": 4.451530612244898e-05,
      "loss": 0.6686,
      "step": 559000
    },
    {
      "epoch": 8.777080062794349,
      "grad_norm": 4.4856390953063965,
      "learning_rate": 4.451432496075353e-05,
      "loss": 0.6677,
      "step": 559100
    },
    {
      "epoch": 8.778649921507064,
      "grad_norm": 4.624337673187256,
      "learning_rate": 4.451334379905809e-05,
      "loss": 0.6346,
      "step": 559200
    },
    {
      "epoch": 8.780219780219781,
      "grad_norm": 3.988532781600952,
      "learning_rate": 4.4512362637362633e-05,
      "loss": 0.6357,
      "step": 559300
    },
    {
      "epoch": 8.781789638932496,
      "grad_norm": 3.189364433288574,
      "learning_rate": 4.451138147566719e-05,
      "loss": 0.6231,
      "step": 559400
    },
    {
      "epoch": 8.783359497645211,
      "grad_norm": 3.7843823432922363,
      "learning_rate": 4.451040031397174e-05,
      "loss": 0.6706,
      "step": 559500
    },
    {
      "epoch": 8.784929356357928,
      "grad_norm": 4.192131042480469,
      "learning_rate": 4.45094191522763e-05,
      "loss": 0.6571,
      "step": 559600
    },
    {
      "epoch": 8.786499215070643,
      "grad_norm": 4.055333614349365,
      "learning_rate": 4.450843799058085e-05,
      "loss": 0.634,
      "step": 559700
    },
    {
      "epoch": 8.78806907378336,
      "grad_norm": 4.419524669647217,
      "learning_rate": 4.45074568288854e-05,
      "loss": 0.6473,
      "step": 559800
    },
    {
      "epoch": 8.789638932496075,
      "grad_norm": 4.126574993133545,
      "learning_rate": 4.450647566718996e-05,
      "loss": 0.6554,
      "step": 559900
    },
    {
      "epoch": 8.791208791208792,
      "grad_norm": 4.475156784057617,
      "learning_rate": 4.4505494505494504e-05,
      "loss": 0.6355,
      "step": 560000
    },
    {
      "epoch": 8.792778649921507,
      "grad_norm": 3.9720458984375,
      "learning_rate": 4.450451334379906e-05,
      "loss": 0.6657,
      "step": 560100
    },
    {
      "epoch": 8.794348508634222,
      "grad_norm": 3.6774404048919678,
      "learning_rate": 4.450353218210361e-05,
      "loss": 0.6213,
      "step": 560200
    },
    {
      "epoch": 8.795918367346939,
      "grad_norm": 3.99025821685791,
      "learning_rate": 4.450255102040817e-05,
      "loss": 0.6392,
      "step": 560300
    },
    {
      "epoch": 8.797488226059654,
      "grad_norm": 4.240264892578125,
      "learning_rate": 4.4501569858712715e-05,
      "loss": 0.6579,
      "step": 560400
    },
    {
      "epoch": 8.799058084772371,
      "grad_norm": 3.417452335357666,
      "learning_rate": 4.450058869701727e-05,
      "loss": 0.6434,
      "step": 560500
    },
    {
      "epoch": 8.800627943485086,
      "grad_norm": 3.3225502967834473,
      "learning_rate": 4.449960753532182e-05,
      "loss": 0.639,
      "step": 560600
    },
    {
      "epoch": 8.802197802197803,
      "grad_norm": 3.087557792663574,
      "learning_rate": 4.4498626373626374e-05,
      "loss": 0.6309,
      "step": 560700
    },
    {
      "epoch": 8.803767660910518,
      "grad_norm": 3.965662956237793,
      "learning_rate": 4.4497645211930925e-05,
      "loss": 0.6417,
      "step": 560800
    },
    {
      "epoch": 8.805337519623233,
      "grad_norm": 3.309995174407959,
      "learning_rate": 4.449666405023548e-05,
      "loss": 0.6485,
      "step": 560900
    },
    {
      "epoch": 8.80690737833595,
      "grad_norm": 4.901336193084717,
      "learning_rate": 4.4495682888540034e-05,
      "loss": 0.6727,
      "step": 561000
    },
    {
      "epoch": 8.808477237048665,
      "grad_norm": 3.5064330101013184,
      "learning_rate": 4.4494701726844585e-05,
      "loss": 0.6421,
      "step": 561100
    },
    {
      "epoch": 8.810047095761382,
      "grad_norm": 4.088964462280273,
      "learning_rate": 4.4493720565149136e-05,
      "loss": 0.6436,
      "step": 561200
    },
    {
      "epoch": 8.811616954474097,
      "grad_norm": 4.304647445678711,
      "learning_rate": 4.4492739403453694e-05,
      "loss": 0.6405,
      "step": 561300
    },
    {
      "epoch": 8.813186813186814,
      "grad_norm": 3.98274302482605,
      "learning_rate": 4.449175824175824e-05,
      "loss": 0.67,
      "step": 561400
    },
    {
      "epoch": 8.814756671899529,
      "grad_norm": 3.8011562824249268,
      "learning_rate": 4.4490777080062796e-05,
      "loss": 0.6296,
      "step": 561500
    },
    {
      "epoch": 8.816326530612244,
      "grad_norm": 3.0933353900909424,
      "learning_rate": 4.448979591836735e-05,
      "loss": 0.636,
      "step": 561600
    },
    {
      "epoch": 8.817896389324961,
      "grad_norm": 3.461668014526367,
      "learning_rate": 4.4488814756671904e-05,
      "loss": 0.6247,
      "step": 561700
    },
    {
      "epoch": 8.819466248037676,
      "grad_norm": 2.7053306102752686,
      "learning_rate": 4.4487833594976455e-05,
      "loss": 0.6466,
      "step": 561800
    },
    {
      "epoch": 8.821036106750393,
      "grad_norm": 3.6608846187591553,
      "learning_rate": 4.4486852433281006e-05,
      "loss": 0.6344,
      "step": 561900
    },
    {
      "epoch": 8.822605965463108,
      "grad_norm": 5.028148651123047,
      "learning_rate": 4.4485871271585564e-05,
      "loss": 0.652,
      "step": 562000
    },
    {
      "epoch": 8.824175824175825,
      "grad_norm": 3.7917840480804443,
      "learning_rate": 4.448489010989011e-05,
      "loss": 0.6189,
      "step": 562100
    },
    {
      "epoch": 8.82574568288854,
      "grad_norm": 1.9886765480041504,
      "learning_rate": 4.4483908948194666e-05,
      "loss": 0.6391,
      "step": 562200
    },
    {
      "epoch": 8.827315541601255,
      "grad_norm": 4.151397705078125,
      "learning_rate": 4.448292778649922e-05,
      "loss": 0.6564,
      "step": 562300
    },
    {
      "epoch": 8.828885400313972,
      "grad_norm": 4.247830867767334,
      "learning_rate": 4.4481946624803775e-05,
      "loss": 0.627,
      "step": 562400
    },
    {
      "epoch": 8.830455259026687,
      "grad_norm": 4.367666244506836,
      "learning_rate": 4.448096546310832e-05,
      "loss": 0.6671,
      "step": 562500
    },
    {
      "epoch": 8.832025117739404,
      "grad_norm": 3.0184926986694336,
      "learning_rate": 4.447998430141288e-05,
      "loss": 0.6622,
      "step": 562600
    },
    {
      "epoch": 8.833594976452119,
      "grad_norm": 3.9979729652404785,
      "learning_rate": 4.447900313971743e-05,
      "loss": 0.668,
      "step": 562700
    },
    {
      "epoch": 8.835164835164836,
      "grad_norm": 3.1912038326263428,
      "learning_rate": 4.447802197802198e-05,
      "loss": 0.6398,
      "step": 562800
    },
    {
      "epoch": 8.83673469387755,
      "grad_norm": 3.602522134780884,
      "learning_rate": 4.447704081632653e-05,
      "loss": 0.6567,
      "step": 562900
    },
    {
      "epoch": 8.838304552590268,
      "grad_norm": 2.561302661895752,
      "learning_rate": 4.447605965463109e-05,
      "loss": 0.6419,
      "step": 563000
    },
    {
      "epoch": 8.839874411302983,
      "grad_norm": 3.5158636569976807,
      "learning_rate": 4.447507849293564e-05,
      "loss": 0.6472,
      "step": 563100
    },
    {
      "epoch": 8.841444270015698,
      "grad_norm": 4.085956573486328,
      "learning_rate": 4.447409733124019e-05,
      "loss": 0.6356,
      "step": 563200
    },
    {
      "epoch": 8.843014128728415,
      "grad_norm": 3.961315155029297,
      "learning_rate": 4.447311616954474e-05,
      "loss": 0.6293,
      "step": 563300
    },
    {
      "epoch": 8.84458398744113,
      "grad_norm": 2.8479888439178467,
      "learning_rate": 4.44721350078493e-05,
      "loss": 0.6327,
      "step": 563400
    },
    {
      "epoch": 8.846153846153847,
      "grad_norm": 4.108383655548096,
      "learning_rate": 4.447115384615384e-05,
      "loss": 0.6216,
      "step": 563500
    },
    {
      "epoch": 8.847723704866562,
      "grad_norm": 3.0711171627044678,
      "learning_rate": 4.44701726844584e-05,
      "loss": 0.6487,
      "step": 563600
    },
    {
      "epoch": 8.849293563579279,
      "grad_norm": 3.954439878463745,
      "learning_rate": 4.446919152276295e-05,
      "loss": 0.625,
      "step": 563700
    },
    {
      "epoch": 8.850863422291994,
      "grad_norm": 3.994515895843506,
      "learning_rate": 4.446821036106751e-05,
      "loss": 0.6204,
      "step": 563800
    },
    {
      "epoch": 8.852433281004709,
      "grad_norm": 4.649466037750244,
      "learning_rate": 4.446722919937206e-05,
      "loss": 0.6774,
      "step": 563900
    },
    {
      "epoch": 8.854003139717426,
      "grad_norm": 3.15256667137146,
      "learning_rate": 4.446624803767661e-05,
      "loss": 0.6686,
      "step": 564000
    },
    {
      "epoch": 8.85557299843014,
      "grad_norm": 5.386981010437012,
      "learning_rate": 4.446526687598117e-05,
      "loss": 0.6726,
      "step": 564100
    },
    {
      "epoch": 8.857142857142858,
      "grad_norm": 3.3913912773132324,
      "learning_rate": 4.446428571428571e-05,
      "loss": 0.6096,
      "step": 564200
    },
    {
      "epoch": 8.858712715855573,
      "grad_norm": 2.5069119930267334,
      "learning_rate": 4.446330455259027e-05,
      "loss": 0.6297,
      "step": 564300
    },
    {
      "epoch": 8.86028257456829,
      "grad_norm": 4.692251682281494,
      "learning_rate": 4.446232339089482e-05,
      "loss": 0.6228,
      "step": 564400
    },
    {
      "epoch": 8.861852433281005,
      "grad_norm": 4.037857532501221,
      "learning_rate": 4.446134222919938e-05,
      "loss": 0.6544,
      "step": 564500
    },
    {
      "epoch": 8.86342229199372,
      "grad_norm": 2.770890951156616,
      "learning_rate": 4.4460361067503924e-05,
      "loss": 0.6561,
      "step": 564600
    },
    {
      "epoch": 8.864992150706437,
      "grad_norm": 3.8150172233581543,
      "learning_rate": 4.445937990580848e-05,
      "loss": 0.6732,
      "step": 564700
    },
    {
      "epoch": 8.866562009419152,
      "grad_norm": 4.233813762664795,
      "learning_rate": 4.445839874411303e-05,
      "loss": 0.6259,
      "step": 564800
    },
    {
      "epoch": 8.868131868131869,
      "grad_norm": 3.139897108078003,
      "learning_rate": 4.445741758241758e-05,
      "loss": 0.6316,
      "step": 564900
    },
    {
      "epoch": 8.869701726844584,
      "grad_norm": 4.993257522583008,
      "learning_rate": 4.4456436420722134e-05,
      "loss": 0.6525,
      "step": 565000
    },
    {
      "epoch": 8.8712715855573,
      "grad_norm": 8.218441009521484,
      "learning_rate": 4.445545525902669e-05,
      "loss": 0.6954,
      "step": 565100
    },
    {
      "epoch": 8.872841444270016,
      "grad_norm": 3.635450839996338,
      "learning_rate": 4.445447409733124e-05,
      "loss": 0.6185,
      "step": 565200
    },
    {
      "epoch": 8.87441130298273,
      "grad_norm": 4.329414367675781,
      "learning_rate": 4.4453492935635794e-05,
      "loss": 0.6466,
      "step": 565300
    },
    {
      "epoch": 8.875981161695448,
      "grad_norm": 3.888765335083008,
      "learning_rate": 4.4452511773940345e-05,
      "loss": 0.6725,
      "step": 565400
    },
    {
      "epoch": 8.877551020408163,
      "grad_norm": 3.3240230083465576,
      "learning_rate": 4.44515306122449e-05,
      "loss": 0.6316,
      "step": 565500
    },
    {
      "epoch": 8.87912087912088,
      "grad_norm": 3.980121612548828,
      "learning_rate": 4.445054945054945e-05,
      "loss": 0.6623,
      "step": 565600
    },
    {
      "epoch": 8.880690737833595,
      "grad_norm": 3.178643226623535,
      "learning_rate": 4.4449568288854005e-05,
      "loss": 0.6593,
      "step": 565700
    },
    {
      "epoch": 8.882260596546312,
      "grad_norm": 3.5818917751312256,
      "learning_rate": 4.4448587127158556e-05,
      "loss": 0.614,
      "step": 565800
    },
    {
      "epoch": 8.883830455259027,
      "grad_norm": 3.376300573348999,
      "learning_rate": 4.444760596546311e-05,
      "loss": 0.6646,
      "step": 565900
    },
    {
      "epoch": 8.885400313971743,
      "grad_norm": 2.7814700603485107,
      "learning_rate": 4.4446624803767664e-05,
      "loss": 0.6527,
      "step": 566000
    },
    {
      "epoch": 8.886970172684459,
      "grad_norm": 3.0908775329589844,
      "learning_rate": 4.4445643642072215e-05,
      "loss": 0.6514,
      "step": 566100
    },
    {
      "epoch": 8.888540031397174,
      "grad_norm": 4.00169038772583,
      "learning_rate": 4.444466248037677e-05,
      "loss": 0.6804,
      "step": 566200
    },
    {
      "epoch": 8.89010989010989,
      "grad_norm": 4.626260757446289,
      "learning_rate": 4.444368131868132e-05,
      "loss": 0.6219,
      "step": 566300
    },
    {
      "epoch": 8.891679748822606,
      "grad_norm": 4.144922733306885,
      "learning_rate": 4.4442700156985875e-05,
      "loss": 0.6361,
      "step": 566400
    },
    {
      "epoch": 8.893249607535322,
      "grad_norm": 3.1737112998962402,
      "learning_rate": 4.4441718995290426e-05,
      "loss": 0.6472,
      "step": 566500
    },
    {
      "epoch": 8.894819466248038,
      "grad_norm": 3.774299383163452,
      "learning_rate": 4.4440737833594984e-05,
      "loss": 0.6005,
      "step": 566600
    },
    {
      "epoch": 8.896389324960754,
      "grad_norm": 4.737890243530273,
      "learning_rate": 4.443975667189953e-05,
      "loss": 0.661,
      "step": 566700
    },
    {
      "epoch": 8.89795918367347,
      "grad_norm": 4.61859130859375,
      "learning_rate": 4.4438775510204086e-05,
      "loss": 0.6187,
      "step": 566800
    },
    {
      "epoch": 8.899529042386185,
      "grad_norm": 4.501461982727051,
      "learning_rate": 4.443779434850864e-05,
      "loss": 0.6241,
      "step": 566900
    },
    {
      "epoch": 8.901098901098901,
      "grad_norm": 2.627760648727417,
      "learning_rate": 4.443681318681319e-05,
      "loss": 0.6358,
      "step": 567000
    },
    {
      "epoch": 8.902668759811617,
      "grad_norm": 4.632437705993652,
      "learning_rate": 4.443583202511774e-05,
      "loss": 0.6292,
      "step": 567100
    },
    {
      "epoch": 8.904238618524333,
      "grad_norm": 3.305931806564331,
      "learning_rate": 4.4434850863422296e-05,
      "loss": 0.6341,
      "step": 567200
    },
    {
      "epoch": 8.905808477237048,
      "grad_norm": 3.3185417652130127,
      "learning_rate": 4.443386970172685e-05,
      "loss": 0.6579,
      "step": 567300
    },
    {
      "epoch": 8.907378335949765,
      "grad_norm": 4.386584758758545,
      "learning_rate": 4.44328885400314e-05,
      "loss": 0.6668,
      "step": 567400
    },
    {
      "epoch": 8.90894819466248,
      "grad_norm": 3.4331185817718506,
      "learning_rate": 4.443190737833595e-05,
      "loss": 0.6472,
      "step": 567500
    },
    {
      "epoch": 8.910518053375196,
      "grad_norm": 5.535481929779053,
      "learning_rate": 4.443092621664051e-05,
      "loss": 0.6043,
      "step": 567600
    },
    {
      "epoch": 8.912087912087912,
      "grad_norm": 4.0937628746032715,
      "learning_rate": 4.442994505494505e-05,
      "loss": 0.624,
      "step": 567700
    },
    {
      "epoch": 8.913657770800627,
      "grad_norm": 3.3204545974731445,
      "learning_rate": 4.442896389324961e-05,
      "loss": 0.6267,
      "step": 567800
    },
    {
      "epoch": 8.915227629513344,
      "grad_norm": 4.244992733001709,
      "learning_rate": 4.442798273155416e-05,
      "loss": 0.6609,
      "step": 567900
    },
    {
      "epoch": 8.91679748822606,
      "grad_norm": 4.202767848968506,
      "learning_rate": 4.442700156985872e-05,
      "loss": 0.6014,
      "step": 568000
    },
    {
      "epoch": 8.918367346938776,
      "grad_norm": 4.857175827026367,
      "learning_rate": 4.442602040816327e-05,
      "loss": 0.6535,
      "step": 568100
    },
    {
      "epoch": 8.919937205651491,
      "grad_norm": 3.5065977573394775,
      "learning_rate": 4.442503924646782e-05,
      "loss": 0.6523,
      "step": 568200
    },
    {
      "epoch": 8.921507064364206,
      "grad_norm": 4.428284168243408,
      "learning_rate": 4.442405808477237e-05,
      "loss": 0.6946,
      "step": 568300
    },
    {
      "epoch": 8.923076923076923,
      "grad_norm": 3.8836138248443604,
      "learning_rate": 4.442307692307692e-05,
      "loss": 0.6421,
      "step": 568400
    },
    {
      "epoch": 8.924646781789638,
      "grad_norm": 4.229650497436523,
      "learning_rate": 4.442209576138148e-05,
      "loss": 0.6473,
      "step": 568500
    },
    {
      "epoch": 8.926216640502355,
      "grad_norm": 3.5428106784820557,
      "learning_rate": 4.442111459968603e-05,
      "loss": 0.6402,
      "step": 568600
    },
    {
      "epoch": 8.92778649921507,
      "grad_norm": 4.462611198425293,
      "learning_rate": 4.442013343799059e-05,
      "loss": 0.645,
      "step": 568700
    },
    {
      "epoch": 8.929356357927787,
      "grad_norm": 3.918684720993042,
      "learning_rate": 4.441915227629513e-05,
      "loss": 0.6403,
      "step": 568800
    },
    {
      "epoch": 8.930926216640502,
      "grad_norm": 4.077979564666748,
      "learning_rate": 4.441817111459969e-05,
      "loss": 0.6285,
      "step": 568900
    },
    {
      "epoch": 8.932496075353217,
      "grad_norm": 2.853278160095215,
      "learning_rate": 4.441718995290424e-05,
      "loss": 0.6252,
      "step": 569000
    },
    {
      "epoch": 8.934065934065934,
      "grad_norm": 4.472587585449219,
      "learning_rate": 4.441620879120879e-05,
      "loss": 0.635,
      "step": 569100
    },
    {
      "epoch": 8.93563579277865,
      "grad_norm": 3.3614041805267334,
      "learning_rate": 4.441522762951334e-05,
      "loss": 0.6516,
      "step": 569200
    },
    {
      "epoch": 8.937205651491366,
      "grad_norm": 3.4545929431915283,
      "learning_rate": 4.44142464678179e-05,
      "loss": 0.6282,
      "step": 569300
    },
    {
      "epoch": 8.938775510204081,
      "grad_norm": 3.969315528869629,
      "learning_rate": 4.441326530612245e-05,
      "loss": 0.6707,
      "step": 569400
    },
    {
      "epoch": 8.940345368916798,
      "grad_norm": 4.298430919647217,
      "learning_rate": 4.4412284144427e-05,
      "loss": 0.6465,
      "step": 569500
    },
    {
      "epoch": 8.941915227629513,
      "grad_norm": 3.849463701248169,
      "learning_rate": 4.4411302982731554e-05,
      "loss": 0.62,
      "step": 569600
    },
    {
      "epoch": 8.943485086342228,
      "grad_norm": 3.210645914077759,
      "learning_rate": 4.441032182103611e-05,
      "loss": 0.6259,
      "step": 569700
    },
    {
      "epoch": 8.945054945054945,
      "grad_norm": 3.0653576850891113,
      "learning_rate": 4.4409340659340656e-05,
      "loss": 0.6082,
      "step": 569800
    },
    {
      "epoch": 8.94662480376766,
      "grad_norm": 3.4595413208007812,
      "learning_rate": 4.4408359497645214e-05,
      "loss": 0.6489,
      "step": 569900
    },
    {
      "epoch": 8.948194662480377,
      "grad_norm": 4.947179794311523,
      "learning_rate": 4.4407378335949765e-05,
      "loss": 0.6737,
      "step": 570000
    },
    {
      "epoch": 8.949764521193092,
      "grad_norm": 5.0166168212890625,
      "learning_rate": 4.440639717425432e-05,
      "loss": 0.6321,
      "step": 570100
    },
    {
      "epoch": 8.95133437990581,
      "grad_norm": 4.871725559234619,
      "learning_rate": 4.440541601255887e-05,
      "loss": 0.6558,
      "step": 570200
    },
    {
      "epoch": 8.952904238618524,
      "grad_norm": 4.770059108734131,
      "learning_rate": 4.4404434850863424e-05,
      "loss": 0.6387,
      "step": 570300
    },
    {
      "epoch": 8.95447409733124,
      "grad_norm": 4.394201278686523,
      "learning_rate": 4.4403453689167975e-05,
      "loss": 0.6604,
      "step": 570400
    },
    {
      "epoch": 8.956043956043956,
      "grad_norm": 3.598999261856079,
      "learning_rate": 4.4402472527472526e-05,
      "loss": 0.6755,
      "step": 570500
    },
    {
      "epoch": 8.957613814756671,
      "grad_norm": 3.009249687194824,
      "learning_rate": 4.4401491365777084e-05,
      "loss": 0.6919,
      "step": 570600
    },
    {
      "epoch": 8.959183673469388,
      "grad_norm": 4.118425369262695,
      "learning_rate": 4.4400510204081635e-05,
      "loss": 0.6848,
      "step": 570700
    },
    {
      "epoch": 8.960753532182103,
      "grad_norm": 3.7398576736450195,
      "learning_rate": 4.439952904238619e-05,
      "loss": 0.621,
      "step": 570800
    },
    {
      "epoch": 8.96232339089482,
      "grad_norm": 4.775282382965088,
      "learning_rate": 4.439854788069074e-05,
      "loss": 0.6262,
      "step": 570900
    },
    {
      "epoch": 8.963893249607535,
      "grad_norm": 5.250401973724365,
      "learning_rate": 4.4397566718995295e-05,
      "loss": 0.6563,
      "step": 571000
    },
    {
      "epoch": 8.96546310832025,
      "grad_norm": 5.3502888679504395,
      "learning_rate": 4.4396585557299846e-05,
      "loss": 0.6587,
      "step": 571100
    },
    {
      "epoch": 8.967032967032967,
      "grad_norm": 4.030148029327393,
      "learning_rate": 4.43956043956044e-05,
      "loss": 0.6404,
      "step": 571200
    },
    {
      "epoch": 8.968602825745682,
      "grad_norm": 3.8257358074188232,
      "learning_rate": 4.439462323390895e-05,
      "loss": 0.6403,
      "step": 571300
    },
    {
      "epoch": 8.970172684458399,
      "grad_norm": 4.462395191192627,
      "learning_rate": 4.4393642072213505e-05,
      "loss": 0.6544,
      "step": 571400
    },
    {
      "epoch": 8.971742543171114,
      "grad_norm": 4.545942783355713,
      "learning_rate": 4.4392660910518056e-05,
      "loss": 0.6658,
      "step": 571500
    },
    {
      "epoch": 8.973312401883831,
      "grad_norm": 2.7545559406280518,
      "learning_rate": 4.439167974882261e-05,
      "loss": 0.6245,
      "step": 571600
    },
    {
      "epoch": 8.974882260596546,
      "grad_norm": 4.199215412139893,
      "learning_rate": 4.439069858712716e-05,
      "loss": 0.6803,
      "step": 571700
    },
    {
      "epoch": 8.976452119309261,
      "grad_norm": 3.0445783138275146,
      "learning_rate": 4.4389717425431716e-05,
      "loss": 0.6613,
      "step": 571800
    },
    {
      "epoch": 8.978021978021978,
      "grad_norm": 3.9185714721679688,
      "learning_rate": 4.438873626373626e-05,
      "loss": 0.6477,
      "step": 571900
    },
    {
      "epoch": 8.979591836734693,
      "grad_norm": 3.4325146675109863,
      "learning_rate": 4.438775510204082e-05,
      "loss": 0.6095,
      "step": 572000
    },
    {
      "epoch": 8.98116169544741,
      "grad_norm": 4.585019111633301,
      "learning_rate": 4.438677394034537e-05,
      "loss": 0.6357,
      "step": 572100
    },
    {
      "epoch": 8.982731554160125,
      "grad_norm": 5.190730571746826,
      "learning_rate": 4.438579277864993e-05,
      "loss": 0.6134,
      "step": 572200
    },
    {
      "epoch": 8.984301412872842,
      "grad_norm": 4.013345241546631,
      "learning_rate": 4.438481161695448e-05,
      "loss": 0.688,
      "step": 572300
    },
    {
      "epoch": 8.985871271585557,
      "grad_norm": 3.9723398685455322,
      "learning_rate": 4.438383045525903e-05,
      "loss": 0.6232,
      "step": 572400
    },
    {
      "epoch": 8.987441130298274,
      "grad_norm": 2.5706491470336914,
      "learning_rate": 4.438284929356358e-05,
      "loss": 0.6762,
      "step": 572500
    },
    {
      "epoch": 8.989010989010989,
      "grad_norm": 3.391627788543701,
      "learning_rate": 4.438186813186813e-05,
      "loss": 0.6055,
      "step": 572600
    },
    {
      "epoch": 8.990580847723704,
      "grad_norm": 4.079648017883301,
      "learning_rate": 4.438088697017269e-05,
      "loss": 0.6029,
      "step": 572700
    },
    {
      "epoch": 8.992150706436421,
      "grad_norm": 3.574521064758301,
      "learning_rate": 4.437990580847724e-05,
      "loss": 0.6172,
      "step": 572800
    },
    {
      "epoch": 8.993720565149136,
      "grad_norm": 3.4576077461242676,
      "learning_rate": 4.43789246467818e-05,
      "loss": 0.6433,
      "step": 572900
    },
    {
      "epoch": 8.995290423861853,
      "grad_norm": 3.8074405193328857,
      "learning_rate": 4.437794348508634e-05,
      "loss": 0.6723,
      "step": 573000
    },
    {
      "epoch": 8.996860282574568,
      "grad_norm": 4.01403284072876,
      "learning_rate": 4.43769623233909e-05,
      "loss": 0.6769,
      "step": 573100
    },
    {
      "epoch": 8.998430141287285,
      "grad_norm": 2.868176221847534,
      "learning_rate": 4.437598116169545e-05,
      "loss": 0.6665,
      "step": 573200
    },
    {
      "epoch": 9.0,
      "grad_norm": 3.872978687286377,
      "learning_rate": 4.4375e-05,
      "loss": 0.6551,
      "step": 573300
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.020621418952942,
      "eval_runtime": 14.6342,
      "eval_samples_per_second": 229.121,
      "eval_steps_per_second": 229.121,
      "step": 573300
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.49833229184150696,
      "eval_runtime": 280.5738,
      "eval_samples_per_second": 227.035,
      "eval_steps_per_second": 227.035,
      "step": 573300
    },
    {
      "epoch": 9.001569858712715,
      "grad_norm": 4.276501178741455,
      "learning_rate": 4.437401883830455e-05,
      "loss": 0.6185,
      "step": 573400
    },
    {
      "epoch": 9.003139717425432,
      "grad_norm": 4.202734470367432,
      "learning_rate": 4.437303767660911e-05,
      "loss": 0.6305,
      "step": 573500
    },
    {
      "epoch": 9.004709576138147,
      "grad_norm": 3.509608745574951,
      "learning_rate": 4.437205651491366e-05,
      "loss": 0.6557,
      "step": 573600
    },
    {
      "epoch": 9.006279434850864,
      "grad_norm": 5.0389509201049805,
      "learning_rate": 4.437107535321821e-05,
      "loss": 0.6179,
      "step": 573700
    },
    {
      "epoch": 9.007849293563579,
      "grad_norm": 3.202655553817749,
      "learning_rate": 4.437009419152276e-05,
      "loss": 0.5888,
      "step": 573800
    },
    {
      "epoch": 9.009419152276296,
      "grad_norm": 4.243131637573242,
      "learning_rate": 4.436911302982732e-05,
      "loss": 0.6562,
      "step": 573900
    },
    {
      "epoch": 9.010989010989011,
      "grad_norm": 4.224087715148926,
      "learning_rate": 4.4368131868131865e-05,
      "loss": 0.6318,
      "step": 574000
    },
    {
      "epoch": 9.012558869701726,
      "grad_norm": 3.9612653255462646,
      "learning_rate": 4.436715070643642e-05,
      "loss": 0.6382,
      "step": 574100
    },
    {
      "epoch": 9.014128728414443,
      "grad_norm": 4.030497074127197,
      "learning_rate": 4.4366169544740974e-05,
      "loss": 0.6249,
      "step": 574200
    },
    {
      "epoch": 9.015698587127158,
      "grad_norm": 4.480139255523682,
      "learning_rate": 4.436518838304553e-05,
      "loss": 0.6688,
      "step": 574300
    },
    {
      "epoch": 9.017268445839875,
      "grad_norm": 3.227296829223633,
      "learning_rate": 4.436420722135008e-05,
      "loss": 0.6366,
      "step": 574400
    },
    {
      "epoch": 9.01883830455259,
      "grad_norm": 3.844406843185425,
      "learning_rate": 4.436322605965463e-05,
      "loss": 0.6162,
      "step": 574500
    },
    {
      "epoch": 9.020408163265307,
      "grad_norm": 4.0624098777771,
      "learning_rate": 4.4362244897959184e-05,
      "loss": 0.6244,
      "step": 574600
    },
    {
      "epoch": 9.021978021978022,
      "grad_norm": 3.7804458141326904,
      "learning_rate": 4.4361263736263735e-05,
      "loss": 0.6151,
      "step": 574700
    },
    {
      "epoch": 9.023547880690737,
      "grad_norm": 4.729689121246338,
      "learning_rate": 4.436028257456829e-05,
      "loss": 0.6826,
      "step": 574800
    },
    {
      "epoch": 9.025117739403454,
      "grad_norm": 4.713578701019287,
      "learning_rate": 4.4359301412872844e-05,
      "loss": 0.6634,
      "step": 574900
    },
    {
      "epoch": 9.026687598116169,
      "grad_norm": 3.2944223880767822,
      "learning_rate": 4.43583202511774e-05,
      "loss": 0.6521,
      "step": 575000
    },
    {
      "epoch": 9.028257456828886,
      "grad_norm": 4.1800761222839355,
      "learning_rate": 4.4357339089481946e-05,
      "loss": 0.6238,
      "step": 575100
    },
    {
      "epoch": 9.029827315541601,
      "grad_norm": 3.8231492042541504,
      "learning_rate": 4.4356357927786504e-05,
      "loss": 0.6486,
      "step": 575200
    },
    {
      "epoch": 9.031397174254318,
      "grad_norm": 2.290706157684326,
      "learning_rate": 4.4355376766091055e-05,
      "loss": 0.6185,
      "step": 575300
    },
    {
      "epoch": 9.032967032967033,
      "grad_norm": 4.644010543823242,
      "learning_rate": 4.4354395604395606e-05,
      "loss": 0.6042,
      "step": 575400
    },
    {
      "epoch": 9.034536891679748,
      "grad_norm": 3.5952961444854736,
      "learning_rate": 4.435341444270016e-05,
      "loss": 0.6445,
      "step": 575500
    },
    {
      "epoch": 9.036106750392465,
      "grad_norm": 3.828535556793213,
      "learning_rate": 4.4352433281004714e-05,
      "loss": 0.6517,
      "step": 575600
    },
    {
      "epoch": 9.03767660910518,
      "grad_norm": 3.615887403488159,
      "learning_rate": 4.4351452119309265e-05,
      "loss": 0.6379,
      "step": 575700
    },
    {
      "epoch": 9.039246467817897,
      "grad_norm": 4.595581531524658,
      "learning_rate": 4.4350470957613816e-05,
      "loss": 0.5879,
      "step": 575800
    },
    {
      "epoch": 9.040816326530612,
      "grad_norm": 4.179681777954102,
      "learning_rate": 4.434948979591837e-05,
      "loss": 0.6119,
      "step": 575900
    },
    {
      "epoch": 9.042386185243329,
      "grad_norm": 3.9014201164245605,
      "learning_rate": 4.4348508634222925e-05,
      "loss": 0.6261,
      "step": 576000
    },
    {
      "epoch": 9.043956043956044,
      "grad_norm": 4.55007266998291,
      "learning_rate": 4.434752747252747e-05,
      "loss": 0.6181,
      "step": 576100
    },
    {
      "epoch": 9.04552590266876,
      "grad_norm": 4.51201868057251,
      "learning_rate": 4.434654631083203e-05,
      "loss": 0.6355,
      "step": 576200
    },
    {
      "epoch": 9.047095761381476,
      "grad_norm": 4.742146968841553,
      "learning_rate": 4.434556514913658e-05,
      "loss": 0.6489,
      "step": 576300
    },
    {
      "epoch": 9.04866562009419,
      "grad_norm": 4.256378650665283,
      "learning_rate": 4.4344583987441136e-05,
      "loss": 0.6618,
      "step": 576400
    },
    {
      "epoch": 9.050235478806908,
      "grad_norm": 3.9624216556549072,
      "learning_rate": 4.434360282574569e-05,
      "loss": 0.6381,
      "step": 576500
    },
    {
      "epoch": 9.051805337519623,
      "grad_norm": 4.116766452789307,
      "learning_rate": 4.434262166405024e-05,
      "loss": 0.6495,
      "step": 576600
    },
    {
      "epoch": 9.05337519623234,
      "grad_norm": 4.055208206176758,
      "learning_rate": 4.434164050235479e-05,
      "loss": 0.6077,
      "step": 576700
    },
    {
      "epoch": 9.054945054945055,
      "grad_norm": 4.2627129554748535,
      "learning_rate": 4.434065934065934e-05,
      "loss": 0.6413,
      "step": 576800
    },
    {
      "epoch": 9.056514913657772,
      "grad_norm": 4.580260753631592,
      "learning_rate": 4.43396781789639e-05,
      "loss": 0.6412,
      "step": 576900
    },
    {
      "epoch": 9.058084772370487,
      "grad_norm": 3.7464683055877686,
      "learning_rate": 4.433869701726845e-05,
      "loss": 0.66,
      "step": 577000
    },
    {
      "epoch": 9.059654631083202,
      "grad_norm": 3.199612617492676,
      "learning_rate": 4.4337715855573006e-05,
      "loss": 0.6684,
      "step": 577100
    },
    {
      "epoch": 9.061224489795919,
      "grad_norm": 3.367013931274414,
      "learning_rate": 4.433673469387755e-05,
      "loss": 0.6034,
      "step": 577200
    },
    {
      "epoch": 9.062794348508634,
      "grad_norm": 3.9326789379119873,
      "learning_rate": 4.433575353218211e-05,
      "loss": 0.6425,
      "step": 577300
    },
    {
      "epoch": 9.06436420722135,
      "grad_norm": 3.6100919246673584,
      "learning_rate": 4.433477237048666e-05,
      "loss": 0.643,
      "step": 577400
    },
    {
      "epoch": 9.065934065934066,
      "grad_norm": 3.049614429473877,
      "learning_rate": 4.433379120879121e-05,
      "loss": 0.6782,
      "step": 577500
    },
    {
      "epoch": 9.067503924646783,
      "grad_norm": 2.3052597045898438,
      "learning_rate": 4.433281004709576e-05,
      "loss": 0.6817,
      "step": 577600
    },
    {
      "epoch": 9.069073783359498,
      "grad_norm": 4.475391864776611,
      "learning_rate": 4.433182888540032e-05,
      "loss": 0.6333,
      "step": 577700
    },
    {
      "epoch": 9.070643642072213,
      "grad_norm": 2.5964272022247314,
      "learning_rate": 4.433084772370487e-05,
      "loss": 0.5835,
      "step": 577800
    },
    {
      "epoch": 9.07221350078493,
      "grad_norm": 3.142853021621704,
      "learning_rate": 4.432986656200942e-05,
      "loss": 0.6449,
      "step": 577900
    },
    {
      "epoch": 9.073783359497645,
      "grad_norm": 4.380989074707031,
      "learning_rate": 4.432888540031397e-05,
      "loss": 0.6553,
      "step": 578000
    },
    {
      "epoch": 9.075353218210362,
      "grad_norm": 3.5855817794799805,
      "learning_rate": 4.432790423861853e-05,
      "loss": 0.6117,
      "step": 578100
    },
    {
      "epoch": 9.076923076923077,
      "grad_norm": 3.9184298515319824,
      "learning_rate": 4.4326923076923074e-05,
      "loss": 0.6344,
      "step": 578200
    },
    {
      "epoch": 9.078492935635794,
      "grad_norm": 3.757733106613159,
      "learning_rate": 4.432594191522763e-05,
      "loss": 0.695,
      "step": 578300
    },
    {
      "epoch": 9.080062794348509,
      "grad_norm": 3.9382376670837402,
      "learning_rate": 4.432496075353218e-05,
      "loss": 0.6543,
      "step": 578400
    },
    {
      "epoch": 9.081632653061224,
      "grad_norm": 3.0806729793548584,
      "learning_rate": 4.432397959183674e-05,
      "loss": 0.6477,
      "step": 578500
    },
    {
      "epoch": 9.08320251177394,
      "grad_norm": 3.7275636196136475,
      "learning_rate": 4.432299843014129e-05,
      "loss": 0.646,
      "step": 578600
    },
    {
      "epoch": 9.084772370486656,
      "grad_norm": 4.100196838378906,
      "learning_rate": 4.432201726844584e-05,
      "loss": 0.642,
      "step": 578700
    },
    {
      "epoch": 9.086342229199373,
      "grad_norm": 2.789552927017212,
      "learning_rate": 4.432103610675039e-05,
      "loss": 0.6587,
      "step": 578800
    },
    {
      "epoch": 9.087912087912088,
      "grad_norm": 3.5481185913085938,
      "learning_rate": 4.4320054945054944e-05,
      "loss": 0.6535,
      "step": 578900
    },
    {
      "epoch": 9.089481946624804,
      "grad_norm": 3.435828924179077,
      "learning_rate": 4.43190737833595e-05,
      "loss": 0.6657,
      "step": 579000
    },
    {
      "epoch": 9.09105180533752,
      "grad_norm": 3.923189640045166,
      "learning_rate": 4.431809262166405e-05,
      "loss": 0.6833,
      "step": 579100
    },
    {
      "epoch": 9.092621664050235,
      "grad_norm": 2.5854263305664062,
      "learning_rate": 4.431711145996861e-05,
      "loss": 0.5777,
      "step": 579200
    },
    {
      "epoch": 9.094191522762952,
      "grad_norm": 4.543500900268555,
      "learning_rate": 4.4316130298273155e-05,
      "loss": 0.6555,
      "step": 579300
    },
    {
      "epoch": 9.095761381475667,
      "grad_norm": 4.353578090667725,
      "learning_rate": 4.431514913657771e-05,
      "loss": 0.6429,
      "step": 579400
    },
    {
      "epoch": 9.097331240188383,
      "grad_norm": 3.620790958404541,
      "learning_rate": 4.4314167974882264e-05,
      "loss": 0.6186,
      "step": 579500
    },
    {
      "epoch": 9.098901098901099,
      "grad_norm": 4.02151346206665,
      "learning_rate": 4.4313186813186815e-05,
      "loss": 0.6204,
      "step": 579600
    },
    {
      "epoch": 9.100470957613815,
      "grad_norm": 3.671782970428467,
      "learning_rate": 4.4312205651491366e-05,
      "loss": 0.6155,
      "step": 579700
    },
    {
      "epoch": 9.10204081632653,
      "grad_norm": 3.921137809753418,
      "learning_rate": 4.4311224489795923e-05,
      "loss": 0.6171,
      "step": 579800
    },
    {
      "epoch": 9.103610675039246,
      "grad_norm": 3.6462562084198,
      "learning_rate": 4.4310243328100474e-05,
      "loss": 0.652,
      "step": 579900
    },
    {
      "epoch": 9.105180533751962,
      "grad_norm": 4.2654571533203125,
      "learning_rate": 4.4309262166405025e-05,
      "loss": 0.654,
      "step": 580000
    },
    {
      "epoch": 9.106750392464678,
      "grad_norm": 5.073788166046143,
      "learning_rate": 4.4308281004709576e-05,
      "loss": 0.6625,
      "step": 580100
    },
    {
      "epoch": 9.108320251177394,
      "grad_norm": 5.466825485229492,
      "learning_rate": 4.4307299843014134e-05,
      "loss": 0.594,
      "step": 580200
    },
    {
      "epoch": 9.10989010989011,
      "grad_norm": 3.111438274383545,
      "learning_rate": 4.430631868131868e-05,
      "loss": 0.6431,
      "step": 580300
    },
    {
      "epoch": 9.111459968602826,
      "grad_norm": 4.185323238372803,
      "learning_rate": 4.4305337519623236e-05,
      "loss": 0.6158,
      "step": 580400
    },
    {
      "epoch": 9.113029827315541,
      "grad_norm": 4.735772132873535,
      "learning_rate": 4.430435635792779e-05,
      "loss": 0.6535,
      "step": 580500
    },
    {
      "epoch": 9.114599686028258,
      "grad_norm": 2.921154022216797,
      "learning_rate": 4.430337519623234e-05,
      "loss": 0.592,
      "step": 580600
    },
    {
      "epoch": 9.116169544740973,
      "grad_norm": 4.527531623840332,
      "learning_rate": 4.4302394034536896e-05,
      "loss": 0.6648,
      "step": 580700
    },
    {
      "epoch": 9.117739403453688,
      "grad_norm": 3.536393404006958,
      "learning_rate": 4.430141287284145e-05,
      "loss": 0.6286,
      "step": 580800
    },
    {
      "epoch": 9.119309262166405,
      "grad_norm": 3.9863102436065674,
      "learning_rate": 4.4300431711146e-05,
      "loss": 0.6156,
      "step": 580900
    },
    {
      "epoch": 9.12087912087912,
      "grad_norm": 4.479996204376221,
      "learning_rate": 4.429945054945055e-05,
      "loss": 0.6224,
      "step": 581000
    },
    {
      "epoch": 9.122448979591837,
      "grad_norm": 4.390353679656982,
      "learning_rate": 4.4298469387755106e-05,
      "loss": 0.6385,
      "step": 581100
    },
    {
      "epoch": 9.124018838304552,
      "grad_norm": 4.810579776763916,
      "learning_rate": 4.429748822605966e-05,
      "loss": 0.6338,
      "step": 581200
    },
    {
      "epoch": 9.12558869701727,
      "grad_norm": 3.508681535720825,
      "learning_rate": 4.429650706436421e-05,
      "loss": 0.6247,
      "step": 581300
    },
    {
      "epoch": 9.127158555729984,
      "grad_norm": 4.906167030334473,
      "learning_rate": 4.429552590266876e-05,
      "loss": 0.6629,
      "step": 581400
    },
    {
      "epoch": 9.1287284144427,
      "grad_norm": 3.832740306854248,
      "learning_rate": 4.429454474097332e-05,
      "loss": 0.6358,
      "step": 581500
    },
    {
      "epoch": 9.130298273155416,
      "grad_norm": 4.1116743087768555,
      "learning_rate": 4.429356357927787e-05,
      "loss": 0.6377,
      "step": 581600
    },
    {
      "epoch": 9.131868131868131,
      "grad_norm": 2.7142512798309326,
      "learning_rate": 4.429258241758242e-05,
      "loss": 0.6345,
      "step": 581700
    },
    {
      "epoch": 9.133437990580848,
      "grad_norm": 4.510814189910889,
      "learning_rate": 4.429160125588697e-05,
      "loss": 0.6328,
      "step": 581800
    },
    {
      "epoch": 9.135007849293563,
      "grad_norm": 3.540004014968872,
      "learning_rate": 4.429062009419153e-05,
      "loss": 0.6363,
      "step": 581900
    },
    {
      "epoch": 9.13657770800628,
      "grad_norm": 4.496328353881836,
      "learning_rate": 4.428963893249607e-05,
      "loss": 0.6195,
      "step": 582000
    },
    {
      "epoch": 9.138147566718995,
      "grad_norm": 3.9383480548858643,
      "learning_rate": 4.428865777080063e-05,
      "loss": 0.6554,
      "step": 582100
    },
    {
      "epoch": 9.13971742543171,
      "grad_norm": 3.3803634643554688,
      "learning_rate": 4.428767660910518e-05,
      "loss": 0.6443,
      "step": 582200
    },
    {
      "epoch": 9.141287284144427,
      "grad_norm": 4.096658706665039,
      "learning_rate": 4.428669544740974e-05,
      "loss": 0.6483,
      "step": 582300
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 5.154634952545166,
      "learning_rate": 4.428571428571428e-05,
      "loss": 0.6365,
      "step": 582400
    },
    {
      "epoch": 9.14442700156986,
      "grad_norm": 4.602187633514404,
      "learning_rate": 4.428473312401884e-05,
      "loss": 0.6763,
      "step": 582500
    },
    {
      "epoch": 9.145996860282574,
      "grad_norm": 2.8970136642456055,
      "learning_rate": 4.428375196232339e-05,
      "loss": 0.6444,
      "step": 582600
    },
    {
      "epoch": 9.147566718995291,
      "grad_norm": 3.7105679512023926,
      "learning_rate": 4.428277080062794e-05,
      "loss": 0.6374,
      "step": 582700
    },
    {
      "epoch": 9.149136577708006,
      "grad_norm": 4.241799354553223,
      "learning_rate": 4.42817896389325e-05,
      "loss": 0.6044,
      "step": 582800
    },
    {
      "epoch": 9.150706436420721,
      "grad_norm": 3.3854894638061523,
      "learning_rate": 4.428080847723705e-05,
      "loss": 0.6476,
      "step": 582900
    },
    {
      "epoch": 9.152276295133438,
      "grad_norm": 3.997213363647461,
      "learning_rate": 4.42798273155416e-05,
      "loss": 0.6211,
      "step": 583000
    },
    {
      "epoch": 9.153846153846153,
      "grad_norm": 3.7747035026550293,
      "learning_rate": 4.427884615384615e-05,
      "loss": 0.636,
      "step": 583100
    },
    {
      "epoch": 9.15541601255887,
      "grad_norm": 3.6283624172210693,
      "learning_rate": 4.427786499215071e-05,
      "loss": 0.6568,
      "step": 583200
    },
    {
      "epoch": 9.156985871271585,
      "grad_norm": 3.4661924839019775,
      "learning_rate": 4.427688383045526e-05,
      "loss": 0.633,
      "step": 583300
    },
    {
      "epoch": 9.158555729984302,
      "grad_norm": 3.199023723602295,
      "learning_rate": 4.427590266875981e-05,
      "loss": 0.6008,
      "step": 583400
    },
    {
      "epoch": 9.160125588697017,
      "grad_norm": 4.908023357391357,
      "learning_rate": 4.4274921507064364e-05,
      "loss": 0.6675,
      "step": 583500
    },
    {
      "epoch": 9.161695447409732,
      "grad_norm": 5.589681148529053,
      "learning_rate": 4.427394034536892e-05,
      "loss": 0.6375,
      "step": 583600
    },
    {
      "epoch": 9.16326530612245,
      "grad_norm": 4.3915510177612305,
      "learning_rate": 4.427295918367347e-05,
      "loss": 0.6115,
      "step": 583700
    },
    {
      "epoch": 9.164835164835164,
      "grad_norm": 3.582932949066162,
      "learning_rate": 4.4271978021978024e-05,
      "loss": 0.6189,
      "step": 583800
    },
    {
      "epoch": 9.166405023547881,
      "grad_norm": 4.05797004699707,
      "learning_rate": 4.4270996860282575e-05,
      "loss": 0.6565,
      "step": 583900
    },
    {
      "epoch": 9.167974882260596,
      "grad_norm": 3.935171127319336,
      "learning_rate": 4.427001569858713e-05,
      "loss": 0.6702,
      "step": 584000
    },
    {
      "epoch": 9.169544740973313,
      "grad_norm": 3.746568441390991,
      "learning_rate": 4.4269034536891677e-05,
      "loss": 0.6678,
      "step": 584100
    },
    {
      "epoch": 9.171114599686028,
      "grad_norm": 3.270693778991699,
      "learning_rate": 4.4268053375196234e-05,
      "loss": 0.6549,
      "step": 584200
    },
    {
      "epoch": 9.172684458398743,
      "grad_norm": 2.887232542037964,
      "learning_rate": 4.4267072213500785e-05,
      "loss": 0.6311,
      "step": 584300
    },
    {
      "epoch": 9.17425431711146,
      "grad_norm": 4.598021984100342,
      "learning_rate": 4.426609105180534e-05,
      "loss": 0.6065,
      "step": 584400
    },
    {
      "epoch": 9.175824175824175,
      "grad_norm": 4.405954837799072,
      "learning_rate": 4.426510989010989e-05,
      "loss": 0.6325,
      "step": 584500
    },
    {
      "epoch": 9.177394034536892,
      "grad_norm": 3.8331234455108643,
      "learning_rate": 4.4264128728414445e-05,
      "loss": 0.6196,
      "step": 584600
    },
    {
      "epoch": 9.178963893249607,
      "grad_norm": 3.9580719470977783,
      "learning_rate": 4.4263147566718996e-05,
      "loss": 0.6302,
      "step": 584700
    },
    {
      "epoch": 9.180533751962324,
      "grad_norm": 3.9733028411865234,
      "learning_rate": 4.426216640502355e-05,
      "loss": 0.6412,
      "step": 584800
    },
    {
      "epoch": 9.182103610675039,
      "grad_norm": 3.781954050064087,
      "learning_rate": 4.4261185243328105e-05,
      "loss": 0.6501,
      "step": 584900
    },
    {
      "epoch": 9.183673469387756,
      "grad_norm": 3.7287518978118896,
      "learning_rate": 4.4260204081632656e-05,
      "loss": 0.6424,
      "step": 585000
    },
    {
      "epoch": 9.185243328100471,
      "grad_norm": 3.3836824893951416,
      "learning_rate": 4.425922291993721e-05,
      "loss": 0.6276,
      "step": 585100
    },
    {
      "epoch": 9.186813186813186,
      "grad_norm": 4.3072943687438965,
      "learning_rate": 4.425824175824176e-05,
      "loss": 0.6391,
      "step": 585200
    },
    {
      "epoch": 9.188383045525903,
      "grad_norm": 3.7263095378875732,
      "learning_rate": 4.4257260596546315e-05,
      "loss": 0.6076,
      "step": 585300
    },
    {
      "epoch": 9.189952904238618,
      "grad_norm": 3.7087643146514893,
      "learning_rate": 4.4256279434850866e-05,
      "loss": 0.6343,
      "step": 585400
    },
    {
      "epoch": 9.191522762951335,
      "grad_norm": 4.009994029998779,
      "learning_rate": 4.425529827315542e-05,
      "loss": 0.6824,
      "step": 585500
    },
    {
      "epoch": 9.19309262166405,
      "grad_norm": 3.8595566749572754,
      "learning_rate": 4.425431711145997e-05,
      "loss": 0.6509,
      "step": 585600
    },
    {
      "epoch": 9.194662480376767,
      "grad_norm": 4.034994602203369,
      "learning_rate": 4.4253335949764526e-05,
      "loss": 0.6091,
      "step": 585700
    },
    {
      "epoch": 9.196232339089482,
      "grad_norm": 2.504563093185425,
      "learning_rate": 4.425235478806908e-05,
      "loss": 0.6368,
      "step": 585800
    },
    {
      "epoch": 9.197802197802197,
      "grad_norm": 3.815308094024658,
      "learning_rate": 4.425137362637363e-05,
      "loss": 0.6553,
      "step": 585900
    },
    {
      "epoch": 9.199372056514914,
      "grad_norm": 3.687608003616333,
      "learning_rate": 4.425039246467818e-05,
      "loss": 0.6093,
      "step": 586000
    },
    {
      "epoch": 9.200941915227629,
      "grad_norm": 4.259789943695068,
      "learning_rate": 4.424941130298274e-05,
      "loss": 0.6391,
      "step": 586100
    },
    {
      "epoch": 9.202511773940346,
      "grad_norm": 3.152130365371704,
      "learning_rate": 4.424843014128728e-05,
      "loss": 0.6085,
      "step": 586200
    },
    {
      "epoch": 9.204081632653061,
      "grad_norm": 4.172093391418457,
      "learning_rate": 4.424744897959184e-05,
      "loss": 0.6764,
      "step": 586300
    },
    {
      "epoch": 9.205651491365778,
      "grad_norm": 2.795974016189575,
      "learning_rate": 4.424646781789639e-05,
      "loss": 0.6775,
      "step": 586400
    },
    {
      "epoch": 9.207221350078493,
      "grad_norm": 3.758168935775757,
      "learning_rate": 4.424548665620095e-05,
      "loss": 0.6672,
      "step": 586500
    },
    {
      "epoch": 9.208791208791208,
      "grad_norm": 4.5814528465271,
      "learning_rate": 4.424450549450549e-05,
      "loss": 0.6449,
      "step": 586600
    },
    {
      "epoch": 9.210361067503925,
      "grad_norm": 4.551886081695557,
      "learning_rate": 4.424352433281005e-05,
      "loss": 0.6069,
      "step": 586700
    },
    {
      "epoch": 9.21193092621664,
      "grad_norm": 3.4766194820404053,
      "learning_rate": 4.42425431711146e-05,
      "loss": 0.6402,
      "step": 586800
    },
    {
      "epoch": 9.213500784929357,
      "grad_norm": 4.071006774902344,
      "learning_rate": 4.424156200941915e-05,
      "loss": 0.659,
      "step": 586900
    },
    {
      "epoch": 9.215070643642072,
      "grad_norm": 3.8031697273254395,
      "learning_rate": 4.424058084772371e-05,
      "loss": 0.648,
      "step": 587000
    },
    {
      "epoch": 9.216640502354789,
      "grad_norm": 4.049775123596191,
      "learning_rate": 4.423959968602826e-05,
      "loss": 0.5889,
      "step": 587100
    },
    {
      "epoch": 9.218210361067504,
      "grad_norm": 3.0793137550354004,
      "learning_rate": 4.423861852433281e-05,
      "loss": 0.6296,
      "step": 587200
    },
    {
      "epoch": 9.219780219780219,
      "grad_norm": 4.115381240844727,
      "learning_rate": 4.423763736263736e-05,
      "loss": 0.6748,
      "step": 587300
    },
    {
      "epoch": 9.221350078492936,
      "grad_norm": 4.566145896911621,
      "learning_rate": 4.423665620094192e-05,
      "loss": 0.6546,
      "step": 587400
    },
    {
      "epoch": 9.222919937205651,
      "grad_norm": 3.3827717304229736,
      "learning_rate": 4.423567503924647e-05,
      "loss": 0.6406,
      "step": 587500
    },
    {
      "epoch": 9.224489795918368,
      "grad_norm": 4.33163595199585,
      "learning_rate": 4.423469387755102e-05,
      "loss": 0.6185,
      "step": 587600
    },
    {
      "epoch": 9.226059654631083,
      "grad_norm": 3.9895529747009277,
      "learning_rate": 4.423371271585557e-05,
      "loss": 0.6711,
      "step": 587700
    },
    {
      "epoch": 9.2276295133438,
      "grad_norm": 3.9664556980133057,
      "learning_rate": 4.423273155416013e-05,
      "loss": 0.6548,
      "step": 587800
    },
    {
      "epoch": 9.229199372056515,
      "grad_norm": 3.4540529251098633,
      "learning_rate": 4.423175039246468e-05,
      "loss": 0.6179,
      "step": 587900
    },
    {
      "epoch": 9.23076923076923,
      "grad_norm": 5.192612648010254,
      "learning_rate": 4.423076923076923e-05,
      "loss": 0.6587,
      "step": 588000
    },
    {
      "epoch": 9.232339089481947,
      "grad_norm": 4.527181625366211,
      "learning_rate": 4.4229788069073784e-05,
      "loss": 0.6138,
      "step": 588100
    },
    {
      "epoch": 9.233908948194662,
      "grad_norm": 4.177854061126709,
      "learning_rate": 4.422880690737834e-05,
      "loss": 0.5989,
      "step": 588200
    },
    {
      "epoch": 9.235478806907379,
      "grad_norm": 4.032135486602783,
      "learning_rate": 4.4227825745682886e-05,
      "loss": 0.6182,
      "step": 588300
    },
    {
      "epoch": 9.237048665620094,
      "grad_norm": 3.535625696182251,
      "learning_rate": 4.422684458398744e-05,
      "loss": 0.6583,
      "step": 588400
    },
    {
      "epoch": 9.23861852433281,
      "grad_norm": 3.870335102081299,
      "learning_rate": 4.4225863422291994e-05,
      "loss": 0.6528,
      "step": 588500
    },
    {
      "epoch": 9.240188383045526,
      "grad_norm": 3.6104390621185303,
      "learning_rate": 4.422488226059655e-05,
      "loss": 0.6759,
      "step": 588600
    },
    {
      "epoch": 9.241758241758241,
      "grad_norm": 3.3881852626800537,
      "learning_rate": 4.4223901098901096e-05,
      "loss": 0.6001,
      "step": 588700
    },
    {
      "epoch": 9.243328100470958,
      "grad_norm": 3.1807854175567627,
      "learning_rate": 4.4222919937205654e-05,
      "loss": 0.6037,
      "step": 588800
    },
    {
      "epoch": 9.244897959183673,
      "grad_norm": 4.309719085693359,
      "learning_rate": 4.4221938775510205e-05,
      "loss": 0.6714,
      "step": 588900
    },
    {
      "epoch": 9.24646781789639,
      "grad_norm": 3.770596981048584,
      "learning_rate": 4.4220957613814756e-05,
      "loss": 0.6758,
      "step": 589000
    },
    {
      "epoch": 9.248037676609105,
      "grad_norm": 4.336422443389893,
      "learning_rate": 4.4219976452119314e-05,
      "loss": 0.6656,
      "step": 589100
    },
    {
      "epoch": 9.249607535321822,
      "grad_norm": 4.355809211730957,
      "learning_rate": 4.4218995290423865e-05,
      "loss": 0.6177,
      "step": 589200
    },
    {
      "epoch": 9.251177394034537,
      "grad_norm": 3.6480636596679688,
      "learning_rate": 4.4218014128728416e-05,
      "loss": 0.6712,
      "step": 589300
    },
    {
      "epoch": 9.252747252747252,
      "grad_norm": 4.0236616134643555,
      "learning_rate": 4.421703296703297e-05,
      "loss": 0.6317,
      "step": 589400
    },
    {
      "epoch": 9.254317111459969,
      "grad_norm": 2.7342042922973633,
      "learning_rate": 4.4216051805337524e-05,
      "loss": 0.5933,
      "step": 589500
    },
    {
      "epoch": 9.255886970172684,
      "grad_norm": 3.6087124347686768,
      "learning_rate": 4.4215070643642075e-05,
      "loss": 0.6339,
      "step": 589600
    },
    {
      "epoch": 9.2574568288854,
      "grad_norm": 4.4313249588012695,
      "learning_rate": 4.4214089481946626e-05,
      "loss": 0.6652,
      "step": 589700
    },
    {
      "epoch": 9.259026687598116,
      "grad_norm": 4.117419242858887,
      "learning_rate": 4.421310832025118e-05,
      "loss": 0.6333,
      "step": 589800
    },
    {
      "epoch": 9.260596546310833,
      "grad_norm": 3.2541580200195312,
      "learning_rate": 4.4212127158555735e-05,
      "loss": 0.6229,
      "step": 589900
    },
    {
      "epoch": 9.262166405023548,
      "grad_norm": 3.57515025138855,
      "learning_rate": 4.4211145996860286e-05,
      "loss": 0.6436,
      "step": 590000
    },
    {
      "epoch": 9.263736263736265,
      "grad_norm": 4.1465983390808105,
      "learning_rate": 4.421016483516484e-05,
      "loss": 0.6441,
      "step": 590100
    },
    {
      "epoch": 9.26530612244898,
      "grad_norm": 2.7372965812683105,
      "learning_rate": 4.420918367346939e-05,
      "loss": 0.6377,
      "step": 590200
    },
    {
      "epoch": 9.266875981161695,
      "grad_norm": 3.8687496185302734,
      "learning_rate": 4.4208202511773946e-05,
      "loss": 0.6586,
      "step": 590300
    },
    {
      "epoch": 9.268445839874412,
      "grad_norm": 3.121110439300537,
      "learning_rate": 4.420722135007849e-05,
      "loss": 0.5939,
      "step": 590400
    },
    {
      "epoch": 9.270015698587127,
      "grad_norm": 3.453235387802124,
      "learning_rate": 4.420624018838305e-05,
      "loss": 0.623,
      "step": 590500
    },
    {
      "epoch": 9.271585557299844,
      "grad_norm": 3.8904881477355957,
      "learning_rate": 4.42052590266876e-05,
      "loss": 0.7136,
      "step": 590600
    },
    {
      "epoch": 9.273155416012559,
      "grad_norm": 3.793118953704834,
      "learning_rate": 4.4204277864992157e-05,
      "loss": 0.6609,
      "step": 590700
    },
    {
      "epoch": 9.274725274725276,
      "grad_norm": 3.9639577865600586,
      "learning_rate": 4.42032967032967e-05,
      "loss": 0.6711,
      "step": 590800
    },
    {
      "epoch": 9.27629513343799,
      "grad_norm": 4.213755130767822,
      "learning_rate": 4.420231554160126e-05,
      "loss": 0.641,
      "step": 590900
    },
    {
      "epoch": 9.277864992150706,
      "grad_norm": 4.7479424476623535,
      "learning_rate": 4.420133437990581e-05,
      "loss": 0.6282,
      "step": 591000
    },
    {
      "epoch": 9.279434850863423,
      "grad_norm": 4.590247631072998,
      "learning_rate": 4.420035321821036e-05,
      "loss": 0.6426,
      "step": 591100
    },
    {
      "epoch": 9.281004709576138,
      "grad_norm": 4.038216590881348,
      "learning_rate": 4.419937205651492e-05,
      "loss": 0.6344,
      "step": 591200
    },
    {
      "epoch": 9.282574568288855,
      "grad_norm": 3.521454334259033,
      "learning_rate": 4.419839089481947e-05,
      "loss": 0.6135,
      "step": 591300
    },
    {
      "epoch": 9.28414442700157,
      "grad_norm": 2.988018274307251,
      "learning_rate": 4.419740973312402e-05,
      "loss": 0.638,
      "step": 591400
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 3.609668016433716,
      "learning_rate": 4.419642857142857e-05,
      "loss": 0.6424,
      "step": 591500
    },
    {
      "epoch": 9.287284144427002,
      "grad_norm": 3.1048426628112793,
      "learning_rate": 4.419544740973313e-05,
      "loss": 0.6645,
      "step": 591600
    },
    {
      "epoch": 9.288854003139717,
      "grad_norm": 4.109087944030762,
      "learning_rate": 4.419446624803768e-05,
      "loss": 0.6527,
      "step": 591700
    },
    {
      "epoch": 9.290423861852434,
      "grad_norm": 4.419119358062744,
      "learning_rate": 4.419348508634223e-05,
      "loss": 0.6128,
      "step": 591800
    },
    {
      "epoch": 9.291993720565149,
      "grad_norm": 5.099108695983887,
      "learning_rate": 4.419250392464678e-05,
      "loss": 0.6731,
      "step": 591900
    },
    {
      "epoch": 9.293563579277865,
      "grad_norm": 5.01422119140625,
      "learning_rate": 4.419152276295134e-05,
      "loss": 0.6038,
      "step": 592000
    },
    {
      "epoch": 9.29513343799058,
      "grad_norm": 3.659001111984253,
      "learning_rate": 4.419054160125589e-05,
      "loss": 0.6864,
      "step": 592100
    },
    {
      "epoch": 9.296703296703297,
      "grad_norm": 3.8144779205322266,
      "learning_rate": 4.418956043956044e-05,
      "loss": 0.6623,
      "step": 592200
    },
    {
      "epoch": 9.298273155416013,
      "grad_norm": 2.8905484676361084,
      "learning_rate": 4.418857927786499e-05,
      "loss": 0.6506,
      "step": 592300
    },
    {
      "epoch": 9.299843014128728,
      "grad_norm": 4.109013080596924,
      "learning_rate": 4.418759811616955e-05,
      "loss": 0.6387,
      "step": 592400
    },
    {
      "epoch": 9.301412872841444,
      "grad_norm": 4.849026203155518,
      "learning_rate": 4.4186616954474095e-05,
      "loss": 0.6467,
      "step": 592500
    },
    {
      "epoch": 9.30298273155416,
      "grad_norm": 2.895051956176758,
      "learning_rate": 4.418563579277865e-05,
      "loss": 0.6075,
      "step": 592600
    },
    {
      "epoch": 9.304552590266876,
      "grad_norm": 4.290606498718262,
      "learning_rate": 4.41846546310832e-05,
      "loss": 0.6558,
      "step": 592700
    },
    {
      "epoch": 9.306122448979592,
      "grad_norm": 4.744297981262207,
      "learning_rate": 4.418367346938776e-05,
      "loss": 0.6367,
      "step": 592800
    },
    {
      "epoch": 9.307692307692308,
      "grad_norm": 2.665552854537964,
      "learning_rate": 4.4182692307692305e-05,
      "loss": 0.6124,
      "step": 592900
    },
    {
      "epoch": 9.309262166405023,
      "grad_norm": 4.51582670211792,
      "learning_rate": 4.418171114599686e-05,
      "loss": 0.6897,
      "step": 593000
    },
    {
      "epoch": 9.310832025117739,
      "grad_norm": 4.307895660400391,
      "learning_rate": 4.4180729984301414e-05,
      "loss": 0.6881,
      "step": 593100
    },
    {
      "epoch": 9.312401883830455,
      "grad_norm": 3.791788339614868,
      "learning_rate": 4.4179748822605965e-05,
      "loss": 0.623,
      "step": 593200
    },
    {
      "epoch": 9.31397174254317,
      "grad_norm": 3.4744889736175537,
      "learning_rate": 4.417876766091052e-05,
      "loss": 0.6687,
      "step": 593300
    },
    {
      "epoch": 9.315541601255887,
      "grad_norm": 3.5325849056243896,
      "learning_rate": 4.4177786499215074e-05,
      "loss": 0.6484,
      "step": 593400
    },
    {
      "epoch": 9.317111459968602,
      "grad_norm": 3.257935047149658,
      "learning_rate": 4.4176805337519625e-05,
      "loss": 0.603,
      "step": 593500
    },
    {
      "epoch": 9.31868131868132,
      "grad_norm": 4.378543376922607,
      "learning_rate": 4.4175824175824176e-05,
      "loss": 0.6253,
      "step": 593600
    },
    {
      "epoch": 9.320251177394034,
      "grad_norm": 3.868880271911621,
      "learning_rate": 4.4174843014128733e-05,
      "loss": 0.6136,
      "step": 593700
    },
    {
      "epoch": 9.321821036106751,
      "grad_norm": 4.728415012359619,
      "learning_rate": 4.4173861852433284e-05,
      "loss": 0.6634,
      "step": 593800
    },
    {
      "epoch": 9.323390894819466,
      "grad_norm": 4.377893447875977,
      "learning_rate": 4.4172880690737835e-05,
      "loss": 0.6298,
      "step": 593900
    },
    {
      "epoch": 9.324960753532181,
      "grad_norm": 4.629810810089111,
      "learning_rate": 4.4171899529042386e-05,
      "loss": 0.6597,
      "step": 594000
    },
    {
      "epoch": 9.326530612244898,
      "grad_norm": 4.41800594329834,
      "learning_rate": 4.4170918367346944e-05,
      "loss": 0.6649,
      "step": 594100
    },
    {
      "epoch": 9.328100470957613,
      "grad_norm": 2.476161479949951,
      "learning_rate": 4.4169937205651495e-05,
      "loss": 0.6556,
      "step": 594200
    },
    {
      "epoch": 9.32967032967033,
      "grad_norm": 4.154817581176758,
      "learning_rate": 4.4168956043956046e-05,
      "loss": 0.6498,
      "step": 594300
    },
    {
      "epoch": 9.331240188383045,
      "grad_norm": 3.591801643371582,
      "learning_rate": 4.41679748822606e-05,
      "loss": 0.6306,
      "step": 594400
    },
    {
      "epoch": 9.332810047095762,
      "grad_norm": 4.7201457023620605,
      "learning_rate": 4.4166993720565155e-05,
      "loss": 0.6395,
      "step": 594500
    },
    {
      "epoch": 9.334379905808477,
      "grad_norm": 3.3911726474761963,
      "learning_rate": 4.41660125588697e-05,
      "loss": 0.6634,
      "step": 594600
    },
    {
      "epoch": 9.335949764521192,
      "grad_norm": 4.0590434074401855,
      "learning_rate": 4.416503139717426e-05,
      "loss": 0.6384,
      "step": 594700
    },
    {
      "epoch": 9.33751962323391,
      "grad_norm": 2.976907730102539,
      "learning_rate": 4.416405023547881e-05,
      "loss": 0.6038,
      "step": 594800
    },
    {
      "epoch": 9.339089481946624,
      "grad_norm": 3.809500217437744,
      "learning_rate": 4.4163069073783366e-05,
      "loss": 0.6226,
      "step": 594900
    },
    {
      "epoch": 9.340659340659341,
      "grad_norm": 3.5622153282165527,
      "learning_rate": 4.416208791208791e-05,
      "loss": 0.6427,
      "step": 595000
    },
    {
      "epoch": 9.342229199372056,
      "grad_norm": 4.349527359008789,
      "learning_rate": 4.416110675039247e-05,
      "loss": 0.6353,
      "step": 595100
    },
    {
      "epoch": 9.343799058084773,
      "grad_norm": 3.2673099040985107,
      "learning_rate": 4.416012558869702e-05,
      "loss": 0.6235,
      "step": 595200
    },
    {
      "epoch": 9.345368916797488,
      "grad_norm": 3.7013909816741943,
      "learning_rate": 4.415914442700157e-05,
      "loss": 0.6295,
      "step": 595300
    },
    {
      "epoch": 9.346938775510203,
      "grad_norm": 4.1130051612854,
      "learning_rate": 4.415816326530613e-05,
      "loss": 0.6394,
      "step": 595400
    },
    {
      "epoch": 9.34850863422292,
      "grad_norm": 4.672569274902344,
      "learning_rate": 4.415718210361068e-05,
      "loss": 0.6276,
      "step": 595500
    },
    {
      "epoch": 9.350078492935635,
      "grad_norm": 4.372800827026367,
      "learning_rate": 4.415620094191523e-05,
      "loss": 0.6814,
      "step": 595600
    },
    {
      "epoch": 9.351648351648352,
      "grad_norm": 3.7609407901763916,
      "learning_rate": 4.415521978021978e-05,
      "loss": 0.6591,
      "step": 595700
    },
    {
      "epoch": 9.353218210361067,
      "grad_norm": 3.4446377754211426,
      "learning_rate": 4.415423861852434e-05,
      "loss": 0.659,
      "step": 595800
    },
    {
      "epoch": 9.354788069073784,
      "grad_norm": 4.080102920532227,
      "learning_rate": 4.415325745682889e-05,
      "loss": 0.6365,
      "step": 595900
    },
    {
      "epoch": 9.3563579277865,
      "grad_norm": 3.762996196746826,
      "learning_rate": 4.415227629513344e-05,
      "loss": 0.6459,
      "step": 596000
    },
    {
      "epoch": 9.357927786499214,
      "grad_norm": 4.621415615081787,
      "learning_rate": 4.415129513343799e-05,
      "loss": 0.6027,
      "step": 596100
    },
    {
      "epoch": 9.359497645211931,
      "grad_norm": 3.4900779724121094,
      "learning_rate": 4.415031397174255e-05,
      "loss": 0.6526,
      "step": 596200
    },
    {
      "epoch": 9.361067503924646,
      "grad_norm": 3.661271095275879,
      "learning_rate": 4.41493328100471e-05,
      "loss": 0.6413,
      "step": 596300
    },
    {
      "epoch": 9.362637362637363,
      "grad_norm": 2.6850390434265137,
      "learning_rate": 4.414835164835165e-05,
      "loss": 0.6107,
      "step": 596400
    },
    {
      "epoch": 9.364207221350078,
      "grad_norm": 3.5140950679779053,
      "learning_rate": 4.41473704866562e-05,
      "loss": 0.6371,
      "step": 596500
    },
    {
      "epoch": 9.365777080062795,
      "grad_norm": 3.8707387447357178,
      "learning_rate": 4.414638932496076e-05,
      "loss": 0.6489,
      "step": 596600
    },
    {
      "epoch": 9.36734693877551,
      "grad_norm": 3.643120288848877,
      "learning_rate": 4.4145408163265304e-05,
      "loss": 0.6065,
      "step": 596700
    },
    {
      "epoch": 9.368916797488225,
      "grad_norm": 4.229783058166504,
      "learning_rate": 4.414442700156986e-05,
      "loss": 0.6333,
      "step": 596800
    },
    {
      "epoch": 9.370486656200942,
      "grad_norm": 2.560737371444702,
      "learning_rate": 4.414344583987441e-05,
      "loss": 0.6786,
      "step": 596900
    },
    {
      "epoch": 9.372056514913657,
      "grad_norm": 3.643693685531616,
      "learning_rate": 4.414246467817897e-05,
      "loss": 0.6304,
      "step": 597000
    },
    {
      "epoch": 9.373626373626374,
      "grad_norm": 2.7599871158599854,
      "learning_rate": 4.4141483516483514e-05,
      "loss": 0.6397,
      "step": 597100
    },
    {
      "epoch": 9.37519623233909,
      "grad_norm": 5.0786452293396,
      "learning_rate": 4.414050235478807e-05,
      "loss": 0.6288,
      "step": 597200
    },
    {
      "epoch": 9.376766091051806,
      "grad_norm": 4.0391764640808105,
      "learning_rate": 4.413952119309262e-05,
      "loss": 0.6348,
      "step": 597300
    },
    {
      "epoch": 9.378335949764521,
      "grad_norm": 2.955796003341675,
      "learning_rate": 4.4138540031397174e-05,
      "loss": 0.6627,
      "step": 597400
    },
    {
      "epoch": 9.379905808477236,
      "grad_norm": 4.409688949584961,
      "learning_rate": 4.413755886970173e-05,
      "loss": 0.6557,
      "step": 597500
    },
    {
      "epoch": 9.381475667189953,
      "grad_norm": 3.6578803062438965,
      "learning_rate": 4.413657770800628e-05,
      "loss": 0.6268,
      "step": 597600
    },
    {
      "epoch": 9.383045525902668,
      "grad_norm": 2.2535903453826904,
      "learning_rate": 4.4135596546310834e-05,
      "loss": 0.6032,
      "step": 597700
    },
    {
      "epoch": 9.384615384615385,
      "grad_norm": 4.629758834838867,
      "learning_rate": 4.4134615384615385e-05,
      "loss": 0.6654,
      "step": 597800
    },
    {
      "epoch": 9.3861852433281,
      "grad_norm": 4.069544315338135,
      "learning_rate": 4.413363422291994e-05,
      "loss": 0.597,
      "step": 597900
    },
    {
      "epoch": 9.387755102040817,
      "grad_norm": 4.547878265380859,
      "learning_rate": 4.4132653061224493e-05,
      "loss": 0.629,
      "step": 598000
    },
    {
      "epoch": 9.389324960753532,
      "grad_norm": 3.1322526931762695,
      "learning_rate": 4.4131671899529044e-05,
      "loss": 0.6316,
      "step": 598100
    },
    {
      "epoch": 9.390894819466247,
      "grad_norm": 3.8272156715393066,
      "learning_rate": 4.4130690737833595e-05,
      "loss": 0.596,
      "step": 598200
    },
    {
      "epoch": 9.392464678178964,
      "grad_norm": 3.565558910369873,
      "learning_rate": 4.412970957613815e-05,
      "loss": 0.6374,
      "step": 598300
    },
    {
      "epoch": 9.394034536891679,
      "grad_norm": 3.316209316253662,
      "learning_rate": 4.4128728414442704e-05,
      "loss": 0.6719,
      "step": 598400
    },
    {
      "epoch": 9.395604395604396,
      "grad_norm": 2.660983085632324,
      "learning_rate": 4.4127747252747255e-05,
      "loss": 0.6582,
      "step": 598500
    },
    {
      "epoch": 9.397174254317111,
      "grad_norm": 4.033944606781006,
      "learning_rate": 4.4126766091051806e-05,
      "loss": 0.6367,
      "step": 598600
    },
    {
      "epoch": 9.398744113029828,
      "grad_norm": 3.8580803871154785,
      "learning_rate": 4.4125784929356364e-05,
      "loss": 0.6237,
      "step": 598700
    },
    {
      "epoch": 9.400313971742543,
      "grad_norm": 3.7990174293518066,
      "learning_rate": 4.412480376766091e-05,
      "loss": 0.6189,
      "step": 598800
    },
    {
      "epoch": 9.40188383045526,
      "grad_norm": 4.24904203414917,
      "learning_rate": 4.4123822605965466e-05,
      "loss": 0.6301,
      "step": 598900
    },
    {
      "epoch": 9.403453689167975,
      "grad_norm": 2.98219895362854,
      "learning_rate": 4.412284144427002e-05,
      "loss": 0.6059,
      "step": 599000
    },
    {
      "epoch": 9.40502354788069,
      "grad_norm": 3.6485838890075684,
      "learning_rate": 4.4121860282574575e-05,
      "loss": 0.6448,
      "step": 599100
    },
    {
      "epoch": 9.406593406593407,
      "grad_norm": 2.9223122596740723,
      "learning_rate": 4.412087912087912e-05,
      "loss": 0.6374,
      "step": 599200
    },
    {
      "epoch": 9.408163265306122,
      "grad_norm": 4.154269695281982,
      "learning_rate": 4.4119897959183676e-05,
      "loss": 0.6872,
      "step": 599300
    },
    {
      "epoch": 9.409733124018839,
      "grad_norm": 3.3387017250061035,
      "learning_rate": 4.411891679748823e-05,
      "loss": 0.6485,
      "step": 599400
    },
    {
      "epoch": 9.411302982731554,
      "grad_norm": 4.264563083648682,
      "learning_rate": 4.411793563579278e-05,
      "loss": 0.6345,
      "step": 599500
    },
    {
      "epoch": 9.41287284144427,
      "grad_norm": 6.306660175323486,
      "learning_rate": 4.4116954474097336e-05,
      "loss": 0.6124,
      "step": 599600
    },
    {
      "epoch": 9.414442700156986,
      "grad_norm": 4.512608528137207,
      "learning_rate": 4.411597331240189e-05,
      "loss": 0.629,
      "step": 599700
    },
    {
      "epoch": 9.416012558869701,
      "grad_norm": 3.7589120864868164,
      "learning_rate": 4.411499215070644e-05,
      "loss": 0.593,
      "step": 599800
    },
    {
      "epoch": 9.417582417582418,
      "grad_norm": 3.6926684379577637,
      "learning_rate": 4.411401098901099e-05,
      "loss": 0.6489,
      "step": 599900
    },
    {
      "epoch": 9.419152276295133,
      "grad_norm": 3.883150100708008,
      "learning_rate": 4.411302982731555e-05,
      "loss": 0.6542,
      "step": 600000
    },
    {
      "epoch": 9.42072213500785,
      "grad_norm": 4.611459255218506,
      "learning_rate": 4.41120486656201e-05,
      "loss": 0.5995,
      "step": 600100
    },
    {
      "epoch": 9.422291993720565,
      "grad_norm": 4.067291736602783,
      "learning_rate": 4.411106750392465e-05,
      "loss": 0.6202,
      "step": 600200
    },
    {
      "epoch": 9.423861852433282,
      "grad_norm": 4.2933526039123535,
      "learning_rate": 4.41100863422292e-05,
      "loss": 0.6459,
      "step": 600300
    },
    {
      "epoch": 9.425431711145997,
      "grad_norm": 3.2754666805267334,
      "learning_rate": 4.410910518053376e-05,
      "loss": 0.6268,
      "step": 600400
    },
    {
      "epoch": 9.427001569858712,
      "grad_norm": 4.377653121948242,
      "learning_rate": 4.410812401883831e-05,
      "loss": 0.6354,
      "step": 600500
    },
    {
      "epoch": 9.428571428571429,
      "grad_norm": 4.487782955169678,
      "learning_rate": 4.410714285714286e-05,
      "loss": 0.6371,
      "step": 600600
    },
    {
      "epoch": 9.430141287284144,
      "grad_norm": 3.976480484008789,
      "learning_rate": 4.410616169544741e-05,
      "loss": 0.6369,
      "step": 600700
    },
    {
      "epoch": 9.43171114599686,
      "grad_norm": 4.3702239990234375,
      "learning_rate": 4.410518053375197e-05,
      "loss": 0.6052,
      "step": 600800
    },
    {
      "epoch": 9.433281004709576,
      "grad_norm": 2.977355480194092,
      "learning_rate": 4.410419937205651e-05,
      "loss": 0.6214,
      "step": 600900
    },
    {
      "epoch": 9.434850863422293,
      "grad_norm": 2.788788318634033,
      "learning_rate": 4.410321821036107e-05,
      "loss": 0.6047,
      "step": 601000
    },
    {
      "epoch": 9.436420722135008,
      "grad_norm": 6.148041725158691,
      "learning_rate": 4.410223704866562e-05,
      "loss": 0.6268,
      "step": 601100
    },
    {
      "epoch": 9.437990580847723,
      "grad_norm": 4.460382461547852,
      "learning_rate": 4.410125588697018e-05,
      "loss": 0.6137,
      "step": 601200
    },
    {
      "epoch": 9.43956043956044,
      "grad_norm": 4.005179405212402,
      "learning_rate": 4.410027472527472e-05,
      "loss": 0.64,
      "step": 601300
    },
    {
      "epoch": 9.441130298273155,
      "grad_norm": 4.5853681564331055,
      "learning_rate": 4.409929356357928e-05,
      "loss": 0.6674,
      "step": 601400
    },
    {
      "epoch": 9.442700156985872,
      "grad_norm": 3.440518617630005,
      "learning_rate": 4.409831240188383e-05,
      "loss": 0.6281,
      "step": 601500
    },
    {
      "epoch": 9.444270015698587,
      "grad_norm": 3.1912131309509277,
      "learning_rate": 4.409733124018838e-05,
      "loss": 0.628,
      "step": 601600
    },
    {
      "epoch": 9.445839874411304,
      "grad_norm": 3.334181547164917,
      "learning_rate": 4.409635007849294e-05,
      "loss": 0.6166,
      "step": 601700
    },
    {
      "epoch": 9.447409733124019,
      "grad_norm": 3.983874797821045,
      "learning_rate": 4.409536891679749e-05,
      "loss": 0.6071,
      "step": 601800
    },
    {
      "epoch": 9.448979591836734,
      "grad_norm": 3.475761651992798,
      "learning_rate": 4.409438775510204e-05,
      "loss": 0.6525,
      "step": 601900
    },
    {
      "epoch": 9.45054945054945,
      "grad_norm": 4.376272678375244,
      "learning_rate": 4.4093406593406594e-05,
      "loss": 0.6477,
      "step": 602000
    },
    {
      "epoch": 9.452119309262166,
      "grad_norm": 2.7588448524475098,
      "learning_rate": 4.409242543171115e-05,
      "loss": 0.6138,
      "step": 602100
    },
    {
      "epoch": 9.453689167974883,
      "grad_norm": 3.9985218048095703,
      "learning_rate": 4.40914442700157e-05,
      "loss": 0.6953,
      "step": 602200
    },
    {
      "epoch": 9.455259026687598,
      "grad_norm": 5.083483695983887,
      "learning_rate": 4.409046310832025e-05,
      "loss": 0.6759,
      "step": 602300
    },
    {
      "epoch": 9.456828885400315,
      "grad_norm": 4.320095539093018,
      "learning_rate": 4.4089481946624804e-05,
      "loss": 0.6298,
      "step": 602400
    },
    {
      "epoch": 9.45839874411303,
      "grad_norm": 2.9164180755615234,
      "learning_rate": 4.408850078492936e-05,
      "loss": 0.6303,
      "step": 602500
    },
    {
      "epoch": 9.459968602825747,
      "grad_norm": 3.143343687057495,
      "learning_rate": 4.408751962323391e-05,
      "loss": 0.632,
      "step": 602600
    },
    {
      "epoch": 9.461538461538462,
      "grad_norm": 3.925668716430664,
      "learning_rate": 4.4086538461538464e-05,
      "loss": 0.5935,
      "step": 602700
    },
    {
      "epoch": 9.463108320251177,
      "grad_norm": 4.510740756988525,
      "learning_rate": 4.4085557299843015e-05,
      "loss": 0.6228,
      "step": 602800
    },
    {
      "epoch": 9.464678178963894,
      "grad_norm": 4.46993350982666,
      "learning_rate": 4.408457613814757e-05,
      "loss": 0.6533,
      "step": 602900
    },
    {
      "epoch": 9.466248037676609,
      "grad_norm": 3.6157405376434326,
      "learning_rate": 4.408359497645212e-05,
      "loss": 0.615,
      "step": 603000
    },
    {
      "epoch": 9.467817896389326,
      "grad_norm": 3.0190327167510986,
      "learning_rate": 4.4082613814756675e-05,
      "loss": 0.5924,
      "step": 603100
    },
    {
      "epoch": 9.46938775510204,
      "grad_norm": 3.1236438751220703,
      "learning_rate": 4.4081632653061226e-05,
      "loss": 0.6106,
      "step": 603200
    },
    {
      "epoch": 9.470957613814758,
      "grad_norm": 3.980372428894043,
      "learning_rate": 4.408065149136578e-05,
      "loss": 0.6231,
      "step": 603300
    },
    {
      "epoch": 9.472527472527473,
      "grad_norm": 4.925960063934326,
      "learning_rate": 4.407967032967033e-05,
      "loss": 0.6667,
      "step": 603400
    },
    {
      "epoch": 9.474097331240188,
      "grad_norm": 3.6862778663635254,
      "learning_rate": 4.4078689167974885e-05,
      "loss": 0.6378,
      "step": 603500
    },
    {
      "epoch": 9.475667189952905,
      "grad_norm": 4.27631950378418,
      "learning_rate": 4.4077708006279436e-05,
      "loss": 0.6822,
      "step": 603600
    },
    {
      "epoch": 9.47723704866562,
      "grad_norm": 4.539275646209717,
      "learning_rate": 4.407672684458399e-05,
      "loss": 0.6563,
      "step": 603700
    },
    {
      "epoch": 9.478806907378337,
      "grad_norm": 4.516127586364746,
      "learning_rate": 4.407574568288854e-05,
      "loss": 0.5924,
      "step": 603800
    },
    {
      "epoch": 9.480376766091052,
      "grad_norm": 3.5561864376068115,
      "learning_rate": 4.4074764521193096e-05,
      "loss": 0.6177,
      "step": 603900
    },
    {
      "epoch": 9.481946624803768,
      "grad_norm": 2.8851916790008545,
      "learning_rate": 4.407378335949765e-05,
      "loss": 0.6308,
      "step": 604000
    },
    {
      "epoch": 9.483516483516484,
      "grad_norm": 4.185103416442871,
      "learning_rate": 4.40728021978022e-05,
      "loss": 0.6581,
      "step": 604100
    },
    {
      "epoch": 9.485086342229199,
      "grad_norm": 4.273683547973633,
      "learning_rate": 4.4071821036106756e-05,
      "loss": 0.6433,
      "step": 604200
    },
    {
      "epoch": 9.486656200941916,
      "grad_norm": 3.4504380226135254,
      "learning_rate": 4.407083987441131e-05,
      "loss": 0.625,
      "step": 604300
    },
    {
      "epoch": 9.48822605965463,
      "grad_norm": 4.039930820465088,
      "learning_rate": 4.406985871271586e-05,
      "loss": 0.6049,
      "step": 604400
    },
    {
      "epoch": 9.489795918367347,
      "grad_norm": 4.281076908111572,
      "learning_rate": 4.406887755102041e-05,
      "loss": 0.6428,
      "step": 604500
    },
    {
      "epoch": 9.491365777080063,
      "grad_norm": 3.3098504543304443,
      "learning_rate": 4.4067896389324967e-05,
      "loss": 0.6074,
      "step": 604600
    },
    {
      "epoch": 9.49293563579278,
      "grad_norm": 4.617206573486328,
      "learning_rate": 4.406691522762951e-05,
      "loss": 0.6232,
      "step": 604700
    },
    {
      "epoch": 9.494505494505495,
      "grad_norm": 2.5603976249694824,
      "learning_rate": 4.406593406593407e-05,
      "loss": 0.6811,
      "step": 604800
    },
    {
      "epoch": 9.49607535321821,
      "grad_norm": 4.336328029632568,
      "learning_rate": 4.406495290423862e-05,
      "loss": 0.6622,
      "step": 604900
    },
    {
      "epoch": 9.497645211930926,
      "grad_norm": 3.7622785568237305,
      "learning_rate": 4.406397174254318e-05,
      "loss": 0.6496,
      "step": 605000
    },
    {
      "epoch": 9.499215070643642,
      "grad_norm": 3.7227532863616943,
      "learning_rate": 4.406299058084772e-05,
      "loss": 0.677,
      "step": 605100
    },
    {
      "epoch": 9.500784929356358,
      "grad_norm": 4.015052795410156,
      "learning_rate": 4.406200941915228e-05,
      "loss": 0.6017,
      "step": 605200
    },
    {
      "epoch": 9.502354788069074,
      "grad_norm": 3.7617645263671875,
      "learning_rate": 4.406102825745683e-05,
      "loss": 0.6638,
      "step": 605300
    },
    {
      "epoch": 9.50392464678179,
      "grad_norm": 3.1516928672790527,
      "learning_rate": 4.406004709576138e-05,
      "loss": 0.6068,
      "step": 605400
    },
    {
      "epoch": 9.505494505494505,
      "grad_norm": 4.145055294036865,
      "learning_rate": 4.405906593406593e-05,
      "loss": 0.624,
      "step": 605500
    },
    {
      "epoch": 9.50706436420722,
      "grad_norm": 3.559891700744629,
      "learning_rate": 4.405808477237049e-05,
      "loss": 0.6159,
      "step": 605600
    },
    {
      "epoch": 9.508634222919937,
      "grad_norm": 3.900806188583374,
      "learning_rate": 4.405710361067504e-05,
      "loss": 0.6477,
      "step": 605700
    },
    {
      "epoch": 9.510204081632653,
      "grad_norm": 4.437175273895264,
      "learning_rate": 4.405612244897959e-05,
      "loss": 0.6359,
      "step": 605800
    },
    {
      "epoch": 9.51177394034537,
      "grad_norm": 3.163242816925049,
      "learning_rate": 4.405514128728414e-05,
      "loss": 0.6411,
      "step": 605900
    },
    {
      "epoch": 9.513343799058084,
      "grad_norm": 4.359118938446045,
      "learning_rate": 4.40541601255887e-05,
      "loss": 0.6432,
      "step": 606000
    },
    {
      "epoch": 9.514913657770801,
      "grad_norm": 3.986407995223999,
      "learning_rate": 4.405317896389325e-05,
      "loss": 0.6208,
      "step": 606100
    },
    {
      "epoch": 9.516483516483516,
      "grad_norm": 2.1552093029022217,
      "learning_rate": 4.40521978021978e-05,
      "loss": 0.6403,
      "step": 606200
    },
    {
      "epoch": 9.518053375196232,
      "grad_norm": 3.5068228244781494,
      "learning_rate": 4.405121664050236e-05,
      "loss": 0.6663,
      "step": 606300
    },
    {
      "epoch": 9.519623233908948,
      "grad_norm": 4.229913711547852,
      "learning_rate": 4.405023547880691e-05,
      "loss": 0.6422,
      "step": 606400
    },
    {
      "epoch": 9.521193092621663,
      "grad_norm": 3.424772024154663,
      "learning_rate": 4.404925431711146e-05,
      "loss": 0.6489,
      "step": 606500
    },
    {
      "epoch": 9.52276295133438,
      "grad_norm": 4.119814395904541,
      "learning_rate": 4.404827315541601e-05,
      "loss": 0.654,
      "step": 606600
    },
    {
      "epoch": 9.524332810047095,
      "grad_norm": 4.064961910247803,
      "learning_rate": 4.404729199372057e-05,
      "loss": 0.6575,
      "step": 606700
    },
    {
      "epoch": 9.525902668759812,
      "grad_norm": 4.61184024810791,
      "learning_rate": 4.4046310832025115e-05,
      "loss": 0.6596,
      "step": 606800
    },
    {
      "epoch": 9.527472527472527,
      "grad_norm": 4.4586944580078125,
      "learning_rate": 4.404532967032967e-05,
      "loss": 0.6555,
      "step": 606900
    },
    {
      "epoch": 9.529042386185242,
      "grad_norm": 3.23734450340271,
      "learning_rate": 4.4044348508634224e-05,
      "loss": 0.6415,
      "step": 607000
    },
    {
      "epoch": 9.53061224489796,
      "grad_norm": 3.6184017658233643,
      "learning_rate": 4.404336734693878e-05,
      "loss": 0.6381,
      "step": 607100
    },
    {
      "epoch": 9.532182103610674,
      "grad_norm": 3.8092098236083984,
      "learning_rate": 4.4042386185243326e-05,
      "loss": 0.6267,
      "step": 607200
    },
    {
      "epoch": 9.533751962323391,
      "grad_norm": 3.806234359741211,
      "learning_rate": 4.4041405023547884e-05,
      "loss": 0.6408,
      "step": 607300
    },
    {
      "epoch": 9.535321821036106,
      "grad_norm": 4.036462306976318,
      "learning_rate": 4.4040423861852435e-05,
      "loss": 0.6318,
      "step": 607400
    },
    {
      "epoch": 9.536891679748823,
      "grad_norm": 3.555108070373535,
      "learning_rate": 4.4039442700156986e-05,
      "loss": 0.6914,
      "step": 607500
    },
    {
      "epoch": 9.538461538461538,
      "grad_norm": 2.733250141143799,
      "learning_rate": 4.403846153846154e-05,
      "loss": 0.6317,
      "step": 607600
    },
    {
      "epoch": 9.540031397174253,
      "grad_norm": 2.776287317276001,
      "learning_rate": 4.4037480376766094e-05,
      "loss": 0.6584,
      "step": 607700
    },
    {
      "epoch": 9.54160125588697,
      "grad_norm": 4.014490127563477,
      "learning_rate": 4.4036499215070645e-05,
      "loss": 0.6304,
      "step": 607800
    },
    {
      "epoch": 9.543171114599685,
      "grad_norm": 3.9011378288269043,
      "learning_rate": 4.4035518053375196e-05,
      "loss": 0.6566,
      "step": 607900
    },
    {
      "epoch": 9.544740973312402,
      "grad_norm": 3.6160123348236084,
      "learning_rate": 4.403453689167975e-05,
      "loss": 0.6172,
      "step": 608000
    },
    {
      "epoch": 9.546310832025117,
      "grad_norm": 4.373866558074951,
      "learning_rate": 4.4033555729984305e-05,
      "loss": 0.5954,
      "step": 608100
    },
    {
      "epoch": 9.547880690737834,
      "grad_norm": 4.547649383544922,
      "learning_rate": 4.4032574568288856e-05,
      "loss": 0.6435,
      "step": 608200
    },
    {
      "epoch": 9.54945054945055,
      "grad_norm": 3.233738660812378,
      "learning_rate": 4.403159340659341e-05,
      "loss": 0.6395,
      "step": 608300
    },
    {
      "epoch": 9.551020408163264,
      "grad_norm": 3.6539807319641113,
      "learning_rate": 4.4030612244897965e-05,
      "loss": 0.6304,
      "step": 608400
    },
    {
      "epoch": 9.552590266875981,
      "grad_norm": 3.101296901702881,
      "learning_rate": 4.4029631083202516e-05,
      "loss": 0.6391,
      "step": 608500
    },
    {
      "epoch": 9.554160125588696,
      "grad_norm": 5.5684638023376465,
      "learning_rate": 4.402864992150707e-05,
      "loss": 0.6454,
      "step": 608600
    },
    {
      "epoch": 9.555729984301413,
      "grad_norm": 4.175632953643799,
      "learning_rate": 4.402766875981162e-05,
      "loss": 0.6519,
      "step": 608700
    },
    {
      "epoch": 9.557299843014128,
      "grad_norm": 4.549045562744141,
      "learning_rate": 4.4026687598116176e-05,
      "loss": 0.6501,
      "step": 608800
    },
    {
      "epoch": 9.558869701726845,
      "grad_norm": 4.0246262550354,
      "learning_rate": 4.402570643642072e-05,
      "loss": 0.6859,
      "step": 608900
    },
    {
      "epoch": 9.56043956043956,
      "grad_norm": 4.122782230377197,
      "learning_rate": 4.402472527472528e-05,
      "loss": 0.6469,
      "step": 609000
    },
    {
      "epoch": 9.562009419152277,
      "grad_norm": 4.346149444580078,
      "learning_rate": 4.402374411302983e-05,
      "loss": 0.6513,
      "step": 609100
    },
    {
      "epoch": 9.563579277864992,
      "grad_norm": 3.5102484226226807,
      "learning_rate": 4.4022762951334386e-05,
      "loss": 0.6317,
      "step": 609200
    },
    {
      "epoch": 9.565149136577707,
      "grad_norm": 4.985483646392822,
      "learning_rate": 4.402178178963893e-05,
      "loss": 0.6472,
      "step": 609300
    },
    {
      "epoch": 9.566718995290424,
      "grad_norm": 3.899754285812378,
      "learning_rate": 4.402080062794349e-05,
      "loss": 0.6509,
      "step": 609400
    },
    {
      "epoch": 9.56828885400314,
      "grad_norm": 3.9271116256713867,
      "learning_rate": 4.401981946624804e-05,
      "loss": 0.6289,
      "step": 609500
    },
    {
      "epoch": 9.569858712715856,
      "grad_norm": 4.760791778564453,
      "learning_rate": 4.401883830455259e-05,
      "loss": 0.6537,
      "step": 609600
    },
    {
      "epoch": 9.571428571428571,
      "grad_norm": 4.059207916259766,
      "learning_rate": 4.401785714285714e-05,
      "loss": 0.6137,
      "step": 609700
    },
    {
      "epoch": 9.572998430141288,
      "grad_norm": 3.590353488922119,
      "learning_rate": 4.40168759811617e-05,
      "loss": 0.6798,
      "step": 609800
    },
    {
      "epoch": 9.574568288854003,
      "grad_norm": 4.08937406539917,
      "learning_rate": 4.401589481946625e-05,
      "loss": 0.6303,
      "step": 609900
    },
    {
      "epoch": 9.576138147566718,
      "grad_norm": 3.242337703704834,
      "learning_rate": 4.40149136577708e-05,
      "loss": 0.641,
      "step": 610000
    },
    {
      "epoch": 9.577708006279435,
      "grad_norm": 3.7473349571228027,
      "learning_rate": 4.401393249607535e-05,
      "loss": 0.6288,
      "step": 610100
    },
    {
      "epoch": 9.57927786499215,
      "grad_norm": 5.020595073699951,
      "learning_rate": 4.401295133437991e-05,
      "loss": 0.6842,
      "step": 610200
    },
    {
      "epoch": 9.580847723704867,
      "grad_norm": 2.5797531604766846,
      "learning_rate": 4.401197017268446e-05,
      "loss": 0.6448,
      "step": 610300
    },
    {
      "epoch": 9.582417582417582,
      "grad_norm": 4.004888534545898,
      "learning_rate": 4.401098901098901e-05,
      "loss": 0.6576,
      "step": 610400
    },
    {
      "epoch": 9.583987441130299,
      "grad_norm": 4.193017959594727,
      "learning_rate": 4.401000784929357e-05,
      "loss": 0.6533,
      "step": 610500
    },
    {
      "epoch": 9.585557299843014,
      "grad_norm": 3.2422432899475098,
      "learning_rate": 4.400902668759812e-05,
      "loss": 0.6349,
      "step": 610600
    },
    {
      "epoch": 9.58712715855573,
      "grad_norm": 4.763458251953125,
      "learning_rate": 4.400804552590267e-05,
      "loss": 0.6322,
      "step": 610700
    },
    {
      "epoch": 9.588697017268446,
      "grad_norm": 4.687840938568115,
      "learning_rate": 4.400706436420722e-05,
      "loss": 0.5999,
      "step": 610800
    },
    {
      "epoch": 9.590266875981161,
      "grad_norm": 5.558664798736572,
      "learning_rate": 4.400608320251178e-05,
      "loss": 0.6717,
      "step": 610900
    },
    {
      "epoch": 9.591836734693878,
      "grad_norm": 3.3884353637695312,
      "learning_rate": 4.4005102040816324e-05,
      "loss": 0.6861,
      "step": 611000
    },
    {
      "epoch": 9.593406593406593,
      "grad_norm": 3.3731119632720947,
      "learning_rate": 4.400412087912088e-05,
      "loss": 0.649,
      "step": 611100
    },
    {
      "epoch": 9.59497645211931,
      "grad_norm": 3.1333189010620117,
      "learning_rate": 4.400313971742543e-05,
      "loss": 0.6405,
      "step": 611200
    },
    {
      "epoch": 9.596546310832025,
      "grad_norm": 2.702329635620117,
      "learning_rate": 4.400215855572999e-05,
      "loss": 0.6323,
      "step": 611300
    },
    {
      "epoch": 9.598116169544742,
      "grad_norm": 3.5817110538482666,
      "learning_rate": 4.4001177394034535e-05,
      "loss": 0.631,
      "step": 611400
    },
    {
      "epoch": 9.599686028257457,
      "grad_norm": 4.505190372467041,
      "learning_rate": 4.400019623233909e-05,
      "loss": 0.6506,
      "step": 611500
    },
    {
      "epoch": 9.601255886970172,
      "grad_norm": 3.059271812438965,
      "learning_rate": 4.3999215070643644e-05,
      "loss": 0.6737,
      "step": 611600
    },
    {
      "epoch": 9.602825745682889,
      "grad_norm": 3.66367244720459,
      "learning_rate": 4.3998233908948195e-05,
      "loss": 0.663,
      "step": 611700
    },
    {
      "epoch": 9.604395604395604,
      "grad_norm": 3.338902711868286,
      "learning_rate": 4.3997252747252746e-05,
      "loss": 0.6252,
      "step": 611800
    },
    {
      "epoch": 9.605965463108321,
      "grad_norm": 3.1131629943847656,
      "learning_rate": 4.3996271585557303e-05,
      "loss": 0.666,
      "step": 611900
    },
    {
      "epoch": 9.607535321821036,
      "grad_norm": 3.636345863342285,
      "learning_rate": 4.3995290423861854e-05,
      "loss": 0.6478,
      "step": 612000
    },
    {
      "epoch": 9.609105180533753,
      "grad_norm": 3.75646710395813,
      "learning_rate": 4.3994309262166405e-05,
      "loss": 0.6396,
      "step": 612100
    },
    {
      "epoch": 9.610675039246468,
      "grad_norm": 3.4939651489257812,
      "learning_rate": 4.3993328100470956e-05,
      "loss": 0.6171,
      "step": 612200
    },
    {
      "epoch": 9.612244897959183,
      "grad_norm": 3.7487170696258545,
      "learning_rate": 4.3992346938775514e-05,
      "loss": 0.6143,
      "step": 612300
    },
    {
      "epoch": 9.6138147566719,
      "grad_norm": 4.074834823608398,
      "learning_rate": 4.3991365777080065e-05,
      "loss": 0.5932,
      "step": 612400
    },
    {
      "epoch": 9.615384615384615,
      "grad_norm": 4.134964942932129,
      "learning_rate": 4.3990384615384616e-05,
      "loss": 0.6442,
      "step": 612500
    },
    {
      "epoch": 9.616954474097332,
      "grad_norm": 2.3546273708343506,
      "learning_rate": 4.3989403453689174e-05,
      "loss": 0.6292,
      "step": 612600
    },
    {
      "epoch": 9.618524332810047,
      "grad_norm": 4.487773418426514,
      "learning_rate": 4.3988422291993725e-05,
      "loss": 0.6031,
      "step": 612700
    },
    {
      "epoch": 9.620094191522764,
      "grad_norm": 3.3363962173461914,
      "learning_rate": 4.3987441130298276e-05,
      "loss": 0.6279,
      "step": 612800
    },
    {
      "epoch": 9.621664050235479,
      "grad_norm": 3.8778278827667236,
      "learning_rate": 4.398645996860283e-05,
      "loss": 0.6595,
      "step": 612900
    },
    {
      "epoch": 9.623233908948194,
      "grad_norm": 3.3716108798980713,
      "learning_rate": 4.3985478806907385e-05,
      "loss": 0.6182,
      "step": 613000
    },
    {
      "epoch": 9.62480376766091,
      "grad_norm": 3.9826719760894775,
      "learning_rate": 4.398449764521193e-05,
      "loss": 0.6624,
      "step": 613100
    },
    {
      "epoch": 9.626373626373626,
      "grad_norm": 4.022034168243408,
      "learning_rate": 4.3983516483516487e-05,
      "loss": 0.6259,
      "step": 613200
    },
    {
      "epoch": 9.627943485086343,
      "grad_norm": 3.502257823944092,
      "learning_rate": 4.398253532182104e-05,
      "loss": 0.6641,
      "step": 613300
    },
    {
      "epoch": 9.629513343799058,
      "grad_norm": 3.8050968647003174,
      "learning_rate": 4.3981554160125595e-05,
      "loss": 0.6577,
      "step": 613400
    },
    {
      "epoch": 9.631083202511775,
      "grad_norm": 5.79041051864624,
      "learning_rate": 4.398057299843014e-05,
      "loss": 0.6512,
      "step": 613500
    },
    {
      "epoch": 9.63265306122449,
      "grad_norm": 3.794861078262329,
      "learning_rate": 4.39795918367347e-05,
      "loss": 0.6607,
      "step": 613600
    },
    {
      "epoch": 9.634222919937205,
      "grad_norm": 4.700565338134766,
      "learning_rate": 4.397861067503925e-05,
      "loss": 0.6529,
      "step": 613700
    },
    {
      "epoch": 9.635792778649922,
      "grad_norm": 3.906846523284912,
      "learning_rate": 4.39776295133438e-05,
      "loss": 0.649,
      "step": 613800
    },
    {
      "epoch": 9.637362637362637,
      "grad_norm": 4.638856410980225,
      "learning_rate": 4.397664835164835e-05,
      "loss": 0.6914,
      "step": 613900
    },
    {
      "epoch": 9.638932496075354,
      "grad_norm": 3.4893102645874023,
      "learning_rate": 4.397566718995291e-05,
      "loss": 0.6897,
      "step": 614000
    },
    {
      "epoch": 9.640502354788069,
      "grad_norm": 3.677182674407959,
      "learning_rate": 4.397468602825746e-05,
      "loss": 0.6516,
      "step": 614100
    },
    {
      "epoch": 9.642072213500786,
      "grad_norm": 3.8441691398620605,
      "learning_rate": 4.397370486656201e-05,
      "loss": 0.5823,
      "step": 614200
    },
    {
      "epoch": 9.6436420722135,
      "grad_norm": 4.512580394744873,
      "learning_rate": 4.397272370486656e-05,
      "loss": 0.615,
      "step": 614300
    },
    {
      "epoch": 9.645211930926216,
      "grad_norm": 2.8070290088653564,
      "learning_rate": 4.397174254317112e-05,
      "loss": 0.6415,
      "step": 614400
    },
    {
      "epoch": 9.646781789638933,
      "grad_norm": 4.498466968536377,
      "learning_rate": 4.397076138147567e-05,
      "loss": 0.6322,
      "step": 614500
    },
    {
      "epoch": 9.648351648351648,
      "grad_norm": 2.6728878021240234,
      "learning_rate": 4.396978021978022e-05,
      "loss": 0.6345,
      "step": 614600
    },
    {
      "epoch": 9.649921507064365,
      "grad_norm": 4.282426834106445,
      "learning_rate": 4.396879905808478e-05,
      "loss": 0.6062,
      "step": 614700
    },
    {
      "epoch": 9.65149136577708,
      "grad_norm": 3.2853353023529053,
      "learning_rate": 4.396781789638933e-05,
      "loss": 0.6446,
      "step": 614800
    },
    {
      "epoch": 9.653061224489797,
      "grad_norm": 4.553531646728516,
      "learning_rate": 4.396683673469388e-05,
      "loss": 0.6492,
      "step": 614900
    },
    {
      "epoch": 9.654631083202512,
      "grad_norm": 3.478520393371582,
      "learning_rate": 4.396585557299843e-05,
      "loss": 0.6251,
      "step": 615000
    },
    {
      "epoch": 9.656200941915227,
      "grad_norm": 3.375216245651245,
      "learning_rate": 4.396487441130299e-05,
      "loss": 0.6093,
      "step": 615100
    },
    {
      "epoch": 9.657770800627944,
      "grad_norm": 4.775806427001953,
      "learning_rate": 4.396389324960753e-05,
      "loss": 0.6423,
      "step": 615200
    },
    {
      "epoch": 9.659340659340659,
      "grad_norm": 3.739304542541504,
      "learning_rate": 4.396291208791209e-05,
      "loss": 0.6014,
      "step": 615300
    },
    {
      "epoch": 9.660910518053376,
      "grad_norm": 4.327025413513184,
      "learning_rate": 4.396193092621664e-05,
      "loss": 0.6297,
      "step": 615400
    },
    {
      "epoch": 9.66248037676609,
      "grad_norm": 4.673077583312988,
      "learning_rate": 4.39609497645212e-05,
      "loss": 0.6036,
      "step": 615500
    },
    {
      "epoch": 9.664050235478808,
      "grad_norm": 4.152600288391113,
      "learning_rate": 4.3959968602825744e-05,
      "loss": 0.6161,
      "step": 615600
    },
    {
      "epoch": 9.665620094191523,
      "grad_norm": 3.62921142578125,
      "learning_rate": 4.39589874411303e-05,
      "loss": 0.6323,
      "step": 615700
    },
    {
      "epoch": 9.667189952904238,
      "grad_norm": 4.054017066955566,
      "learning_rate": 4.395800627943485e-05,
      "loss": 0.6738,
      "step": 615800
    },
    {
      "epoch": 9.668759811616955,
      "grad_norm": 4.185325622558594,
      "learning_rate": 4.3957025117739404e-05,
      "loss": 0.6352,
      "step": 615900
    },
    {
      "epoch": 9.67032967032967,
      "grad_norm": 2.86398983001709,
      "learning_rate": 4.3956043956043955e-05,
      "loss": 0.6496,
      "step": 616000
    },
    {
      "epoch": 9.671899529042387,
      "grad_norm": 4.0204010009765625,
      "learning_rate": 4.395506279434851e-05,
      "loss": 0.6158,
      "step": 616100
    },
    {
      "epoch": 9.673469387755102,
      "grad_norm": 4.564062595367432,
      "learning_rate": 4.3954081632653063e-05,
      "loss": 0.6679,
      "step": 616200
    },
    {
      "epoch": 9.675039246467819,
      "grad_norm": 4.5321736335754395,
      "learning_rate": 4.3953100470957614e-05,
      "loss": 0.6241,
      "step": 616300
    },
    {
      "epoch": 9.676609105180534,
      "grad_norm": 3.2435216903686523,
      "learning_rate": 4.3952119309262165e-05,
      "loss": 0.6574,
      "step": 616400
    },
    {
      "epoch": 9.678178963893249,
      "grad_norm": 4.783077716827393,
      "learning_rate": 4.395113814756672e-05,
      "loss": 0.6281,
      "step": 616500
    },
    {
      "epoch": 9.679748822605966,
      "grad_norm": 3.4082083702087402,
      "learning_rate": 4.3950156985871274e-05,
      "loss": 0.596,
      "step": 616600
    },
    {
      "epoch": 9.68131868131868,
      "grad_norm": 2.9373931884765625,
      "learning_rate": 4.3949175824175825e-05,
      "loss": 0.6279,
      "step": 616700
    },
    {
      "epoch": 9.682888540031398,
      "grad_norm": 3.9443705081939697,
      "learning_rate": 4.394819466248038e-05,
      "loss": 0.6463,
      "step": 616800
    },
    {
      "epoch": 9.684458398744113,
      "grad_norm": 3.263409376144409,
      "learning_rate": 4.3947213500784934e-05,
      "loss": 0.6648,
      "step": 616900
    },
    {
      "epoch": 9.68602825745683,
      "grad_norm": 3.5146970748901367,
      "learning_rate": 4.3946232339089485e-05,
      "loss": 0.6748,
      "step": 617000
    },
    {
      "epoch": 9.687598116169545,
      "grad_norm": 3.570185899734497,
      "learning_rate": 4.3945251177394036e-05,
      "loss": 0.6215,
      "step": 617100
    },
    {
      "epoch": 9.68916797488226,
      "grad_norm": 4.037125110626221,
      "learning_rate": 4.3944270015698594e-05,
      "loss": 0.6363,
      "step": 617200
    },
    {
      "epoch": 9.690737833594977,
      "grad_norm": 3.61472749710083,
      "learning_rate": 4.394328885400314e-05,
      "loss": 0.6206,
      "step": 617300
    },
    {
      "epoch": 9.692307692307692,
      "grad_norm": 3.3515663146972656,
      "learning_rate": 4.3942307692307695e-05,
      "loss": 0.6665,
      "step": 617400
    },
    {
      "epoch": 9.693877551020408,
      "grad_norm": 2.3899927139282227,
      "learning_rate": 4.3941326530612246e-05,
      "loss": 0.6137,
      "step": 617500
    },
    {
      "epoch": 9.695447409733124,
      "grad_norm": 3.1537208557128906,
      "learning_rate": 4.3940345368916804e-05,
      "loss": 0.6389,
      "step": 617600
    },
    {
      "epoch": 9.69701726844584,
      "grad_norm": 4.074238300323486,
      "learning_rate": 4.393936420722135e-05,
      "loss": 0.6508,
      "step": 617700
    },
    {
      "epoch": 9.698587127158556,
      "grad_norm": 3.7857861518859863,
      "learning_rate": 4.3938383045525906e-05,
      "loss": 0.6253,
      "step": 617800
    },
    {
      "epoch": 9.700156985871272,
      "grad_norm": 4.2979350090026855,
      "learning_rate": 4.393740188383046e-05,
      "loss": 0.6352,
      "step": 617900
    },
    {
      "epoch": 9.701726844583987,
      "grad_norm": 4.299069404602051,
      "learning_rate": 4.393642072213501e-05,
      "loss": 0.6207,
      "step": 618000
    },
    {
      "epoch": 9.703296703296703,
      "grad_norm": 2.7442870140075684,
      "learning_rate": 4.393543956043956e-05,
      "loss": 0.6422,
      "step": 618100
    },
    {
      "epoch": 9.70486656200942,
      "grad_norm": 3.739853858947754,
      "learning_rate": 4.393445839874412e-05,
      "loss": 0.6888,
      "step": 618200
    },
    {
      "epoch": 9.706436420722135,
      "grad_norm": 3.7531604766845703,
      "learning_rate": 4.393347723704867e-05,
      "loss": 0.6345,
      "step": 618300
    },
    {
      "epoch": 9.708006279434851,
      "grad_norm": 3.9057810306549072,
      "learning_rate": 4.393249607535322e-05,
      "loss": 0.6349,
      "step": 618400
    },
    {
      "epoch": 9.709576138147566,
      "grad_norm": 3.604731559753418,
      "learning_rate": 4.393151491365777e-05,
      "loss": 0.6191,
      "step": 618500
    },
    {
      "epoch": 9.711145996860283,
      "grad_norm": 3.638193368911743,
      "learning_rate": 4.393053375196233e-05,
      "loss": 0.6551,
      "step": 618600
    },
    {
      "epoch": 9.712715855572998,
      "grad_norm": 4.440779685974121,
      "learning_rate": 4.392955259026688e-05,
      "loss": 0.649,
      "step": 618700
    },
    {
      "epoch": 9.714285714285714,
      "grad_norm": 3.3638508319854736,
      "learning_rate": 4.392857142857143e-05,
      "loss": 0.6288,
      "step": 618800
    },
    {
      "epoch": 9.71585557299843,
      "grad_norm": 3.580610513687134,
      "learning_rate": 4.392759026687599e-05,
      "loss": 0.6166,
      "step": 618900
    },
    {
      "epoch": 9.717425431711145,
      "grad_norm": 3.333390951156616,
      "learning_rate": 4.392660910518054e-05,
      "loss": 0.6439,
      "step": 619000
    },
    {
      "epoch": 9.718995290423862,
      "grad_norm": 4.2074875831604,
      "learning_rate": 4.392562794348509e-05,
      "loss": 0.6289,
      "step": 619100
    },
    {
      "epoch": 9.720565149136577,
      "grad_norm": 4.5496134757995605,
      "learning_rate": 4.392464678178964e-05,
      "loss": 0.5967,
      "step": 619200
    },
    {
      "epoch": 9.722135007849294,
      "grad_norm": 4.078187942504883,
      "learning_rate": 4.39236656200942e-05,
      "loss": 0.6601,
      "step": 619300
    },
    {
      "epoch": 9.72370486656201,
      "grad_norm": 3.0712850093841553,
      "learning_rate": 4.392268445839874e-05,
      "loss": 0.6529,
      "step": 619400
    },
    {
      "epoch": 9.725274725274724,
      "grad_norm": 4.906290054321289,
      "learning_rate": 4.39217032967033e-05,
      "loss": 0.6352,
      "step": 619500
    },
    {
      "epoch": 9.726844583987441,
      "grad_norm": 2.60261869430542,
      "learning_rate": 4.392072213500785e-05,
      "loss": 0.667,
      "step": 619600
    },
    {
      "epoch": 9.728414442700156,
      "grad_norm": 3.7763798236846924,
      "learning_rate": 4.391974097331241e-05,
      "loss": 0.6314,
      "step": 619700
    },
    {
      "epoch": 9.729984301412873,
      "grad_norm": 3.8660967350006104,
      "learning_rate": 4.391875981161695e-05,
      "loss": 0.6146,
      "step": 619800
    },
    {
      "epoch": 9.731554160125588,
      "grad_norm": 3.75880765914917,
      "learning_rate": 4.391777864992151e-05,
      "loss": 0.6519,
      "step": 619900
    },
    {
      "epoch": 9.733124018838305,
      "grad_norm": 4.344946384429932,
      "learning_rate": 4.391679748822606e-05,
      "loss": 0.6553,
      "step": 620000
    },
    {
      "epoch": 9.73469387755102,
      "grad_norm": 4.590633392333984,
      "learning_rate": 4.391581632653061e-05,
      "loss": 0.6321,
      "step": 620100
    },
    {
      "epoch": 9.736263736263737,
      "grad_norm": 3.750028371810913,
      "learning_rate": 4.3914835164835164e-05,
      "loss": 0.6527,
      "step": 620200
    },
    {
      "epoch": 9.737833594976452,
      "grad_norm": 4.509251117706299,
      "learning_rate": 4.391385400313972e-05,
      "loss": 0.6468,
      "step": 620300
    },
    {
      "epoch": 9.739403453689167,
      "grad_norm": 4.313596248626709,
      "learning_rate": 4.391287284144427e-05,
      "loss": 0.6769,
      "step": 620400
    },
    {
      "epoch": 9.740973312401884,
      "grad_norm": 3.561962127685547,
      "learning_rate": 4.391189167974882e-05,
      "loss": 0.6376,
      "step": 620500
    },
    {
      "epoch": 9.7425431711146,
      "grad_norm": 4.116892337799072,
      "learning_rate": 4.3910910518053374e-05,
      "loss": 0.6548,
      "step": 620600
    },
    {
      "epoch": 9.744113029827316,
      "grad_norm": 3.742690324783325,
      "learning_rate": 4.390992935635793e-05,
      "loss": 0.6442,
      "step": 620700
    },
    {
      "epoch": 9.745682888540031,
      "grad_norm": 4.158529758453369,
      "learning_rate": 4.390894819466248e-05,
      "loss": 0.6606,
      "step": 620800
    },
    {
      "epoch": 9.747252747252748,
      "grad_norm": 3.42997670173645,
      "learning_rate": 4.3907967032967034e-05,
      "loss": 0.6687,
      "step": 620900
    },
    {
      "epoch": 9.748822605965463,
      "grad_norm": 3.075289487838745,
      "learning_rate": 4.390698587127159e-05,
      "loss": 0.6675,
      "step": 621000
    },
    {
      "epoch": 9.750392464678178,
      "grad_norm": 4.45829963684082,
      "learning_rate": 4.390600470957614e-05,
      "loss": 0.5773,
      "step": 621100
    },
    {
      "epoch": 9.751962323390895,
      "grad_norm": 3.261223316192627,
      "learning_rate": 4.3905023547880694e-05,
      "loss": 0.6113,
      "step": 621200
    },
    {
      "epoch": 9.75353218210361,
      "grad_norm": 4.460567474365234,
      "learning_rate": 4.3904042386185245e-05,
      "loss": 0.6155,
      "step": 621300
    },
    {
      "epoch": 9.755102040816327,
      "grad_norm": 4.264063835144043,
      "learning_rate": 4.39030612244898e-05,
      "loss": 0.6239,
      "step": 621400
    },
    {
      "epoch": 9.756671899529042,
      "grad_norm": 3.5326006412506104,
      "learning_rate": 4.390208006279435e-05,
      "loss": 0.6665,
      "step": 621500
    },
    {
      "epoch": 9.758241758241759,
      "grad_norm": 3.8156449794769287,
      "learning_rate": 4.3901098901098904e-05,
      "loss": 0.6647,
      "step": 621600
    },
    {
      "epoch": 9.759811616954474,
      "grad_norm": 2.5330073833465576,
      "learning_rate": 4.3900117739403455e-05,
      "loss": 0.636,
      "step": 621700
    },
    {
      "epoch": 9.76138147566719,
      "grad_norm": 2.710867404937744,
      "learning_rate": 4.389913657770801e-05,
      "loss": 0.632,
      "step": 621800
    },
    {
      "epoch": 9.762951334379906,
      "grad_norm": 4.31583833694458,
      "learning_rate": 4.389815541601256e-05,
      "loss": 0.6675,
      "step": 621900
    },
    {
      "epoch": 9.764521193092621,
      "grad_norm": 5.057156085968018,
      "learning_rate": 4.3897174254317115e-05,
      "loss": 0.6583,
      "step": 622000
    },
    {
      "epoch": 9.766091051805338,
      "grad_norm": 3.628460645675659,
      "learning_rate": 4.3896193092621666e-05,
      "loss": 0.641,
      "step": 622100
    },
    {
      "epoch": 9.767660910518053,
      "grad_norm": 2.9683663845062256,
      "learning_rate": 4.389521193092622e-05,
      "loss": 0.5818,
      "step": 622200
    },
    {
      "epoch": 9.76923076923077,
      "grad_norm": 3.5725793838500977,
      "learning_rate": 4.389423076923077e-05,
      "loss": 0.6826,
      "step": 622300
    },
    {
      "epoch": 9.770800627943485,
      "grad_norm": 3.1081395149230957,
      "learning_rate": 4.3893249607535326e-05,
      "loss": 0.648,
      "step": 622400
    },
    {
      "epoch": 9.7723704866562,
      "grad_norm": 5.2286858558654785,
      "learning_rate": 4.389226844583988e-05,
      "loss": 0.6582,
      "step": 622500
    },
    {
      "epoch": 9.773940345368917,
      "grad_norm": 3.999187469482422,
      "learning_rate": 4.389128728414443e-05,
      "loss": 0.6206,
      "step": 622600
    },
    {
      "epoch": 9.775510204081632,
      "grad_norm": 3.8287625312805176,
      "learning_rate": 4.389030612244898e-05,
      "loss": 0.6396,
      "step": 622700
    },
    {
      "epoch": 9.777080062794349,
      "grad_norm": 3.996140956878662,
      "learning_rate": 4.3889324960753537e-05,
      "loss": 0.6085,
      "step": 622800
    },
    {
      "epoch": 9.778649921507064,
      "grad_norm": 3.6524899005889893,
      "learning_rate": 4.388834379905809e-05,
      "loss": 0.6327,
      "step": 622900
    },
    {
      "epoch": 9.780219780219781,
      "grad_norm": 2.4978749752044678,
      "learning_rate": 4.388736263736264e-05,
      "loss": 0.6263,
      "step": 623000
    },
    {
      "epoch": 9.781789638932496,
      "grad_norm": 3.6024138927459717,
      "learning_rate": 4.3886381475667196e-05,
      "loss": 0.6446,
      "step": 623100
    },
    {
      "epoch": 9.783359497645211,
      "grad_norm": 4.611268520355225,
      "learning_rate": 4.388540031397175e-05,
      "loss": 0.5721,
      "step": 623200
    },
    {
      "epoch": 9.784929356357928,
      "grad_norm": 3.8919105529785156,
      "learning_rate": 4.38844191522763e-05,
      "loss": 0.6402,
      "step": 623300
    },
    {
      "epoch": 9.786499215070643,
      "grad_norm": 2.5864791870117188,
      "learning_rate": 4.388343799058085e-05,
      "loss": 0.6267,
      "step": 623400
    },
    {
      "epoch": 9.78806907378336,
      "grad_norm": 3.758162498474121,
      "learning_rate": 4.388245682888541e-05,
      "loss": 0.6448,
      "step": 623500
    },
    {
      "epoch": 9.789638932496075,
      "grad_norm": 3.81994366645813,
      "learning_rate": 4.388147566718995e-05,
      "loss": 0.6017,
      "step": 623600
    },
    {
      "epoch": 9.791208791208792,
      "grad_norm": 4.310797691345215,
      "learning_rate": 4.388049450549451e-05,
      "loss": 0.6532,
      "step": 623700
    },
    {
      "epoch": 9.792778649921507,
      "grad_norm": 2.657047986984253,
      "learning_rate": 4.387951334379906e-05,
      "loss": 0.6215,
      "step": 623800
    },
    {
      "epoch": 9.794348508634222,
      "grad_norm": 3.401066303253174,
      "learning_rate": 4.387853218210362e-05,
      "loss": 0.6226,
      "step": 623900
    },
    {
      "epoch": 9.795918367346939,
      "grad_norm": 4.365729808807373,
      "learning_rate": 4.387755102040816e-05,
      "loss": 0.6607,
      "step": 624000
    },
    {
      "epoch": 9.797488226059654,
      "grad_norm": 4.66830587387085,
      "learning_rate": 4.387656985871272e-05,
      "loss": 0.6197,
      "step": 624100
    },
    {
      "epoch": 9.799058084772371,
      "grad_norm": 4.647177219390869,
      "learning_rate": 4.387558869701727e-05,
      "loss": 0.6361,
      "step": 624200
    },
    {
      "epoch": 9.800627943485086,
      "grad_norm": 3.9938137531280518,
      "learning_rate": 4.387460753532182e-05,
      "loss": 0.6317,
      "step": 624300
    },
    {
      "epoch": 9.802197802197803,
      "grad_norm": 3.5457093715667725,
      "learning_rate": 4.387362637362637e-05,
      "loss": 0.6167,
      "step": 624400
    },
    {
      "epoch": 9.803767660910518,
      "grad_norm": 2.6918954849243164,
      "learning_rate": 4.387264521193093e-05,
      "loss": 0.6259,
      "step": 624500
    },
    {
      "epoch": 9.805337519623233,
      "grad_norm": 4.416221618652344,
      "learning_rate": 4.387166405023548e-05,
      "loss": 0.6102,
      "step": 624600
    },
    {
      "epoch": 9.80690737833595,
      "grad_norm": 2.8357739448547363,
      "learning_rate": 4.387068288854003e-05,
      "loss": 0.6019,
      "step": 624700
    },
    {
      "epoch": 9.808477237048665,
      "grad_norm": 3.939213514328003,
      "learning_rate": 4.386970172684458e-05,
      "loss": 0.6284,
      "step": 624800
    },
    {
      "epoch": 9.810047095761382,
      "grad_norm": 4.740400314331055,
      "learning_rate": 4.386872056514914e-05,
      "loss": 0.6004,
      "step": 624900
    },
    {
      "epoch": 9.811616954474097,
      "grad_norm": 3.460897922515869,
      "learning_rate": 4.386773940345369e-05,
      "loss": 0.6518,
      "step": 625000
    },
    {
      "epoch": 9.813186813186814,
      "grad_norm": 3.8868460655212402,
      "learning_rate": 4.386675824175824e-05,
      "loss": 0.6777,
      "step": 625100
    },
    {
      "epoch": 9.814756671899529,
      "grad_norm": 3.454247236251831,
      "learning_rate": 4.38657770800628e-05,
      "loss": 0.6529,
      "step": 625200
    },
    {
      "epoch": 9.816326530612244,
      "grad_norm": 4.709226131439209,
      "learning_rate": 4.386479591836735e-05,
      "loss": 0.6182,
      "step": 625300
    },
    {
      "epoch": 9.817896389324961,
      "grad_norm": 5.339489459991455,
      "learning_rate": 4.38638147566719e-05,
      "loss": 0.6601,
      "step": 625400
    },
    {
      "epoch": 9.819466248037676,
      "grad_norm": 2.7944841384887695,
      "learning_rate": 4.3862833594976454e-05,
      "loss": 0.6239,
      "step": 625500
    },
    {
      "epoch": 9.821036106750393,
      "grad_norm": 3.9804232120513916,
      "learning_rate": 4.386185243328101e-05,
      "loss": 0.6711,
      "step": 625600
    },
    {
      "epoch": 9.822605965463108,
      "grad_norm": 4.550668716430664,
      "learning_rate": 4.3860871271585556e-05,
      "loss": 0.5854,
      "step": 625700
    },
    {
      "epoch": 9.824175824175825,
      "grad_norm": 3.179419994354248,
      "learning_rate": 4.3859890109890113e-05,
      "loss": 0.6669,
      "step": 625800
    },
    {
      "epoch": 9.82574568288854,
      "grad_norm": 3.941678285598755,
      "learning_rate": 4.3858908948194664e-05,
      "loss": 0.6508,
      "step": 625900
    },
    {
      "epoch": 9.827315541601255,
      "grad_norm": 3.9080417156219482,
      "learning_rate": 4.3857927786499215e-05,
      "loss": 0.6673,
      "step": 626000
    },
    {
      "epoch": 9.828885400313972,
      "grad_norm": 3.6411237716674805,
      "learning_rate": 4.3856946624803766e-05,
      "loss": 0.6425,
      "step": 626100
    },
    {
      "epoch": 9.830455259026687,
      "grad_norm": 4.847797393798828,
      "learning_rate": 4.3855965463108324e-05,
      "loss": 0.6389,
      "step": 626200
    },
    {
      "epoch": 9.832025117739404,
      "grad_norm": 5.292966842651367,
      "learning_rate": 4.3854984301412875e-05,
      "loss": 0.6681,
      "step": 626300
    },
    {
      "epoch": 9.833594976452119,
      "grad_norm": 3.87274432182312,
      "learning_rate": 4.3854003139717426e-05,
      "loss": 0.6272,
      "step": 626400
    },
    {
      "epoch": 9.835164835164836,
      "grad_norm": 4.417809009552002,
      "learning_rate": 4.385302197802198e-05,
      "loss": 0.6762,
      "step": 626500
    },
    {
      "epoch": 9.83673469387755,
      "grad_norm": 4.280989646911621,
      "learning_rate": 4.3852040816326535e-05,
      "loss": 0.6007,
      "step": 626600
    },
    {
      "epoch": 9.838304552590268,
      "grad_norm": 4.270031929016113,
      "learning_rate": 4.3851059654631086e-05,
      "loss": 0.6257,
      "step": 626700
    },
    {
      "epoch": 9.839874411302983,
      "grad_norm": 3.1644179821014404,
      "learning_rate": 4.385007849293564e-05,
      "loss": 0.6314,
      "step": 626800
    },
    {
      "epoch": 9.841444270015698,
      "grad_norm": 3.163424253463745,
      "learning_rate": 4.384909733124019e-05,
      "loss": 0.6503,
      "step": 626900
    },
    {
      "epoch": 9.843014128728415,
      "grad_norm": 2.877885341644287,
      "learning_rate": 4.3848116169544746e-05,
      "loss": 0.6532,
      "step": 627000
    },
    {
      "epoch": 9.84458398744113,
      "grad_norm": 3.963271379470825,
      "learning_rate": 4.3847135007849297e-05,
      "loss": 0.6425,
      "step": 627100
    },
    {
      "epoch": 9.846153846153847,
      "grad_norm": 3.6742851734161377,
      "learning_rate": 4.384615384615385e-05,
      "loss": 0.6204,
      "step": 627200
    },
    {
      "epoch": 9.847723704866562,
      "grad_norm": 4.094838619232178,
      "learning_rate": 4.3845172684458405e-05,
      "loss": 0.6457,
      "step": 627300
    },
    {
      "epoch": 9.849293563579279,
      "grad_norm": 2.949932098388672,
      "learning_rate": 4.384419152276295e-05,
      "loss": 0.6421,
      "step": 627400
    },
    {
      "epoch": 9.850863422291994,
      "grad_norm": 4.806472301483154,
      "learning_rate": 4.384321036106751e-05,
      "loss": 0.6372,
      "step": 627500
    },
    {
      "epoch": 9.852433281004709,
      "grad_norm": 3.8162927627563477,
      "learning_rate": 4.384222919937206e-05,
      "loss": 0.6296,
      "step": 627600
    },
    {
      "epoch": 9.854003139717426,
      "grad_norm": 3.4485232830047607,
      "learning_rate": 4.3841248037676616e-05,
      "loss": 0.6747,
      "step": 627700
    },
    {
      "epoch": 9.85557299843014,
      "grad_norm": 3.462498903274536,
      "learning_rate": 4.384026687598116e-05,
      "loss": 0.6566,
      "step": 627800
    },
    {
      "epoch": 9.857142857142858,
      "grad_norm": 4.065741062164307,
      "learning_rate": 4.383928571428572e-05,
      "loss": 0.5944,
      "step": 627900
    },
    {
      "epoch": 9.858712715855573,
      "grad_norm": 2.646062135696411,
      "learning_rate": 4.383830455259027e-05,
      "loss": 0.6322,
      "step": 628000
    },
    {
      "epoch": 9.86028257456829,
      "grad_norm": 4.841952323913574,
      "learning_rate": 4.383732339089482e-05,
      "loss": 0.6307,
      "step": 628100
    },
    {
      "epoch": 9.861852433281005,
      "grad_norm": 3.7055065631866455,
      "learning_rate": 4.383634222919937e-05,
      "loss": 0.5942,
      "step": 628200
    },
    {
      "epoch": 9.86342229199372,
      "grad_norm": 4.907088279724121,
      "learning_rate": 4.383536106750393e-05,
      "loss": 0.6511,
      "step": 628300
    },
    {
      "epoch": 9.864992150706437,
      "grad_norm": 4.436858654022217,
      "learning_rate": 4.383437990580848e-05,
      "loss": 0.6325,
      "step": 628400
    },
    {
      "epoch": 9.866562009419152,
      "grad_norm": 4.458293914794922,
      "learning_rate": 4.383339874411303e-05,
      "loss": 0.6828,
      "step": 628500
    },
    {
      "epoch": 9.868131868131869,
      "grad_norm": 4.875789165496826,
      "learning_rate": 4.383241758241758e-05,
      "loss": 0.6282,
      "step": 628600
    },
    {
      "epoch": 9.869701726844584,
      "grad_norm": 4.936241149902344,
      "learning_rate": 4.383143642072214e-05,
      "loss": 0.6589,
      "step": 628700
    },
    {
      "epoch": 9.8712715855573,
      "grad_norm": 4.197950839996338,
      "learning_rate": 4.383045525902669e-05,
      "loss": 0.6483,
      "step": 628800
    },
    {
      "epoch": 9.872841444270016,
      "grad_norm": 3.628412961959839,
      "learning_rate": 4.382947409733124e-05,
      "loss": 0.6166,
      "step": 628900
    },
    {
      "epoch": 9.87441130298273,
      "grad_norm": 3.4056050777435303,
      "learning_rate": 4.382849293563579e-05,
      "loss": 0.6722,
      "step": 629000
    },
    {
      "epoch": 9.875981161695448,
      "grad_norm": 4.333085060119629,
      "learning_rate": 4.382751177394035e-05,
      "loss": 0.6454,
      "step": 629100
    },
    {
      "epoch": 9.877551020408163,
      "grad_norm": 3.3419442176818848,
      "learning_rate": 4.38265306122449e-05,
      "loss": 0.6615,
      "step": 629200
    },
    {
      "epoch": 9.87912087912088,
      "grad_norm": 3.9133145809173584,
      "learning_rate": 4.382554945054945e-05,
      "loss": 0.637,
      "step": 629300
    },
    {
      "epoch": 9.880690737833595,
      "grad_norm": 4.277343273162842,
      "learning_rate": 4.382456828885401e-05,
      "loss": 0.68,
      "step": 629400
    },
    {
      "epoch": 9.882260596546312,
      "grad_norm": 4.192484378814697,
      "learning_rate": 4.3823587127158554e-05,
      "loss": 0.6764,
      "step": 629500
    },
    {
      "epoch": 9.883830455259027,
      "grad_norm": 4.26676082611084,
      "learning_rate": 4.382260596546311e-05,
      "loss": 0.6556,
      "step": 629600
    },
    {
      "epoch": 9.885400313971743,
      "grad_norm": 3.4550459384918213,
      "learning_rate": 4.382162480376766e-05,
      "loss": 0.6213,
      "step": 629700
    },
    {
      "epoch": 9.886970172684459,
      "grad_norm": 4.2321295738220215,
      "learning_rate": 4.382064364207222e-05,
      "loss": 0.6518,
      "step": 629800
    },
    {
      "epoch": 9.888540031397174,
      "grad_norm": 4.755461692810059,
      "learning_rate": 4.3819662480376765e-05,
      "loss": 0.6163,
      "step": 629900
    },
    {
      "epoch": 9.89010989010989,
      "grad_norm": 5.122980117797852,
      "learning_rate": 4.381868131868132e-05,
      "loss": 0.6628,
      "step": 630000
    },
    {
      "epoch": 9.891679748822606,
      "grad_norm": 3.9852325916290283,
      "learning_rate": 4.3817700156985873e-05,
      "loss": 0.649,
      "step": 630100
    },
    {
      "epoch": 9.893249607535322,
      "grad_norm": 4.291991710662842,
      "learning_rate": 4.3816718995290424e-05,
      "loss": 0.6496,
      "step": 630200
    },
    {
      "epoch": 9.894819466248038,
      "grad_norm": 4.054962635040283,
      "learning_rate": 4.3815737833594975e-05,
      "loss": 0.6148,
      "step": 630300
    },
    {
      "epoch": 9.896389324960754,
      "grad_norm": 3.2820956707000732,
      "learning_rate": 4.381475667189953e-05,
      "loss": 0.6335,
      "step": 630400
    },
    {
      "epoch": 9.89795918367347,
      "grad_norm": 4.1014556884765625,
      "learning_rate": 4.3813775510204084e-05,
      "loss": 0.6431,
      "step": 630500
    },
    {
      "epoch": 9.899529042386185,
      "grad_norm": 4.444730281829834,
      "learning_rate": 4.3812794348508635e-05,
      "loss": 0.5955,
      "step": 630600
    },
    {
      "epoch": 9.901098901098901,
      "grad_norm": 4.193126678466797,
      "learning_rate": 4.3811813186813186e-05,
      "loss": 0.6446,
      "step": 630700
    },
    {
      "epoch": 9.902668759811617,
      "grad_norm": 3.414092779159546,
      "learning_rate": 4.3810832025117744e-05,
      "loss": 0.6249,
      "step": 630800
    },
    {
      "epoch": 9.904238618524333,
      "grad_norm": 3.9313950538635254,
      "learning_rate": 4.3809850863422295e-05,
      "loss": 0.6343,
      "step": 630900
    },
    {
      "epoch": 9.905808477237048,
      "grad_norm": 3.882542610168457,
      "learning_rate": 4.3808869701726846e-05,
      "loss": 0.6219,
      "step": 631000
    },
    {
      "epoch": 9.907378335949765,
      "grad_norm": 3.1912200450897217,
      "learning_rate": 4.38078885400314e-05,
      "loss": 0.6171,
      "step": 631100
    },
    {
      "epoch": 9.90894819466248,
      "grad_norm": 3.1711978912353516,
      "learning_rate": 4.3806907378335955e-05,
      "loss": 0.6648,
      "step": 631200
    },
    {
      "epoch": 9.910518053375196,
      "grad_norm": 3.275099515914917,
      "learning_rate": 4.3805926216640506e-05,
      "loss": 0.6177,
      "step": 631300
    },
    {
      "epoch": 9.912087912087912,
      "grad_norm": 3.8719794750213623,
      "learning_rate": 4.3804945054945057e-05,
      "loss": 0.6721,
      "step": 631400
    },
    {
      "epoch": 9.913657770800627,
      "grad_norm": 4.2121453285217285,
      "learning_rate": 4.3803963893249614e-05,
      "loss": 0.6148,
      "step": 631500
    },
    {
      "epoch": 9.915227629513344,
      "grad_norm": 3.265451431274414,
      "learning_rate": 4.380298273155416e-05,
      "loss": 0.6422,
      "step": 631600
    },
    {
      "epoch": 9.91679748822606,
      "grad_norm": 3.371504306793213,
      "learning_rate": 4.3802001569858716e-05,
      "loss": 0.6439,
      "step": 631700
    },
    {
      "epoch": 9.918367346938776,
      "grad_norm": 3.9714269638061523,
      "learning_rate": 4.380102040816327e-05,
      "loss": 0.6129,
      "step": 631800
    },
    {
      "epoch": 9.919937205651491,
      "grad_norm": 2.477513551712036,
      "learning_rate": 4.3800039246467825e-05,
      "loss": 0.6382,
      "step": 631900
    },
    {
      "epoch": 9.921507064364206,
      "grad_norm": 3.3717217445373535,
      "learning_rate": 4.379905808477237e-05,
      "loss": 0.6347,
      "step": 632000
    },
    {
      "epoch": 9.923076923076923,
      "grad_norm": 3.508091449737549,
      "learning_rate": 4.379807692307693e-05,
      "loss": 0.594,
      "step": 632100
    },
    {
      "epoch": 9.924646781789638,
      "grad_norm": 3.094939708709717,
      "learning_rate": 4.379709576138148e-05,
      "loss": 0.6249,
      "step": 632200
    },
    {
      "epoch": 9.926216640502355,
      "grad_norm": 3.865330219268799,
      "learning_rate": 4.379611459968603e-05,
      "loss": 0.6612,
      "step": 632300
    },
    {
      "epoch": 9.92778649921507,
      "grad_norm": 4.285464286804199,
      "learning_rate": 4.379513343799058e-05,
      "loss": 0.6133,
      "step": 632400
    },
    {
      "epoch": 9.929356357927787,
      "grad_norm": 3.1110758781433105,
      "learning_rate": 4.379415227629514e-05,
      "loss": 0.6393,
      "step": 632500
    },
    {
      "epoch": 9.930926216640502,
      "grad_norm": 3.1513564586639404,
      "learning_rate": 4.379317111459969e-05,
      "loss": 0.6192,
      "step": 632600
    },
    {
      "epoch": 9.932496075353217,
      "grad_norm": 4.113479137420654,
      "learning_rate": 4.379218995290424e-05,
      "loss": 0.6751,
      "step": 632700
    },
    {
      "epoch": 9.934065934065934,
      "grad_norm": 3.0222158432006836,
      "learning_rate": 4.379120879120879e-05,
      "loss": 0.6349,
      "step": 632800
    },
    {
      "epoch": 9.93563579277865,
      "grad_norm": 3.9218714237213135,
      "learning_rate": 4.379022762951335e-05,
      "loss": 0.6847,
      "step": 632900
    },
    {
      "epoch": 9.937205651491366,
      "grad_norm": 3.844855785369873,
      "learning_rate": 4.37892464678179e-05,
      "loss": 0.6262,
      "step": 633000
    },
    {
      "epoch": 9.938775510204081,
      "grad_norm": 4.393561840057373,
      "learning_rate": 4.378826530612245e-05,
      "loss": 0.6595,
      "step": 633100
    },
    {
      "epoch": 9.940345368916798,
      "grad_norm": 3.4335579872131348,
      "learning_rate": 4.3787284144427e-05,
      "loss": 0.6471,
      "step": 633200
    },
    {
      "epoch": 9.941915227629513,
      "grad_norm": 4.110380172729492,
      "learning_rate": 4.378630298273156e-05,
      "loss": 0.6557,
      "step": 633300
    },
    {
      "epoch": 9.943485086342228,
      "grad_norm": 5.7784810066223145,
      "learning_rate": 4.378532182103611e-05,
      "loss": 0.6292,
      "step": 633400
    },
    {
      "epoch": 9.945054945054945,
      "grad_norm": 4.6134233474731445,
      "learning_rate": 4.378434065934066e-05,
      "loss": 0.643,
      "step": 633500
    },
    {
      "epoch": 9.94662480376766,
      "grad_norm": 3.746351718902588,
      "learning_rate": 4.378335949764522e-05,
      "loss": 0.6356,
      "step": 633600
    },
    {
      "epoch": 9.948194662480377,
      "grad_norm": 3.735839605331421,
      "learning_rate": 4.378237833594976e-05,
      "loss": 0.6314,
      "step": 633700
    },
    {
      "epoch": 9.949764521193092,
      "grad_norm": 3.847181558609009,
      "learning_rate": 4.378139717425432e-05,
      "loss": 0.6354,
      "step": 633800
    },
    {
      "epoch": 9.95133437990581,
      "grad_norm": 5.145418643951416,
      "learning_rate": 4.378041601255887e-05,
      "loss": 0.5921,
      "step": 633900
    },
    {
      "epoch": 9.952904238618524,
      "grad_norm": 4.853922367095947,
      "learning_rate": 4.377943485086343e-05,
      "loss": 0.6248,
      "step": 634000
    },
    {
      "epoch": 9.95447409733124,
      "grad_norm": 2.277501344680786,
      "learning_rate": 4.3778453689167974e-05,
      "loss": 0.663,
      "step": 634100
    },
    {
      "epoch": 9.956043956043956,
      "grad_norm": 4.857813835144043,
      "learning_rate": 4.377747252747253e-05,
      "loss": 0.5645,
      "step": 634200
    },
    {
      "epoch": 9.957613814756671,
      "grad_norm": 4.498058319091797,
      "learning_rate": 4.377649136577708e-05,
      "loss": 0.6575,
      "step": 634300
    },
    {
      "epoch": 9.959183673469388,
      "grad_norm": 4.364256381988525,
      "learning_rate": 4.377551020408163e-05,
      "loss": 0.6116,
      "step": 634400
    },
    {
      "epoch": 9.960753532182103,
      "grad_norm": 3.2823073863983154,
      "learning_rate": 4.3774529042386184e-05,
      "loss": 0.6067,
      "step": 634500
    },
    {
      "epoch": 9.96232339089482,
      "grad_norm": 3.2294678688049316,
      "learning_rate": 4.377354788069074e-05,
      "loss": 0.6315,
      "step": 634600
    },
    {
      "epoch": 9.963893249607535,
      "grad_norm": 3.46704363822937,
      "learning_rate": 4.377256671899529e-05,
      "loss": 0.6268,
      "step": 634700
    },
    {
      "epoch": 9.96546310832025,
      "grad_norm": 3.3459737300872803,
      "learning_rate": 4.3771585557299844e-05,
      "loss": 0.6122,
      "step": 634800
    },
    {
      "epoch": 9.967032967032967,
      "grad_norm": 3.852193593978882,
      "learning_rate": 4.3770604395604395e-05,
      "loss": 0.6301,
      "step": 634900
    },
    {
      "epoch": 9.968602825745682,
      "grad_norm": 3.3096840381622314,
      "learning_rate": 4.376962323390895e-05,
      "loss": 0.6646,
      "step": 635000
    },
    {
      "epoch": 9.970172684458399,
      "grad_norm": 4.3298797607421875,
      "learning_rate": 4.3768642072213504e-05,
      "loss": 0.6454,
      "step": 635100
    },
    {
      "epoch": 9.971742543171114,
      "grad_norm": 3.7418792247772217,
      "learning_rate": 4.3767660910518055e-05,
      "loss": 0.6509,
      "step": 635200
    },
    {
      "epoch": 9.973312401883831,
      "grad_norm": 3.7832090854644775,
      "learning_rate": 4.3766679748822606e-05,
      "loss": 0.6438,
      "step": 635300
    },
    {
      "epoch": 9.974882260596546,
      "grad_norm": 4.248976707458496,
      "learning_rate": 4.3765698587127164e-05,
      "loss": 0.6554,
      "step": 635400
    },
    {
      "epoch": 9.976452119309261,
      "grad_norm": 3.9852499961853027,
      "learning_rate": 4.3764717425431715e-05,
      "loss": 0.5954,
      "step": 635500
    },
    {
      "epoch": 9.978021978021978,
      "grad_norm": 4.685482501983643,
      "learning_rate": 4.3763736263736265e-05,
      "loss": 0.6738,
      "step": 635600
    },
    {
      "epoch": 9.979591836734693,
      "grad_norm": 5.051169395446777,
      "learning_rate": 4.376275510204082e-05,
      "loss": 0.6486,
      "step": 635700
    },
    {
      "epoch": 9.98116169544741,
      "grad_norm": 3.536027193069458,
      "learning_rate": 4.376177394034537e-05,
      "loss": 0.6231,
      "step": 635800
    },
    {
      "epoch": 9.982731554160125,
      "grad_norm": 3.1816678047180176,
      "learning_rate": 4.3760792778649925e-05,
      "loss": 0.6389,
      "step": 635900
    },
    {
      "epoch": 9.984301412872842,
      "grad_norm": 3.97847580909729,
      "learning_rate": 4.3759811616954476e-05,
      "loss": 0.6999,
      "step": 636000
    },
    {
      "epoch": 9.985871271585557,
      "grad_norm": 3.820483684539795,
      "learning_rate": 4.3758830455259034e-05,
      "loss": 0.6088,
      "step": 636100
    },
    {
      "epoch": 9.987441130298274,
      "grad_norm": 3.6788547039031982,
      "learning_rate": 4.375784929356358e-05,
      "loss": 0.6283,
      "step": 636200
    },
    {
      "epoch": 9.989010989010989,
      "grad_norm": 4.270578861236572,
      "learning_rate": 4.3756868131868136e-05,
      "loss": 0.5851,
      "step": 636300
    },
    {
      "epoch": 9.990580847723704,
      "grad_norm": 3.3676819801330566,
      "learning_rate": 4.375588697017269e-05,
      "loss": 0.6039,
      "step": 636400
    },
    {
      "epoch": 9.992150706436421,
      "grad_norm": 3.3410348892211914,
      "learning_rate": 4.375490580847724e-05,
      "loss": 0.5929,
      "step": 636500
    },
    {
      "epoch": 9.993720565149136,
      "grad_norm": 4.511521816253662,
      "learning_rate": 4.375392464678179e-05,
      "loss": 0.6665,
      "step": 636600
    },
    {
      "epoch": 9.995290423861853,
      "grad_norm": 4.214759349822998,
      "learning_rate": 4.3752943485086347e-05,
      "loss": 0.6342,
      "step": 636700
    },
    {
      "epoch": 9.996860282574568,
      "grad_norm": 4.383114337921143,
      "learning_rate": 4.37519623233909e-05,
      "loss": 0.6443,
      "step": 636800
    },
    {
      "epoch": 9.998430141287285,
      "grad_norm": 3.4905757904052734,
      "learning_rate": 4.375098116169545e-05,
      "loss": 0.6693,
      "step": 636900
    },
    {
      "epoch": 10.0,
      "grad_norm": 4.428003787994385,
      "learning_rate": 4.375e-05,
      "loss": 0.6601,
      "step": 637000
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.0186415910720825,
      "eval_runtime": 14.6746,
      "eval_samples_per_second": 228.49,
      "eval_steps_per_second": 228.49,
      "step": 637000
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.4953402578830719,
      "eval_runtime": 281.0332,
      "eval_samples_per_second": 226.664,
      "eval_steps_per_second": 226.664,
      "step": 637000
    },
    {
      "epoch": 10.001569858712715,
      "grad_norm": 3.947523593902588,
      "learning_rate": 4.374901883830456e-05,
      "loss": 0.6204,
      "step": 637100
    },
    {
      "epoch": 10.003139717425432,
      "grad_norm": 3.6285176277160645,
      "learning_rate": 4.37480376766091e-05,
      "loss": 0.6376,
      "step": 637200
    },
    {
      "epoch": 10.004709576138147,
      "grad_norm": 4.408929347991943,
      "learning_rate": 4.374705651491366e-05,
      "loss": 0.6257,
      "step": 637300
    },
    {
      "epoch": 10.006279434850864,
      "grad_norm": 4.179021835327148,
      "learning_rate": 4.374607535321821e-05,
      "loss": 0.6107,
      "step": 637400
    },
    {
      "epoch": 10.007849293563579,
      "grad_norm": 4.968554973602295,
      "learning_rate": 4.374509419152277e-05,
      "loss": 0.6502,
      "step": 637500
    },
    {
      "epoch": 10.009419152276296,
      "grad_norm": 3.5083413124084473,
      "learning_rate": 4.374411302982732e-05,
      "loss": 0.6155,
      "step": 637600
    },
    {
      "epoch": 10.010989010989011,
      "grad_norm": 2.6616101264953613,
      "learning_rate": 4.374313186813187e-05,
      "loss": 0.678,
      "step": 637700
    },
    {
      "epoch": 10.012558869701726,
      "grad_norm": 3.975782632827759,
      "learning_rate": 4.374215070643643e-05,
      "loss": 0.5967,
      "step": 637800
    },
    {
      "epoch": 10.014128728414443,
      "grad_norm": 3.520308494567871,
      "learning_rate": 4.374116954474097e-05,
      "loss": 0.6601,
      "step": 637900
    },
    {
      "epoch": 10.015698587127158,
      "grad_norm": 3.264300584793091,
      "learning_rate": 4.374018838304553e-05,
      "loss": 0.6377,
      "step": 638000
    },
    {
      "epoch": 10.017268445839875,
      "grad_norm": 4.336093902587891,
      "learning_rate": 4.373920722135008e-05,
      "loss": 0.6145,
      "step": 638100
    },
    {
      "epoch": 10.01883830455259,
      "grad_norm": 4.34981632232666,
      "learning_rate": 4.373822605965464e-05,
      "loss": 0.6634,
      "step": 638200
    },
    {
      "epoch": 10.020408163265307,
      "grad_norm": 4.731996059417725,
      "learning_rate": 4.373724489795918e-05,
      "loss": 0.6598,
      "step": 638300
    },
    {
      "epoch": 10.021978021978022,
      "grad_norm": 3.4499058723449707,
      "learning_rate": 4.373626373626374e-05,
      "loss": 0.6249,
      "step": 638400
    },
    {
      "epoch": 10.023547880690737,
      "grad_norm": 3.6746065616607666,
      "learning_rate": 4.373528257456829e-05,
      "loss": 0.6686,
      "step": 638500
    },
    {
      "epoch": 10.025117739403454,
      "grad_norm": 3.750798463821411,
      "learning_rate": 4.373430141287284e-05,
      "loss": 0.5961,
      "step": 638600
    },
    {
      "epoch": 10.026687598116169,
      "grad_norm": 2.5349693298339844,
      "learning_rate": 4.373332025117739e-05,
      "loss": 0.6604,
      "step": 638700
    },
    {
      "epoch": 10.028257456828886,
      "grad_norm": 4.8978447914123535,
      "learning_rate": 4.373233908948195e-05,
      "loss": 0.6692,
      "step": 638800
    },
    {
      "epoch": 10.029827315541601,
      "grad_norm": 4.5154619216918945,
      "learning_rate": 4.37313579277865e-05,
      "loss": 0.6078,
      "step": 638900
    },
    {
      "epoch": 10.031397174254318,
      "grad_norm": 4.1118059158325195,
      "learning_rate": 4.373037676609105e-05,
      "loss": 0.5959,
      "step": 639000
    },
    {
      "epoch": 10.032967032967033,
      "grad_norm": 2.3667712211608887,
      "learning_rate": 4.3729395604395604e-05,
      "loss": 0.6396,
      "step": 639100
    },
    {
      "epoch": 10.034536891679748,
      "grad_norm": 2.191866397857666,
      "learning_rate": 4.372841444270016e-05,
      "loss": 0.6176,
      "step": 639200
    },
    {
      "epoch": 10.036106750392465,
      "grad_norm": 3.820002317428589,
      "learning_rate": 4.3727433281004706e-05,
      "loss": 0.6367,
      "step": 639300
    },
    {
      "epoch": 10.03767660910518,
      "grad_norm": 3.704099178314209,
      "learning_rate": 4.3726452119309264e-05,
      "loss": 0.64,
      "step": 639400
    },
    {
      "epoch": 10.039246467817897,
      "grad_norm": 3.9862453937530518,
      "learning_rate": 4.3725470957613815e-05,
      "loss": 0.6656,
      "step": 639500
    },
    {
      "epoch": 10.040816326530612,
      "grad_norm": 3.7548105716705322,
      "learning_rate": 4.372448979591837e-05,
      "loss": 0.5891,
      "step": 639600
    },
    {
      "epoch": 10.042386185243329,
      "grad_norm": 4.295073509216309,
      "learning_rate": 4.3723508634222923e-05,
      "loss": 0.6739,
      "step": 639700
    },
    {
      "epoch": 10.043956043956044,
      "grad_norm": 3.553973913192749,
      "learning_rate": 4.3722527472527474e-05,
      "loss": 0.6671,
      "step": 639800
    },
    {
      "epoch": 10.04552590266876,
      "grad_norm": 3.2954561710357666,
      "learning_rate": 4.372154631083203e-05,
      "loss": 0.6409,
      "step": 639900
    },
    {
      "epoch": 10.047095761381476,
      "grad_norm": 3.299802780151367,
      "learning_rate": 4.3720565149136576e-05,
      "loss": 0.5929,
      "step": 640000
    },
    {
      "epoch": 10.04866562009419,
      "grad_norm": 4.49484395980835,
      "learning_rate": 4.3719583987441134e-05,
      "loss": 0.6183,
      "step": 640100
    },
    {
      "epoch": 10.050235478806908,
      "grad_norm": 3.639657735824585,
      "learning_rate": 4.3718602825745685e-05,
      "loss": 0.6493,
      "step": 640200
    },
    {
      "epoch": 10.051805337519623,
      "grad_norm": 5.524317264556885,
      "learning_rate": 4.371762166405024e-05,
      "loss": 0.6366,
      "step": 640300
    },
    {
      "epoch": 10.05337519623234,
      "grad_norm": 4.140779972076416,
      "learning_rate": 4.371664050235479e-05,
      "loss": 0.6239,
      "step": 640400
    },
    {
      "epoch": 10.054945054945055,
      "grad_norm": 4.07643461227417,
      "learning_rate": 4.3715659340659345e-05,
      "loss": 0.6532,
      "step": 640500
    },
    {
      "epoch": 10.056514913657772,
      "grad_norm": 3.153230905532837,
      "learning_rate": 4.3714678178963896e-05,
      "loss": 0.635,
      "step": 640600
    },
    {
      "epoch": 10.058084772370487,
      "grad_norm": 2.2077577114105225,
      "learning_rate": 4.371369701726845e-05,
      "loss": 0.621,
      "step": 640700
    },
    {
      "epoch": 10.059654631083202,
      "grad_norm": 4.801191806793213,
      "learning_rate": 4.3712715855573e-05,
      "loss": 0.6344,
      "step": 640800
    },
    {
      "epoch": 10.061224489795919,
      "grad_norm": 7.19056510925293,
      "learning_rate": 4.3711734693877556e-05,
      "loss": 0.6389,
      "step": 640900
    },
    {
      "epoch": 10.062794348508634,
      "grad_norm": 4.422199726104736,
      "learning_rate": 4.3710753532182107e-05,
      "loss": 0.6389,
      "step": 641000
    },
    {
      "epoch": 10.06436420722135,
      "grad_norm": 4.107962131500244,
      "learning_rate": 4.370977237048666e-05,
      "loss": 0.6454,
      "step": 641100
    },
    {
      "epoch": 10.065934065934066,
      "grad_norm": 3.2455556392669678,
      "learning_rate": 4.370879120879121e-05,
      "loss": 0.5962,
      "step": 641200
    },
    {
      "epoch": 10.067503924646783,
      "grad_norm": 2.329456329345703,
      "learning_rate": 4.3707810047095766e-05,
      "loss": 0.6252,
      "step": 641300
    },
    {
      "epoch": 10.069073783359498,
      "grad_norm": 2.5778074264526367,
      "learning_rate": 4.370682888540031e-05,
      "loss": 0.6426,
      "step": 641400
    },
    {
      "epoch": 10.070643642072213,
      "grad_norm": 3.796252727508545,
      "learning_rate": 4.370584772370487e-05,
      "loss": 0.6245,
      "step": 641500
    },
    {
      "epoch": 10.07221350078493,
      "grad_norm": 4.058815002441406,
      "learning_rate": 4.370486656200942e-05,
      "loss": 0.6519,
      "step": 641600
    },
    {
      "epoch": 10.073783359497645,
      "grad_norm": 3.398080825805664,
      "learning_rate": 4.370388540031398e-05,
      "loss": 0.6075,
      "step": 641700
    },
    {
      "epoch": 10.075353218210362,
      "grad_norm": 4.500586032867432,
      "learning_rate": 4.370290423861853e-05,
      "loss": 0.669,
      "step": 641800
    },
    {
      "epoch": 10.076923076923077,
      "grad_norm": 4.577963352203369,
      "learning_rate": 4.370192307692308e-05,
      "loss": 0.63,
      "step": 641900
    },
    {
      "epoch": 10.078492935635794,
      "grad_norm": 4.364316940307617,
      "learning_rate": 4.370094191522764e-05,
      "loss": 0.6361,
      "step": 642000
    },
    {
      "epoch": 10.080062794348509,
      "grad_norm": 3.412496566772461,
      "learning_rate": 4.369996075353218e-05,
      "loss": 0.6197,
      "step": 642100
    },
    {
      "epoch": 10.081632653061224,
      "grad_norm": 3.048153877258301,
      "learning_rate": 4.369897959183674e-05,
      "loss": 0.5872,
      "step": 642200
    },
    {
      "epoch": 10.08320251177394,
      "grad_norm": 4.458601951599121,
      "learning_rate": 4.369799843014129e-05,
      "loss": 0.6068,
      "step": 642300
    },
    {
      "epoch": 10.084772370486656,
      "grad_norm": 4.563486099243164,
      "learning_rate": 4.369701726844585e-05,
      "loss": 0.609,
      "step": 642400
    },
    {
      "epoch": 10.086342229199373,
      "grad_norm": 4.442155838012695,
      "learning_rate": 4.369603610675039e-05,
      "loss": 0.6256,
      "step": 642500
    },
    {
      "epoch": 10.087912087912088,
      "grad_norm": 3.0896575450897217,
      "learning_rate": 4.369505494505495e-05,
      "loss": 0.6294,
      "step": 642600
    },
    {
      "epoch": 10.089481946624804,
      "grad_norm": 3.393801212310791,
      "learning_rate": 4.36940737833595e-05,
      "loss": 0.6475,
      "step": 642700
    },
    {
      "epoch": 10.09105180533752,
      "grad_norm": 3.954770088195801,
      "learning_rate": 4.369309262166405e-05,
      "loss": 0.652,
      "step": 642800
    },
    {
      "epoch": 10.092621664050235,
      "grad_norm": 2.6511127948760986,
      "learning_rate": 4.36921114599686e-05,
      "loss": 0.6239,
      "step": 642900
    },
    {
      "epoch": 10.094191522762952,
      "grad_norm": 4.280567646026611,
      "learning_rate": 4.369113029827316e-05,
      "loss": 0.5853,
      "step": 643000
    },
    {
      "epoch": 10.095761381475667,
      "grad_norm": 2.5628955364227295,
      "learning_rate": 4.369014913657771e-05,
      "loss": 0.6681,
      "step": 643100
    },
    {
      "epoch": 10.097331240188383,
      "grad_norm": 3.219700574874878,
      "learning_rate": 4.368916797488226e-05,
      "loss": 0.6462,
      "step": 643200
    },
    {
      "epoch": 10.098901098901099,
      "grad_norm": 4.37423038482666,
      "learning_rate": 4.368818681318681e-05,
      "loss": 0.6587,
      "step": 643300
    },
    {
      "epoch": 10.100470957613815,
      "grad_norm": 3.4719176292419434,
      "learning_rate": 4.368720565149137e-05,
      "loss": 0.6362,
      "step": 643400
    },
    {
      "epoch": 10.10204081632653,
      "grad_norm": 4.16779899597168,
      "learning_rate": 4.3686224489795915e-05,
      "loss": 0.6776,
      "step": 643500
    },
    {
      "epoch": 10.103610675039246,
      "grad_norm": 3.865368127822876,
      "learning_rate": 4.368524332810047e-05,
      "loss": 0.61,
      "step": 643600
    },
    {
      "epoch": 10.105180533751962,
      "grad_norm": 3.3857011795043945,
      "learning_rate": 4.3684262166405024e-05,
      "loss": 0.6308,
      "step": 643700
    },
    {
      "epoch": 10.106750392464678,
      "grad_norm": 4.494075298309326,
      "learning_rate": 4.368328100470958e-05,
      "loss": 0.5906,
      "step": 643800
    },
    {
      "epoch": 10.108320251177394,
      "grad_norm": 2.8177103996276855,
      "learning_rate": 4.368229984301413e-05,
      "loss": 0.6276,
      "step": 643900
    },
    {
      "epoch": 10.10989010989011,
      "grad_norm": 3.8992927074432373,
      "learning_rate": 4.3681318681318683e-05,
      "loss": 0.6179,
      "step": 644000
    },
    {
      "epoch": 10.111459968602826,
      "grad_norm": 4.330191135406494,
      "learning_rate": 4.368033751962324e-05,
      "loss": 0.6368,
      "step": 644100
    },
    {
      "epoch": 10.113029827315541,
      "grad_norm": 4.36320686340332,
      "learning_rate": 4.3679356357927785e-05,
      "loss": 0.6833,
      "step": 644200
    },
    {
      "epoch": 10.114599686028258,
      "grad_norm": 3.297179698944092,
      "learning_rate": 4.367837519623234e-05,
      "loss": 0.6531,
      "step": 644300
    },
    {
      "epoch": 10.116169544740973,
      "grad_norm": 4.015166282653809,
      "learning_rate": 4.3677394034536894e-05,
      "loss": 0.637,
      "step": 644400
    },
    {
      "epoch": 10.117739403453688,
      "grad_norm": 5.086737155914307,
      "learning_rate": 4.367641287284145e-05,
      "loss": 0.638,
      "step": 644500
    },
    {
      "epoch": 10.119309262166405,
      "grad_norm": 4.01643180847168,
      "learning_rate": 4.3675431711145996e-05,
      "loss": 0.604,
      "step": 644600
    },
    {
      "epoch": 10.12087912087912,
      "grad_norm": 3.0884392261505127,
      "learning_rate": 4.3674450549450554e-05,
      "loss": 0.5976,
      "step": 644700
    },
    {
      "epoch": 10.122448979591837,
      "grad_norm": 4.256844520568848,
      "learning_rate": 4.3673469387755105e-05,
      "loss": 0.6358,
      "step": 644800
    },
    {
      "epoch": 10.124018838304552,
      "grad_norm": 3.63032603263855,
      "learning_rate": 4.3672488226059656e-05,
      "loss": 0.6779,
      "step": 644900
    },
    {
      "epoch": 10.12558869701727,
      "grad_norm": 3.8645379543304443,
      "learning_rate": 4.367150706436421e-05,
      "loss": 0.6325,
      "step": 645000
    },
    {
      "epoch": 10.127158555729984,
      "grad_norm": 2.815380811691284,
      "learning_rate": 4.3670525902668765e-05,
      "loss": 0.6226,
      "step": 645100
    },
    {
      "epoch": 10.1287284144427,
      "grad_norm": 3.7552683353424072,
      "learning_rate": 4.3669544740973316e-05,
      "loss": 0.6383,
      "step": 645200
    },
    {
      "epoch": 10.130298273155416,
      "grad_norm": 4.495595455169678,
      "learning_rate": 4.3668563579277867e-05,
      "loss": 0.612,
      "step": 645300
    },
    {
      "epoch": 10.131868131868131,
      "grad_norm": 3.7658627033233643,
      "learning_rate": 4.366758241758242e-05,
      "loss": 0.6259,
      "step": 645400
    },
    {
      "epoch": 10.133437990580848,
      "grad_norm": 3.365112543106079,
      "learning_rate": 4.3666601255886975e-05,
      "loss": 0.6304,
      "step": 645500
    },
    {
      "epoch": 10.135007849293563,
      "grad_norm": 4.432056903839111,
      "learning_rate": 4.366562009419152e-05,
      "loss": 0.6228,
      "step": 645600
    },
    {
      "epoch": 10.13657770800628,
      "grad_norm": 4.229884624481201,
      "learning_rate": 4.366463893249608e-05,
      "loss": 0.5966,
      "step": 645700
    },
    {
      "epoch": 10.138147566718995,
      "grad_norm": 3.7856123447418213,
      "learning_rate": 4.366365777080063e-05,
      "loss": 0.6158,
      "step": 645800
    },
    {
      "epoch": 10.13971742543171,
      "grad_norm": 4.3700714111328125,
      "learning_rate": 4.3662676609105186e-05,
      "loss": 0.6225,
      "step": 645900
    },
    {
      "epoch": 10.141287284144427,
      "grad_norm": 3.8094325065612793,
      "learning_rate": 4.366169544740974e-05,
      "loss": 0.6583,
      "step": 646000
    },
    {
      "epoch": 10.142857142857142,
      "grad_norm": 3.541121244430542,
      "learning_rate": 4.366071428571429e-05,
      "loss": 0.676,
      "step": 646100
    },
    {
      "epoch": 10.14442700156986,
      "grad_norm": 3.4867820739746094,
      "learning_rate": 4.365973312401884e-05,
      "loss": 0.5982,
      "step": 646200
    },
    {
      "epoch": 10.145996860282574,
      "grad_norm": 3.8201828002929688,
      "learning_rate": 4.365875196232339e-05,
      "loss": 0.6328,
      "step": 646300
    },
    {
      "epoch": 10.147566718995291,
      "grad_norm": 3.849961042404175,
      "learning_rate": 4.365777080062795e-05,
      "loss": 0.6208,
      "step": 646400
    },
    {
      "epoch": 10.149136577708006,
      "grad_norm": 4.757487773895264,
      "learning_rate": 4.36567896389325e-05,
      "loss": 0.5945,
      "step": 646500
    },
    {
      "epoch": 10.150706436420721,
      "grad_norm": 4.0627665519714355,
      "learning_rate": 4.3655808477237056e-05,
      "loss": 0.6479,
      "step": 646600
    },
    {
      "epoch": 10.152276295133438,
      "grad_norm": 3.174255132675171,
      "learning_rate": 4.36548273155416e-05,
      "loss": 0.6454,
      "step": 646700
    },
    {
      "epoch": 10.153846153846153,
      "grad_norm": 4.28978157043457,
      "learning_rate": 4.365384615384616e-05,
      "loss": 0.5848,
      "step": 646800
    },
    {
      "epoch": 10.15541601255887,
      "grad_norm": 3.954918384552002,
      "learning_rate": 4.365286499215071e-05,
      "loss": 0.633,
      "step": 646900
    },
    {
      "epoch": 10.156985871271585,
      "grad_norm": 4.323586463928223,
      "learning_rate": 4.365188383045526e-05,
      "loss": 0.6361,
      "step": 647000
    },
    {
      "epoch": 10.158555729984302,
      "grad_norm": 3.561560869216919,
      "learning_rate": 4.365090266875981e-05,
      "loss": 0.6057,
      "step": 647100
    },
    {
      "epoch": 10.160125588697017,
      "grad_norm": 3.6751983165740967,
      "learning_rate": 4.364992150706437e-05,
      "loss": 0.697,
      "step": 647200
    },
    {
      "epoch": 10.161695447409732,
      "grad_norm": 2.706040143966675,
      "learning_rate": 4.364894034536892e-05,
      "loss": 0.5955,
      "step": 647300
    },
    {
      "epoch": 10.16326530612245,
      "grad_norm": 3.637260675430298,
      "learning_rate": 4.364795918367347e-05,
      "loss": 0.6155,
      "step": 647400
    },
    {
      "epoch": 10.164835164835164,
      "grad_norm": 3.530586004257202,
      "learning_rate": 4.364697802197802e-05,
      "loss": 0.6277,
      "step": 647500
    },
    {
      "epoch": 10.166405023547881,
      "grad_norm": 3.9612791538238525,
      "learning_rate": 4.364599686028258e-05,
      "loss": 0.6363,
      "step": 647600
    },
    {
      "epoch": 10.167974882260596,
      "grad_norm": 4.122403621673584,
      "learning_rate": 4.3645015698587124e-05,
      "loss": 0.6477,
      "step": 647700
    },
    {
      "epoch": 10.169544740973313,
      "grad_norm": 3.091526985168457,
      "learning_rate": 4.364403453689168e-05,
      "loss": 0.6383,
      "step": 647800
    },
    {
      "epoch": 10.171114599686028,
      "grad_norm": 3.59234881401062,
      "learning_rate": 4.364305337519623e-05,
      "loss": 0.6251,
      "step": 647900
    },
    {
      "epoch": 10.172684458398743,
      "grad_norm": 4.301174163818359,
      "learning_rate": 4.364207221350079e-05,
      "loss": 0.594,
      "step": 648000
    },
    {
      "epoch": 10.17425431711146,
      "grad_norm": 4.8247809410095215,
      "learning_rate": 4.364109105180534e-05,
      "loss": 0.6473,
      "step": 648100
    },
    {
      "epoch": 10.175824175824175,
      "grad_norm": 5.0205397605896,
      "learning_rate": 4.364010989010989e-05,
      "loss": 0.6028,
      "step": 648200
    },
    {
      "epoch": 10.177394034536892,
      "grad_norm": 4.100291728973389,
      "learning_rate": 4.3639128728414443e-05,
      "loss": 0.6335,
      "step": 648300
    },
    {
      "epoch": 10.178963893249607,
      "grad_norm": 3.1887402534484863,
      "learning_rate": 4.3638147566718994e-05,
      "loss": 0.6327,
      "step": 648400
    },
    {
      "epoch": 10.180533751962324,
      "grad_norm": 4.259189128875732,
      "learning_rate": 4.363716640502355e-05,
      "loss": 0.6239,
      "step": 648500
    },
    {
      "epoch": 10.182103610675039,
      "grad_norm": 3.3611552715301514,
      "learning_rate": 4.36361852433281e-05,
      "loss": 0.6457,
      "step": 648600
    },
    {
      "epoch": 10.183673469387756,
      "grad_norm": 4.678618907928467,
      "learning_rate": 4.3635204081632654e-05,
      "loss": 0.6226,
      "step": 648700
    },
    {
      "epoch": 10.185243328100471,
      "grad_norm": 3.345628499984741,
      "learning_rate": 4.3634222919937205e-05,
      "loss": 0.633,
      "step": 648800
    },
    {
      "epoch": 10.186813186813186,
      "grad_norm": 4.037337779998779,
      "learning_rate": 4.363324175824176e-05,
      "loss": 0.6069,
      "step": 648900
    },
    {
      "epoch": 10.188383045525903,
      "grad_norm": 3.964617967605591,
      "learning_rate": 4.3632260596546314e-05,
      "loss": 0.6188,
      "step": 649000
    },
    {
      "epoch": 10.189952904238618,
      "grad_norm": 5.154536247253418,
      "learning_rate": 4.3631279434850865e-05,
      "loss": 0.6239,
      "step": 649100
    },
    {
      "epoch": 10.191522762951335,
      "grad_norm": 4.192240238189697,
      "learning_rate": 4.3630298273155416e-05,
      "loss": 0.6142,
      "step": 649200
    },
    {
      "epoch": 10.19309262166405,
      "grad_norm": 4.075326919555664,
      "learning_rate": 4.3629317111459974e-05,
      "loss": 0.6156,
      "step": 649300
    },
    {
      "epoch": 10.194662480376767,
      "grad_norm": 3.6886520385742188,
      "learning_rate": 4.362833594976452e-05,
      "loss": 0.6065,
      "step": 649400
    },
    {
      "epoch": 10.196232339089482,
      "grad_norm": 4.824091911315918,
      "learning_rate": 4.3627354788069076e-05,
      "loss": 0.6251,
      "step": 649500
    },
    {
      "epoch": 10.197802197802197,
      "grad_norm": 3.8920950889587402,
      "learning_rate": 4.3626373626373626e-05,
      "loss": 0.6245,
      "step": 649600
    },
    {
      "epoch": 10.199372056514914,
      "grad_norm": 3.3733296394348145,
      "learning_rate": 4.3625392464678184e-05,
      "loss": 0.6063,
      "step": 649700
    },
    {
      "epoch": 10.200941915227629,
      "grad_norm": 3.6511521339416504,
      "learning_rate": 4.362441130298273e-05,
      "loss": 0.6025,
      "step": 649800
    },
    {
      "epoch": 10.202511773940346,
      "grad_norm": 3.191847324371338,
      "learning_rate": 4.3623430141287286e-05,
      "loss": 0.6145,
      "step": 649900
    },
    {
      "epoch": 10.204081632653061,
      "grad_norm": 4.5242228507995605,
      "learning_rate": 4.362244897959184e-05,
      "loss": 0.6212,
      "step": 650000
    },
    {
      "epoch": 10.205651491365778,
      "grad_norm": 3.5792880058288574,
      "learning_rate": 4.362146781789639e-05,
      "loss": 0.6079,
      "step": 650100
    },
    {
      "epoch": 10.207221350078493,
      "grad_norm": 4.386048793792725,
      "learning_rate": 4.3620486656200946e-05,
      "loss": 0.6374,
      "step": 650200
    },
    {
      "epoch": 10.208791208791208,
      "grad_norm": 2.5876667499542236,
      "learning_rate": 4.36195054945055e-05,
      "loss": 0.6453,
      "step": 650300
    },
    {
      "epoch": 10.210361067503925,
      "grad_norm": 3.188455581665039,
      "learning_rate": 4.361852433281005e-05,
      "loss": 0.6378,
      "step": 650400
    },
    {
      "epoch": 10.21193092621664,
      "grad_norm": 3.707012176513672,
      "learning_rate": 4.36175431711146e-05,
      "loss": 0.6286,
      "step": 650500
    },
    {
      "epoch": 10.213500784929357,
      "grad_norm": 3.9178452491760254,
      "learning_rate": 4.361656200941916e-05,
      "loss": 0.6284,
      "step": 650600
    },
    {
      "epoch": 10.215070643642072,
      "grad_norm": 4.845878601074219,
      "learning_rate": 4.361558084772371e-05,
      "loss": 0.6245,
      "step": 650700
    },
    {
      "epoch": 10.216640502354789,
      "grad_norm": 3.2748606204986572,
      "learning_rate": 4.361459968602826e-05,
      "loss": 0.6255,
      "step": 650800
    },
    {
      "epoch": 10.218210361067504,
      "grad_norm": 3.287710189819336,
      "learning_rate": 4.361361852433281e-05,
      "loss": 0.6269,
      "step": 650900
    },
    {
      "epoch": 10.219780219780219,
      "grad_norm": 3.5088744163513184,
      "learning_rate": 4.361263736263737e-05,
      "loss": 0.6308,
      "step": 651000
    },
    {
      "epoch": 10.221350078492936,
      "grad_norm": 3.465195417404175,
      "learning_rate": 4.361165620094192e-05,
      "loss": 0.6394,
      "step": 651100
    },
    {
      "epoch": 10.222919937205651,
      "grad_norm": 4.630601406097412,
      "learning_rate": 4.361067503924647e-05,
      "loss": 0.6669,
      "step": 651200
    },
    {
      "epoch": 10.224489795918368,
      "grad_norm": 4.810345649719238,
      "learning_rate": 4.360969387755102e-05,
      "loss": 0.6596,
      "step": 651300
    },
    {
      "epoch": 10.226059654631083,
      "grad_norm": 3.762697458267212,
      "learning_rate": 4.360871271585558e-05,
      "loss": 0.5905,
      "step": 651400
    },
    {
      "epoch": 10.2276295133438,
      "grad_norm": 4.454661846160889,
      "learning_rate": 4.360773155416012e-05,
      "loss": 0.6034,
      "step": 651500
    },
    {
      "epoch": 10.229199372056515,
      "grad_norm": 3.705216646194458,
      "learning_rate": 4.360675039246468e-05,
      "loss": 0.6182,
      "step": 651600
    },
    {
      "epoch": 10.23076923076923,
      "grad_norm": 3.4234001636505127,
      "learning_rate": 4.360576923076923e-05,
      "loss": 0.6374,
      "step": 651700
    },
    {
      "epoch": 10.232339089481947,
      "grad_norm": 4.108715057373047,
      "learning_rate": 4.360478806907379e-05,
      "loss": 0.6218,
      "step": 651800
    },
    {
      "epoch": 10.233908948194662,
      "grad_norm": 3.0273003578186035,
      "learning_rate": 4.360380690737833e-05,
      "loss": 0.6411,
      "step": 651900
    },
    {
      "epoch": 10.235478806907379,
      "grad_norm": 4.256343364715576,
      "learning_rate": 4.360282574568289e-05,
      "loss": 0.6173,
      "step": 652000
    },
    {
      "epoch": 10.237048665620094,
      "grad_norm": 3.6468541622161865,
      "learning_rate": 4.360184458398744e-05,
      "loss": 0.6521,
      "step": 652100
    },
    {
      "epoch": 10.23861852433281,
      "grad_norm": 4.343191146850586,
      "learning_rate": 4.360086342229199e-05,
      "loss": 0.5961,
      "step": 652200
    },
    {
      "epoch": 10.240188383045526,
      "grad_norm": 4.476140975952148,
      "learning_rate": 4.359988226059655e-05,
      "loss": 0.6667,
      "step": 652300
    },
    {
      "epoch": 10.241758241758241,
      "grad_norm": 4.351665496826172,
      "learning_rate": 4.35989010989011e-05,
      "loss": 0.6253,
      "step": 652400
    },
    {
      "epoch": 10.243328100470958,
      "grad_norm": 4.375169277191162,
      "learning_rate": 4.359791993720565e-05,
      "loss": 0.6185,
      "step": 652500
    },
    {
      "epoch": 10.244897959183673,
      "grad_norm": 4.433133602142334,
      "learning_rate": 4.35969387755102e-05,
      "loss": 0.6372,
      "step": 652600
    },
    {
      "epoch": 10.24646781789639,
      "grad_norm": 4.475712299346924,
      "learning_rate": 4.359595761381476e-05,
      "loss": 0.62,
      "step": 652700
    },
    {
      "epoch": 10.248037676609105,
      "grad_norm": 4.367974281311035,
      "learning_rate": 4.359497645211931e-05,
      "loss": 0.6341,
      "step": 652800
    },
    {
      "epoch": 10.249607535321822,
      "grad_norm": 3.9385783672332764,
      "learning_rate": 4.359399529042386e-05,
      "loss": 0.6363,
      "step": 652900
    },
    {
      "epoch": 10.251177394034537,
      "grad_norm": 4.293942451477051,
      "learning_rate": 4.3593014128728414e-05,
      "loss": 0.6698,
      "step": 653000
    },
    {
      "epoch": 10.252747252747252,
      "grad_norm": 4.130006313323975,
      "learning_rate": 4.359203296703297e-05,
      "loss": 0.6713,
      "step": 653100
    },
    {
      "epoch": 10.254317111459969,
      "grad_norm": 4.556601047515869,
      "learning_rate": 4.359105180533752e-05,
      "loss": 0.6224,
      "step": 653200
    },
    {
      "epoch": 10.255886970172684,
      "grad_norm": 5.11386775970459,
      "learning_rate": 4.3590070643642074e-05,
      "loss": 0.6084,
      "step": 653300
    },
    {
      "epoch": 10.2574568288854,
      "grad_norm": 4.08015775680542,
      "learning_rate": 4.3589089481946625e-05,
      "loss": 0.6168,
      "step": 653400
    },
    {
      "epoch": 10.259026687598116,
      "grad_norm": 3.9018757343292236,
      "learning_rate": 4.358810832025118e-05,
      "loss": 0.6226,
      "step": 653500
    },
    {
      "epoch": 10.260596546310833,
      "grad_norm": 4.004836082458496,
      "learning_rate": 4.358712715855573e-05,
      "loss": 0.6636,
      "step": 653600
    },
    {
      "epoch": 10.262166405023548,
      "grad_norm": 2.7876670360565186,
      "learning_rate": 4.3586145996860284e-05,
      "loss": 0.5758,
      "step": 653700
    },
    {
      "epoch": 10.263736263736265,
      "grad_norm": 3.1323294639587402,
      "learning_rate": 4.3585164835164835e-05,
      "loss": 0.6197,
      "step": 653800
    },
    {
      "epoch": 10.26530612244898,
      "grad_norm": 4.649648666381836,
      "learning_rate": 4.358418367346939e-05,
      "loss": 0.6551,
      "step": 653900
    },
    {
      "epoch": 10.266875981161695,
      "grad_norm": 3.4479823112487793,
      "learning_rate": 4.358320251177394e-05,
      "loss": 0.7043,
      "step": 654000
    },
    {
      "epoch": 10.268445839874412,
      "grad_norm": 4.290175914764404,
      "learning_rate": 4.3582221350078495e-05,
      "loss": 0.6747,
      "step": 654100
    },
    {
      "epoch": 10.270015698587127,
      "grad_norm": 3.137406587600708,
      "learning_rate": 4.3581240188383046e-05,
      "loss": 0.6329,
      "step": 654200
    },
    {
      "epoch": 10.271585557299844,
      "grad_norm": 4.585862636566162,
      "learning_rate": 4.35802590266876e-05,
      "loss": 0.6448,
      "step": 654300
    },
    {
      "epoch": 10.273155416012559,
      "grad_norm": 4.201182842254639,
      "learning_rate": 4.3579277864992155e-05,
      "loss": 0.592,
      "step": 654400
    },
    {
      "epoch": 10.274725274725276,
      "grad_norm": 3.8405206203460693,
      "learning_rate": 4.3578296703296706e-05,
      "loss": 0.6822,
      "step": 654500
    },
    {
      "epoch": 10.27629513343799,
      "grad_norm": 5.563170433044434,
      "learning_rate": 4.357731554160126e-05,
      "loss": 0.6469,
      "step": 654600
    },
    {
      "epoch": 10.277864992150706,
      "grad_norm": 3.6458301544189453,
      "learning_rate": 4.357633437990581e-05,
      "loss": 0.6546,
      "step": 654700
    },
    {
      "epoch": 10.279434850863423,
      "grad_norm": 4.421452045440674,
      "learning_rate": 4.3575353218210366e-05,
      "loss": 0.6546,
      "step": 654800
    },
    {
      "epoch": 10.281004709576138,
      "grad_norm": 4.2278618812561035,
      "learning_rate": 4.3574372056514917e-05,
      "loss": 0.6155,
      "step": 654900
    },
    {
      "epoch": 10.282574568288855,
      "grad_norm": 3.289816379547119,
      "learning_rate": 4.357339089481947e-05,
      "loss": 0.5943,
      "step": 655000
    },
    {
      "epoch": 10.28414442700157,
      "grad_norm": 4.824111461639404,
      "learning_rate": 4.357240973312402e-05,
      "loss": 0.6143,
      "step": 655100
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 4.203465938568115,
      "learning_rate": 4.3571428571428576e-05,
      "loss": 0.6642,
      "step": 655200
    },
    {
      "epoch": 10.287284144427002,
      "grad_norm": 3.4651741981506348,
      "learning_rate": 4.357044740973313e-05,
      "loss": 0.6107,
      "step": 655300
    },
    {
      "epoch": 10.288854003139717,
      "grad_norm": 4.629232883453369,
      "learning_rate": 4.356946624803768e-05,
      "loss": 0.6358,
      "step": 655400
    },
    {
      "epoch": 10.290423861852434,
      "grad_norm": 4.167332649230957,
      "learning_rate": 4.356848508634223e-05,
      "loss": 0.677,
      "step": 655500
    },
    {
      "epoch": 10.291993720565149,
      "grad_norm": 4.145299434661865,
      "learning_rate": 4.356750392464679e-05,
      "loss": 0.6118,
      "step": 655600
    },
    {
      "epoch": 10.293563579277865,
      "grad_norm": 3.8592357635498047,
      "learning_rate": 4.356652276295133e-05,
      "loss": 0.6258,
      "step": 655700
    },
    {
      "epoch": 10.29513343799058,
      "grad_norm": 3.4004411697387695,
      "learning_rate": 4.356554160125589e-05,
      "loss": 0.6294,
      "step": 655800
    },
    {
      "epoch": 10.296703296703297,
      "grad_norm": 3.6246142387390137,
      "learning_rate": 4.356456043956044e-05,
      "loss": 0.6488,
      "step": 655900
    },
    {
      "epoch": 10.298273155416013,
      "grad_norm": 4.002788543701172,
      "learning_rate": 4.3563579277865e-05,
      "loss": 0.636,
      "step": 656000
    },
    {
      "epoch": 10.299843014128728,
      "grad_norm": 3.196310043334961,
      "learning_rate": 4.356259811616954e-05,
      "loss": 0.6716,
      "step": 656100
    },
    {
      "epoch": 10.301412872841444,
      "grad_norm": 3.5426135063171387,
      "learning_rate": 4.35616169544741e-05,
      "loss": 0.6354,
      "step": 656200
    },
    {
      "epoch": 10.30298273155416,
      "grad_norm": 3.6830453872680664,
      "learning_rate": 4.356063579277865e-05,
      "loss": 0.6175,
      "step": 656300
    },
    {
      "epoch": 10.304552590266876,
      "grad_norm": 4.147963047027588,
      "learning_rate": 4.35596546310832e-05,
      "loss": 0.6408,
      "step": 656400
    },
    {
      "epoch": 10.306122448979592,
      "grad_norm": 3.837913990020752,
      "learning_rate": 4.355867346938776e-05,
      "loss": 0.6113,
      "step": 656500
    },
    {
      "epoch": 10.307692307692308,
      "grad_norm": 3.4910099506378174,
      "learning_rate": 4.355769230769231e-05,
      "loss": 0.6428,
      "step": 656600
    },
    {
      "epoch": 10.309262166405023,
      "grad_norm": 3.531099796295166,
      "learning_rate": 4.355671114599686e-05,
      "loss": 0.6816,
      "step": 656700
    },
    {
      "epoch": 10.310832025117739,
      "grad_norm": 3.6047918796539307,
      "learning_rate": 4.355572998430141e-05,
      "loss": 0.6259,
      "step": 656800
    },
    {
      "epoch": 10.312401883830455,
      "grad_norm": 3.087787389755249,
      "learning_rate": 4.355474882260597e-05,
      "loss": 0.6429,
      "step": 656900
    },
    {
      "epoch": 10.31397174254317,
      "grad_norm": 4.6287994384765625,
      "learning_rate": 4.355376766091052e-05,
      "loss": 0.6358,
      "step": 657000
    },
    {
      "epoch": 10.315541601255887,
      "grad_norm": 4.490466117858887,
      "learning_rate": 4.355278649921507e-05,
      "loss": 0.6166,
      "step": 657100
    },
    {
      "epoch": 10.317111459968602,
      "grad_norm": 5.309512138366699,
      "learning_rate": 4.355180533751962e-05,
      "loss": 0.6262,
      "step": 657200
    },
    {
      "epoch": 10.31868131868132,
      "grad_norm": 3.165591239929199,
      "learning_rate": 4.355082417582418e-05,
      "loss": 0.5993,
      "step": 657300
    },
    {
      "epoch": 10.320251177394034,
      "grad_norm": 4.0667338371276855,
      "learning_rate": 4.354984301412873e-05,
      "loss": 0.637,
      "step": 657400
    },
    {
      "epoch": 10.321821036106751,
      "grad_norm": 4.569896697998047,
      "learning_rate": 4.354886185243328e-05,
      "loss": 0.6505,
      "step": 657500
    },
    {
      "epoch": 10.323390894819466,
      "grad_norm": 4.503228187561035,
      "learning_rate": 4.3547880690737834e-05,
      "loss": 0.6085,
      "step": 657600
    },
    {
      "epoch": 10.324960753532181,
      "grad_norm": 3.9994254112243652,
      "learning_rate": 4.354689952904239e-05,
      "loss": 0.5769,
      "step": 657700
    },
    {
      "epoch": 10.326530612244898,
      "grad_norm": 2.3925721645355225,
      "learning_rate": 4.3545918367346936e-05,
      "loss": 0.6454,
      "step": 657800
    },
    {
      "epoch": 10.328100470957613,
      "grad_norm": 5.228300094604492,
      "learning_rate": 4.3544937205651493e-05,
      "loss": 0.6043,
      "step": 657900
    },
    {
      "epoch": 10.32967032967033,
      "grad_norm": 3.9920711517333984,
      "learning_rate": 4.3543956043956044e-05,
      "loss": 0.6456,
      "step": 658000
    },
    {
      "epoch": 10.331240188383045,
      "grad_norm": 4.172553539276123,
      "learning_rate": 4.35429748822606e-05,
      "loss": 0.6382,
      "step": 658100
    },
    {
      "epoch": 10.332810047095762,
      "grad_norm": 2.887925863265991,
      "learning_rate": 4.3541993720565146e-05,
      "loss": 0.6278,
      "step": 658200
    },
    {
      "epoch": 10.334379905808477,
      "grad_norm": 3.8533754348754883,
      "learning_rate": 4.3541012558869704e-05,
      "loss": 0.6139,
      "step": 658300
    },
    {
      "epoch": 10.335949764521192,
      "grad_norm": 3.653911828994751,
      "learning_rate": 4.3540031397174255e-05,
      "loss": 0.6393,
      "step": 658400
    },
    {
      "epoch": 10.33751962323391,
      "grad_norm": 4.198744297027588,
      "learning_rate": 4.3539050235478806e-05,
      "loss": 0.6391,
      "step": 658500
    },
    {
      "epoch": 10.339089481946624,
      "grad_norm": 1.878326416015625,
      "learning_rate": 4.3538069073783364e-05,
      "loss": 0.615,
      "step": 658600
    },
    {
      "epoch": 10.340659340659341,
      "grad_norm": 3.4511470794677734,
      "learning_rate": 4.3537087912087915e-05,
      "loss": 0.6423,
      "step": 658700
    },
    {
      "epoch": 10.342229199372056,
      "grad_norm": 4.167327880859375,
      "learning_rate": 4.3536106750392466e-05,
      "loss": 0.6423,
      "step": 658800
    },
    {
      "epoch": 10.343799058084773,
      "grad_norm": 3.354036569595337,
      "learning_rate": 4.353512558869702e-05,
      "loss": 0.5887,
      "step": 658900
    },
    {
      "epoch": 10.345368916797488,
      "grad_norm": 2.8741931915283203,
      "learning_rate": 4.3534144427001575e-05,
      "loss": 0.6186,
      "step": 659000
    },
    {
      "epoch": 10.346938775510203,
      "grad_norm": 4.541380405426025,
      "learning_rate": 4.3533163265306126e-05,
      "loss": 0.6369,
      "step": 659100
    },
    {
      "epoch": 10.34850863422292,
      "grad_norm": 4.341525554656982,
      "learning_rate": 4.3532182103610677e-05,
      "loss": 0.6078,
      "step": 659200
    },
    {
      "epoch": 10.350078492935635,
      "grad_norm": 4.053306579589844,
      "learning_rate": 4.353120094191523e-05,
      "loss": 0.6161,
      "step": 659300
    },
    {
      "epoch": 10.351648351648352,
      "grad_norm": 4.683150768280029,
      "learning_rate": 4.3530219780219785e-05,
      "loss": 0.6371,
      "step": 659400
    },
    {
      "epoch": 10.353218210361067,
      "grad_norm": 4.084310531616211,
      "learning_rate": 4.3529238618524336e-05,
      "loss": 0.6684,
      "step": 659500
    },
    {
      "epoch": 10.354788069073784,
      "grad_norm": 4.851578712463379,
      "learning_rate": 4.352825745682889e-05,
      "loss": 0.6646,
      "step": 659600
    },
    {
      "epoch": 10.3563579277865,
      "grad_norm": 4.503328800201416,
      "learning_rate": 4.352727629513344e-05,
      "loss": 0.6284,
      "step": 659700
    },
    {
      "epoch": 10.357927786499214,
      "grad_norm": 2.82024884223938,
      "learning_rate": 4.3526295133437996e-05,
      "loss": 0.6328,
      "step": 659800
    },
    {
      "epoch": 10.359497645211931,
      "grad_norm": 3.740783452987671,
      "learning_rate": 4.352531397174254e-05,
      "loss": 0.6408,
      "step": 659900
    },
    {
      "epoch": 10.361067503924646,
      "grad_norm": 4.603211402893066,
      "learning_rate": 4.35243328100471e-05,
      "loss": 0.6482,
      "step": 660000
    },
    {
      "epoch": 10.362637362637363,
      "grad_norm": 5.046565055847168,
      "learning_rate": 4.352335164835165e-05,
      "loss": 0.6736,
      "step": 660100
    },
    {
      "epoch": 10.364207221350078,
      "grad_norm": 4.221959590911865,
      "learning_rate": 4.352237048665621e-05,
      "loss": 0.6289,
      "step": 660200
    },
    {
      "epoch": 10.365777080062795,
      "grad_norm": 3.8484325408935547,
      "learning_rate": 4.352138932496075e-05,
      "loss": 0.6662,
      "step": 660300
    },
    {
      "epoch": 10.36734693877551,
      "grad_norm": 2.515716314315796,
      "learning_rate": 4.352040816326531e-05,
      "loss": 0.6472,
      "step": 660400
    },
    {
      "epoch": 10.368916797488225,
      "grad_norm": 3.520444393157959,
      "learning_rate": 4.351942700156986e-05,
      "loss": 0.5947,
      "step": 660500
    },
    {
      "epoch": 10.370486656200942,
      "grad_norm": 4.263891696929932,
      "learning_rate": 4.351844583987441e-05,
      "loss": 0.6731,
      "step": 660600
    },
    {
      "epoch": 10.372056514913657,
      "grad_norm": 3.474834680557251,
      "learning_rate": 4.351746467817897e-05,
      "loss": 0.6431,
      "step": 660700
    },
    {
      "epoch": 10.373626373626374,
      "grad_norm": 4.445960998535156,
      "learning_rate": 4.351648351648352e-05,
      "loss": 0.6375,
      "step": 660800
    },
    {
      "epoch": 10.37519623233909,
      "grad_norm": 4.4935407638549805,
      "learning_rate": 4.351550235478807e-05,
      "loss": 0.6179,
      "step": 660900
    },
    {
      "epoch": 10.376766091051806,
      "grad_norm": 3.7447550296783447,
      "learning_rate": 4.351452119309262e-05,
      "loss": 0.6493,
      "step": 661000
    },
    {
      "epoch": 10.378335949764521,
      "grad_norm": 3.9949238300323486,
      "learning_rate": 4.351354003139718e-05,
      "loss": 0.6443,
      "step": 661100
    },
    {
      "epoch": 10.379905808477236,
      "grad_norm": 2.9979307651519775,
      "learning_rate": 4.351255886970173e-05,
      "loss": 0.6582,
      "step": 661200
    },
    {
      "epoch": 10.381475667189953,
      "grad_norm": 4.138557434082031,
      "learning_rate": 4.351157770800628e-05,
      "loss": 0.6738,
      "step": 661300
    },
    {
      "epoch": 10.383045525902668,
      "grad_norm": 3.345564603805542,
      "learning_rate": 4.351059654631083e-05,
      "loss": 0.6266,
      "step": 661400
    },
    {
      "epoch": 10.384615384615385,
      "grad_norm": 3.335606098175049,
      "learning_rate": 4.350961538461539e-05,
      "loss": 0.6365,
      "step": 661500
    },
    {
      "epoch": 10.3861852433281,
      "grad_norm": 3.3958864212036133,
      "learning_rate": 4.350863422291994e-05,
      "loss": 0.6031,
      "step": 661600
    },
    {
      "epoch": 10.387755102040817,
      "grad_norm": 3.811444044113159,
      "learning_rate": 4.350765306122449e-05,
      "loss": 0.6504,
      "step": 661700
    },
    {
      "epoch": 10.389324960753532,
      "grad_norm": 5.552262783050537,
      "learning_rate": 4.350667189952904e-05,
      "loss": 0.6322,
      "step": 661800
    },
    {
      "epoch": 10.390894819466247,
      "grad_norm": 4.1282477378845215,
      "learning_rate": 4.35056907378336e-05,
      "loss": 0.664,
      "step": 661900
    },
    {
      "epoch": 10.392464678178964,
      "grad_norm": 3.4871747493743896,
      "learning_rate": 4.3504709576138145e-05,
      "loss": 0.6642,
      "step": 662000
    },
    {
      "epoch": 10.394034536891679,
      "grad_norm": 4.192725658416748,
      "learning_rate": 4.35037284144427e-05,
      "loss": 0.6476,
      "step": 662100
    },
    {
      "epoch": 10.395604395604396,
      "grad_norm": 4.1357645988464355,
      "learning_rate": 4.3502747252747253e-05,
      "loss": 0.6351,
      "step": 662200
    },
    {
      "epoch": 10.397174254317111,
      "grad_norm": 4.086609363555908,
      "learning_rate": 4.350176609105181e-05,
      "loss": 0.6596,
      "step": 662300
    },
    {
      "epoch": 10.398744113029828,
      "grad_norm": 3.8148579597473145,
      "learning_rate": 4.3500784929356355e-05,
      "loss": 0.6738,
      "step": 662400
    },
    {
      "epoch": 10.400313971742543,
      "grad_norm": 3.5314948558807373,
      "learning_rate": 4.349980376766091e-05,
      "loss": 0.6143,
      "step": 662500
    },
    {
      "epoch": 10.40188383045526,
      "grad_norm": 3.6713595390319824,
      "learning_rate": 4.3498822605965464e-05,
      "loss": 0.6095,
      "step": 662600
    },
    {
      "epoch": 10.403453689167975,
      "grad_norm": 4.009807586669922,
      "learning_rate": 4.3497841444270015e-05,
      "loss": 0.64,
      "step": 662700
    },
    {
      "epoch": 10.40502354788069,
      "grad_norm": 4.351227283477783,
      "learning_rate": 4.349686028257457e-05,
      "loss": 0.6181,
      "step": 662800
    },
    {
      "epoch": 10.406593406593407,
      "grad_norm": 6.548427581787109,
      "learning_rate": 4.3495879120879124e-05,
      "loss": 0.6451,
      "step": 662900
    },
    {
      "epoch": 10.408163265306122,
      "grad_norm": 3.1452274322509766,
      "learning_rate": 4.3494897959183675e-05,
      "loss": 0.6545,
      "step": 663000
    },
    {
      "epoch": 10.409733124018839,
      "grad_norm": 3.8799996376037598,
      "learning_rate": 4.3493916797488226e-05,
      "loss": 0.6305,
      "step": 663100
    },
    {
      "epoch": 10.411302982731554,
      "grad_norm": 3.403093099594116,
      "learning_rate": 4.3492935635792784e-05,
      "loss": 0.6479,
      "step": 663200
    },
    {
      "epoch": 10.41287284144427,
      "grad_norm": 3.7018725872039795,
      "learning_rate": 4.3491954474097335e-05,
      "loss": 0.6577,
      "step": 663300
    },
    {
      "epoch": 10.414442700156986,
      "grad_norm": 3.1177923679351807,
      "learning_rate": 4.3490973312401886e-05,
      "loss": 0.603,
      "step": 663400
    },
    {
      "epoch": 10.416012558869701,
      "grad_norm": 3.1347434520721436,
      "learning_rate": 4.3489992150706437e-05,
      "loss": 0.6513,
      "step": 663500
    },
    {
      "epoch": 10.417582417582418,
      "grad_norm": 3.8409032821655273,
      "learning_rate": 4.3489010989010994e-05,
      "loss": 0.6255,
      "step": 663600
    },
    {
      "epoch": 10.419152276295133,
      "grad_norm": 4.318478107452393,
      "learning_rate": 4.3488029827315545e-05,
      "loss": 0.6352,
      "step": 663700
    },
    {
      "epoch": 10.42072213500785,
      "grad_norm": 3.9586329460144043,
      "learning_rate": 4.3487048665620096e-05,
      "loss": 0.6081,
      "step": 663800
    },
    {
      "epoch": 10.422291993720565,
      "grad_norm": 2.3825843334198,
      "learning_rate": 4.348606750392465e-05,
      "loss": 0.6045,
      "step": 663900
    },
    {
      "epoch": 10.423861852433282,
      "grad_norm": 3.501567840576172,
      "learning_rate": 4.3485086342229205e-05,
      "loss": 0.6504,
      "step": 664000
    },
    {
      "epoch": 10.425431711145997,
      "grad_norm": 4.0694379806518555,
      "learning_rate": 4.348410518053375e-05,
      "loss": 0.614,
      "step": 664100
    },
    {
      "epoch": 10.427001569858712,
      "grad_norm": 2.853102922439575,
      "learning_rate": 4.348312401883831e-05,
      "loss": 0.5981,
      "step": 664200
    },
    {
      "epoch": 10.428571428571429,
      "grad_norm": 2.9663851261138916,
      "learning_rate": 4.348214285714286e-05,
      "loss": 0.5939,
      "step": 664300
    },
    {
      "epoch": 10.430141287284144,
      "grad_norm": 5.261198997497559,
      "learning_rate": 4.3481161695447416e-05,
      "loss": 0.648,
      "step": 664400
    },
    {
      "epoch": 10.43171114599686,
      "grad_norm": 3.3475124835968018,
      "learning_rate": 4.348018053375196e-05,
      "loss": 0.6079,
      "step": 664500
    },
    {
      "epoch": 10.433281004709576,
      "grad_norm": 4.451969623565674,
      "learning_rate": 4.347919937205652e-05,
      "loss": 0.6008,
      "step": 664600
    },
    {
      "epoch": 10.434850863422293,
      "grad_norm": 3.8195090293884277,
      "learning_rate": 4.347821821036107e-05,
      "loss": 0.6038,
      "step": 664700
    },
    {
      "epoch": 10.436420722135008,
      "grad_norm": 2.4455370903015137,
      "learning_rate": 4.347723704866562e-05,
      "loss": 0.6403,
      "step": 664800
    },
    {
      "epoch": 10.437990580847723,
      "grad_norm": 4.433640003204346,
      "learning_rate": 4.347625588697018e-05,
      "loss": 0.6412,
      "step": 664900
    },
    {
      "epoch": 10.43956043956044,
      "grad_norm": 3.4670939445495605,
      "learning_rate": 4.347527472527473e-05,
      "loss": 0.6323,
      "step": 665000
    },
    {
      "epoch": 10.441130298273155,
      "grad_norm": 4.413777828216553,
      "learning_rate": 4.347429356357928e-05,
      "loss": 0.606,
      "step": 665100
    },
    {
      "epoch": 10.442700156985872,
      "grad_norm": 4.171713829040527,
      "learning_rate": 4.347331240188383e-05,
      "loss": 0.6312,
      "step": 665200
    },
    {
      "epoch": 10.444270015698587,
      "grad_norm": 3.8602540493011475,
      "learning_rate": 4.347233124018839e-05,
      "loss": 0.5926,
      "step": 665300
    },
    {
      "epoch": 10.445839874411304,
      "grad_norm": 3.2415285110473633,
      "learning_rate": 4.347135007849294e-05,
      "loss": 0.651,
      "step": 665400
    },
    {
      "epoch": 10.447409733124019,
      "grad_norm": 3.4347801208496094,
      "learning_rate": 4.347036891679749e-05,
      "loss": 0.632,
      "step": 665500
    },
    {
      "epoch": 10.448979591836734,
      "grad_norm": 4.092181205749512,
      "learning_rate": 4.346938775510204e-05,
      "loss": 0.6237,
      "step": 665600
    },
    {
      "epoch": 10.45054945054945,
      "grad_norm": 4.08157205581665,
      "learning_rate": 4.34684065934066e-05,
      "loss": 0.6256,
      "step": 665700
    },
    {
      "epoch": 10.452119309262166,
      "grad_norm": 4.293157577514648,
      "learning_rate": 4.346742543171115e-05,
      "loss": 0.6498,
      "step": 665800
    },
    {
      "epoch": 10.453689167974883,
      "grad_norm": 3.1969594955444336,
      "learning_rate": 4.34664442700157e-05,
      "loss": 0.6311,
      "step": 665900
    },
    {
      "epoch": 10.455259026687598,
      "grad_norm": 3.723771572113037,
      "learning_rate": 4.346546310832025e-05,
      "loss": 0.6718,
      "step": 666000
    },
    {
      "epoch": 10.456828885400315,
      "grad_norm": 3.2788898944854736,
      "learning_rate": 4.346448194662481e-05,
      "loss": 0.6213,
      "step": 666100
    },
    {
      "epoch": 10.45839874411303,
      "grad_norm": 3.8528661727905273,
      "learning_rate": 4.3463500784929354e-05,
      "loss": 0.646,
      "step": 666200
    },
    {
      "epoch": 10.459968602825747,
      "grad_norm": 4.298381328582764,
      "learning_rate": 4.346251962323391e-05,
      "loss": 0.6552,
      "step": 666300
    },
    {
      "epoch": 10.461538461538462,
      "grad_norm": 3.6545157432556152,
      "learning_rate": 4.346153846153846e-05,
      "loss": 0.6252,
      "step": 666400
    },
    {
      "epoch": 10.463108320251177,
      "grad_norm": 2.9201860427856445,
      "learning_rate": 4.346055729984302e-05,
      "loss": 0.6376,
      "step": 666500
    },
    {
      "epoch": 10.464678178963894,
      "grad_norm": 4.0623345375061035,
      "learning_rate": 4.3459576138147564e-05,
      "loss": 0.6502,
      "step": 666600
    },
    {
      "epoch": 10.466248037676609,
      "grad_norm": 3.0707273483276367,
      "learning_rate": 4.345859497645212e-05,
      "loss": 0.647,
      "step": 666700
    },
    {
      "epoch": 10.467817896389326,
      "grad_norm": 4.526501655578613,
      "learning_rate": 4.345761381475667e-05,
      "loss": 0.6202,
      "step": 666800
    },
    {
      "epoch": 10.46938775510204,
      "grad_norm": 5.028892517089844,
      "learning_rate": 4.3456632653061224e-05,
      "loss": 0.6372,
      "step": 666900
    },
    {
      "epoch": 10.470957613814758,
      "grad_norm": 5.290232181549072,
      "learning_rate": 4.345565149136578e-05,
      "loss": 0.62,
      "step": 667000
    },
    {
      "epoch": 10.472527472527473,
      "grad_norm": 4.249442100524902,
      "learning_rate": 4.345467032967033e-05,
      "loss": 0.6352,
      "step": 667100
    },
    {
      "epoch": 10.474097331240188,
      "grad_norm": 3.607173204421997,
      "learning_rate": 4.3453689167974884e-05,
      "loss": 0.6551,
      "step": 667200
    },
    {
      "epoch": 10.475667189952905,
      "grad_norm": 5.38084602355957,
      "learning_rate": 4.3452708006279435e-05,
      "loss": 0.631,
      "step": 667300
    },
    {
      "epoch": 10.47723704866562,
      "grad_norm": 4.380521774291992,
      "learning_rate": 4.345172684458399e-05,
      "loss": 0.6501,
      "step": 667400
    },
    {
      "epoch": 10.478806907378337,
      "grad_norm": 3.821225166320801,
      "learning_rate": 4.3450745682888544e-05,
      "loss": 0.6173,
      "step": 667500
    },
    {
      "epoch": 10.480376766091052,
      "grad_norm": 3.461049795150757,
      "learning_rate": 4.3449764521193095e-05,
      "loss": 0.6255,
      "step": 667600
    },
    {
      "epoch": 10.481946624803768,
      "grad_norm": 3.132619857788086,
      "learning_rate": 4.3448783359497646e-05,
      "loss": 0.6547,
      "step": 667700
    },
    {
      "epoch": 10.483516483516484,
      "grad_norm": 3.6044094562530518,
      "learning_rate": 4.34478021978022e-05,
      "loss": 0.6433,
      "step": 667800
    },
    {
      "epoch": 10.485086342229199,
      "grad_norm": 3.832268476486206,
      "learning_rate": 4.3446821036106754e-05,
      "loss": 0.606,
      "step": 667900
    },
    {
      "epoch": 10.486656200941916,
      "grad_norm": 4.754904270172119,
      "learning_rate": 4.3445839874411305e-05,
      "loss": 0.6532,
      "step": 668000
    },
    {
      "epoch": 10.48822605965463,
      "grad_norm": 4.166969299316406,
      "learning_rate": 4.3444858712715856e-05,
      "loss": 0.6243,
      "step": 668100
    },
    {
      "epoch": 10.489795918367347,
      "grad_norm": 4.122654914855957,
      "learning_rate": 4.3443877551020414e-05,
      "loss": 0.6513,
      "step": 668200
    },
    {
      "epoch": 10.491365777080063,
      "grad_norm": 3.3395168781280518,
      "learning_rate": 4.344289638932496e-05,
      "loss": 0.6487,
      "step": 668300
    },
    {
      "epoch": 10.49293563579278,
      "grad_norm": 5.829944133758545,
      "learning_rate": 4.3441915227629516e-05,
      "loss": 0.65,
      "step": 668400
    },
    {
      "epoch": 10.494505494505495,
      "grad_norm": 4.212115287780762,
      "learning_rate": 4.344093406593407e-05,
      "loss": 0.6201,
      "step": 668500
    },
    {
      "epoch": 10.49607535321821,
      "grad_norm": 3.8967812061309814,
      "learning_rate": 4.3439952904238625e-05,
      "loss": 0.6203,
      "step": 668600
    },
    {
      "epoch": 10.497645211930926,
      "grad_norm": 3.1054329872131348,
      "learning_rate": 4.343897174254317e-05,
      "loss": 0.65,
      "step": 668700
    },
    {
      "epoch": 10.499215070643642,
      "grad_norm": 3.595026731491089,
      "learning_rate": 4.343799058084773e-05,
      "loss": 0.6233,
      "step": 668800
    },
    {
      "epoch": 10.500784929356358,
      "grad_norm": 4.331570148468018,
      "learning_rate": 4.343700941915228e-05,
      "loss": 0.6416,
      "step": 668900
    },
    {
      "epoch": 10.502354788069074,
      "grad_norm": 4.425694465637207,
      "learning_rate": 4.343602825745683e-05,
      "loss": 0.6384,
      "step": 669000
    },
    {
      "epoch": 10.50392464678179,
      "grad_norm": 4.678558349609375,
      "learning_rate": 4.3435047095761386e-05,
      "loss": 0.6675,
      "step": 669100
    },
    {
      "epoch": 10.505494505494505,
      "grad_norm": 5.381808280944824,
      "learning_rate": 4.343406593406594e-05,
      "loss": 0.6121,
      "step": 669200
    },
    {
      "epoch": 10.50706436420722,
      "grad_norm": 5.570821285247803,
      "learning_rate": 4.343308477237049e-05,
      "loss": 0.6091,
      "step": 669300
    },
    {
      "epoch": 10.508634222919937,
      "grad_norm": 3.234088182449341,
      "learning_rate": 4.343210361067504e-05,
      "loss": 0.6522,
      "step": 669400
    },
    {
      "epoch": 10.510204081632653,
      "grad_norm": 4.278469562530518,
      "learning_rate": 4.34311224489796e-05,
      "loss": 0.6682,
      "step": 669500
    },
    {
      "epoch": 10.51177394034537,
      "grad_norm": 2.357395887374878,
      "learning_rate": 4.343014128728415e-05,
      "loss": 0.6516,
      "step": 669600
    },
    {
      "epoch": 10.513343799058084,
      "grad_norm": 3.758136510848999,
      "learning_rate": 4.34291601255887e-05,
      "loss": 0.6539,
      "step": 669700
    },
    {
      "epoch": 10.514913657770801,
      "grad_norm": 3.3275413513183594,
      "learning_rate": 4.342817896389325e-05,
      "loss": 0.6415,
      "step": 669800
    },
    {
      "epoch": 10.516483516483516,
      "grad_norm": 4.107250690460205,
      "learning_rate": 4.342719780219781e-05,
      "loss": 0.6558,
      "step": 669900
    },
    {
      "epoch": 10.518053375196232,
      "grad_norm": 3.5803093910217285,
      "learning_rate": 4.342621664050236e-05,
      "loss": 0.6275,
      "step": 670000
    },
    {
      "epoch": 10.519623233908948,
      "grad_norm": 2.9670629501342773,
      "learning_rate": 4.342523547880691e-05,
      "loss": 0.6091,
      "step": 670100
    },
    {
      "epoch": 10.521193092621663,
      "grad_norm": 4.284875392913818,
      "learning_rate": 4.342425431711146e-05,
      "loss": 0.6642,
      "step": 670200
    },
    {
      "epoch": 10.52276295133438,
      "grad_norm": 3.8023529052734375,
      "learning_rate": 4.342327315541602e-05,
      "loss": 0.6318,
      "step": 670300
    },
    {
      "epoch": 10.524332810047095,
      "grad_norm": 3.640371799468994,
      "learning_rate": 4.342229199372056e-05,
      "loss": 0.6286,
      "step": 670400
    },
    {
      "epoch": 10.525902668759812,
      "grad_norm": 4.081928253173828,
      "learning_rate": 4.342131083202512e-05,
      "loss": 0.6721,
      "step": 670500
    },
    {
      "epoch": 10.527472527472527,
      "grad_norm": 4.260765552520752,
      "learning_rate": 4.342032967032967e-05,
      "loss": 0.6253,
      "step": 670600
    },
    {
      "epoch": 10.529042386185242,
      "grad_norm": 4.216564178466797,
      "learning_rate": 4.341934850863423e-05,
      "loss": 0.6519,
      "step": 670700
    },
    {
      "epoch": 10.53061224489796,
      "grad_norm": 3.5145676136016846,
      "learning_rate": 4.341836734693877e-05,
      "loss": 0.6097,
      "step": 670800
    },
    {
      "epoch": 10.532182103610674,
      "grad_norm": 4.002441883087158,
      "learning_rate": 4.341738618524333e-05,
      "loss": 0.6001,
      "step": 670900
    },
    {
      "epoch": 10.533751962323391,
      "grad_norm": 2.8274848461151123,
      "learning_rate": 4.341640502354788e-05,
      "loss": 0.6295,
      "step": 671000
    },
    {
      "epoch": 10.535321821036106,
      "grad_norm": 3.232698678970337,
      "learning_rate": 4.341542386185243e-05,
      "loss": 0.6462,
      "step": 671100
    },
    {
      "epoch": 10.536891679748823,
      "grad_norm": 4.207461833953857,
      "learning_rate": 4.341444270015699e-05,
      "loss": 0.6512,
      "step": 671200
    },
    {
      "epoch": 10.538461538461538,
      "grad_norm": 4.065217018127441,
      "learning_rate": 4.341346153846154e-05,
      "loss": 0.6295,
      "step": 671300
    },
    {
      "epoch": 10.540031397174253,
      "grad_norm": 4.144855499267578,
      "learning_rate": 4.341248037676609e-05,
      "loss": 0.6064,
      "step": 671400
    },
    {
      "epoch": 10.54160125588697,
      "grad_norm": 4.208333969116211,
      "learning_rate": 4.3411499215070644e-05,
      "loss": 0.6459,
      "step": 671500
    },
    {
      "epoch": 10.543171114599685,
      "grad_norm": 5.369257926940918,
      "learning_rate": 4.34105180533752e-05,
      "loss": 0.6456,
      "step": 671600
    },
    {
      "epoch": 10.544740973312402,
      "grad_norm": 3.2857701778411865,
      "learning_rate": 4.340953689167975e-05,
      "loss": 0.6634,
      "step": 671700
    },
    {
      "epoch": 10.546310832025117,
      "grad_norm": 4.779021739959717,
      "learning_rate": 4.3408555729984304e-05,
      "loss": 0.6336,
      "step": 671800
    },
    {
      "epoch": 10.547880690737834,
      "grad_norm": 4.10936164855957,
      "learning_rate": 4.3407574568288854e-05,
      "loss": 0.6265,
      "step": 671900
    },
    {
      "epoch": 10.54945054945055,
      "grad_norm": 4.450709819793701,
      "learning_rate": 4.340659340659341e-05,
      "loss": 0.6548,
      "step": 672000
    },
    {
      "epoch": 10.551020408163264,
      "grad_norm": 4.848249912261963,
      "learning_rate": 4.3405612244897956e-05,
      "loss": 0.6462,
      "step": 672100
    },
    {
      "epoch": 10.552590266875981,
      "grad_norm": 4.172489166259766,
      "learning_rate": 4.3404631083202514e-05,
      "loss": 0.6618,
      "step": 672200
    },
    {
      "epoch": 10.554160125588696,
      "grad_norm": 3.839843988418579,
      "learning_rate": 4.3403649921507065e-05,
      "loss": 0.6576,
      "step": 672300
    },
    {
      "epoch": 10.555729984301413,
      "grad_norm": 4.108659267425537,
      "learning_rate": 4.340266875981162e-05,
      "loss": 0.6177,
      "step": 672400
    },
    {
      "epoch": 10.557299843014128,
      "grad_norm": 3.629579782485962,
      "learning_rate": 4.340168759811617e-05,
      "loss": 0.6419,
      "step": 672500
    },
    {
      "epoch": 10.558869701726845,
      "grad_norm": 4.658148765563965,
      "learning_rate": 4.3400706436420725e-05,
      "loss": 0.6325,
      "step": 672600
    },
    {
      "epoch": 10.56043956043956,
      "grad_norm": 2.989933490753174,
      "learning_rate": 4.3399725274725276e-05,
      "loss": 0.5879,
      "step": 672700
    },
    {
      "epoch": 10.562009419152277,
      "grad_norm": 4.901402950286865,
      "learning_rate": 4.339874411302983e-05,
      "loss": 0.6015,
      "step": 672800
    },
    {
      "epoch": 10.563579277864992,
      "grad_norm": 3.546342372894287,
      "learning_rate": 4.339776295133438e-05,
      "loss": 0.6043,
      "step": 672900
    },
    {
      "epoch": 10.565149136577707,
      "grad_norm": 3.756892442703247,
      "learning_rate": 4.3396781789638936e-05,
      "loss": 0.6309,
      "step": 673000
    },
    {
      "epoch": 10.566718995290424,
      "grad_norm": 2.946901798248291,
      "learning_rate": 4.3395800627943487e-05,
      "loss": 0.6175,
      "step": 673100
    },
    {
      "epoch": 10.56828885400314,
      "grad_norm": 3.324244737625122,
      "learning_rate": 4.339481946624804e-05,
      "loss": 0.6222,
      "step": 673200
    },
    {
      "epoch": 10.569858712715856,
      "grad_norm": 4.396817684173584,
      "learning_rate": 4.3393838304552595e-05,
      "loss": 0.6219,
      "step": 673300
    },
    {
      "epoch": 10.571428571428571,
      "grad_norm": 2.9829227924346924,
      "learning_rate": 4.3392857142857146e-05,
      "loss": 0.6534,
      "step": 673400
    },
    {
      "epoch": 10.572998430141288,
      "grad_norm": 3.7579851150512695,
      "learning_rate": 4.33918759811617e-05,
      "loss": 0.6494,
      "step": 673500
    },
    {
      "epoch": 10.574568288854003,
      "grad_norm": 4.55385684967041,
      "learning_rate": 4.339089481946625e-05,
      "loss": 0.6556,
      "step": 673600
    },
    {
      "epoch": 10.576138147566718,
      "grad_norm": 3.963749408721924,
      "learning_rate": 4.3389913657770806e-05,
      "loss": 0.6341,
      "step": 673700
    },
    {
      "epoch": 10.577708006279435,
      "grad_norm": 5.139797687530518,
      "learning_rate": 4.338893249607536e-05,
      "loss": 0.5812,
      "step": 673800
    },
    {
      "epoch": 10.57927786499215,
      "grad_norm": 3.2675442695617676,
      "learning_rate": 4.338795133437991e-05,
      "loss": 0.626,
      "step": 673900
    },
    {
      "epoch": 10.580847723704867,
      "grad_norm": 5.043467044830322,
      "learning_rate": 4.338697017268446e-05,
      "loss": 0.6183,
      "step": 674000
    },
    {
      "epoch": 10.582417582417582,
      "grad_norm": 3.3457276821136475,
      "learning_rate": 4.338598901098902e-05,
      "loss": 0.6446,
      "step": 674100
    },
    {
      "epoch": 10.583987441130299,
      "grad_norm": 2.9255130290985107,
      "learning_rate": 4.338500784929356e-05,
      "loss": 0.6432,
      "step": 674200
    },
    {
      "epoch": 10.585557299843014,
      "grad_norm": 4.118520259857178,
      "learning_rate": 4.338402668759812e-05,
      "loss": 0.6075,
      "step": 674300
    },
    {
      "epoch": 10.58712715855573,
      "grad_norm": 4.702977180480957,
      "learning_rate": 4.338304552590267e-05,
      "loss": 0.6229,
      "step": 674400
    },
    {
      "epoch": 10.588697017268446,
      "grad_norm": 3.197624921798706,
      "learning_rate": 4.338206436420723e-05,
      "loss": 0.6052,
      "step": 674500
    },
    {
      "epoch": 10.590266875981161,
      "grad_norm": 3.85317063331604,
      "learning_rate": 4.338108320251177e-05,
      "loss": 0.6318,
      "step": 674600
    },
    {
      "epoch": 10.591836734693878,
      "grad_norm": 3.312507152557373,
      "learning_rate": 4.338010204081633e-05,
      "loss": 0.6602,
      "step": 674700
    },
    {
      "epoch": 10.593406593406593,
      "grad_norm": 3.0308332443237305,
      "learning_rate": 4.337912087912088e-05,
      "loss": 0.623,
      "step": 674800
    },
    {
      "epoch": 10.59497645211931,
      "grad_norm": 3.435774803161621,
      "learning_rate": 4.337813971742543e-05,
      "loss": 0.6436,
      "step": 674900
    },
    {
      "epoch": 10.596546310832025,
      "grad_norm": 4.1067070960998535,
      "learning_rate": 4.337715855572998e-05,
      "loss": 0.6458,
      "step": 675000
    },
    {
      "epoch": 10.598116169544742,
      "grad_norm": 3.009817123413086,
      "learning_rate": 4.337617739403454e-05,
      "loss": 0.622,
      "step": 675100
    },
    {
      "epoch": 10.599686028257457,
      "grad_norm": 3.6958844661712646,
      "learning_rate": 4.337519623233909e-05,
      "loss": 0.631,
      "step": 675200
    },
    {
      "epoch": 10.601255886970172,
      "grad_norm": 4.291160583496094,
      "learning_rate": 4.337421507064364e-05,
      "loss": 0.6206,
      "step": 675300
    },
    {
      "epoch": 10.602825745682889,
      "grad_norm": 2.670252561569214,
      "learning_rate": 4.33732339089482e-05,
      "loss": 0.6568,
      "step": 675400
    },
    {
      "epoch": 10.604395604395604,
      "grad_norm": 3.7636749744415283,
      "learning_rate": 4.337225274725275e-05,
      "loss": 0.6402,
      "step": 675500
    },
    {
      "epoch": 10.605965463108321,
      "grad_norm": 4.301273345947266,
      "learning_rate": 4.33712715855573e-05,
      "loss": 0.6395,
      "step": 675600
    },
    {
      "epoch": 10.607535321821036,
      "grad_norm": 3.4598450660705566,
      "learning_rate": 4.337029042386185e-05,
      "loss": 0.5969,
      "step": 675700
    },
    {
      "epoch": 10.609105180533753,
      "grad_norm": 3.948580265045166,
      "learning_rate": 4.336930926216641e-05,
      "loss": 0.6313,
      "step": 675800
    },
    {
      "epoch": 10.610675039246468,
      "grad_norm": 3.855997323989868,
      "learning_rate": 4.336832810047096e-05,
      "loss": 0.6164,
      "step": 675900
    },
    {
      "epoch": 10.612244897959183,
      "grad_norm": 3.5484113693237305,
      "learning_rate": 4.336734693877551e-05,
      "loss": 0.6326,
      "step": 676000
    },
    {
      "epoch": 10.6138147566719,
      "grad_norm": 3.3356857299804688,
      "learning_rate": 4.3366365777080063e-05,
      "loss": 0.6235,
      "step": 676100
    },
    {
      "epoch": 10.615384615384615,
      "grad_norm": 3.9002761840820312,
      "learning_rate": 4.336538461538462e-05,
      "loss": 0.6598,
      "step": 676200
    },
    {
      "epoch": 10.616954474097332,
      "grad_norm": 3.502702236175537,
      "learning_rate": 4.3364403453689165e-05,
      "loss": 0.6445,
      "step": 676300
    },
    {
      "epoch": 10.618524332810047,
      "grad_norm": 3.9518187046051025,
      "learning_rate": 4.336342229199372e-05,
      "loss": 0.6848,
      "step": 676400
    },
    {
      "epoch": 10.620094191522764,
      "grad_norm": 4.023534297943115,
      "learning_rate": 4.3362441130298274e-05,
      "loss": 0.6484,
      "step": 676500
    },
    {
      "epoch": 10.621664050235479,
      "grad_norm": 3.0991740226745605,
      "learning_rate": 4.336145996860283e-05,
      "loss": 0.5967,
      "step": 676600
    },
    {
      "epoch": 10.623233908948194,
      "grad_norm": 2.7899205684661865,
      "learning_rate": 4.3360478806907376e-05,
      "loss": 0.6577,
      "step": 676700
    },
    {
      "epoch": 10.62480376766091,
      "grad_norm": 3.727447271347046,
      "learning_rate": 4.3359497645211934e-05,
      "loss": 0.6174,
      "step": 676800
    },
    {
      "epoch": 10.626373626373626,
      "grad_norm": 3.6965198516845703,
      "learning_rate": 4.3358516483516485e-05,
      "loss": 0.6501,
      "step": 676900
    },
    {
      "epoch": 10.627943485086343,
      "grad_norm": 3.4295029640197754,
      "learning_rate": 4.3357535321821036e-05,
      "loss": 0.6666,
      "step": 677000
    },
    {
      "epoch": 10.629513343799058,
      "grad_norm": 3.825913667678833,
      "learning_rate": 4.335655416012559e-05,
      "loss": 0.6219,
      "step": 677100
    },
    {
      "epoch": 10.631083202511775,
      "grad_norm": 3.855426788330078,
      "learning_rate": 4.3355572998430145e-05,
      "loss": 0.6226,
      "step": 677200
    },
    {
      "epoch": 10.63265306122449,
      "grad_norm": 3.8968939781188965,
      "learning_rate": 4.3354591836734696e-05,
      "loss": 0.6072,
      "step": 677300
    },
    {
      "epoch": 10.634222919937205,
      "grad_norm": 1.8604620695114136,
      "learning_rate": 4.3353610675039247e-05,
      "loss": 0.615,
      "step": 677400
    },
    {
      "epoch": 10.635792778649922,
      "grad_norm": 3.323767900466919,
      "learning_rate": 4.3352629513343804e-05,
      "loss": 0.6732,
      "step": 677500
    },
    {
      "epoch": 10.637362637362637,
      "grad_norm": 4.341554164886475,
      "learning_rate": 4.3351648351648355e-05,
      "loss": 0.6339,
      "step": 677600
    },
    {
      "epoch": 10.638932496075354,
      "grad_norm": 2.3733975887298584,
      "learning_rate": 4.3350667189952906e-05,
      "loss": 0.6228,
      "step": 677700
    },
    {
      "epoch": 10.640502354788069,
      "grad_norm": 3.5823047161102295,
      "learning_rate": 4.334968602825746e-05,
      "loss": 0.6635,
      "step": 677800
    },
    {
      "epoch": 10.642072213500786,
      "grad_norm": 3.6690845489501953,
      "learning_rate": 4.3348704866562015e-05,
      "loss": 0.6397,
      "step": 677900
    },
    {
      "epoch": 10.6436420722135,
      "grad_norm": 3.8918356895446777,
      "learning_rate": 4.3347723704866566e-05,
      "loss": 0.6416,
      "step": 678000
    },
    {
      "epoch": 10.645211930926216,
      "grad_norm": 3.0977120399475098,
      "learning_rate": 4.334674254317112e-05,
      "loss": 0.6338,
      "step": 678100
    },
    {
      "epoch": 10.646781789638933,
      "grad_norm": 4.555840015411377,
      "learning_rate": 4.334576138147567e-05,
      "loss": 0.6715,
      "step": 678200
    },
    {
      "epoch": 10.648351648351648,
      "grad_norm": 3.3484315872192383,
      "learning_rate": 4.3344780219780226e-05,
      "loss": 0.6574,
      "step": 678300
    },
    {
      "epoch": 10.649921507064365,
      "grad_norm": 4.293969631195068,
      "learning_rate": 4.334379905808477e-05,
      "loss": 0.6477,
      "step": 678400
    },
    {
      "epoch": 10.65149136577708,
      "grad_norm": 3.4410877227783203,
      "learning_rate": 4.334281789638933e-05,
      "loss": 0.6263,
      "step": 678500
    },
    {
      "epoch": 10.653061224489797,
      "grad_norm": 2.9338107109069824,
      "learning_rate": 4.334183673469388e-05,
      "loss": 0.6024,
      "step": 678600
    },
    {
      "epoch": 10.654631083202512,
      "grad_norm": 3.3366000652313232,
      "learning_rate": 4.3340855572998436e-05,
      "loss": 0.6179,
      "step": 678700
    },
    {
      "epoch": 10.656200941915227,
      "grad_norm": 4.019524574279785,
      "learning_rate": 4.333987441130298e-05,
      "loss": 0.6689,
      "step": 678800
    },
    {
      "epoch": 10.657770800627944,
      "grad_norm": 3.953228712081909,
      "learning_rate": 4.333889324960754e-05,
      "loss": 0.6411,
      "step": 678900
    },
    {
      "epoch": 10.659340659340659,
      "grad_norm": 4.556556224822998,
      "learning_rate": 4.333791208791209e-05,
      "loss": 0.6378,
      "step": 679000
    },
    {
      "epoch": 10.660910518053376,
      "grad_norm": 3.8681142330169678,
      "learning_rate": 4.333693092621664e-05,
      "loss": 0.6385,
      "step": 679100
    },
    {
      "epoch": 10.66248037676609,
      "grad_norm": 3.7520527839660645,
      "learning_rate": 4.333594976452119e-05,
      "loss": 0.6467,
      "step": 679200
    },
    {
      "epoch": 10.664050235478808,
      "grad_norm": 3.2628111839294434,
      "learning_rate": 4.333496860282575e-05,
      "loss": 0.6127,
      "step": 679300
    },
    {
      "epoch": 10.665620094191523,
      "grad_norm": 2.823984146118164,
      "learning_rate": 4.33339874411303e-05,
      "loss": 0.6164,
      "step": 679400
    },
    {
      "epoch": 10.667189952904238,
      "grad_norm": 3.3807733058929443,
      "learning_rate": 4.333300627943485e-05,
      "loss": 0.6167,
      "step": 679500
    },
    {
      "epoch": 10.668759811616955,
      "grad_norm": 4.214356422424316,
      "learning_rate": 4.333202511773941e-05,
      "loss": 0.6393,
      "step": 679600
    },
    {
      "epoch": 10.67032967032967,
      "grad_norm": 3.9841415882110596,
      "learning_rate": 4.333104395604396e-05,
      "loss": 0.6458,
      "step": 679700
    },
    {
      "epoch": 10.671899529042387,
      "grad_norm": 2.131011486053467,
      "learning_rate": 4.333006279434851e-05,
      "loss": 0.637,
      "step": 679800
    },
    {
      "epoch": 10.673469387755102,
      "grad_norm": 3.7982404232025146,
      "learning_rate": 4.332908163265306e-05,
      "loss": 0.6751,
      "step": 679900
    },
    {
      "epoch": 10.675039246467819,
      "grad_norm": 3.8896608352661133,
      "learning_rate": 4.332810047095762e-05,
      "loss": 0.63,
      "step": 680000
    },
    {
      "epoch": 10.676609105180534,
      "grad_norm": 3.27834415435791,
      "learning_rate": 4.332711930926217e-05,
      "loss": 0.6426,
      "step": 680100
    },
    {
      "epoch": 10.678178963893249,
      "grad_norm": 4.1122236251831055,
      "learning_rate": 4.332613814756672e-05,
      "loss": 0.6557,
      "step": 680200
    },
    {
      "epoch": 10.679748822605966,
      "grad_norm": 4.196679592132568,
      "learning_rate": 4.332515698587127e-05,
      "loss": 0.6384,
      "step": 680300
    },
    {
      "epoch": 10.68131868131868,
      "grad_norm": 4.445956230163574,
      "learning_rate": 4.332417582417583e-05,
      "loss": 0.6375,
      "step": 680400
    },
    {
      "epoch": 10.682888540031398,
      "grad_norm": 3.796640634536743,
      "learning_rate": 4.3323194662480374e-05,
      "loss": 0.6262,
      "step": 680500
    },
    {
      "epoch": 10.684458398744113,
      "grad_norm": 3.3294291496276855,
      "learning_rate": 4.332221350078493e-05,
      "loss": 0.594,
      "step": 680600
    },
    {
      "epoch": 10.68602825745683,
      "grad_norm": 3.7905032634735107,
      "learning_rate": 4.332123233908948e-05,
      "loss": 0.6127,
      "step": 680700
    },
    {
      "epoch": 10.687598116169545,
      "grad_norm": 3.1240134239196777,
      "learning_rate": 4.332025117739404e-05,
      "loss": 0.6472,
      "step": 680800
    },
    {
      "epoch": 10.68916797488226,
      "grad_norm": 4.226770401000977,
      "learning_rate": 4.3319270015698585e-05,
      "loss": 0.6454,
      "step": 680900
    },
    {
      "epoch": 10.690737833594977,
      "grad_norm": 3.7246716022491455,
      "learning_rate": 4.331828885400314e-05,
      "loss": 0.6712,
      "step": 681000
    },
    {
      "epoch": 10.692307692307692,
      "grad_norm": 3.978921413421631,
      "learning_rate": 4.3317307692307694e-05,
      "loss": 0.6434,
      "step": 681100
    },
    {
      "epoch": 10.693877551020408,
      "grad_norm": 3.4615890979766846,
      "learning_rate": 4.3316326530612245e-05,
      "loss": 0.651,
      "step": 681200
    },
    {
      "epoch": 10.695447409733124,
      "grad_norm": 3.7661685943603516,
      "learning_rate": 4.3315345368916796e-05,
      "loss": 0.6163,
      "step": 681300
    },
    {
      "epoch": 10.69701726844584,
      "grad_norm": 4.81654691696167,
      "learning_rate": 4.3314364207221354e-05,
      "loss": 0.6668,
      "step": 681400
    },
    {
      "epoch": 10.698587127158556,
      "grad_norm": 4.295612335205078,
      "learning_rate": 4.3313383045525905e-05,
      "loss": 0.6156,
      "step": 681500
    },
    {
      "epoch": 10.700156985871272,
      "grad_norm": 3.2790777683258057,
      "learning_rate": 4.3312401883830456e-05,
      "loss": 0.6627,
      "step": 681600
    },
    {
      "epoch": 10.701726844583987,
      "grad_norm": 3.3005456924438477,
      "learning_rate": 4.3311420722135007e-05,
      "loss": 0.6316,
      "step": 681700
    },
    {
      "epoch": 10.703296703296703,
      "grad_norm": 4.542166233062744,
      "learning_rate": 4.3310439560439564e-05,
      "loss": 0.6692,
      "step": 681800
    },
    {
      "epoch": 10.70486656200942,
      "grad_norm": 4.415400505065918,
      "learning_rate": 4.3309458398744115e-05,
      "loss": 0.6271,
      "step": 681900
    },
    {
      "epoch": 10.706436420722135,
      "grad_norm": 3.4820377826690674,
      "learning_rate": 4.3308477237048666e-05,
      "loss": 0.5957,
      "step": 682000
    },
    {
      "epoch": 10.708006279434851,
      "grad_norm": 3.7933385372161865,
      "learning_rate": 4.3307496075353224e-05,
      "loss": 0.632,
      "step": 682100
    },
    {
      "epoch": 10.709576138147566,
      "grad_norm": 4.021242141723633,
      "learning_rate": 4.3306514913657775e-05,
      "loss": 0.6222,
      "step": 682200
    },
    {
      "epoch": 10.711145996860283,
      "grad_norm": 4.070423603057861,
      "learning_rate": 4.3305533751962326e-05,
      "loss": 0.6418,
      "step": 682300
    },
    {
      "epoch": 10.712715855572998,
      "grad_norm": 3.653829336166382,
      "learning_rate": 4.330455259026688e-05,
      "loss": 0.6345,
      "step": 682400
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 4.531373500823975,
      "learning_rate": 4.3303571428571435e-05,
      "loss": 0.647,
      "step": 682500
    },
    {
      "epoch": 10.71585557299843,
      "grad_norm": 3.7957890033721924,
      "learning_rate": 4.330259026687598e-05,
      "loss": 0.64,
      "step": 682600
    },
    {
      "epoch": 10.717425431711145,
      "grad_norm": 4.612626075744629,
      "learning_rate": 4.330160910518054e-05,
      "loss": 0.654,
      "step": 682700
    },
    {
      "epoch": 10.718995290423862,
      "grad_norm": 4.105223655700684,
      "learning_rate": 4.330062794348509e-05,
      "loss": 0.6717,
      "step": 682800
    },
    {
      "epoch": 10.720565149136577,
      "grad_norm": 4.209285259246826,
      "learning_rate": 4.3299646781789645e-05,
      "loss": 0.682,
      "step": 682900
    },
    {
      "epoch": 10.722135007849294,
      "grad_norm": 2.752700090408325,
      "learning_rate": 4.329866562009419e-05,
      "loss": 0.6693,
      "step": 683000
    },
    {
      "epoch": 10.72370486656201,
      "grad_norm": 3.290410280227661,
      "learning_rate": 4.329768445839875e-05,
      "loss": 0.6022,
      "step": 683100
    },
    {
      "epoch": 10.725274725274724,
      "grad_norm": 5.287116050720215,
      "learning_rate": 4.32967032967033e-05,
      "loss": 0.6324,
      "step": 683200
    },
    {
      "epoch": 10.726844583987441,
      "grad_norm": 3.1836459636688232,
      "learning_rate": 4.329572213500785e-05,
      "loss": 0.622,
      "step": 683300
    },
    {
      "epoch": 10.728414442700156,
      "grad_norm": 4.318589687347412,
      "learning_rate": 4.32947409733124e-05,
      "loss": 0.6116,
      "step": 683400
    },
    {
      "epoch": 10.729984301412873,
      "grad_norm": 3.530761480331421,
      "learning_rate": 4.329375981161696e-05,
      "loss": 0.6225,
      "step": 683500
    },
    {
      "epoch": 10.731554160125588,
      "grad_norm": 4.774860858917236,
      "learning_rate": 4.329277864992151e-05,
      "loss": 0.6538,
      "step": 683600
    },
    {
      "epoch": 10.733124018838305,
      "grad_norm": 4.129735469818115,
      "learning_rate": 4.329179748822606e-05,
      "loss": 0.6356,
      "step": 683700
    },
    {
      "epoch": 10.73469387755102,
      "grad_norm": 2.5836844444274902,
      "learning_rate": 4.329081632653061e-05,
      "loss": 0.6257,
      "step": 683800
    },
    {
      "epoch": 10.736263736263737,
      "grad_norm": 3.0222041606903076,
      "learning_rate": 4.328983516483517e-05,
      "loss": 0.661,
      "step": 683900
    },
    {
      "epoch": 10.737833594976452,
      "grad_norm": 3.121450424194336,
      "learning_rate": 4.328885400313972e-05,
      "loss": 0.6124,
      "step": 684000
    },
    {
      "epoch": 10.739403453689167,
      "grad_norm": 3.4809165000915527,
      "learning_rate": 4.328787284144427e-05,
      "loss": 0.6093,
      "step": 684100
    },
    {
      "epoch": 10.740973312401884,
      "grad_norm": 5.339559555053711,
      "learning_rate": 4.328689167974883e-05,
      "loss": 0.6625,
      "step": 684200
    },
    {
      "epoch": 10.7425431711146,
      "grad_norm": 3.3151698112487793,
      "learning_rate": 4.328591051805338e-05,
      "loss": 0.6763,
      "step": 684300
    },
    {
      "epoch": 10.744113029827316,
      "grad_norm": 4.288384437561035,
      "learning_rate": 4.328492935635793e-05,
      "loss": 0.588,
      "step": 684400
    },
    {
      "epoch": 10.745682888540031,
      "grad_norm": 4.065545558929443,
      "learning_rate": 4.328394819466248e-05,
      "loss": 0.6215,
      "step": 684500
    },
    {
      "epoch": 10.747252747252748,
      "grad_norm": 2.8883156776428223,
      "learning_rate": 4.328296703296704e-05,
      "loss": 0.6395,
      "step": 684600
    },
    {
      "epoch": 10.748822605965463,
      "grad_norm": 4.4537811279296875,
      "learning_rate": 4.3281985871271583e-05,
      "loss": 0.6183,
      "step": 684700
    },
    {
      "epoch": 10.750392464678178,
      "grad_norm": 4.06857442855835,
      "learning_rate": 4.328100470957614e-05,
      "loss": 0.5992,
      "step": 684800
    },
    {
      "epoch": 10.751962323390895,
      "grad_norm": 4.755162715911865,
      "learning_rate": 4.328002354788069e-05,
      "loss": 0.681,
      "step": 684900
    },
    {
      "epoch": 10.75353218210361,
      "grad_norm": 5.490437030792236,
      "learning_rate": 4.327904238618525e-05,
      "loss": 0.5964,
      "step": 685000
    },
    {
      "epoch": 10.755102040816327,
      "grad_norm": 3.443838596343994,
      "learning_rate": 4.3278061224489794e-05,
      "loss": 0.6171,
      "step": 685100
    },
    {
      "epoch": 10.756671899529042,
      "grad_norm": 3.551079273223877,
      "learning_rate": 4.327708006279435e-05,
      "loss": 0.6162,
      "step": 685200
    },
    {
      "epoch": 10.758241758241759,
      "grad_norm": 4.454923629760742,
      "learning_rate": 4.32760989010989e-05,
      "loss": 0.6393,
      "step": 685300
    },
    {
      "epoch": 10.759811616954474,
      "grad_norm": 4.117960453033447,
      "learning_rate": 4.3275117739403454e-05,
      "loss": 0.6123,
      "step": 685400
    },
    {
      "epoch": 10.76138147566719,
      "grad_norm": 4.1010966300964355,
      "learning_rate": 4.3274136577708005e-05,
      "loss": 0.6565,
      "step": 685500
    },
    {
      "epoch": 10.762951334379906,
      "grad_norm": 3.256579637527466,
      "learning_rate": 4.327315541601256e-05,
      "loss": 0.6096,
      "step": 685600
    },
    {
      "epoch": 10.764521193092621,
      "grad_norm": 5.09393835067749,
      "learning_rate": 4.3272174254317114e-05,
      "loss": 0.6556,
      "step": 685700
    },
    {
      "epoch": 10.766091051805338,
      "grad_norm": 4.986861705780029,
      "learning_rate": 4.3271193092621665e-05,
      "loss": 0.6318,
      "step": 685800
    },
    {
      "epoch": 10.767660910518053,
      "grad_norm": 4.6325883865356445,
      "learning_rate": 4.3270211930926216e-05,
      "loss": 0.6371,
      "step": 685900
    },
    {
      "epoch": 10.76923076923077,
      "grad_norm": 3.132988691329956,
      "learning_rate": 4.326923076923077e-05,
      "loss": 0.649,
      "step": 686000
    },
    {
      "epoch": 10.770800627943485,
      "grad_norm": 4.409566879272461,
      "learning_rate": 4.3268249607535324e-05,
      "loss": 0.6397,
      "step": 686100
    },
    {
      "epoch": 10.7723704866562,
      "grad_norm": 4.835293292999268,
      "learning_rate": 4.3267268445839875e-05,
      "loss": 0.6102,
      "step": 686200
    },
    {
      "epoch": 10.773940345368917,
      "grad_norm": 3.2447292804718018,
      "learning_rate": 4.326628728414443e-05,
      "loss": 0.634,
      "step": 686300
    },
    {
      "epoch": 10.775510204081632,
      "grad_norm": 3.3743457794189453,
      "learning_rate": 4.3265306122448984e-05,
      "loss": 0.6441,
      "step": 686400
    },
    {
      "epoch": 10.777080062794349,
      "grad_norm": 4.445589065551758,
      "learning_rate": 4.3264324960753535e-05,
      "loss": 0.6668,
      "step": 686500
    },
    {
      "epoch": 10.778649921507064,
      "grad_norm": 3.595285177230835,
      "learning_rate": 4.3263343799058086e-05,
      "loss": 0.6545,
      "step": 686600
    },
    {
      "epoch": 10.780219780219781,
      "grad_norm": 3.7890379428863525,
      "learning_rate": 4.3262362637362644e-05,
      "loss": 0.6415,
      "step": 686700
    },
    {
      "epoch": 10.781789638932496,
      "grad_norm": 4.787410736083984,
      "learning_rate": 4.326138147566719e-05,
      "loss": 0.628,
      "step": 686800
    },
    {
      "epoch": 10.783359497645211,
      "grad_norm": 3.9896976947784424,
      "learning_rate": 4.3260400313971746e-05,
      "loss": 0.6795,
      "step": 686900
    },
    {
      "epoch": 10.784929356357928,
      "grad_norm": 4.011497497558594,
      "learning_rate": 4.32594191522763e-05,
      "loss": 0.655,
      "step": 687000
    },
    {
      "epoch": 10.786499215070643,
      "grad_norm": 4.142080307006836,
      "learning_rate": 4.3258437990580854e-05,
      "loss": 0.6316,
      "step": 687100
    },
    {
      "epoch": 10.78806907378336,
      "grad_norm": 4.5249528884887695,
      "learning_rate": 4.32574568288854e-05,
      "loss": 0.6195,
      "step": 687200
    },
    {
      "epoch": 10.789638932496075,
      "grad_norm": 4.106212615966797,
      "learning_rate": 4.3256475667189956e-05,
      "loss": 0.6087,
      "step": 687300
    },
    {
      "epoch": 10.791208791208792,
      "grad_norm": 3.750079393386841,
      "learning_rate": 4.325549450549451e-05,
      "loss": 0.6484,
      "step": 687400
    },
    {
      "epoch": 10.792778649921507,
      "grad_norm": 3.0200612545013428,
      "learning_rate": 4.325451334379906e-05,
      "loss": 0.6288,
      "step": 687500
    },
    {
      "epoch": 10.794348508634222,
      "grad_norm": 3.52227783203125,
      "learning_rate": 4.325353218210361e-05,
      "loss": 0.5858,
      "step": 687600
    },
    {
      "epoch": 10.795918367346939,
      "grad_norm": 3.474012851715088,
      "learning_rate": 4.325255102040817e-05,
      "loss": 0.6427,
      "step": 687700
    },
    {
      "epoch": 10.797488226059654,
      "grad_norm": 3.137591600418091,
      "learning_rate": 4.325156985871272e-05,
      "loss": 0.6347,
      "step": 687800
    },
    {
      "epoch": 10.799058084772371,
      "grad_norm": 3.9191629886627197,
      "learning_rate": 4.325058869701727e-05,
      "loss": 0.6528,
      "step": 687900
    },
    {
      "epoch": 10.800627943485086,
      "grad_norm": 4.086188316345215,
      "learning_rate": 4.324960753532182e-05,
      "loss": 0.6323,
      "step": 688000
    },
    {
      "epoch": 10.802197802197803,
      "grad_norm": 3.512211799621582,
      "learning_rate": 4.324862637362638e-05,
      "loss": 0.6104,
      "step": 688100
    },
    {
      "epoch": 10.803767660910518,
      "grad_norm": 3.7544608116149902,
      "learning_rate": 4.324764521193093e-05,
      "loss": 0.6194,
      "step": 688200
    },
    {
      "epoch": 10.805337519623233,
      "grad_norm": 3.9853262901306152,
      "learning_rate": 4.324666405023548e-05,
      "loss": 0.6543,
      "step": 688300
    },
    {
      "epoch": 10.80690737833595,
      "grad_norm": 3.566481590270996,
      "learning_rate": 4.324568288854004e-05,
      "loss": 0.6188,
      "step": 688400
    },
    {
      "epoch": 10.808477237048665,
      "grad_norm": 3.6752166748046875,
      "learning_rate": 4.324470172684459e-05,
      "loss": 0.631,
      "step": 688500
    },
    {
      "epoch": 10.810047095761382,
      "grad_norm": 4.199258804321289,
      "learning_rate": 4.324372056514914e-05,
      "loss": 0.6986,
      "step": 688600
    },
    {
      "epoch": 10.811616954474097,
      "grad_norm": 2.7486178874969482,
      "learning_rate": 4.324273940345369e-05,
      "loss": 0.6468,
      "step": 688700
    },
    {
      "epoch": 10.813186813186814,
      "grad_norm": 4.688164234161377,
      "learning_rate": 4.324175824175825e-05,
      "loss": 0.6617,
      "step": 688800
    },
    {
      "epoch": 10.814756671899529,
      "grad_norm": 4.235719680786133,
      "learning_rate": 4.324077708006279e-05,
      "loss": 0.6316,
      "step": 688900
    },
    {
      "epoch": 10.816326530612244,
      "grad_norm": 3.4594566822052,
      "learning_rate": 4.323979591836735e-05,
      "loss": 0.6596,
      "step": 689000
    },
    {
      "epoch": 10.817896389324961,
      "grad_norm": 4.227926254272461,
      "learning_rate": 4.32388147566719e-05,
      "loss": 0.6588,
      "step": 689100
    },
    {
      "epoch": 10.819466248037676,
      "grad_norm": 2.636021852493286,
      "learning_rate": 4.323783359497646e-05,
      "loss": 0.6368,
      "step": 689200
    },
    {
      "epoch": 10.821036106750393,
      "grad_norm": 4.123103141784668,
      "learning_rate": 4.3236852433281e-05,
      "loss": 0.5891,
      "step": 689300
    },
    {
      "epoch": 10.822605965463108,
      "grad_norm": 4.023954391479492,
      "learning_rate": 4.323587127158556e-05,
      "loss": 0.6322,
      "step": 689400
    },
    {
      "epoch": 10.824175824175825,
      "grad_norm": 3.8978636264801025,
      "learning_rate": 4.323489010989011e-05,
      "loss": 0.6192,
      "step": 689500
    },
    {
      "epoch": 10.82574568288854,
      "grad_norm": 3.081042766571045,
      "learning_rate": 4.323390894819466e-05,
      "loss": 0.633,
      "step": 689600
    },
    {
      "epoch": 10.827315541601255,
      "grad_norm": 4.3579535484313965,
      "learning_rate": 4.3232927786499214e-05,
      "loss": 0.6471,
      "step": 689700
    },
    {
      "epoch": 10.828885400313972,
      "grad_norm": 3.163715124130249,
      "learning_rate": 4.323194662480377e-05,
      "loss": 0.6094,
      "step": 689800
    },
    {
      "epoch": 10.830455259026687,
      "grad_norm": 3.737636089324951,
      "learning_rate": 4.323096546310832e-05,
      "loss": 0.6065,
      "step": 689900
    },
    {
      "epoch": 10.832025117739404,
      "grad_norm": 3.363901376724243,
      "learning_rate": 4.3229984301412874e-05,
      "loss": 0.6113,
      "step": 690000
    },
    {
      "epoch": 10.833594976452119,
      "grad_norm": 2.090934991836548,
      "learning_rate": 4.3229003139717424e-05,
      "loss": 0.6496,
      "step": 690100
    },
    {
      "epoch": 10.835164835164836,
      "grad_norm": 4.080581188201904,
      "learning_rate": 4.322802197802198e-05,
      "loss": 0.6142,
      "step": 690200
    },
    {
      "epoch": 10.83673469387755,
      "grad_norm": 3.6895909309387207,
      "learning_rate": 4.322704081632653e-05,
      "loss": 0.6613,
      "step": 690300
    },
    {
      "epoch": 10.838304552590268,
      "grad_norm": 2.771350622177124,
      "learning_rate": 4.3226059654631084e-05,
      "loss": 0.6164,
      "step": 690400
    },
    {
      "epoch": 10.839874411302983,
      "grad_norm": 4.092861175537109,
      "learning_rate": 4.322507849293564e-05,
      "loss": 0.6674,
      "step": 690500
    },
    {
      "epoch": 10.841444270015698,
      "grad_norm": 3.2683217525482178,
      "learning_rate": 4.322409733124019e-05,
      "loss": 0.6102,
      "step": 690600
    },
    {
      "epoch": 10.843014128728415,
      "grad_norm": 4.080906391143799,
      "learning_rate": 4.3223116169544744e-05,
      "loss": 0.6338,
      "step": 690700
    },
    {
      "epoch": 10.84458398744113,
      "grad_norm": 3.7205867767333984,
      "learning_rate": 4.3222135007849295e-05,
      "loss": 0.6249,
      "step": 690800
    },
    {
      "epoch": 10.846153846153847,
      "grad_norm": 3.339522123336792,
      "learning_rate": 4.322115384615385e-05,
      "loss": 0.6805,
      "step": 690900
    },
    {
      "epoch": 10.847723704866562,
      "grad_norm": 4.550182342529297,
      "learning_rate": 4.32201726844584e-05,
      "loss": 0.6197,
      "step": 691000
    },
    {
      "epoch": 10.849293563579279,
      "grad_norm": 4.2068915367126465,
      "learning_rate": 4.3219191522762955e-05,
      "loss": 0.6294,
      "step": 691100
    },
    {
      "epoch": 10.850863422291994,
      "grad_norm": 3.9299709796905518,
      "learning_rate": 4.3218210361067506e-05,
      "loss": 0.6288,
      "step": 691200
    },
    {
      "epoch": 10.852433281004709,
      "grad_norm": 3.725243091583252,
      "learning_rate": 4.321722919937206e-05,
      "loss": 0.6321,
      "step": 691300
    },
    {
      "epoch": 10.854003139717426,
      "grad_norm": 4.414248943328857,
      "learning_rate": 4.321624803767661e-05,
      "loss": 0.6078,
      "step": 691400
    },
    {
      "epoch": 10.85557299843014,
      "grad_norm": 2.5065932273864746,
      "learning_rate": 4.3215266875981165e-05,
      "loss": 0.5918,
      "step": 691500
    },
    {
      "epoch": 10.857142857142858,
      "grad_norm": 3.7692387104034424,
      "learning_rate": 4.3214285714285716e-05,
      "loss": 0.6667,
      "step": 691600
    },
    {
      "epoch": 10.858712715855573,
      "grad_norm": 3.2454047203063965,
      "learning_rate": 4.321330455259027e-05,
      "loss": 0.6341,
      "step": 691700
    },
    {
      "epoch": 10.86028257456829,
      "grad_norm": 4.813572883605957,
      "learning_rate": 4.321232339089482e-05,
      "loss": 0.6372,
      "step": 691800
    },
    {
      "epoch": 10.861852433281005,
      "grad_norm": 3.2848563194274902,
      "learning_rate": 4.3211342229199376e-05,
      "loss": 0.6084,
      "step": 691900
    },
    {
      "epoch": 10.86342229199372,
      "grad_norm": 3.8167436122894287,
      "learning_rate": 4.321036106750393e-05,
      "loss": 0.63,
      "step": 692000
    },
    {
      "epoch": 10.864992150706437,
      "grad_norm": 4.555552005767822,
      "learning_rate": 4.320937990580848e-05,
      "loss": 0.6376,
      "step": 692100
    },
    {
      "epoch": 10.866562009419152,
      "grad_norm": 4.53578519821167,
      "learning_rate": 4.320839874411303e-05,
      "loss": 0.6692,
      "step": 692200
    },
    {
      "epoch": 10.868131868131869,
      "grad_norm": 4.493069648742676,
      "learning_rate": 4.320741758241759e-05,
      "loss": 0.6461,
      "step": 692300
    },
    {
      "epoch": 10.869701726844584,
      "grad_norm": 4.188683986663818,
      "learning_rate": 4.320643642072214e-05,
      "loss": 0.7028,
      "step": 692400
    },
    {
      "epoch": 10.8712715855573,
      "grad_norm": 3.7414536476135254,
      "learning_rate": 4.320545525902669e-05,
      "loss": 0.6326,
      "step": 692500
    },
    {
      "epoch": 10.872841444270016,
      "grad_norm": 3.941490411758423,
      "learning_rate": 4.3204474097331246e-05,
      "loss": 0.6595,
      "step": 692600
    },
    {
      "epoch": 10.87441130298273,
      "grad_norm": 3.1032159328460693,
      "learning_rate": 4.32034929356358e-05,
      "loss": 0.6342,
      "step": 692700
    },
    {
      "epoch": 10.875981161695448,
      "grad_norm": 3.4897353649139404,
      "learning_rate": 4.320251177394035e-05,
      "loss": 0.6455,
      "step": 692800
    },
    {
      "epoch": 10.877551020408163,
      "grad_norm": 3.8946876525878906,
      "learning_rate": 4.32015306122449e-05,
      "loss": 0.6345,
      "step": 692900
    },
    {
      "epoch": 10.87912087912088,
      "grad_norm": 5.24995231628418,
      "learning_rate": 4.320054945054946e-05,
      "loss": 0.6069,
      "step": 693000
    },
    {
      "epoch": 10.880690737833595,
      "grad_norm": 4.472599029541016,
      "learning_rate": 4.3199568288854e-05,
      "loss": 0.6272,
      "step": 693100
    },
    {
      "epoch": 10.882260596546312,
      "grad_norm": 3.281139612197876,
      "learning_rate": 4.319858712715856e-05,
      "loss": 0.6366,
      "step": 693200
    },
    {
      "epoch": 10.883830455259027,
      "grad_norm": 4.22873592376709,
      "learning_rate": 4.319760596546311e-05,
      "loss": 0.627,
      "step": 693300
    },
    {
      "epoch": 10.885400313971743,
      "grad_norm": 4.568287372589111,
      "learning_rate": 4.319662480376767e-05,
      "loss": 0.6275,
      "step": 693400
    },
    {
      "epoch": 10.886970172684459,
      "grad_norm": 3.576460599899292,
      "learning_rate": 4.319564364207221e-05,
      "loss": 0.66,
      "step": 693500
    },
    {
      "epoch": 10.888540031397174,
      "grad_norm": 5.0138373374938965,
      "learning_rate": 4.319466248037677e-05,
      "loss": 0.6446,
      "step": 693600
    },
    {
      "epoch": 10.89010989010989,
      "grad_norm": 3.7861289978027344,
      "learning_rate": 4.319368131868132e-05,
      "loss": 0.6387,
      "step": 693700
    },
    {
      "epoch": 10.891679748822606,
      "grad_norm": 4.013305187225342,
      "learning_rate": 4.319270015698587e-05,
      "loss": 0.6501,
      "step": 693800
    },
    {
      "epoch": 10.893249607535322,
      "grad_norm": 3.6794142723083496,
      "learning_rate": 4.319171899529042e-05,
      "loss": 0.6675,
      "step": 693900
    },
    {
      "epoch": 10.894819466248038,
      "grad_norm": 3.9658799171447754,
      "learning_rate": 4.319073783359498e-05,
      "loss": 0.6093,
      "step": 694000
    },
    {
      "epoch": 10.896389324960754,
      "grad_norm": 3.6029908657073975,
      "learning_rate": 4.318975667189953e-05,
      "loss": 0.6168,
      "step": 694100
    },
    {
      "epoch": 10.89795918367347,
      "grad_norm": 3.100181818008423,
      "learning_rate": 4.318877551020408e-05,
      "loss": 0.6006,
      "step": 694200
    },
    {
      "epoch": 10.899529042386185,
      "grad_norm": 3.8826982975006104,
      "learning_rate": 4.3187794348508633e-05,
      "loss": 0.6432,
      "step": 694300
    },
    {
      "epoch": 10.901098901098901,
      "grad_norm": 3.9110476970672607,
      "learning_rate": 4.318681318681319e-05,
      "loss": 0.6331,
      "step": 694400
    },
    {
      "epoch": 10.902668759811617,
      "grad_norm": 3.905895709991455,
      "learning_rate": 4.318583202511774e-05,
      "loss": 0.6595,
      "step": 694500
    },
    {
      "epoch": 10.904238618524333,
      "grad_norm": 4.035303115844727,
      "learning_rate": 4.318485086342229e-05,
      "loss": 0.6464,
      "step": 694600
    },
    {
      "epoch": 10.905808477237048,
      "grad_norm": 3.114633083343506,
      "learning_rate": 4.318386970172685e-05,
      "loss": 0.6346,
      "step": 694700
    },
    {
      "epoch": 10.907378335949765,
      "grad_norm": 4.0604071617126465,
      "learning_rate": 4.3182888540031395e-05,
      "loss": 0.6372,
      "step": 694800
    },
    {
      "epoch": 10.90894819466248,
      "grad_norm": 3.817485809326172,
      "learning_rate": 4.318190737833595e-05,
      "loss": 0.6332,
      "step": 694900
    },
    {
      "epoch": 10.910518053375196,
      "grad_norm": 4.16301155090332,
      "learning_rate": 4.3180926216640504e-05,
      "loss": 0.6264,
      "step": 695000
    },
    {
      "epoch": 10.912087912087912,
      "grad_norm": 3.295785903930664,
      "learning_rate": 4.317994505494506e-05,
      "loss": 0.663,
      "step": 695100
    },
    {
      "epoch": 10.913657770800627,
      "grad_norm": 4.315247535705566,
      "learning_rate": 4.3178963893249606e-05,
      "loss": 0.6427,
      "step": 695200
    },
    {
      "epoch": 10.915227629513344,
      "grad_norm": 4.114363670349121,
      "learning_rate": 4.3177982731554164e-05,
      "loss": 0.6498,
      "step": 695300
    },
    {
      "epoch": 10.91679748822606,
      "grad_norm": 4.017918586730957,
      "learning_rate": 4.3177001569858715e-05,
      "loss": 0.6113,
      "step": 695400
    },
    {
      "epoch": 10.918367346938776,
      "grad_norm": 4.861684799194336,
      "learning_rate": 4.3176020408163266e-05,
      "loss": 0.6497,
      "step": 695500
    },
    {
      "epoch": 10.919937205651491,
      "grad_norm": 4.067112922668457,
      "learning_rate": 4.3175039246467817e-05,
      "loss": 0.6424,
      "step": 695600
    },
    {
      "epoch": 10.921507064364206,
      "grad_norm": 4.170499801635742,
      "learning_rate": 4.3174058084772374e-05,
      "loss": 0.6173,
      "step": 695700
    },
    {
      "epoch": 10.923076923076923,
      "grad_norm": 4.452065467834473,
      "learning_rate": 4.3173076923076925e-05,
      "loss": 0.6443,
      "step": 695800
    },
    {
      "epoch": 10.924646781789638,
      "grad_norm": 3.915336847305298,
      "learning_rate": 4.3172095761381476e-05,
      "loss": 0.6678,
      "step": 695900
    },
    {
      "epoch": 10.926216640502355,
      "grad_norm": 3.7163009643554688,
      "learning_rate": 4.317111459968603e-05,
      "loss": 0.5769,
      "step": 696000
    },
    {
      "epoch": 10.92778649921507,
      "grad_norm": 3.5236544609069824,
      "learning_rate": 4.3170133437990585e-05,
      "loss": 0.6449,
      "step": 696100
    },
    {
      "epoch": 10.929356357927787,
      "grad_norm": 4.010019779205322,
      "learning_rate": 4.3169152276295136e-05,
      "loss": 0.65,
      "step": 696200
    },
    {
      "epoch": 10.930926216640502,
      "grad_norm": 4.612865924835205,
      "learning_rate": 4.316817111459969e-05,
      "loss": 0.6432,
      "step": 696300
    },
    {
      "epoch": 10.932496075353217,
      "grad_norm": 3.1737799644470215,
      "learning_rate": 4.316718995290424e-05,
      "loss": 0.6368,
      "step": 696400
    },
    {
      "epoch": 10.934065934065934,
      "grad_norm": 3.2499337196350098,
      "learning_rate": 4.3166208791208796e-05,
      "loss": 0.5972,
      "step": 696500
    },
    {
      "epoch": 10.93563579277865,
      "grad_norm": 4.46315860748291,
      "learning_rate": 4.316522762951335e-05,
      "loss": 0.6244,
      "step": 696600
    },
    {
      "epoch": 10.937205651491366,
      "grad_norm": 4.185070037841797,
      "learning_rate": 4.31642464678179e-05,
      "loss": 0.6468,
      "step": 696700
    },
    {
      "epoch": 10.938775510204081,
      "grad_norm": 3.5566086769104004,
      "learning_rate": 4.3163265306122455e-05,
      "loss": 0.6422,
      "step": 696800
    },
    {
      "epoch": 10.940345368916798,
      "grad_norm": 3.2596404552459717,
      "learning_rate": 4.3162284144427e-05,
      "loss": 0.62,
      "step": 696900
    },
    {
      "epoch": 10.941915227629513,
      "grad_norm": 4.128676414489746,
      "learning_rate": 4.316130298273156e-05,
      "loss": 0.6216,
      "step": 697000
    },
    {
      "epoch": 10.943485086342228,
      "grad_norm": 3.6979727745056152,
      "learning_rate": 4.316032182103611e-05,
      "loss": 0.6406,
      "step": 697100
    },
    {
      "epoch": 10.945054945054945,
      "grad_norm": 5.173203945159912,
      "learning_rate": 4.3159340659340666e-05,
      "loss": 0.63,
      "step": 697200
    },
    {
      "epoch": 10.94662480376766,
      "grad_norm": 1.861701250076294,
      "learning_rate": 4.315835949764521e-05,
      "loss": 0.6546,
      "step": 697300
    },
    {
      "epoch": 10.948194662480377,
      "grad_norm": 4.158417701721191,
      "learning_rate": 4.315737833594977e-05,
      "loss": 0.6001,
      "step": 697400
    },
    {
      "epoch": 10.949764521193092,
      "grad_norm": 3.8150596618652344,
      "learning_rate": 4.315639717425432e-05,
      "loss": 0.6685,
      "step": 697500
    },
    {
      "epoch": 10.95133437990581,
      "grad_norm": 3.6037437915802,
      "learning_rate": 4.315541601255887e-05,
      "loss": 0.6229,
      "step": 697600
    },
    {
      "epoch": 10.952904238618524,
      "grad_norm": 3.8537778854370117,
      "learning_rate": 4.315443485086342e-05,
      "loss": 0.6422,
      "step": 697700
    },
    {
      "epoch": 10.95447409733124,
      "grad_norm": 3.6099512577056885,
      "learning_rate": 4.315345368916798e-05,
      "loss": 0.6482,
      "step": 697800
    },
    {
      "epoch": 10.956043956043956,
      "grad_norm": 4.223391532897949,
      "learning_rate": 4.315247252747253e-05,
      "loss": 0.61,
      "step": 697900
    },
    {
      "epoch": 10.957613814756671,
      "grad_norm": 3.8713347911834717,
      "learning_rate": 4.315149136577708e-05,
      "loss": 0.6218,
      "step": 698000
    },
    {
      "epoch": 10.959183673469388,
      "grad_norm": 4.0649003982543945,
      "learning_rate": 4.315051020408163e-05,
      "loss": 0.6111,
      "step": 698100
    },
    {
      "epoch": 10.960753532182103,
      "grad_norm": 4.186936855316162,
      "learning_rate": 4.314952904238619e-05,
      "loss": 0.6193,
      "step": 698200
    },
    {
      "epoch": 10.96232339089482,
      "grad_norm": 3.487797975540161,
      "learning_rate": 4.314854788069074e-05,
      "loss": 0.6716,
      "step": 698300
    },
    {
      "epoch": 10.963893249607535,
      "grad_norm": 3.60690975189209,
      "learning_rate": 4.314756671899529e-05,
      "loss": 0.6333,
      "step": 698400
    },
    {
      "epoch": 10.96546310832025,
      "grad_norm": 2.242988348007202,
      "learning_rate": 4.314658555729984e-05,
      "loss": 0.6511,
      "step": 698500
    },
    {
      "epoch": 10.967032967032967,
      "grad_norm": 3.274674415588379,
      "learning_rate": 4.31456043956044e-05,
      "loss": 0.644,
      "step": 698600
    },
    {
      "epoch": 10.968602825745682,
      "grad_norm": 3.316014528274536,
      "learning_rate": 4.314462323390895e-05,
      "loss": 0.6599,
      "step": 698700
    },
    {
      "epoch": 10.970172684458399,
      "grad_norm": 4.01392936706543,
      "learning_rate": 4.31436420722135e-05,
      "loss": 0.6628,
      "step": 698800
    },
    {
      "epoch": 10.971742543171114,
      "grad_norm": 3.3651063442230225,
      "learning_rate": 4.314266091051806e-05,
      "loss": 0.6299,
      "step": 698900
    },
    {
      "epoch": 10.973312401883831,
      "grad_norm": 3.092308521270752,
      "learning_rate": 4.3141679748822604e-05,
      "loss": 0.5805,
      "step": 699000
    },
    {
      "epoch": 10.974882260596546,
      "grad_norm": 4.156678199768066,
      "learning_rate": 4.314069858712716e-05,
      "loss": 0.6241,
      "step": 699100
    },
    {
      "epoch": 10.976452119309261,
      "grad_norm": 2.564582347869873,
      "learning_rate": 4.313971742543171e-05,
      "loss": 0.5935,
      "step": 699200
    },
    {
      "epoch": 10.978021978021978,
      "grad_norm": 3.805677652359009,
      "learning_rate": 4.313873626373627e-05,
      "loss": 0.6459,
      "step": 699300
    },
    {
      "epoch": 10.979591836734693,
      "grad_norm": 4.004954814910889,
      "learning_rate": 4.3137755102040815e-05,
      "loss": 0.6152,
      "step": 699400
    },
    {
      "epoch": 10.98116169544741,
      "grad_norm": 4.29968786239624,
      "learning_rate": 4.313677394034537e-05,
      "loss": 0.6335,
      "step": 699500
    },
    {
      "epoch": 10.982731554160125,
      "grad_norm": 2.7449140548706055,
      "learning_rate": 4.3135792778649924e-05,
      "loss": 0.6071,
      "step": 699600
    },
    {
      "epoch": 10.984301412872842,
      "grad_norm": 4.058915138244629,
      "learning_rate": 4.3134811616954475e-05,
      "loss": 0.6347,
      "step": 699700
    },
    {
      "epoch": 10.985871271585557,
      "grad_norm": 4.2915120124816895,
      "learning_rate": 4.3133830455259026e-05,
      "loss": 0.6473,
      "step": 699800
    },
    {
      "epoch": 10.987441130298274,
      "grad_norm": 4.012323379516602,
      "learning_rate": 4.313284929356358e-05,
      "loss": 0.6185,
      "step": 699900
    },
    {
      "epoch": 10.989010989010989,
      "grad_norm": 3.906813859939575,
      "learning_rate": 4.3131868131868134e-05,
      "loss": 0.6615,
      "step": 700000
    },
    {
      "epoch": 10.990580847723704,
      "grad_norm": 4.2817254066467285,
      "learning_rate": 4.3130886970172685e-05,
      "loss": 0.6245,
      "step": 700100
    },
    {
      "epoch": 10.992150706436421,
      "grad_norm": 3.2340450286865234,
      "learning_rate": 4.3129905808477236e-05,
      "loss": 0.6179,
      "step": 700200
    },
    {
      "epoch": 10.993720565149136,
      "grad_norm": 2.757188558578491,
      "learning_rate": 4.3128924646781794e-05,
      "loss": 0.5967,
      "step": 700300
    },
    {
      "epoch": 10.995290423861853,
      "grad_norm": 4.226624011993408,
      "learning_rate": 4.3127943485086345e-05,
      "loss": 0.6347,
      "step": 700400
    },
    {
      "epoch": 10.996860282574568,
      "grad_norm": 3.1108462810516357,
      "learning_rate": 4.3126962323390896e-05,
      "loss": 0.6081,
      "step": 700500
    },
    {
      "epoch": 10.998430141287285,
      "grad_norm": 4.639113426208496,
      "learning_rate": 4.312598116169545e-05,
      "loss": 0.6214,
      "step": 700600
    },
    {
      "epoch": 11.0,
      "grad_norm": 2.6194775104522705,
      "learning_rate": 4.3125000000000005e-05,
      "loss": 0.6908,
      "step": 700700
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.0360296964645386,
      "eval_runtime": 14.6955,
      "eval_samples_per_second": 228.164,
      "eval_steps_per_second": 228.164,
      "step": 700700
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.49424654245376587,
      "eval_runtime": 283.9611,
      "eval_samples_per_second": 224.326,
      "eval_steps_per_second": 224.326,
      "step": 700700
    },
    {
      "epoch": 11.001569858712715,
      "grad_norm": 2.8832857608795166,
      "learning_rate": 4.3124018838304556e-05,
      "loss": 0.6599,
      "step": 700800
    },
    {
      "epoch": 11.003139717425432,
      "grad_norm": 4.547738075256348,
      "learning_rate": 4.312303767660911e-05,
      "loss": 0.66,
      "step": 700900
    },
    {
      "epoch": 11.004709576138147,
      "grad_norm": 3.819866895675659,
      "learning_rate": 4.3122056514913664e-05,
      "loss": 0.6309,
      "step": 701000
    },
    {
      "epoch": 11.006279434850864,
      "grad_norm": 3.3829987049102783,
      "learning_rate": 4.312107535321821e-05,
      "loss": 0.6763,
      "step": 701100
    },
    {
      "epoch": 11.007849293563579,
      "grad_norm": 4.024527072906494,
      "learning_rate": 4.3120094191522766e-05,
      "loss": 0.6308,
      "step": 701200
    },
    {
      "epoch": 11.009419152276296,
      "grad_norm": 5.096527099609375,
      "learning_rate": 4.311911302982732e-05,
      "loss": 0.6211,
      "step": 701300
    },
    {
      "epoch": 11.010989010989011,
      "grad_norm": 3.0466675758361816,
      "learning_rate": 4.3118131868131875e-05,
      "loss": 0.6302,
      "step": 701400
    },
    {
      "epoch": 11.012558869701726,
      "grad_norm": 4.424627304077148,
      "learning_rate": 4.311715070643642e-05,
      "loss": 0.6278,
      "step": 701500
    },
    {
      "epoch": 11.014128728414443,
      "grad_norm": 3.725966453552246,
      "learning_rate": 4.311616954474098e-05,
      "loss": 0.6184,
      "step": 701600
    },
    {
      "epoch": 11.015698587127158,
      "grad_norm": 3.558983087539673,
      "learning_rate": 4.311518838304553e-05,
      "loss": 0.6001,
      "step": 701700
    },
    {
      "epoch": 11.017268445839875,
      "grad_norm": 3.8156027793884277,
      "learning_rate": 4.311420722135008e-05,
      "loss": 0.5879,
      "step": 701800
    },
    {
      "epoch": 11.01883830455259,
      "grad_norm": 4.223824977874756,
      "learning_rate": 4.311322605965463e-05,
      "loss": 0.6222,
      "step": 701900
    },
    {
      "epoch": 11.020408163265307,
      "grad_norm": 3.289994955062866,
      "learning_rate": 4.311224489795919e-05,
      "loss": 0.5962,
      "step": 702000
    },
    {
      "epoch": 11.021978021978022,
      "grad_norm": 4.659826755523682,
      "learning_rate": 4.311126373626374e-05,
      "loss": 0.6205,
      "step": 702100
    },
    {
      "epoch": 11.023547880690737,
      "grad_norm": 4.152022361755371,
      "learning_rate": 4.311028257456829e-05,
      "loss": 0.6622,
      "step": 702200
    },
    {
      "epoch": 11.025117739403454,
      "grad_norm": 3.844876527786255,
      "learning_rate": 4.310930141287284e-05,
      "loss": 0.6231,
      "step": 702300
    },
    {
      "epoch": 11.026687598116169,
      "grad_norm": 4.523307800292969,
      "learning_rate": 4.31083202511774e-05,
      "loss": 0.647,
      "step": 702400
    },
    {
      "epoch": 11.028257456828886,
      "grad_norm": 2.9635589122772217,
      "learning_rate": 4.310733908948195e-05,
      "loss": 0.6342,
      "step": 702500
    },
    {
      "epoch": 11.029827315541601,
      "grad_norm": 3.014993667602539,
      "learning_rate": 4.31063579277865e-05,
      "loss": 0.6248,
      "step": 702600
    },
    {
      "epoch": 11.031397174254318,
      "grad_norm": 4.353433609008789,
      "learning_rate": 4.310537676609105e-05,
      "loss": 0.6262,
      "step": 702700
    },
    {
      "epoch": 11.032967032967033,
      "grad_norm": 4.191205978393555,
      "learning_rate": 4.310439560439561e-05,
      "loss": 0.6024,
      "step": 702800
    },
    {
      "epoch": 11.034536891679748,
      "grad_norm": 4.2140350341796875,
      "learning_rate": 4.310341444270016e-05,
      "loss": 0.6596,
      "step": 702900
    },
    {
      "epoch": 11.036106750392465,
      "grad_norm": 3.94425106048584,
      "learning_rate": 4.310243328100471e-05,
      "loss": 0.6471,
      "step": 703000
    },
    {
      "epoch": 11.03767660910518,
      "grad_norm": 3.404165506362915,
      "learning_rate": 4.310145211930927e-05,
      "loss": 0.5845,
      "step": 703100
    },
    {
      "epoch": 11.039246467817897,
      "grad_norm": 4.208406925201416,
      "learning_rate": 4.310047095761381e-05,
      "loss": 0.6142,
      "step": 703200
    },
    {
      "epoch": 11.040816326530612,
      "grad_norm": 4.62391471862793,
      "learning_rate": 4.309948979591837e-05,
      "loss": 0.6413,
      "step": 703300
    },
    {
      "epoch": 11.042386185243329,
      "grad_norm": 3.0543618202209473,
      "learning_rate": 4.309850863422292e-05,
      "loss": 0.6152,
      "step": 703400
    },
    {
      "epoch": 11.043956043956044,
      "grad_norm": 4.0608015060424805,
      "learning_rate": 4.309752747252748e-05,
      "loss": 0.601,
      "step": 703500
    },
    {
      "epoch": 11.04552590266876,
      "grad_norm": 4.863288879394531,
      "learning_rate": 4.3096546310832024e-05,
      "loss": 0.6262,
      "step": 703600
    },
    {
      "epoch": 11.047095761381476,
      "grad_norm": 3.7488651275634766,
      "learning_rate": 4.309556514913658e-05,
      "loss": 0.6185,
      "step": 703700
    },
    {
      "epoch": 11.04866562009419,
      "grad_norm": 4.290128707885742,
      "learning_rate": 4.309458398744113e-05,
      "loss": 0.6156,
      "step": 703800
    },
    {
      "epoch": 11.050235478806908,
      "grad_norm": 2.390913724899292,
      "learning_rate": 4.3093602825745684e-05,
      "loss": 0.5658,
      "step": 703900
    },
    {
      "epoch": 11.051805337519623,
      "grad_norm": 3.571443557739258,
      "learning_rate": 4.3092621664050235e-05,
      "loss": 0.6185,
      "step": 704000
    },
    {
      "epoch": 11.05337519623234,
      "grad_norm": 3.104278326034546,
      "learning_rate": 4.309164050235479e-05,
      "loss": 0.641,
      "step": 704100
    },
    {
      "epoch": 11.054945054945055,
      "grad_norm": 3.1452553272247314,
      "learning_rate": 4.309065934065934e-05,
      "loss": 0.6644,
      "step": 704200
    },
    {
      "epoch": 11.056514913657772,
      "grad_norm": 4.710837364196777,
      "learning_rate": 4.3089678178963894e-05,
      "loss": 0.6366,
      "step": 704300
    },
    {
      "epoch": 11.058084772370487,
      "grad_norm": 3.163140296936035,
      "learning_rate": 4.3088697017268445e-05,
      "loss": 0.6225,
      "step": 704400
    },
    {
      "epoch": 11.059654631083202,
      "grad_norm": 4.7133684158325195,
      "learning_rate": 4.3087715855573e-05,
      "loss": 0.6027,
      "step": 704500
    },
    {
      "epoch": 11.061224489795919,
      "grad_norm": 4.514279842376709,
      "learning_rate": 4.3086734693877554e-05,
      "loss": 0.7148,
      "step": 704600
    },
    {
      "epoch": 11.062794348508634,
      "grad_norm": 4.017209529876709,
      "learning_rate": 4.3085753532182105e-05,
      "loss": 0.6491,
      "step": 704700
    },
    {
      "epoch": 11.06436420722135,
      "grad_norm": 2.5193800926208496,
      "learning_rate": 4.3084772370486656e-05,
      "loss": 0.6032,
      "step": 704800
    },
    {
      "epoch": 11.065934065934066,
      "grad_norm": 3.947619915008545,
      "learning_rate": 4.3083791208791214e-05,
      "loss": 0.6106,
      "step": 704900
    },
    {
      "epoch": 11.067503924646783,
      "grad_norm": 3.88984751701355,
      "learning_rate": 4.3082810047095765e-05,
      "loss": 0.6419,
      "step": 705000
    },
    {
      "epoch": 11.069073783359498,
      "grad_norm": 3.525448799133301,
      "learning_rate": 4.3081828885400316e-05,
      "loss": 0.6127,
      "step": 705100
    },
    {
      "epoch": 11.070643642072213,
      "grad_norm": 3.604729652404785,
      "learning_rate": 4.3080847723704873e-05,
      "loss": 0.6286,
      "step": 705200
    },
    {
      "epoch": 11.07221350078493,
      "grad_norm": 2.453885316848755,
      "learning_rate": 4.307986656200942e-05,
      "loss": 0.6508,
      "step": 705300
    },
    {
      "epoch": 11.073783359497645,
      "grad_norm": 3.0365421772003174,
      "learning_rate": 4.3078885400313975e-05,
      "loss": 0.6164,
      "step": 705400
    },
    {
      "epoch": 11.075353218210362,
      "grad_norm": 4.304742336273193,
      "learning_rate": 4.3077904238618526e-05,
      "loss": 0.6376,
      "step": 705500
    },
    {
      "epoch": 11.076923076923077,
      "grad_norm": 3.9258530139923096,
      "learning_rate": 4.3076923076923084e-05,
      "loss": 0.6477,
      "step": 705600
    },
    {
      "epoch": 11.078492935635794,
      "grad_norm": 3.713810682296753,
      "learning_rate": 4.307594191522763e-05,
      "loss": 0.6246,
      "step": 705700
    },
    {
      "epoch": 11.080062794348509,
      "grad_norm": 3.972015142440796,
      "learning_rate": 4.3074960753532186e-05,
      "loss": 0.6193,
      "step": 705800
    },
    {
      "epoch": 11.081632653061224,
      "grad_norm": 4.018720626831055,
      "learning_rate": 4.307397959183674e-05,
      "loss": 0.6386,
      "step": 705900
    },
    {
      "epoch": 11.08320251177394,
      "grad_norm": 4.049192905426025,
      "learning_rate": 4.307299843014129e-05,
      "loss": 0.6337,
      "step": 706000
    },
    {
      "epoch": 11.084772370486656,
      "grad_norm": 4.650759220123291,
      "learning_rate": 4.307201726844584e-05,
      "loss": 0.5926,
      "step": 706100
    },
    {
      "epoch": 11.086342229199373,
      "grad_norm": 4.5131096839904785,
      "learning_rate": 4.30710361067504e-05,
      "loss": 0.6368,
      "step": 706200
    },
    {
      "epoch": 11.087912087912088,
      "grad_norm": 3.9162869453430176,
      "learning_rate": 4.307005494505495e-05,
      "loss": 0.6083,
      "step": 706300
    },
    {
      "epoch": 11.089481946624804,
      "grad_norm": 2.9860448837280273,
      "learning_rate": 4.30690737833595e-05,
      "loss": 0.5914,
      "step": 706400
    },
    {
      "epoch": 11.09105180533752,
      "grad_norm": 3.7206785678863525,
      "learning_rate": 4.306809262166405e-05,
      "loss": 0.6048,
      "step": 706500
    },
    {
      "epoch": 11.092621664050235,
      "grad_norm": 2.6826553344726562,
      "learning_rate": 4.306711145996861e-05,
      "loss": 0.5926,
      "step": 706600
    },
    {
      "epoch": 11.094191522762952,
      "grad_norm": 3.848926544189453,
      "learning_rate": 4.306613029827316e-05,
      "loss": 0.6092,
      "step": 706700
    },
    {
      "epoch": 11.095761381475667,
      "grad_norm": 4.1455230712890625,
      "learning_rate": 4.306514913657771e-05,
      "loss": 0.6132,
      "step": 706800
    },
    {
      "epoch": 11.097331240188383,
      "grad_norm": 4.31754207611084,
      "learning_rate": 4.306416797488226e-05,
      "loss": 0.5814,
      "step": 706900
    },
    {
      "epoch": 11.098901098901099,
      "grad_norm": 4.176183223724365,
      "learning_rate": 4.306318681318682e-05,
      "loss": 0.6351,
      "step": 707000
    },
    {
      "epoch": 11.100470957613815,
      "grad_norm": 4.205347537994385,
      "learning_rate": 4.306220565149137e-05,
      "loss": 0.6448,
      "step": 707100
    },
    {
      "epoch": 11.10204081632653,
      "grad_norm": 3.939545154571533,
      "learning_rate": 4.306122448979592e-05,
      "loss": 0.6236,
      "step": 707200
    },
    {
      "epoch": 11.103610675039246,
      "grad_norm": 3.2121098041534424,
      "learning_rate": 4.306024332810048e-05,
      "loss": 0.6342,
      "step": 707300
    },
    {
      "epoch": 11.105180533751962,
      "grad_norm": 3.7186460494995117,
      "learning_rate": 4.305926216640502e-05,
      "loss": 0.6169,
      "step": 707400
    },
    {
      "epoch": 11.106750392464678,
      "grad_norm": 4.69944953918457,
      "learning_rate": 4.305828100470958e-05,
      "loss": 0.6007,
      "step": 707500
    },
    {
      "epoch": 11.108320251177394,
      "grad_norm": 4.136687755584717,
      "learning_rate": 4.305729984301413e-05,
      "loss": 0.6239,
      "step": 707600
    },
    {
      "epoch": 11.10989010989011,
      "grad_norm": 4.147644519805908,
      "learning_rate": 4.305631868131869e-05,
      "loss": 0.6438,
      "step": 707700
    },
    {
      "epoch": 11.111459968602826,
      "grad_norm": 4.168058395385742,
      "learning_rate": 4.305533751962323e-05,
      "loss": 0.6261,
      "step": 707800
    },
    {
      "epoch": 11.113029827315541,
      "grad_norm": 3.9948558807373047,
      "learning_rate": 4.305435635792779e-05,
      "loss": 0.6227,
      "step": 707900
    },
    {
      "epoch": 11.114599686028258,
      "grad_norm": 3.859035015106201,
      "learning_rate": 4.305337519623234e-05,
      "loss": 0.6119,
      "step": 708000
    },
    {
      "epoch": 11.116169544740973,
      "grad_norm": 5.123030185699463,
      "learning_rate": 4.305239403453689e-05,
      "loss": 0.6235,
      "step": 708100
    },
    {
      "epoch": 11.117739403453688,
      "grad_norm": 3.7462761402130127,
      "learning_rate": 4.3051412872841443e-05,
      "loss": 0.6332,
      "step": 708200
    },
    {
      "epoch": 11.119309262166405,
      "grad_norm": 4.520572662353516,
      "learning_rate": 4.3050431711146e-05,
      "loss": 0.65,
      "step": 708300
    },
    {
      "epoch": 11.12087912087912,
      "grad_norm": 3.7736403942108154,
      "learning_rate": 4.304945054945055e-05,
      "loss": 0.6055,
      "step": 708400
    },
    {
      "epoch": 11.122448979591837,
      "grad_norm": 4.559826850891113,
      "learning_rate": 4.30484693877551e-05,
      "loss": 0.6136,
      "step": 708500
    },
    {
      "epoch": 11.124018838304552,
      "grad_norm": 1.9714592695236206,
      "learning_rate": 4.3047488226059654e-05,
      "loss": 0.6143,
      "step": 708600
    },
    {
      "epoch": 11.12558869701727,
      "grad_norm": 3.1399683952331543,
      "learning_rate": 4.304650706436421e-05,
      "loss": 0.5941,
      "step": 708700
    },
    {
      "epoch": 11.127158555729984,
      "grad_norm": 5.028720378875732,
      "learning_rate": 4.304552590266876e-05,
      "loss": 0.6059,
      "step": 708800
    },
    {
      "epoch": 11.1287284144427,
      "grad_norm": 3.702035427093506,
      "learning_rate": 4.3044544740973314e-05,
      "loss": 0.6157,
      "step": 708900
    },
    {
      "epoch": 11.130298273155416,
      "grad_norm": 4.31995964050293,
      "learning_rate": 4.3043563579277865e-05,
      "loss": 0.6375,
      "step": 709000
    },
    {
      "epoch": 11.131868131868131,
      "grad_norm": 3.923738718032837,
      "learning_rate": 4.304258241758242e-05,
      "loss": 0.5731,
      "step": 709100
    },
    {
      "epoch": 11.133437990580848,
      "grad_norm": 4.61243200302124,
      "learning_rate": 4.3041601255886974e-05,
      "loss": 0.6677,
      "step": 709200
    },
    {
      "epoch": 11.135007849293563,
      "grad_norm": 4.052369117736816,
      "learning_rate": 4.3040620094191525e-05,
      "loss": 0.6308,
      "step": 709300
    },
    {
      "epoch": 11.13657770800628,
      "grad_norm": 2.9109811782836914,
      "learning_rate": 4.303963893249608e-05,
      "loss": 0.5958,
      "step": 709400
    },
    {
      "epoch": 11.138147566718995,
      "grad_norm": 4.197318077087402,
      "learning_rate": 4.3038657770800627e-05,
      "loss": 0.6175,
      "step": 709500
    },
    {
      "epoch": 11.13971742543171,
      "grad_norm": 3.0405330657958984,
      "learning_rate": 4.3037676609105184e-05,
      "loss": 0.6505,
      "step": 709600
    },
    {
      "epoch": 11.141287284144427,
      "grad_norm": 3.5410687923431396,
      "learning_rate": 4.3036695447409735e-05,
      "loss": 0.6526,
      "step": 709700
    },
    {
      "epoch": 11.142857142857142,
      "grad_norm": 2.837935209274292,
      "learning_rate": 4.303571428571429e-05,
      "loss": 0.6034,
      "step": 709800
    },
    {
      "epoch": 11.14442700156986,
      "grad_norm": 4.747807502746582,
      "learning_rate": 4.303473312401884e-05,
      "loss": 0.6413,
      "step": 709900
    },
    {
      "epoch": 11.145996860282574,
      "grad_norm": 4.428218364715576,
      "learning_rate": 4.3033751962323395e-05,
      "loss": 0.6302,
      "step": 710000
    },
    {
      "epoch": 11.147566718995291,
      "grad_norm": 4.5180230140686035,
      "learning_rate": 4.3032770800627946e-05,
      "loss": 0.6227,
      "step": 710100
    },
    {
      "epoch": 11.149136577708006,
      "grad_norm": 3.4326870441436768,
      "learning_rate": 4.30317896389325e-05,
      "loss": 0.6146,
      "step": 710200
    },
    {
      "epoch": 11.150706436420721,
      "grad_norm": 3.629758596420288,
      "learning_rate": 4.303080847723705e-05,
      "loss": 0.6507,
      "step": 710300
    },
    {
      "epoch": 11.152276295133438,
      "grad_norm": 4.652272701263428,
      "learning_rate": 4.3029827315541606e-05,
      "loss": 0.5923,
      "step": 710400
    },
    {
      "epoch": 11.153846153846153,
      "grad_norm": 5.200087070465088,
      "learning_rate": 4.302884615384616e-05,
      "loss": 0.615,
      "step": 710500
    },
    {
      "epoch": 11.15541601255887,
      "grad_norm": 3.6406612396240234,
      "learning_rate": 4.302786499215071e-05,
      "loss": 0.6485,
      "step": 710600
    },
    {
      "epoch": 11.156985871271585,
      "grad_norm": 4.786310195922852,
      "learning_rate": 4.302688383045526e-05,
      "loss": 0.6167,
      "step": 710700
    },
    {
      "epoch": 11.158555729984302,
      "grad_norm": 3.5616002082824707,
      "learning_rate": 4.3025902668759816e-05,
      "loss": 0.6042,
      "step": 710800
    },
    {
      "epoch": 11.160125588697017,
      "grad_norm": 3.9644033908843994,
      "learning_rate": 4.302492150706437e-05,
      "loss": 0.6099,
      "step": 710900
    },
    {
      "epoch": 11.161695447409732,
      "grad_norm": 3.0341827869415283,
      "learning_rate": 4.302394034536892e-05,
      "loss": 0.6565,
      "step": 711000
    },
    {
      "epoch": 11.16326530612245,
      "grad_norm": 4.543163776397705,
      "learning_rate": 4.302295918367347e-05,
      "loss": 0.6075,
      "step": 711100
    },
    {
      "epoch": 11.164835164835164,
      "grad_norm": 2.826080322265625,
      "learning_rate": 4.302197802197803e-05,
      "loss": 0.6133,
      "step": 711200
    },
    {
      "epoch": 11.166405023547881,
      "grad_norm": 4.074195384979248,
      "learning_rate": 4.302099686028258e-05,
      "loss": 0.633,
      "step": 711300
    },
    {
      "epoch": 11.167974882260596,
      "grad_norm": 3.320023536682129,
      "learning_rate": 4.302001569858713e-05,
      "loss": 0.6183,
      "step": 711400
    },
    {
      "epoch": 11.169544740973313,
      "grad_norm": 4.460172653198242,
      "learning_rate": 4.301903453689169e-05,
      "loss": 0.6215,
      "step": 711500
    },
    {
      "epoch": 11.171114599686028,
      "grad_norm": 4.091284275054932,
      "learning_rate": 4.301805337519623e-05,
      "loss": 0.6342,
      "step": 711600
    },
    {
      "epoch": 11.172684458398743,
      "grad_norm": 4.939128398895264,
      "learning_rate": 4.301707221350079e-05,
      "loss": 0.6625,
      "step": 711700
    },
    {
      "epoch": 11.17425431711146,
      "grad_norm": 4.047102451324463,
      "learning_rate": 4.301609105180534e-05,
      "loss": 0.6521,
      "step": 711800
    },
    {
      "epoch": 11.175824175824175,
      "grad_norm": 3.8742258548736572,
      "learning_rate": 4.30151098901099e-05,
      "loss": 0.6328,
      "step": 711900
    },
    {
      "epoch": 11.177394034536892,
      "grad_norm": 3.5351808071136475,
      "learning_rate": 4.301412872841444e-05,
      "loss": 0.6612,
      "step": 712000
    },
    {
      "epoch": 11.178963893249607,
      "grad_norm": 3.6662631034851074,
      "learning_rate": 4.3013147566719e-05,
      "loss": 0.6298,
      "step": 712100
    },
    {
      "epoch": 11.180533751962324,
      "grad_norm": 3.5980749130249023,
      "learning_rate": 4.301216640502355e-05,
      "loss": 0.64,
      "step": 712200
    },
    {
      "epoch": 11.182103610675039,
      "grad_norm": 4.667815208435059,
      "learning_rate": 4.30111852433281e-05,
      "loss": 0.6407,
      "step": 712300
    },
    {
      "epoch": 11.183673469387756,
      "grad_norm": 4.200761318206787,
      "learning_rate": 4.301020408163265e-05,
      "loss": 0.6447,
      "step": 712400
    },
    {
      "epoch": 11.185243328100471,
      "grad_norm": 3.933819055557251,
      "learning_rate": 4.300922291993721e-05,
      "loss": 0.6286,
      "step": 712500
    },
    {
      "epoch": 11.186813186813186,
      "grad_norm": 3.970305919647217,
      "learning_rate": 4.300824175824176e-05,
      "loss": 0.6558,
      "step": 712600
    },
    {
      "epoch": 11.188383045525903,
      "grad_norm": 3.515316963195801,
      "learning_rate": 4.300726059654631e-05,
      "loss": 0.6196,
      "step": 712700
    },
    {
      "epoch": 11.189952904238618,
      "grad_norm": 3.51348614692688,
      "learning_rate": 4.300627943485086e-05,
      "loss": 0.6503,
      "step": 712800
    },
    {
      "epoch": 11.191522762951335,
      "grad_norm": 3.9207394123077393,
      "learning_rate": 4.300529827315542e-05,
      "loss": 0.6475,
      "step": 712900
    },
    {
      "epoch": 11.19309262166405,
      "grad_norm": 3.2241647243499756,
      "learning_rate": 4.300431711145997e-05,
      "loss": 0.6557,
      "step": 713000
    },
    {
      "epoch": 11.194662480376767,
      "grad_norm": 4.308250904083252,
      "learning_rate": 4.300333594976452e-05,
      "loss": 0.5812,
      "step": 713100
    },
    {
      "epoch": 11.196232339089482,
      "grad_norm": 3.75100040435791,
      "learning_rate": 4.3002354788069074e-05,
      "loss": 0.635,
      "step": 713200
    },
    {
      "epoch": 11.197802197802197,
      "grad_norm": 3.7737226486206055,
      "learning_rate": 4.300137362637363e-05,
      "loss": 0.6416,
      "step": 713300
    },
    {
      "epoch": 11.199372056514914,
      "grad_norm": 4.015284538269043,
      "learning_rate": 4.300039246467818e-05,
      "loss": 0.631,
      "step": 713400
    },
    {
      "epoch": 11.200941915227629,
      "grad_norm": 3.906778573989868,
      "learning_rate": 4.2999411302982734e-05,
      "loss": 0.6066,
      "step": 713500
    },
    {
      "epoch": 11.202511773940346,
      "grad_norm": 4.399133205413818,
      "learning_rate": 4.299843014128729e-05,
      "loss": 0.6263,
      "step": 713600
    },
    {
      "epoch": 11.204081632653061,
      "grad_norm": 3.765939950942993,
      "learning_rate": 4.2997448979591836e-05,
      "loss": 0.6391,
      "step": 713700
    },
    {
      "epoch": 11.205651491365778,
      "grad_norm": 5.719653606414795,
      "learning_rate": 4.299646781789639e-05,
      "loss": 0.6064,
      "step": 713800
    },
    {
      "epoch": 11.207221350078493,
      "grad_norm": 2.987849473953247,
      "learning_rate": 4.2995486656200944e-05,
      "loss": 0.5962,
      "step": 713900
    },
    {
      "epoch": 11.208791208791208,
      "grad_norm": 4.243715286254883,
      "learning_rate": 4.29945054945055e-05,
      "loss": 0.6242,
      "step": 714000
    },
    {
      "epoch": 11.210361067503925,
      "grad_norm": 4.702935695648193,
      "learning_rate": 4.2993524332810046e-05,
      "loss": 0.6101,
      "step": 714100
    },
    {
      "epoch": 11.21193092621664,
      "grad_norm": 3.30773663520813,
      "learning_rate": 4.2992543171114604e-05,
      "loss": 0.6542,
      "step": 714200
    },
    {
      "epoch": 11.213500784929357,
      "grad_norm": 3.39876127243042,
      "learning_rate": 4.2991562009419155e-05,
      "loss": 0.6265,
      "step": 714300
    },
    {
      "epoch": 11.215070643642072,
      "grad_norm": 3.9712657928466797,
      "learning_rate": 4.2990580847723706e-05,
      "loss": 0.6316,
      "step": 714400
    },
    {
      "epoch": 11.216640502354789,
      "grad_norm": 4.001103401184082,
      "learning_rate": 4.298959968602826e-05,
      "loss": 0.6102,
      "step": 714500
    },
    {
      "epoch": 11.218210361067504,
      "grad_norm": 4.285107612609863,
      "learning_rate": 4.2988618524332815e-05,
      "loss": 0.639,
      "step": 714600
    },
    {
      "epoch": 11.219780219780219,
      "grad_norm": 4.315701007843018,
      "learning_rate": 4.2987637362637366e-05,
      "loss": 0.6413,
      "step": 714700
    },
    {
      "epoch": 11.221350078492936,
      "grad_norm": 4.535362720489502,
      "learning_rate": 4.298665620094192e-05,
      "loss": 0.6512,
      "step": 714800
    },
    {
      "epoch": 11.222919937205651,
      "grad_norm": 3.7270302772521973,
      "learning_rate": 4.298567503924647e-05,
      "loss": 0.6501,
      "step": 714900
    },
    {
      "epoch": 11.224489795918368,
      "grad_norm": 4.568798065185547,
      "learning_rate": 4.2984693877551025e-05,
      "loss": 0.6276,
      "step": 715000
    },
    {
      "epoch": 11.226059654631083,
      "grad_norm": 3.9399988651275635,
      "learning_rate": 4.2983712715855576e-05,
      "loss": 0.6342,
      "step": 715100
    },
    {
      "epoch": 11.2276295133438,
      "grad_norm": 4.437524318695068,
      "learning_rate": 4.298273155416013e-05,
      "loss": 0.6206,
      "step": 715200
    },
    {
      "epoch": 11.229199372056515,
      "grad_norm": 4.08397912979126,
      "learning_rate": 4.298175039246468e-05,
      "loss": 0.6323,
      "step": 715300
    },
    {
      "epoch": 11.23076923076923,
      "grad_norm": 4.8088274002075195,
      "learning_rate": 4.2980769230769236e-05,
      "loss": 0.6492,
      "step": 715400
    },
    {
      "epoch": 11.232339089481947,
      "grad_norm": 3.488419771194458,
      "learning_rate": 4.297978806907379e-05,
      "loss": 0.674,
      "step": 715500
    },
    {
      "epoch": 11.233908948194662,
      "grad_norm": 3.8960821628570557,
      "learning_rate": 4.297880690737834e-05,
      "loss": 0.6374,
      "step": 715600
    },
    {
      "epoch": 11.235478806907379,
      "grad_norm": 3.781672716140747,
      "learning_rate": 4.2977825745682896e-05,
      "loss": 0.611,
      "step": 715700
    },
    {
      "epoch": 11.237048665620094,
      "grad_norm": 2.7026607990264893,
      "learning_rate": 4.297684458398744e-05,
      "loss": 0.6112,
      "step": 715800
    },
    {
      "epoch": 11.23861852433281,
      "grad_norm": 4.536919593811035,
      "learning_rate": 4.2975863422292e-05,
      "loss": 0.6168,
      "step": 715900
    },
    {
      "epoch": 11.240188383045526,
      "grad_norm": 3.826570987701416,
      "learning_rate": 4.297488226059655e-05,
      "loss": 0.657,
      "step": 716000
    },
    {
      "epoch": 11.241758241758241,
      "grad_norm": 4.525138854980469,
      "learning_rate": 4.2973901098901107e-05,
      "loss": 0.589,
      "step": 716100
    },
    {
      "epoch": 11.243328100470958,
      "grad_norm": 3.559356927871704,
      "learning_rate": 4.297291993720565e-05,
      "loss": 0.6159,
      "step": 716200
    },
    {
      "epoch": 11.244897959183673,
      "grad_norm": 3.4812140464782715,
      "learning_rate": 4.297193877551021e-05,
      "loss": 0.6028,
      "step": 716300
    },
    {
      "epoch": 11.24646781789639,
      "grad_norm": 4.212573051452637,
      "learning_rate": 4.297095761381476e-05,
      "loss": 0.6141,
      "step": 716400
    },
    {
      "epoch": 11.248037676609105,
      "grad_norm": 4.73187780380249,
      "learning_rate": 4.296997645211931e-05,
      "loss": 0.5731,
      "step": 716500
    },
    {
      "epoch": 11.249607535321822,
      "grad_norm": 4.014882564544678,
      "learning_rate": 4.296899529042386e-05,
      "loss": 0.6328,
      "step": 716600
    },
    {
      "epoch": 11.251177394034537,
      "grad_norm": 4.463264465332031,
      "learning_rate": 4.296801412872842e-05,
      "loss": 0.6291,
      "step": 716700
    },
    {
      "epoch": 11.252747252747252,
      "grad_norm": 2.8823912143707275,
      "learning_rate": 4.2967032967032963e-05,
      "loss": 0.622,
      "step": 716800
    },
    {
      "epoch": 11.254317111459969,
      "grad_norm": 3.1127288341522217,
      "learning_rate": 4.296605180533752e-05,
      "loss": 0.6438,
      "step": 716900
    },
    {
      "epoch": 11.255886970172684,
      "grad_norm": 4.254656791687012,
      "learning_rate": 4.296507064364207e-05,
      "loss": 0.5884,
      "step": 717000
    },
    {
      "epoch": 11.2574568288854,
      "grad_norm": 4.092825412750244,
      "learning_rate": 4.296408948194663e-05,
      "loss": 0.6325,
      "step": 717100
    },
    {
      "epoch": 11.259026687598116,
      "grad_norm": 2.795442819595337,
      "learning_rate": 4.2963108320251174e-05,
      "loss": 0.6508,
      "step": 717200
    },
    {
      "epoch": 11.260596546310833,
      "grad_norm": 3.7819502353668213,
      "learning_rate": 4.296212715855573e-05,
      "loss": 0.6802,
      "step": 717300
    },
    {
      "epoch": 11.262166405023548,
      "grad_norm": 2.9861016273498535,
      "learning_rate": 4.296114599686028e-05,
      "loss": 0.6015,
      "step": 717400
    },
    {
      "epoch": 11.263736263736265,
      "grad_norm": 4.251194953918457,
      "learning_rate": 4.2960164835164834e-05,
      "loss": 0.6574,
      "step": 717500
    },
    {
      "epoch": 11.26530612244898,
      "grad_norm": 3.4026787281036377,
      "learning_rate": 4.295918367346939e-05,
      "loss": 0.5971,
      "step": 717600
    },
    {
      "epoch": 11.266875981161695,
      "grad_norm": 2.8666954040527344,
      "learning_rate": 4.295820251177394e-05,
      "loss": 0.6188,
      "step": 717700
    },
    {
      "epoch": 11.268445839874412,
      "grad_norm": 3.3998959064483643,
      "learning_rate": 4.29572213500785e-05,
      "loss": 0.5988,
      "step": 717800
    },
    {
      "epoch": 11.270015698587127,
      "grad_norm": 3.2756924629211426,
      "learning_rate": 4.2956240188383045e-05,
      "loss": 0.6513,
      "step": 717900
    },
    {
      "epoch": 11.271585557299844,
      "grad_norm": 2.5149407386779785,
      "learning_rate": 4.29552590266876e-05,
      "loss": 0.6201,
      "step": 718000
    },
    {
      "epoch": 11.273155416012559,
      "grad_norm": 3.981623411178589,
      "learning_rate": 4.295427786499215e-05,
      "loss": 0.6352,
      "step": 718100
    },
    {
      "epoch": 11.274725274725276,
      "grad_norm": 3.9503555297851562,
      "learning_rate": 4.2953296703296704e-05,
      "loss": 0.6478,
      "step": 718200
    },
    {
      "epoch": 11.27629513343799,
      "grad_norm": 4.1526103019714355,
      "learning_rate": 4.2952315541601255e-05,
      "loss": 0.5919,
      "step": 718300
    },
    {
      "epoch": 11.277864992150706,
      "grad_norm": 3.701765537261963,
      "learning_rate": 4.295133437990581e-05,
      "loss": 0.6036,
      "step": 718400
    },
    {
      "epoch": 11.279434850863423,
      "grad_norm": 3.936957836151123,
      "learning_rate": 4.2950353218210364e-05,
      "loss": 0.6555,
      "step": 718500
    },
    {
      "epoch": 11.281004709576138,
      "grad_norm": 4.485635280609131,
      "learning_rate": 4.2949372056514915e-05,
      "loss": 0.5817,
      "step": 718600
    },
    {
      "epoch": 11.282574568288855,
      "grad_norm": 4.402475357055664,
      "learning_rate": 4.2948390894819466e-05,
      "loss": 0.6442,
      "step": 718700
    },
    {
      "epoch": 11.28414442700157,
      "grad_norm": 3.5854780673980713,
      "learning_rate": 4.2947409733124024e-05,
      "loss": 0.6422,
      "step": 718800
    },
    {
      "epoch": 11.285714285714286,
      "grad_norm": 3.929800033569336,
      "learning_rate": 4.294642857142857e-05,
      "loss": 0.6309,
      "step": 718900
    },
    {
      "epoch": 11.287284144427002,
      "grad_norm": 4.932857036590576,
      "learning_rate": 4.2945447409733126e-05,
      "loss": 0.6137,
      "step": 719000
    },
    {
      "epoch": 11.288854003139717,
      "grad_norm": 4.199489593505859,
      "learning_rate": 4.294446624803768e-05,
      "loss": 0.5595,
      "step": 719100
    },
    {
      "epoch": 11.290423861852434,
      "grad_norm": 4.7448930740356445,
      "learning_rate": 4.2943485086342234e-05,
      "loss": 0.6452,
      "step": 719200
    },
    {
      "epoch": 11.291993720565149,
      "grad_norm": 3.8788068294525146,
      "learning_rate": 4.294250392464678e-05,
      "loss": 0.6219,
      "step": 719300
    },
    {
      "epoch": 11.293563579277865,
      "grad_norm": 5.485243797302246,
      "learning_rate": 4.2941522762951336e-05,
      "loss": 0.5861,
      "step": 719400
    },
    {
      "epoch": 11.29513343799058,
      "grad_norm": 3.0373973846435547,
      "learning_rate": 4.294054160125589e-05,
      "loss": 0.6292,
      "step": 719500
    },
    {
      "epoch": 11.296703296703297,
      "grad_norm": 4.059914588928223,
      "learning_rate": 4.293956043956044e-05,
      "loss": 0.6316,
      "step": 719600
    },
    {
      "epoch": 11.298273155416013,
      "grad_norm": 2.819749355316162,
      "learning_rate": 4.2938579277864996e-05,
      "loss": 0.5805,
      "step": 719700
    },
    {
      "epoch": 11.299843014128728,
      "grad_norm": 2.350651264190674,
      "learning_rate": 4.293759811616955e-05,
      "loss": 0.6639,
      "step": 719800
    },
    {
      "epoch": 11.301412872841444,
      "grad_norm": 3.039260149002075,
      "learning_rate": 4.2936616954474105e-05,
      "loss": 0.6244,
      "step": 719900
    },
    {
      "epoch": 11.30298273155416,
      "grad_norm": 3.9568381309509277,
      "learning_rate": 4.293563579277865e-05,
      "loss": 0.659,
      "step": 720000
    },
    {
      "epoch": 11.304552590266876,
      "grad_norm": 2.9532418251037598,
      "learning_rate": 4.293465463108321e-05,
      "loss": 0.6346,
      "step": 720100
    },
    {
      "epoch": 11.306122448979592,
      "grad_norm": 4.011509895324707,
      "learning_rate": 4.293367346938776e-05,
      "loss": 0.6174,
      "step": 720200
    },
    {
      "epoch": 11.307692307692308,
      "grad_norm": 4.725865364074707,
      "learning_rate": 4.293269230769231e-05,
      "loss": 0.6187,
      "step": 720300
    },
    {
      "epoch": 11.309262166405023,
      "grad_norm": 2.9642655849456787,
      "learning_rate": 4.293171114599686e-05,
      "loss": 0.6333,
      "step": 720400
    },
    {
      "epoch": 11.310832025117739,
      "grad_norm": 3.896670341491699,
      "learning_rate": 4.293072998430142e-05,
      "loss": 0.6206,
      "step": 720500
    },
    {
      "epoch": 11.312401883830455,
      "grad_norm": 3.26138973236084,
      "learning_rate": 4.292974882260597e-05,
      "loss": 0.6519,
      "step": 720600
    },
    {
      "epoch": 11.31397174254317,
      "grad_norm": 3.9268150329589844,
      "learning_rate": 4.292876766091052e-05,
      "loss": 0.6194,
      "step": 720700
    },
    {
      "epoch": 11.315541601255887,
      "grad_norm": 3.711744785308838,
      "learning_rate": 4.292778649921507e-05,
      "loss": 0.6566,
      "step": 720800
    },
    {
      "epoch": 11.317111459968602,
      "grad_norm": 3.463731050491333,
      "learning_rate": 4.292680533751963e-05,
      "loss": 0.6337,
      "step": 720900
    },
    {
      "epoch": 11.31868131868132,
      "grad_norm": 3.215912103652954,
      "learning_rate": 4.292582417582417e-05,
      "loss": 0.6687,
      "step": 721000
    },
    {
      "epoch": 11.320251177394034,
      "grad_norm": 3.261580467224121,
      "learning_rate": 4.292484301412873e-05,
      "loss": 0.6362,
      "step": 721100
    },
    {
      "epoch": 11.321821036106751,
      "grad_norm": 3.1578540802001953,
      "learning_rate": 4.292386185243328e-05,
      "loss": 0.6125,
      "step": 721200
    },
    {
      "epoch": 11.323390894819466,
      "grad_norm": 3.7599222660064697,
      "learning_rate": 4.292288069073784e-05,
      "loss": 0.6276,
      "step": 721300
    },
    {
      "epoch": 11.324960753532181,
      "grad_norm": 4.517221450805664,
      "learning_rate": 4.292189952904238e-05,
      "loss": 0.6225,
      "step": 721400
    },
    {
      "epoch": 11.326530612244898,
      "grad_norm": 4.2921247482299805,
      "learning_rate": 4.292091836734694e-05,
      "loss": 0.6556,
      "step": 721500
    },
    {
      "epoch": 11.328100470957613,
      "grad_norm": 3.940732479095459,
      "learning_rate": 4.291993720565149e-05,
      "loss": 0.63,
      "step": 721600
    },
    {
      "epoch": 11.32967032967033,
      "grad_norm": 3.424614429473877,
      "learning_rate": 4.291895604395604e-05,
      "loss": 0.6168,
      "step": 721700
    },
    {
      "epoch": 11.331240188383045,
      "grad_norm": 3.3073344230651855,
      "learning_rate": 4.29179748822606e-05,
      "loss": 0.6275,
      "step": 721800
    },
    {
      "epoch": 11.332810047095762,
      "grad_norm": 3.391725778579712,
      "learning_rate": 4.291699372056515e-05,
      "loss": 0.6109,
      "step": 721900
    },
    {
      "epoch": 11.334379905808477,
      "grad_norm": 3.2647907733917236,
      "learning_rate": 4.291601255886971e-05,
      "loss": 0.5789,
      "step": 722000
    },
    {
      "epoch": 11.335949764521192,
      "grad_norm": 3.6101279258728027,
      "learning_rate": 4.2915031397174254e-05,
      "loss": 0.5992,
      "step": 722100
    },
    {
      "epoch": 11.33751962323391,
      "grad_norm": 3.6418864727020264,
      "learning_rate": 4.291405023547881e-05,
      "loss": 0.6243,
      "step": 722200
    },
    {
      "epoch": 11.339089481946624,
      "grad_norm": 3.7312309741973877,
      "learning_rate": 4.291306907378336e-05,
      "loss": 0.639,
      "step": 722300
    },
    {
      "epoch": 11.340659340659341,
      "grad_norm": 4.4925971031188965,
      "learning_rate": 4.291208791208791e-05,
      "loss": 0.6166,
      "step": 722400
    },
    {
      "epoch": 11.342229199372056,
      "grad_norm": 4.556014060974121,
      "learning_rate": 4.2911106750392464e-05,
      "loss": 0.6172,
      "step": 722500
    },
    {
      "epoch": 11.343799058084773,
      "grad_norm": 2.705118417739868,
      "learning_rate": 4.291012558869702e-05,
      "loss": 0.5798,
      "step": 722600
    },
    {
      "epoch": 11.345368916797488,
      "grad_norm": 3.8894271850585938,
      "learning_rate": 4.290914442700157e-05,
      "loss": 0.6447,
      "step": 722700
    },
    {
      "epoch": 11.346938775510203,
      "grad_norm": 3.178596258163452,
      "learning_rate": 4.2908163265306124e-05,
      "loss": 0.6165,
      "step": 722800
    },
    {
      "epoch": 11.34850863422292,
      "grad_norm": 3.8329594135284424,
      "learning_rate": 4.2907182103610675e-05,
      "loss": 0.605,
      "step": 722900
    },
    {
      "epoch": 11.350078492935635,
      "grad_norm": 4.269583702087402,
      "learning_rate": 4.290620094191523e-05,
      "loss": 0.6128,
      "step": 723000
    },
    {
      "epoch": 11.351648351648352,
      "grad_norm": 3.9405457973480225,
      "learning_rate": 4.290521978021978e-05,
      "loss": 0.6479,
      "step": 723100
    },
    {
      "epoch": 11.353218210361067,
      "grad_norm": 4.67746639251709,
      "learning_rate": 4.2904238618524335e-05,
      "loss": 0.6195,
      "step": 723200
    },
    {
      "epoch": 11.354788069073784,
      "grad_norm": 3.923518180847168,
      "learning_rate": 4.2903257456828886e-05,
      "loss": 0.6167,
      "step": 723300
    },
    {
      "epoch": 11.3563579277865,
      "grad_norm": 4.391024589538574,
      "learning_rate": 4.290227629513344e-05,
      "loss": 0.6331,
      "step": 723400
    },
    {
      "epoch": 11.357927786499214,
      "grad_norm": 4.331775188446045,
      "learning_rate": 4.290129513343799e-05,
      "loss": 0.6844,
      "step": 723500
    },
    {
      "epoch": 11.359497645211931,
      "grad_norm": 3.132998466491699,
      "learning_rate": 4.2900313971742545e-05,
      "loss": 0.5908,
      "step": 723600
    },
    {
      "epoch": 11.361067503924646,
      "grad_norm": 3.7406466007232666,
      "learning_rate": 4.2899332810047096e-05,
      "loss": 0.6153,
      "step": 723700
    },
    {
      "epoch": 11.362637362637363,
      "grad_norm": 4.514536380767822,
      "learning_rate": 4.289835164835165e-05,
      "loss": 0.6554,
      "step": 723800
    },
    {
      "epoch": 11.364207221350078,
      "grad_norm": 4.775074481964111,
      "learning_rate": 4.2897370486656205e-05,
      "loss": 0.6469,
      "step": 723900
    },
    {
      "epoch": 11.365777080062795,
      "grad_norm": 4.1294169425964355,
      "learning_rate": 4.2896389324960756e-05,
      "loss": 0.687,
      "step": 724000
    },
    {
      "epoch": 11.36734693877551,
      "grad_norm": 4.607363224029541,
      "learning_rate": 4.2895408163265314e-05,
      "loss": 0.6303,
      "step": 724100
    },
    {
      "epoch": 11.368916797488225,
      "grad_norm": 2.4156789779663086,
      "learning_rate": 4.289442700156986e-05,
      "loss": 0.6353,
      "step": 724200
    },
    {
      "epoch": 11.370486656200942,
      "grad_norm": 3.3616344928741455,
      "learning_rate": 4.2893445839874416e-05,
      "loss": 0.6274,
      "step": 724300
    },
    {
      "epoch": 11.372056514913657,
      "grad_norm": 4.686558246612549,
      "learning_rate": 4.289246467817897e-05,
      "loss": 0.6165,
      "step": 724400
    },
    {
      "epoch": 11.373626373626374,
      "grad_norm": 3.4107744693756104,
      "learning_rate": 4.289148351648352e-05,
      "loss": 0.6181,
      "step": 724500
    },
    {
      "epoch": 11.37519623233909,
      "grad_norm": 4.508298873901367,
      "learning_rate": 4.289050235478807e-05,
      "loss": 0.626,
      "step": 724600
    },
    {
      "epoch": 11.376766091051806,
      "grad_norm": 4.082301139831543,
      "learning_rate": 4.2889521193092626e-05,
      "loss": 0.6004,
      "step": 724700
    },
    {
      "epoch": 11.378335949764521,
      "grad_norm": 3.6760730743408203,
      "learning_rate": 4.288854003139718e-05,
      "loss": 0.621,
      "step": 724800
    },
    {
      "epoch": 11.379905808477236,
      "grad_norm": 4.802374839782715,
      "learning_rate": 4.288755886970173e-05,
      "loss": 0.6242,
      "step": 724900
    },
    {
      "epoch": 11.381475667189953,
      "grad_norm": 4.092060565948486,
      "learning_rate": 4.288657770800628e-05,
      "loss": 0.6113,
      "step": 725000
    },
    {
      "epoch": 11.383045525902668,
      "grad_norm": 4.301047325134277,
      "learning_rate": 4.288559654631084e-05,
      "loss": 0.666,
      "step": 725100
    },
    {
      "epoch": 11.384615384615385,
      "grad_norm": 5.124945640563965,
      "learning_rate": 4.288461538461538e-05,
      "loss": 0.619,
      "step": 725200
    },
    {
      "epoch": 11.3861852433281,
      "grad_norm": 3.6613118648529053,
      "learning_rate": 4.288363422291994e-05,
      "loss": 0.6329,
      "step": 725300
    },
    {
      "epoch": 11.387755102040817,
      "grad_norm": 4.2015767097473145,
      "learning_rate": 4.288265306122449e-05,
      "loss": 0.6149,
      "step": 725400
    },
    {
      "epoch": 11.389324960753532,
      "grad_norm": 3.4377176761627197,
      "learning_rate": 4.288167189952905e-05,
      "loss": 0.6364,
      "step": 725500
    },
    {
      "epoch": 11.390894819466247,
      "grad_norm": 3.897663116455078,
      "learning_rate": 4.288069073783359e-05,
      "loss": 0.656,
      "step": 725600
    },
    {
      "epoch": 11.392464678178964,
      "grad_norm": 5.8161091804504395,
      "learning_rate": 4.287970957613815e-05,
      "loss": 0.6566,
      "step": 725700
    },
    {
      "epoch": 11.394034536891679,
      "grad_norm": 4.574726581573486,
      "learning_rate": 4.28787284144427e-05,
      "loss": 0.618,
      "step": 725800
    },
    {
      "epoch": 11.395604395604396,
      "grad_norm": 3.225332021713257,
      "learning_rate": 4.287774725274725e-05,
      "loss": 0.6079,
      "step": 725900
    },
    {
      "epoch": 11.397174254317111,
      "grad_norm": 2.4806995391845703,
      "learning_rate": 4.287676609105181e-05,
      "loss": 0.6196,
      "step": 726000
    },
    {
      "epoch": 11.398744113029828,
      "grad_norm": 3.7615301609039307,
      "learning_rate": 4.287578492935636e-05,
      "loss": 0.6358,
      "step": 726100
    },
    {
      "epoch": 11.400313971742543,
      "grad_norm": 3.8515048027038574,
      "learning_rate": 4.287480376766091e-05,
      "loss": 0.6061,
      "step": 726200
    },
    {
      "epoch": 11.40188383045526,
      "grad_norm": 3.4981510639190674,
      "learning_rate": 4.287382260596546e-05,
      "loss": 0.6442,
      "step": 726300
    },
    {
      "epoch": 11.403453689167975,
      "grad_norm": 4.404617786407471,
      "learning_rate": 4.287284144427002e-05,
      "loss": 0.634,
      "step": 726400
    },
    {
      "epoch": 11.40502354788069,
      "grad_norm": 3.849045991897583,
      "learning_rate": 4.287186028257457e-05,
      "loss": 0.6215,
      "step": 726500
    },
    {
      "epoch": 11.406593406593407,
      "grad_norm": 4.039031505584717,
      "learning_rate": 4.287087912087912e-05,
      "loss": 0.6137,
      "step": 726600
    },
    {
      "epoch": 11.408163265306122,
      "grad_norm": 3.6518914699554443,
      "learning_rate": 4.286989795918367e-05,
      "loss": 0.6488,
      "step": 726700
    },
    {
      "epoch": 11.409733124018839,
      "grad_norm": 3.65501070022583,
      "learning_rate": 4.286891679748823e-05,
      "loss": 0.6528,
      "step": 726800
    },
    {
      "epoch": 11.411302982731554,
      "grad_norm": 4.717836856842041,
      "learning_rate": 4.286793563579278e-05,
      "loss": 0.6386,
      "step": 726900
    },
    {
      "epoch": 11.41287284144427,
      "grad_norm": 2.4986391067504883,
      "learning_rate": 4.286695447409733e-05,
      "loss": 0.6038,
      "step": 727000
    },
    {
      "epoch": 11.414442700156986,
      "grad_norm": 4.4005022048950195,
      "learning_rate": 4.2865973312401884e-05,
      "loss": 0.5989,
      "step": 727100
    },
    {
      "epoch": 11.416012558869701,
      "grad_norm": 3.183924674987793,
      "learning_rate": 4.286499215070644e-05,
      "loss": 0.6073,
      "step": 727200
    },
    {
      "epoch": 11.417582417582418,
      "grad_norm": 3.6942224502563477,
      "learning_rate": 4.2864010989010986e-05,
      "loss": 0.6378,
      "step": 727300
    },
    {
      "epoch": 11.419152276295133,
      "grad_norm": 4.069188117980957,
      "learning_rate": 4.2863029827315544e-05,
      "loss": 0.6497,
      "step": 727400
    },
    {
      "epoch": 11.42072213500785,
      "grad_norm": 4.5901360511779785,
      "learning_rate": 4.2862048665620095e-05,
      "loss": 0.6708,
      "step": 727500
    },
    {
      "epoch": 11.422291993720565,
      "grad_norm": 3.932065010070801,
      "learning_rate": 4.286106750392465e-05,
      "loss": 0.6209,
      "step": 727600
    },
    {
      "epoch": 11.423861852433282,
      "grad_norm": 3.9075260162353516,
      "learning_rate": 4.2860086342229197e-05,
      "loss": 0.6546,
      "step": 727700
    },
    {
      "epoch": 11.425431711145997,
      "grad_norm": 4.546010971069336,
      "learning_rate": 4.2859105180533754e-05,
      "loss": 0.6487,
      "step": 727800
    },
    {
      "epoch": 11.427001569858712,
      "grad_norm": 3.652578830718994,
      "learning_rate": 4.2858124018838305e-05,
      "loss": 0.6394,
      "step": 727900
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 3.77494478225708,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.6005,
      "step": 728000
    },
    {
      "epoch": 11.430141287284144,
      "grad_norm": 3.7423791885375977,
      "learning_rate": 4.2856161695447414e-05,
      "loss": 0.6156,
      "step": 728100
    },
    {
      "epoch": 11.43171114599686,
      "grad_norm": 4.43934440612793,
      "learning_rate": 4.2855180533751965e-05,
      "loss": 0.622,
      "step": 728200
    },
    {
      "epoch": 11.433281004709576,
      "grad_norm": 3.9215471744537354,
      "learning_rate": 4.2854199372056516e-05,
      "loss": 0.6362,
      "step": 728300
    },
    {
      "epoch": 11.434850863422293,
      "grad_norm": 4.179991722106934,
      "learning_rate": 4.285321821036107e-05,
      "loss": 0.6312,
      "step": 728400
    },
    {
      "epoch": 11.436420722135008,
      "grad_norm": 4.046120643615723,
      "learning_rate": 4.2852237048665625e-05,
      "loss": 0.6516,
      "step": 728500
    },
    {
      "epoch": 11.437990580847723,
      "grad_norm": 4.01140832901001,
      "learning_rate": 4.2851255886970176e-05,
      "loss": 0.6254,
      "step": 728600
    },
    {
      "epoch": 11.43956043956044,
      "grad_norm": 5.071422576904297,
      "learning_rate": 4.285027472527473e-05,
      "loss": 0.6643,
      "step": 728700
    },
    {
      "epoch": 11.441130298273155,
      "grad_norm": 4.6046528816223145,
      "learning_rate": 4.284929356357928e-05,
      "loss": 0.6105,
      "step": 728800
    },
    {
      "epoch": 11.442700156985872,
      "grad_norm": 4.34207010269165,
      "learning_rate": 4.2848312401883835e-05,
      "loss": 0.6373,
      "step": 728900
    },
    {
      "epoch": 11.444270015698587,
      "grad_norm": 3.042280673980713,
      "learning_rate": 4.2847331240188386e-05,
      "loss": 0.6158,
      "step": 729000
    },
    {
      "epoch": 11.445839874411304,
      "grad_norm": 4.099647521972656,
      "learning_rate": 4.284635007849294e-05,
      "loss": 0.6378,
      "step": 729100
    },
    {
      "epoch": 11.447409733124019,
      "grad_norm": 3.539386749267578,
      "learning_rate": 4.284536891679749e-05,
      "loss": 0.5976,
      "step": 729200
    },
    {
      "epoch": 11.448979591836734,
      "grad_norm": 3.657320737838745,
      "learning_rate": 4.2844387755102046e-05,
      "loss": 0.6661,
      "step": 729300
    },
    {
      "epoch": 11.45054945054945,
      "grad_norm": 3.5612242221832275,
      "learning_rate": 4.284340659340659e-05,
      "loss": 0.6418,
      "step": 729400
    },
    {
      "epoch": 11.452119309262166,
      "grad_norm": 4.357673168182373,
      "learning_rate": 4.284242543171115e-05,
      "loss": 0.634,
      "step": 729500
    },
    {
      "epoch": 11.453689167974883,
      "grad_norm": 3.4128968715667725,
      "learning_rate": 4.28414442700157e-05,
      "loss": 0.604,
      "step": 729600
    },
    {
      "epoch": 11.455259026687598,
      "grad_norm": 4.223750114440918,
      "learning_rate": 4.284046310832026e-05,
      "loss": 0.6305,
      "step": 729700
    },
    {
      "epoch": 11.456828885400315,
      "grad_norm": 4.364292621612549,
      "learning_rate": 4.28394819466248e-05,
      "loss": 0.6422,
      "step": 729800
    },
    {
      "epoch": 11.45839874411303,
      "grad_norm": 4.420878887176514,
      "learning_rate": 4.283850078492936e-05,
      "loss": 0.6085,
      "step": 729900
    },
    {
      "epoch": 11.459968602825747,
      "grad_norm": 3.015488386154175,
      "learning_rate": 4.283751962323391e-05,
      "loss": 0.6758,
      "step": 730000
    },
    {
      "epoch": 11.461538461538462,
      "grad_norm": 3.2525322437286377,
      "learning_rate": 4.283653846153846e-05,
      "loss": 0.6217,
      "step": 730100
    },
    {
      "epoch": 11.463108320251177,
      "grad_norm": 4.8052077293396,
      "learning_rate": 4.283555729984302e-05,
      "loss": 0.6223,
      "step": 730200
    },
    {
      "epoch": 11.464678178963894,
      "grad_norm": 3.82496976852417,
      "learning_rate": 4.283457613814757e-05,
      "loss": 0.6194,
      "step": 730300
    },
    {
      "epoch": 11.466248037676609,
      "grad_norm": 4.0951642990112305,
      "learning_rate": 4.283359497645212e-05,
      "loss": 0.6194,
      "step": 730400
    },
    {
      "epoch": 11.467817896389326,
      "grad_norm": 4.713027477264404,
      "learning_rate": 4.283261381475667e-05,
      "loss": 0.6043,
      "step": 730500
    },
    {
      "epoch": 11.46938775510204,
      "grad_norm": 3.9695346355438232,
      "learning_rate": 4.283163265306123e-05,
      "loss": 0.6117,
      "step": 730600
    },
    {
      "epoch": 11.470957613814758,
      "grad_norm": 4.773877143859863,
      "learning_rate": 4.283065149136578e-05,
      "loss": 0.6378,
      "step": 730700
    },
    {
      "epoch": 11.472527472527473,
      "grad_norm": 3.7379095554351807,
      "learning_rate": 4.282967032967033e-05,
      "loss": 0.6524,
      "step": 730800
    },
    {
      "epoch": 11.474097331240188,
      "grad_norm": 4.093835830688477,
      "learning_rate": 4.282868916797488e-05,
      "loss": 0.6097,
      "step": 730900
    },
    {
      "epoch": 11.475667189952905,
      "grad_norm": 2.9272823333740234,
      "learning_rate": 4.282770800627944e-05,
      "loss": 0.5786,
      "step": 731000
    },
    {
      "epoch": 11.47723704866562,
      "grad_norm": 3.582763910293579,
      "learning_rate": 4.282672684458399e-05,
      "loss": 0.6131,
      "step": 731100
    },
    {
      "epoch": 11.478806907378337,
      "grad_norm": 4.055395126342773,
      "learning_rate": 4.282574568288854e-05,
      "loss": 0.6181,
      "step": 731200
    },
    {
      "epoch": 11.480376766091052,
      "grad_norm": 3.6088194847106934,
      "learning_rate": 4.282476452119309e-05,
      "loss": 0.5936,
      "step": 731300
    },
    {
      "epoch": 11.481946624803768,
      "grad_norm": 3.760622978210449,
      "learning_rate": 4.282378335949765e-05,
      "loss": 0.6203,
      "step": 731400
    },
    {
      "epoch": 11.483516483516484,
      "grad_norm": 4.087231159210205,
      "learning_rate": 4.2822802197802195e-05,
      "loss": 0.6084,
      "step": 731500
    },
    {
      "epoch": 11.485086342229199,
      "grad_norm": 4.027937412261963,
      "learning_rate": 4.282182103610675e-05,
      "loss": 0.6196,
      "step": 731600
    },
    {
      "epoch": 11.486656200941916,
      "grad_norm": 3.9172556400299072,
      "learning_rate": 4.2820839874411304e-05,
      "loss": 0.6385,
      "step": 731700
    },
    {
      "epoch": 11.48822605965463,
      "grad_norm": 4.4820556640625,
      "learning_rate": 4.281985871271586e-05,
      "loss": 0.6434,
      "step": 731800
    },
    {
      "epoch": 11.489795918367347,
      "grad_norm": 4.516066074371338,
      "learning_rate": 4.2818877551020406e-05,
      "loss": 0.5765,
      "step": 731900
    },
    {
      "epoch": 11.491365777080063,
      "grad_norm": 4.300570011138916,
      "learning_rate": 4.281789638932496e-05,
      "loss": 0.6744,
      "step": 732000
    },
    {
      "epoch": 11.49293563579278,
      "grad_norm": 3.3454389572143555,
      "learning_rate": 4.2816915227629514e-05,
      "loss": 0.6633,
      "step": 732100
    },
    {
      "epoch": 11.494505494505495,
      "grad_norm": 4.894193172454834,
      "learning_rate": 4.2815934065934065e-05,
      "loss": 0.655,
      "step": 732200
    },
    {
      "epoch": 11.49607535321821,
      "grad_norm": 2.7797768115997314,
      "learning_rate": 4.281495290423862e-05,
      "loss": 0.6351,
      "step": 732300
    },
    {
      "epoch": 11.497645211930926,
      "grad_norm": 4.028643608093262,
      "learning_rate": 4.2813971742543174e-05,
      "loss": 0.6227,
      "step": 732400
    },
    {
      "epoch": 11.499215070643642,
      "grad_norm": 3.0252764225006104,
      "learning_rate": 4.2812990580847725e-05,
      "loss": 0.5895,
      "step": 732500
    },
    {
      "epoch": 11.500784929356358,
      "grad_norm": 4.65347957611084,
      "learning_rate": 4.2812009419152276e-05,
      "loss": 0.6563,
      "step": 732600
    },
    {
      "epoch": 11.502354788069074,
      "grad_norm": 3.0564889907836914,
      "learning_rate": 4.2811028257456834e-05,
      "loss": 0.6184,
      "step": 732700
    },
    {
      "epoch": 11.50392464678179,
      "grad_norm": 3.638817310333252,
      "learning_rate": 4.2810047095761385e-05,
      "loss": 0.6236,
      "step": 732800
    },
    {
      "epoch": 11.505494505494505,
      "grad_norm": 4.011698246002197,
      "learning_rate": 4.2809065934065936e-05,
      "loss": 0.5938,
      "step": 732900
    },
    {
      "epoch": 11.50706436420722,
      "grad_norm": 4.0403218269348145,
      "learning_rate": 4.280808477237049e-05,
      "loss": 0.7219,
      "step": 733000
    },
    {
      "epoch": 11.508634222919937,
      "grad_norm": 3.7149710655212402,
      "learning_rate": 4.2807103610675044e-05,
      "loss": 0.6272,
      "step": 733100
    },
    {
      "epoch": 11.510204081632653,
      "grad_norm": 4.895012378692627,
      "learning_rate": 4.2806122448979595e-05,
      "loss": 0.6131,
      "step": 733200
    },
    {
      "epoch": 11.51177394034537,
      "grad_norm": 3.7895238399505615,
      "learning_rate": 4.2805141287284146e-05,
      "loss": 0.6824,
      "step": 733300
    },
    {
      "epoch": 11.513343799058084,
      "grad_norm": 4.875609397888184,
      "learning_rate": 4.28041601255887e-05,
      "loss": 0.6318,
      "step": 733400
    },
    {
      "epoch": 11.514913657770801,
      "grad_norm": 4.377143383026123,
      "learning_rate": 4.2803178963893255e-05,
      "loss": 0.6416,
      "step": 733500
    },
    {
      "epoch": 11.516483516483516,
      "grad_norm": 3.7422752380371094,
      "learning_rate": 4.28021978021978e-05,
      "loss": 0.64,
      "step": 733600
    },
    {
      "epoch": 11.518053375196232,
      "grad_norm": 4.075031757354736,
      "learning_rate": 4.280121664050236e-05,
      "loss": 0.6171,
      "step": 733700
    },
    {
      "epoch": 11.519623233908948,
      "grad_norm": 4.520390510559082,
      "learning_rate": 4.280023547880691e-05,
      "loss": 0.6182,
      "step": 733800
    },
    {
      "epoch": 11.521193092621663,
      "grad_norm": 2.6531219482421875,
      "learning_rate": 4.2799254317111466e-05,
      "loss": 0.587,
      "step": 733900
    },
    {
      "epoch": 11.52276295133438,
      "grad_norm": 3.317737579345703,
      "learning_rate": 4.279827315541601e-05,
      "loss": 0.6146,
      "step": 734000
    },
    {
      "epoch": 11.524332810047095,
      "grad_norm": 2.7644882202148438,
      "learning_rate": 4.279729199372057e-05,
      "loss": 0.612,
      "step": 734100
    },
    {
      "epoch": 11.525902668759812,
      "grad_norm": 3.088305711746216,
      "learning_rate": 4.279631083202512e-05,
      "loss": 0.639,
      "step": 734200
    },
    {
      "epoch": 11.527472527472527,
      "grad_norm": 4.338909149169922,
      "learning_rate": 4.279532967032967e-05,
      "loss": 0.6165,
      "step": 734300
    },
    {
      "epoch": 11.529042386185242,
      "grad_norm": 3.74540376663208,
      "learning_rate": 4.279434850863423e-05,
      "loss": 0.62,
      "step": 734400
    },
    {
      "epoch": 11.53061224489796,
      "grad_norm": 4.16405725479126,
      "learning_rate": 4.279336734693878e-05,
      "loss": 0.6391,
      "step": 734500
    },
    {
      "epoch": 11.532182103610674,
      "grad_norm": 4.151664733886719,
      "learning_rate": 4.279238618524333e-05,
      "loss": 0.6439,
      "step": 734600
    },
    {
      "epoch": 11.533751962323391,
      "grad_norm": 3.717374086380005,
      "learning_rate": 4.279140502354788e-05,
      "loss": 0.5992,
      "step": 734700
    },
    {
      "epoch": 11.535321821036106,
      "grad_norm": 4.399399757385254,
      "learning_rate": 4.279042386185244e-05,
      "loss": 0.6314,
      "step": 734800
    },
    {
      "epoch": 11.536891679748823,
      "grad_norm": 4.0456647872924805,
      "learning_rate": 4.278944270015699e-05,
      "loss": 0.668,
      "step": 734900
    },
    {
      "epoch": 11.538461538461538,
      "grad_norm": 4.567436695098877,
      "learning_rate": 4.278846153846154e-05,
      "loss": 0.6491,
      "step": 735000
    },
    {
      "epoch": 11.540031397174253,
      "grad_norm": 3.6074140071868896,
      "learning_rate": 4.278748037676609e-05,
      "loss": 0.6529,
      "step": 735100
    },
    {
      "epoch": 11.54160125588697,
      "grad_norm": 3.965514898300171,
      "learning_rate": 4.278649921507065e-05,
      "loss": 0.625,
      "step": 735200
    },
    {
      "epoch": 11.543171114599685,
      "grad_norm": 4.266119003295898,
      "learning_rate": 4.27855180533752e-05,
      "loss": 0.6038,
      "step": 735300
    },
    {
      "epoch": 11.544740973312402,
      "grad_norm": 3.5586016178131104,
      "learning_rate": 4.278453689167975e-05,
      "loss": 0.6345,
      "step": 735400
    },
    {
      "epoch": 11.546310832025117,
      "grad_norm": 2.635627031326294,
      "learning_rate": 4.27835557299843e-05,
      "loss": 0.6492,
      "step": 735500
    },
    {
      "epoch": 11.547880690737834,
      "grad_norm": 4.106646537780762,
      "learning_rate": 4.278257456828886e-05,
      "loss": 0.6541,
      "step": 735600
    },
    {
      "epoch": 11.54945054945055,
      "grad_norm": 3.761552333831787,
      "learning_rate": 4.2781593406593404e-05,
      "loss": 0.6945,
      "step": 735700
    },
    {
      "epoch": 11.551020408163264,
      "grad_norm": 3.9858717918395996,
      "learning_rate": 4.278061224489796e-05,
      "loss": 0.6388,
      "step": 735800
    },
    {
      "epoch": 11.552590266875981,
      "grad_norm": 2.837580680847168,
      "learning_rate": 4.277963108320251e-05,
      "loss": 0.6538,
      "step": 735900
    },
    {
      "epoch": 11.554160125588696,
      "grad_norm": 3.966423273086548,
      "learning_rate": 4.277864992150707e-05,
      "loss": 0.675,
      "step": 736000
    },
    {
      "epoch": 11.555729984301413,
      "grad_norm": 4.398496150970459,
      "learning_rate": 4.2777668759811615e-05,
      "loss": 0.6277,
      "step": 736100
    },
    {
      "epoch": 11.557299843014128,
      "grad_norm": 2.2370247840881348,
      "learning_rate": 4.277668759811617e-05,
      "loss": 0.6225,
      "step": 736200
    },
    {
      "epoch": 11.558869701726845,
      "grad_norm": 3.8151159286499023,
      "learning_rate": 4.277570643642072e-05,
      "loss": 0.6478,
      "step": 736300
    },
    {
      "epoch": 11.56043956043956,
      "grad_norm": 4.5519585609436035,
      "learning_rate": 4.2774725274725274e-05,
      "loss": 0.6158,
      "step": 736400
    },
    {
      "epoch": 11.562009419152277,
      "grad_norm": 3.5921335220336914,
      "learning_rate": 4.277374411302983e-05,
      "loss": 0.6141,
      "step": 736500
    },
    {
      "epoch": 11.563579277864992,
      "grad_norm": 4.26971960067749,
      "learning_rate": 4.277276295133438e-05,
      "loss": 0.6318,
      "step": 736600
    },
    {
      "epoch": 11.565149136577707,
      "grad_norm": 4.289635181427002,
      "learning_rate": 4.2771781789638934e-05,
      "loss": 0.6586,
      "step": 736700
    },
    {
      "epoch": 11.566718995290424,
      "grad_norm": 5.112853050231934,
      "learning_rate": 4.2770800627943485e-05,
      "loss": 0.6513,
      "step": 736800
    },
    {
      "epoch": 11.56828885400314,
      "grad_norm": 4.32602071762085,
      "learning_rate": 4.276981946624804e-05,
      "loss": 0.6382,
      "step": 736900
    },
    {
      "epoch": 11.569858712715856,
      "grad_norm": 3.991431951522827,
      "learning_rate": 4.2768838304552594e-05,
      "loss": 0.6101,
      "step": 737000
    },
    {
      "epoch": 11.571428571428571,
      "grad_norm": 4.652533054351807,
      "learning_rate": 4.2767857142857145e-05,
      "loss": 0.638,
      "step": 737100
    },
    {
      "epoch": 11.572998430141288,
      "grad_norm": 4.161648750305176,
      "learning_rate": 4.2766875981161696e-05,
      "loss": 0.6192,
      "step": 737200
    },
    {
      "epoch": 11.574568288854003,
      "grad_norm": 2.81858491897583,
      "learning_rate": 4.2765894819466253e-05,
      "loss": 0.6083,
      "step": 737300
    },
    {
      "epoch": 11.576138147566718,
      "grad_norm": 3.908173084259033,
      "learning_rate": 4.2764913657770804e-05,
      "loss": 0.6075,
      "step": 737400
    },
    {
      "epoch": 11.577708006279435,
      "grad_norm": 4.095958709716797,
      "learning_rate": 4.2763932496075355e-05,
      "loss": 0.6571,
      "step": 737500
    },
    {
      "epoch": 11.57927786499215,
      "grad_norm": 4.202815055847168,
      "learning_rate": 4.2762951334379906e-05,
      "loss": 0.6257,
      "step": 737600
    },
    {
      "epoch": 11.580847723704867,
      "grad_norm": 4.5651960372924805,
      "learning_rate": 4.2761970172684464e-05,
      "loss": 0.6368,
      "step": 737700
    },
    {
      "epoch": 11.582417582417582,
      "grad_norm": 4.508058547973633,
      "learning_rate": 4.276098901098901e-05,
      "loss": 0.6454,
      "step": 737800
    },
    {
      "epoch": 11.583987441130299,
      "grad_norm": 3.818742036819458,
      "learning_rate": 4.2760007849293566e-05,
      "loss": 0.5974,
      "step": 737900
    },
    {
      "epoch": 11.585557299843014,
      "grad_norm": 3.9726381301879883,
      "learning_rate": 4.275902668759812e-05,
      "loss": 0.6041,
      "step": 738000
    },
    {
      "epoch": 11.58712715855573,
      "grad_norm": 4.166539192199707,
      "learning_rate": 4.2758045525902675e-05,
      "loss": 0.6245,
      "step": 738100
    },
    {
      "epoch": 11.588697017268446,
      "grad_norm": 4.035303115844727,
      "learning_rate": 4.275706436420722e-05,
      "loss": 0.6091,
      "step": 738200
    },
    {
      "epoch": 11.590266875981161,
      "grad_norm": 3.4626529216766357,
      "learning_rate": 4.275608320251178e-05,
      "loss": 0.6594,
      "step": 738300
    },
    {
      "epoch": 11.591836734693878,
      "grad_norm": 3.660409688949585,
      "learning_rate": 4.275510204081633e-05,
      "loss": 0.5848,
      "step": 738400
    },
    {
      "epoch": 11.593406593406593,
      "grad_norm": 3.9432756900787354,
      "learning_rate": 4.275412087912088e-05,
      "loss": 0.6179,
      "step": 738500
    },
    {
      "epoch": 11.59497645211931,
      "grad_norm": 3.769516944885254,
      "learning_rate": 4.2753139717425437e-05,
      "loss": 0.6048,
      "step": 738600
    },
    {
      "epoch": 11.596546310832025,
      "grad_norm": 3.3581817150115967,
      "learning_rate": 4.275215855572999e-05,
      "loss": 0.6446,
      "step": 738700
    },
    {
      "epoch": 11.598116169544742,
      "grad_norm": 2.734123706817627,
      "learning_rate": 4.275117739403454e-05,
      "loss": 0.6393,
      "step": 738800
    },
    {
      "epoch": 11.599686028257457,
      "grad_norm": 4.6395440101623535,
      "learning_rate": 4.275019623233909e-05,
      "loss": 0.6601,
      "step": 738900
    },
    {
      "epoch": 11.601255886970172,
      "grad_norm": 4.7285566329956055,
      "learning_rate": 4.274921507064365e-05,
      "loss": 0.6258,
      "step": 739000
    },
    {
      "epoch": 11.602825745682889,
      "grad_norm": 3.8749890327453613,
      "learning_rate": 4.27482339089482e-05,
      "loss": 0.5807,
      "step": 739100
    },
    {
      "epoch": 11.604395604395604,
      "grad_norm": 4.659102916717529,
      "learning_rate": 4.274725274725275e-05,
      "loss": 0.6442,
      "step": 739200
    },
    {
      "epoch": 11.605965463108321,
      "grad_norm": 3.608433961868286,
      "learning_rate": 4.27462715855573e-05,
      "loss": 0.6518,
      "step": 739300
    },
    {
      "epoch": 11.607535321821036,
      "grad_norm": 3.966951608657837,
      "learning_rate": 4.274529042386186e-05,
      "loss": 0.686,
      "step": 739400
    },
    {
      "epoch": 11.609105180533753,
      "grad_norm": 4.297347068786621,
      "learning_rate": 4.27443092621664e-05,
      "loss": 0.6163,
      "step": 739500
    },
    {
      "epoch": 11.610675039246468,
      "grad_norm": 4.123383522033691,
      "learning_rate": 4.274332810047096e-05,
      "loss": 0.5979,
      "step": 739600
    },
    {
      "epoch": 11.612244897959183,
      "grad_norm": 3.654290199279785,
      "learning_rate": 4.274234693877551e-05,
      "loss": 0.6404,
      "step": 739700
    },
    {
      "epoch": 11.6138147566719,
      "grad_norm": 4.324629783630371,
      "learning_rate": 4.274136577708007e-05,
      "loss": 0.6788,
      "step": 739800
    },
    {
      "epoch": 11.615384615384615,
      "grad_norm": 3.167942762374878,
      "learning_rate": 4.274038461538461e-05,
      "loss": 0.6088,
      "step": 739900
    },
    {
      "epoch": 11.616954474097332,
      "grad_norm": 3.3388893604278564,
      "learning_rate": 4.273940345368917e-05,
      "loss": 0.6791,
      "step": 740000
    },
    {
      "epoch": 11.618524332810047,
      "grad_norm": 3.241055727005005,
      "learning_rate": 4.273842229199372e-05,
      "loss": 0.6042,
      "step": 740100
    },
    {
      "epoch": 11.620094191522764,
      "grad_norm": 4.743276596069336,
      "learning_rate": 4.273744113029827e-05,
      "loss": 0.6058,
      "step": 740200
    },
    {
      "epoch": 11.621664050235479,
      "grad_norm": 3.6806795597076416,
      "learning_rate": 4.2736459968602824e-05,
      "loss": 0.6477,
      "step": 740300
    },
    {
      "epoch": 11.623233908948194,
      "grad_norm": 3.185182809829712,
      "learning_rate": 4.273547880690738e-05,
      "loss": 0.6033,
      "step": 740400
    },
    {
      "epoch": 11.62480376766091,
      "grad_norm": 4.878663063049316,
      "learning_rate": 4.273449764521193e-05,
      "loss": 0.6145,
      "step": 740500
    },
    {
      "epoch": 11.626373626373626,
      "grad_norm": 4.148140907287598,
      "learning_rate": 4.273351648351648e-05,
      "loss": 0.6423,
      "step": 740600
    },
    {
      "epoch": 11.627943485086343,
      "grad_norm": 5.352768421173096,
      "learning_rate": 4.273253532182104e-05,
      "loss": 0.6498,
      "step": 740700
    },
    {
      "epoch": 11.629513343799058,
      "grad_norm": 3.300870418548584,
      "learning_rate": 4.273155416012559e-05,
      "loss": 0.6156,
      "step": 740800
    },
    {
      "epoch": 11.631083202511775,
      "grad_norm": 4.187376499176025,
      "learning_rate": 4.273057299843014e-05,
      "loss": 0.6163,
      "step": 740900
    },
    {
      "epoch": 11.63265306122449,
      "grad_norm": 4.769982814788818,
      "learning_rate": 4.2729591836734694e-05,
      "loss": 0.5843,
      "step": 741000
    },
    {
      "epoch": 11.634222919937205,
      "grad_norm": 4.943914413452148,
      "learning_rate": 4.272861067503925e-05,
      "loss": 0.6133,
      "step": 741100
    },
    {
      "epoch": 11.635792778649922,
      "grad_norm": 3.6435792446136475,
      "learning_rate": 4.27276295133438e-05,
      "loss": 0.641,
      "step": 741200
    },
    {
      "epoch": 11.637362637362637,
      "grad_norm": 4.254157066345215,
      "learning_rate": 4.2726648351648354e-05,
      "loss": 0.673,
      "step": 741300
    },
    {
      "epoch": 11.638932496075354,
      "grad_norm": 2.919861078262329,
      "learning_rate": 4.2725667189952905e-05,
      "loss": 0.6232,
      "step": 741400
    },
    {
      "epoch": 11.640502354788069,
      "grad_norm": 4.196002006530762,
      "learning_rate": 4.272468602825746e-05,
      "loss": 0.5941,
      "step": 741500
    },
    {
      "epoch": 11.642072213500786,
      "grad_norm": 3.2701330184936523,
      "learning_rate": 4.2723704866562007e-05,
      "loss": 0.5815,
      "step": 741600
    },
    {
      "epoch": 11.6436420722135,
      "grad_norm": 4.508233547210693,
      "learning_rate": 4.2722723704866564e-05,
      "loss": 0.6138,
      "step": 741700
    },
    {
      "epoch": 11.645211930926216,
      "grad_norm": 2.751664161682129,
      "learning_rate": 4.2721742543171115e-05,
      "loss": 0.5925,
      "step": 741800
    },
    {
      "epoch": 11.646781789638933,
      "grad_norm": 4.098391056060791,
      "learning_rate": 4.272076138147567e-05,
      "loss": 0.6739,
      "step": 741900
    },
    {
      "epoch": 11.648351648351648,
      "grad_norm": 3.208181858062744,
      "learning_rate": 4.271978021978022e-05,
      "loss": 0.6405,
      "step": 742000
    },
    {
      "epoch": 11.649921507064365,
      "grad_norm": 4.103714466094971,
      "learning_rate": 4.2718799058084775e-05,
      "loss": 0.6295,
      "step": 742100
    },
    {
      "epoch": 11.65149136577708,
      "grad_norm": 3.6459739208221436,
      "learning_rate": 4.2717817896389326e-05,
      "loss": 0.6267,
      "step": 742200
    },
    {
      "epoch": 11.653061224489797,
      "grad_norm": 3.3775546550750732,
      "learning_rate": 4.271683673469388e-05,
      "loss": 0.5954,
      "step": 742300
    },
    {
      "epoch": 11.654631083202512,
      "grad_norm": 3.68479585647583,
      "learning_rate": 4.271585557299843e-05,
      "loss": 0.6045,
      "step": 742400
    },
    {
      "epoch": 11.656200941915227,
      "grad_norm": 3.9524738788604736,
      "learning_rate": 4.2714874411302986e-05,
      "loss": 0.6694,
      "step": 742500
    },
    {
      "epoch": 11.657770800627944,
      "grad_norm": 4.41046142578125,
      "learning_rate": 4.271389324960754e-05,
      "loss": 0.5942,
      "step": 742600
    },
    {
      "epoch": 11.659340659340659,
      "grad_norm": 5.103127956390381,
      "learning_rate": 4.271291208791209e-05,
      "loss": 0.6551,
      "step": 742700
    },
    {
      "epoch": 11.660910518053376,
      "grad_norm": 2.463397979736328,
      "learning_rate": 4.2711930926216645e-05,
      "loss": 0.5922,
      "step": 742800
    },
    {
      "epoch": 11.66248037676609,
      "grad_norm": 3.526660203933716,
      "learning_rate": 4.2710949764521196e-05,
      "loss": 0.5998,
      "step": 742900
    },
    {
      "epoch": 11.664050235478808,
      "grad_norm": 3.8238916397094727,
      "learning_rate": 4.270996860282575e-05,
      "loss": 0.5941,
      "step": 743000
    },
    {
      "epoch": 11.665620094191523,
      "grad_norm": 3.72212815284729,
      "learning_rate": 4.27089874411303e-05,
      "loss": 0.6037,
      "step": 743100
    },
    {
      "epoch": 11.667189952904238,
      "grad_norm": 4.069952964782715,
      "learning_rate": 4.2708006279434856e-05,
      "loss": 0.6466,
      "step": 743200
    },
    {
      "epoch": 11.668759811616955,
      "grad_norm": 4.195617198944092,
      "learning_rate": 4.270702511773941e-05,
      "loss": 0.5979,
      "step": 743300
    },
    {
      "epoch": 11.67032967032967,
      "grad_norm": 3.8943939208984375,
      "learning_rate": 4.270604395604396e-05,
      "loss": 0.6435,
      "step": 743400
    },
    {
      "epoch": 11.671899529042387,
      "grad_norm": 2.307628631591797,
      "learning_rate": 4.270506279434851e-05,
      "loss": 0.6308,
      "step": 743500
    },
    {
      "epoch": 11.673469387755102,
      "grad_norm": 4.163110733032227,
      "learning_rate": 4.270408163265307e-05,
      "loss": 0.6028,
      "step": 743600
    },
    {
      "epoch": 11.675039246467819,
      "grad_norm": 7.110783100128174,
      "learning_rate": 4.270310047095761e-05,
      "loss": 0.5813,
      "step": 743700
    },
    {
      "epoch": 11.676609105180534,
      "grad_norm": 3.2072033882141113,
      "learning_rate": 4.270211930926217e-05,
      "loss": 0.667,
      "step": 743800
    },
    {
      "epoch": 11.678178963893249,
      "grad_norm": 3.964574098587036,
      "learning_rate": 4.270113814756672e-05,
      "loss": 0.633,
      "step": 743900
    },
    {
      "epoch": 11.679748822605966,
      "grad_norm": 2.9307358264923096,
      "learning_rate": 4.270015698587128e-05,
      "loss": 0.6263,
      "step": 744000
    },
    {
      "epoch": 11.68131868131868,
      "grad_norm": 4.883869647979736,
      "learning_rate": 4.269917582417582e-05,
      "loss": 0.6121,
      "step": 744100
    },
    {
      "epoch": 11.682888540031398,
      "grad_norm": 2.3513894081115723,
      "learning_rate": 4.269819466248038e-05,
      "loss": 0.6372,
      "step": 744200
    },
    {
      "epoch": 11.684458398744113,
      "grad_norm": 4.5325493812561035,
      "learning_rate": 4.269721350078493e-05,
      "loss": 0.6333,
      "step": 744300
    },
    {
      "epoch": 11.68602825745683,
      "grad_norm": 4.252142906188965,
      "learning_rate": 4.269623233908948e-05,
      "loss": 0.6191,
      "step": 744400
    },
    {
      "epoch": 11.687598116169545,
      "grad_norm": 4.380984783172607,
      "learning_rate": 4.269525117739403e-05,
      "loss": 0.6402,
      "step": 744500
    },
    {
      "epoch": 11.68916797488226,
      "grad_norm": 3.6599838733673096,
      "learning_rate": 4.269427001569859e-05,
      "loss": 0.6336,
      "step": 744600
    },
    {
      "epoch": 11.690737833594977,
      "grad_norm": 4.4688897132873535,
      "learning_rate": 4.269328885400314e-05,
      "loss": 0.6447,
      "step": 744700
    },
    {
      "epoch": 11.692307692307692,
      "grad_norm": 4.552374839782715,
      "learning_rate": 4.269230769230769e-05,
      "loss": 0.611,
      "step": 744800
    },
    {
      "epoch": 11.693877551020408,
      "grad_norm": 4.172004222869873,
      "learning_rate": 4.269132653061225e-05,
      "loss": 0.596,
      "step": 744900
    },
    {
      "epoch": 11.695447409733124,
      "grad_norm": 4.009403705596924,
      "learning_rate": 4.26903453689168e-05,
      "loss": 0.6221,
      "step": 745000
    },
    {
      "epoch": 11.69701726844584,
      "grad_norm": 3.8235669136047363,
      "learning_rate": 4.268936420722135e-05,
      "loss": 0.6346,
      "step": 745100
    },
    {
      "epoch": 11.698587127158556,
      "grad_norm": 3.116527557373047,
      "learning_rate": 4.26883830455259e-05,
      "loss": 0.6333,
      "step": 745200
    },
    {
      "epoch": 11.700156985871272,
      "grad_norm": 4.790338516235352,
      "learning_rate": 4.268740188383046e-05,
      "loss": 0.6165,
      "step": 745300
    },
    {
      "epoch": 11.701726844583987,
      "grad_norm": 4.1417059898376465,
      "learning_rate": 4.268642072213501e-05,
      "loss": 0.6573,
      "step": 745400
    },
    {
      "epoch": 11.703296703296703,
      "grad_norm": 3.746093273162842,
      "learning_rate": 4.268543956043956e-05,
      "loss": 0.6706,
      "step": 745500
    },
    {
      "epoch": 11.70486656200942,
      "grad_norm": 2.747128486633301,
      "learning_rate": 4.2684458398744114e-05,
      "loss": 0.635,
      "step": 745600
    },
    {
      "epoch": 11.706436420722135,
      "grad_norm": 3.1577587127685547,
      "learning_rate": 4.268347723704867e-05,
      "loss": 0.6157,
      "step": 745700
    },
    {
      "epoch": 11.708006279434851,
      "grad_norm": 5.038592338562012,
      "learning_rate": 4.2682496075353216e-05,
      "loss": 0.6557,
      "step": 745800
    },
    {
      "epoch": 11.709576138147566,
      "grad_norm": 3.9797208309173584,
      "learning_rate": 4.268151491365777e-05,
      "loss": 0.6369,
      "step": 745900
    },
    {
      "epoch": 11.711145996860283,
      "grad_norm": 3.8998188972473145,
      "learning_rate": 4.2680533751962324e-05,
      "loss": 0.6056,
      "step": 746000
    },
    {
      "epoch": 11.712715855572998,
      "grad_norm": 5.141523361206055,
      "learning_rate": 4.267955259026688e-05,
      "loss": 0.6266,
      "step": 746100
    },
    {
      "epoch": 11.714285714285714,
      "grad_norm": 4.088139533996582,
      "learning_rate": 4.2678571428571426e-05,
      "loss": 0.6341,
      "step": 746200
    },
    {
      "epoch": 11.71585557299843,
      "grad_norm": 4.08833646774292,
      "learning_rate": 4.2677590266875984e-05,
      "loss": 0.6305,
      "step": 746300
    },
    {
      "epoch": 11.717425431711145,
      "grad_norm": 3.5299274921417236,
      "learning_rate": 4.2676609105180535e-05,
      "loss": 0.6338,
      "step": 746400
    },
    {
      "epoch": 11.718995290423862,
      "grad_norm": 2.548417806625366,
      "learning_rate": 4.2675627943485086e-05,
      "loss": 0.6047,
      "step": 746500
    },
    {
      "epoch": 11.720565149136577,
      "grad_norm": 3.720036506652832,
      "learning_rate": 4.267464678178964e-05,
      "loss": 0.6551,
      "step": 746600
    },
    {
      "epoch": 11.722135007849294,
      "grad_norm": 3.119300127029419,
      "learning_rate": 4.2673665620094195e-05,
      "loss": 0.6539,
      "step": 746700
    },
    {
      "epoch": 11.72370486656201,
      "grad_norm": 2.243448257446289,
      "learning_rate": 4.2672684458398746e-05,
      "loss": 0.6308,
      "step": 746800
    },
    {
      "epoch": 11.725274725274724,
      "grad_norm": 4.261849880218506,
      "learning_rate": 4.26717032967033e-05,
      "loss": 0.6657,
      "step": 746900
    },
    {
      "epoch": 11.726844583987441,
      "grad_norm": 4.140477657318115,
      "learning_rate": 4.2670722135007854e-05,
      "loss": 0.6538,
      "step": 747000
    },
    {
      "epoch": 11.728414442700156,
      "grad_norm": 3.3314027786254883,
      "learning_rate": 4.2669740973312405e-05,
      "loss": 0.6609,
      "step": 747100
    },
    {
      "epoch": 11.729984301412873,
      "grad_norm": 2.561490058898926,
      "learning_rate": 4.2668759811616956e-05,
      "loss": 0.6623,
      "step": 747200
    },
    {
      "epoch": 11.731554160125588,
      "grad_norm": 3.535656213760376,
      "learning_rate": 4.266777864992151e-05,
      "loss": 0.6174,
      "step": 747300
    },
    {
      "epoch": 11.733124018838305,
      "grad_norm": 3.526054859161377,
      "learning_rate": 4.2666797488226065e-05,
      "loss": 0.6301,
      "step": 747400
    },
    {
      "epoch": 11.73469387755102,
      "grad_norm": 3.043895721435547,
      "learning_rate": 4.2665816326530616e-05,
      "loss": 0.6349,
      "step": 747500
    },
    {
      "epoch": 11.736263736263737,
      "grad_norm": 4.424382209777832,
      "learning_rate": 4.266483516483517e-05,
      "loss": 0.649,
      "step": 747600
    },
    {
      "epoch": 11.737833594976452,
      "grad_norm": 3.741342306137085,
      "learning_rate": 4.266385400313972e-05,
      "loss": 0.6434,
      "step": 747700
    },
    {
      "epoch": 11.739403453689167,
      "grad_norm": 3.599236249923706,
      "learning_rate": 4.2662872841444276e-05,
      "loss": 0.5878,
      "step": 747800
    },
    {
      "epoch": 11.740973312401884,
      "grad_norm": 3.2192277908325195,
      "learning_rate": 4.266189167974882e-05,
      "loss": 0.5992,
      "step": 747900
    },
    {
      "epoch": 11.7425431711146,
      "grad_norm": 4.8192138671875,
      "learning_rate": 4.266091051805338e-05,
      "loss": 0.6346,
      "step": 748000
    },
    {
      "epoch": 11.744113029827316,
      "grad_norm": 3.893458366394043,
      "learning_rate": 4.265992935635793e-05,
      "loss": 0.6314,
      "step": 748100
    },
    {
      "epoch": 11.745682888540031,
      "grad_norm": 4.087647914886475,
      "learning_rate": 4.2658948194662487e-05,
      "loss": 0.6123,
      "step": 748200
    },
    {
      "epoch": 11.747252747252748,
      "grad_norm": 3.992553949356079,
      "learning_rate": 4.265796703296703e-05,
      "loss": 0.6279,
      "step": 748300
    },
    {
      "epoch": 11.748822605965463,
      "grad_norm": 2.957057237625122,
      "learning_rate": 4.265698587127159e-05,
      "loss": 0.6364,
      "step": 748400
    },
    {
      "epoch": 11.750392464678178,
      "grad_norm": 4.635494709014893,
      "learning_rate": 4.265600470957614e-05,
      "loss": 0.6168,
      "step": 748500
    },
    {
      "epoch": 11.751962323390895,
      "grad_norm": 3.474146604537964,
      "learning_rate": 4.265502354788069e-05,
      "loss": 0.5917,
      "step": 748600
    },
    {
      "epoch": 11.75353218210361,
      "grad_norm": 4.3424601554870605,
      "learning_rate": 4.265404238618524e-05,
      "loss": 0.6493,
      "step": 748700
    },
    {
      "epoch": 11.755102040816327,
      "grad_norm": 4.556342124938965,
      "learning_rate": 4.26530612244898e-05,
      "loss": 0.6292,
      "step": 748800
    },
    {
      "epoch": 11.756671899529042,
      "grad_norm": 3.311654806137085,
      "learning_rate": 4.265208006279435e-05,
      "loss": 0.6529,
      "step": 748900
    },
    {
      "epoch": 11.758241758241759,
      "grad_norm": 4.390620231628418,
      "learning_rate": 4.26510989010989e-05,
      "loss": 0.6046,
      "step": 749000
    },
    {
      "epoch": 11.759811616954474,
      "grad_norm": 4.0728278160095215,
      "learning_rate": 4.265011773940346e-05,
      "loss": 0.7036,
      "step": 749100
    },
    {
      "epoch": 11.76138147566719,
      "grad_norm": 3.8838694095611572,
      "learning_rate": 4.264913657770801e-05,
      "loss": 0.6375,
      "step": 749200
    },
    {
      "epoch": 11.762951334379906,
      "grad_norm": 3.978416919708252,
      "learning_rate": 4.264815541601256e-05,
      "loss": 0.6532,
      "step": 749300
    },
    {
      "epoch": 11.764521193092621,
      "grad_norm": 3.2893614768981934,
      "learning_rate": 4.264717425431711e-05,
      "loss": 0.5985,
      "step": 749400
    },
    {
      "epoch": 11.766091051805338,
      "grad_norm": 4.700531482696533,
      "learning_rate": 4.264619309262167e-05,
      "loss": 0.6436,
      "step": 749500
    },
    {
      "epoch": 11.767660910518053,
      "grad_norm": 3.8917620182037354,
      "learning_rate": 4.264521193092622e-05,
      "loss": 0.656,
      "step": 749600
    },
    {
      "epoch": 11.76923076923077,
      "grad_norm": 4.1988630294799805,
      "learning_rate": 4.264423076923077e-05,
      "loss": 0.5915,
      "step": 749700
    },
    {
      "epoch": 11.770800627943485,
      "grad_norm": 4.447784900665283,
      "learning_rate": 4.264324960753532e-05,
      "loss": 0.6386,
      "step": 749800
    },
    {
      "epoch": 11.7723704866562,
      "grad_norm": 4.162527084350586,
      "learning_rate": 4.264226844583988e-05,
      "loss": 0.618,
      "step": 749900
    },
    {
      "epoch": 11.773940345368917,
      "grad_norm": 3.588038921356201,
      "learning_rate": 4.2641287284144425e-05,
      "loss": 0.6571,
      "step": 750000
    },
    {
      "epoch": 11.775510204081632,
      "grad_norm": 3.6614339351654053,
      "learning_rate": 4.264030612244898e-05,
      "loss": 0.6695,
      "step": 750100
    },
    {
      "epoch": 11.777080062794349,
      "grad_norm": 4.443347454071045,
      "learning_rate": 4.263932496075353e-05,
      "loss": 0.6322,
      "step": 750200
    },
    {
      "epoch": 11.778649921507064,
      "grad_norm": 3.5293471813201904,
      "learning_rate": 4.263834379905809e-05,
      "loss": 0.6202,
      "step": 750300
    },
    {
      "epoch": 11.780219780219781,
      "grad_norm": 4.47603702545166,
      "learning_rate": 4.2637362637362635e-05,
      "loss": 0.6383,
      "step": 750400
    },
    {
      "epoch": 11.781789638932496,
      "grad_norm": 3.5405116081237793,
      "learning_rate": 4.263638147566719e-05,
      "loss": 0.6494,
      "step": 750500
    },
    {
      "epoch": 11.783359497645211,
      "grad_norm": 3.1569554805755615,
      "learning_rate": 4.2635400313971744e-05,
      "loss": 0.6128,
      "step": 750600
    },
    {
      "epoch": 11.784929356357928,
      "grad_norm": 3.66788911819458,
      "learning_rate": 4.2634419152276295e-05,
      "loss": 0.6539,
      "step": 750700
    },
    {
      "epoch": 11.786499215070643,
      "grad_norm": 5.813560962677002,
      "learning_rate": 4.2633437990580846e-05,
      "loss": 0.6274,
      "step": 750800
    },
    {
      "epoch": 11.78806907378336,
      "grad_norm": 4.295534133911133,
      "learning_rate": 4.2632456828885404e-05,
      "loss": 0.6449,
      "step": 750900
    },
    {
      "epoch": 11.789638932496075,
      "grad_norm": 2.9946157932281494,
      "learning_rate": 4.2631475667189955e-05,
      "loss": 0.6528,
      "step": 751000
    },
    {
      "epoch": 11.791208791208792,
      "grad_norm": 7.76456356048584,
      "learning_rate": 4.2630494505494506e-05,
      "loss": 0.6282,
      "step": 751100
    },
    {
      "epoch": 11.792778649921507,
      "grad_norm": 3.9794070720672607,
      "learning_rate": 4.2629513343799063e-05,
      "loss": 0.6764,
      "step": 751200
    },
    {
      "epoch": 11.794348508634222,
      "grad_norm": 4.264665603637695,
      "learning_rate": 4.2628532182103614e-05,
      "loss": 0.6364,
      "step": 751300
    },
    {
      "epoch": 11.795918367346939,
      "grad_norm": 3.2733640670776367,
      "learning_rate": 4.2627551020408165e-05,
      "loss": 0.5755,
      "step": 751400
    },
    {
      "epoch": 11.797488226059654,
      "grad_norm": 3.1114001274108887,
      "learning_rate": 4.2626569858712716e-05,
      "loss": 0.6521,
      "step": 751500
    },
    {
      "epoch": 11.799058084772371,
      "grad_norm": 4.056136608123779,
      "learning_rate": 4.2625588697017274e-05,
      "loss": 0.6131,
      "step": 751600
    },
    {
      "epoch": 11.800627943485086,
      "grad_norm": 3.7677319049835205,
      "learning_rate": 4.2624607535321825e-05,
      "loss": 0.6082,
      "step": 751700
    },
    {
      "epoch": 11.802197802197803,
      "grad_norm": 4.417548179626465,
      "learning_rate": 4.2623626373626376e-05,
      "loss": 0.6505,
      "step": 751800
    },
    {
      "epoch": 11.803767660910518,
      "grad_norm": 3.9487884044647217,
      "learning_rate": 4.262264521193093e-05,
      "loss": 0.6393,
      "step": 751900
    },
    {
      "epoch": 11.805337519623233,
      "grad_norm": 4.885613918304443,
      "learning_rate": 4.2621664050235485e-05,
      "loss": 0.6473,
      "step": 752000
    },
    {
      "epoch": 11.80690737833595,
      "grad_norm": 2.736295461654663,
      "learning_rate": 4.262068288854003e-05,
      "loss": 0.6369,
      "step": 752100
    },
    {
      "epoch": 11.808477237048665,
      "grad_norm": 3.998782157897949,
      "learning_rate": 4.261970172684459e-05,
      "loss": 0.6117,
      "step": 752200
    },
    {
      "epoch": 11.810047095761382,
      "grad_norm": 4.235556125640869,
      "learning_rate": 4.261872056514914e-05,
      "loss": 0.6552,
      "step": 752300
    },
    {
      "epoch": 11.811616954474097,
      "grad_norm": 4.158586025238037,
      "learning_rate": 4.2617739403453696e-05,
      "loss": 0.5854,
      "step": 752400
    },
    {
      "epoch": 11.813186813186814,
      "grad_norm": 4.471328258514404,
      "learning_rate": 4.261675824175824e-05,
      "loss": 0.596,
      "step": 752500
    },
    {
      "epoch": 11.814756671899529,
      "grad_norm": 3.377411127090454,
      "learning_rate": 4.26157770800628e-05,
      "loss": 0.6643,
      "step": 752600
    },
    {
      "epoch": 11.816326530612244,
      "grad_norm": 3.7427961826324463,
      "learning_rate": 4.261479591836735e-05,
      "loss": 0.6165,
      "step": 752700
    },
    {
      "epoch": 11.817896389324961,
      "grad_norm": 4.0481486320495605,
      "learning_rate": 4.26138147566719e-05,
      "loss": 0.6264,
      "step": 752800
    },
    {
      "epoch": 11.819466248037676,
      "grad_norm": 4.502457141876221,
      "learning_rate": 4.261283359497645e-05,
      "loss": 0.6323,
      "step": 752900
    },
    {
      "epoch": 11.821036106750393,
      "grad_norm": 2.7175607681274414,
      "learning_rate": 4.261185243328101e-05,
      "loss": 0.6419,
      "step": 753000
    },
    {
      "epoch": 11.822605965463108,
      "grad_norm": 4.025915622711182,
      "learning_rate": 4.261087127158556e-05,
      "loss": 0.6103,
      "step": 753100
    },
    {
      "epoch": 11.824175824175825,
      "grad_norm": 4.4594268798828125,
      "learning_rate": 4.260989010989011e-05,
      "loss": 0.6169,
      "step": 753200
    },
    {
      "epoch": 11.82574568288854,
      "grad_norm": 3.549938917160034,
      "learning_rate": 4.260890894819467e-05,
      "loss": 0.5734,
      "step": 753300
    },
    {
      "epoch": 11.827315541601255,
      "grad_norm": 3.72808575630188,
      "learning_rate": 4.260792778649922e-05,
      "loss": 0.6432,
      "step": 753400
    },
    {
      "epoch": 11.828885400313972,
      "grad_norm": 4.055661678314209,
      "learning_rate": 4.260694662480377e-05,
      "loss": 0.638,
      "step": 753500
    },
    {
      "epoch": 11.830455259026687,
      "grad_norm": 4.573185443878174,
      "learning_rate": 4.260596546310832e-05,
      "loss": 0.6259,
      "step": 753600
    },
    {
      "epoch": 11.832025117739404,
      "grad_norm": 3.912914991378784,
      "learning_rate": 4.260498430141288e-05,
      "loss": 0.6517,
      "step": 753700
    },
    {
      "epoch": 11.833594976452119,
      "grad_norm": 3.2954418659210205,
      "learning_rate": 4.260400313971743e-05,
      "loss": 0.6664,
      "step": 753800
    },
    {
      "epoch": 11.835164835164836,
      "grad_norm": 4.609512805938721,
      "learning_rate": 4.260302197802198e-05,
      "loss": 0.6239,
      "step": 753900
    },
    {
      "epoch": 11.83673469387755,
      "grad_norm": 5.217845916748047,
      "learning_rate": 4.260204081632653e-05,
      "loss": 0.7103,
      "step": 754000
    },
    {
      "epoch": 11.838304552590268,
      "grad_norm": 4.280200958251953,
      "learning_rate": 4.260105965463109e-05,
      "loss": 0.6693,
      "step": 754100
    },
    {
      "epoch": 11.839874411302983,
      "grad_norm": 3.42229962348938,
      "learning_rate": 4.2600078492935634e-05,
      "loss": 0.6362,
      "step": 754200
    },
    {
      "epoch": 11.841444270015698,
      "grad_norm": 3.165297746658325,
      "learning_rate": 4.259909733124019e-05,
      "loss": 0.6569,
      "step": 754300
    },
    {
      "epoch": 11.843014128728415,
      "grad_norm": 4.105804443359375,
      "learning_rate": 4.259811616954474e-05,
      "loss": 0.6069,
      "step": 754400
    },
    {
      "epoch": 11.84458398744113,
      "grad_norm": 3.9391961097717285,
      "learning_rate": 4.25971350078493e-05,
      "loss": 0.6741,
      "step": 754500
    },
    {
      "epoch": 11.846153846153847,
      "grad_norm": 3.4747331142425537,
      "learning_rate": 4.2596153846153844e-05,
      "loss": 0.6195,
      "step": 754600
    },
    {
      "epoch": 11.847723704866562,
      "grad_norm": 4.072582721710205,
      "learning_rate": 4.25951726844584e-05,
      "loss": 0.6269,
      "step": 754700
    },
    {
      "epoch": 11.849293563579279,
      "grad_norm": 3.579678773880005,
      "learning_rate": 4.259419152276295e-05,
      "loss": 0.6166,
      "step": 754800
    },
    {
      "epoch": 11.850863422291994,
      "grad_norm": 3.5348212718963623,
      "learning_rate": 4.2593210361067504e-05,
      "loss": 0.6463,
      "step": 754900
    },
    {
      "epoch": 11.852433281004709,
      "grad_norm": 2.9257469177246094,
      "learning_rate": 4.2592229199372055e-05,
      "loss": 0.6395,
      "step": 755000
    },
    {
      "epoch": 11.854003139717426,
      "grad_norm": 4.145582675933838,
      "learning_rate": 4.259124803767661e-05,
      "loss": 0.6164,
      "step": 755100
    },
    {
      "epoch": 11.85557299843014,
      "grad_norm": 3.0386836528778076,
      "learning_rate": 4.2590266875981164e-05,
      "loss": 0.6351,
      "step": 755200
    },
    {
      "epoch": 11.857142857142858,
      "grad_norm": 3.822570323944092,
      "learning_rate": 4.2589285714285715e-05,
      "loss": 0.6376,
      "step": 755300
    },
    {
      "epoch": 11.858712715855573,
      "grad_norm": 4.511036396026611,
      "learning_rate": 4.258830455259027e-05,
      "loss": 0.6299,
      "step": 755400
    },
    {
      "epoch": 11.86028257456829,
      "grad_norm": 3.5244693756103516,
      "learning_rate": 4.2587323390894823e-05,
      "loss": 0.6091,
      "step": 755500
    },
    {
      "epoch": 11.861852433281005,
      "grad_norm": 4.136532306671143,
      "learning_rate": 4.2586342229199374e-05,
      "loss": 0.6469,
      "step": 755600
    },
    {
      "epoch": 11.86342229199372,
      "grad_norm": 4.2645792961120605,
      "learning_rate": 4.2585361067503925e-05,
      "loss": 0.6055,
      "step": 755700
    },
    {
      "epoch": 11.864992150706437,
      "grad_norm": 3.9712636470794678,
      "learning_rate": 4.258437990580848e-05,
      "loss": 0.6269,
      "step": 755800
    },
    {
      "epoch": 11.866562009419152,
      "grad_norm": 3.787947177886963,
      "learning_rate": 4.2583398744113034e-05,
      "loss": 0.598,
      "step": 755900
    },
    {
      "epoch": 11.868131868131869,
      "grad_norm": 3.543351411819458,
      "learning_rate": 4.2582417582417585e-05,
      "loss": 0.6777,
      "step": 756000
    },
    {
      "epoch": 11.869701726844584,
      "grad_norm": 3.812194585800171,
      "learning_rate": 4.2581436420722136e-05,
      "loss": 0.6242,
      "step": 756100
    },
    {
      "epoch": 11.8712715855573,
      "grad_norm": 3.1733901500701904,
      "learning_rate": 4.2580455259026694e-05,
      "loss": 0.6416,
      "step": 756200
    },
    {
      "epoch": 11.872841444270016,
      "grad_norm": 4.0261735916137695,
      "learning_rate": 4.257947409733124e-05,
      "loss": 0.6082,
      "step": 756300
    },
    {
      "epoch": 11.87441130298273,
      "grad_norm": 4.370480537414551,
      "learning_rate": 4.2578492935635796e-05,
      "loss": 0.6619,
      "step": 756400
    },
    {
      "epoch": 11.875981161695448,
      "grad_norm": 3.308048963546753,
      "learning_rate": 4.257751177394035e-05,
      "loss": 0.646,
      "step": 756500
    },
    {
      "epoch": 11.877551020408163,
      "grad_norm": 3.1635823249816895,
      "learning_rate": 4.2576530612244905e-05,
      "loss": 0.6194,
      "step": 756600
    },
    {
      "epoch": 11.87912087912088,
      "grad_norm": 2.511706829071045,
      "learning_rate": 4.257554945054945e-05,
      "loss": 0.6037,
      "step": 756700
    },
    {
      "epoch": 11.880690737833595,
      "grad_norm": 4.390808582305908,
      "learning_rate": 4.2574568288854006e-05,
      "loss": 0.6434,
      "step": 756800
    },
    {
      "epoch": 11.882260596546312,
      "grad_norm": 3.878030300140381,
      "learning_rate": 4.257358712715856e-05,
      "loss": 0.5999,
      "step": 756900
    },
    {
      "epoch": 11.883830455259027,
      "grad_norm": 4.688639163970947,
      "learning_rate": 4.257260596546311e-05,
      "loss": 0.6519,
      "step": 757000
    },
    {
      "epoch": 11.885400313971743,
      "grad_norm": 3.7259180545806885,
      "learning_rate": 4.257162480376766e-05,
      "loss": 0.6563,
      "step": 757100
    },
    {
      "epoch": 11.886970172684459,
      "grad_norm": 4.456695079803467,
      "learning_rate": 4.257064364207222e-05,
      "loss": 0.6261,
      "step": 757200
    },
    {
      "epoch": 11.888540031397174,
      "grad_norm": 4.337622165679932,
      "learning_rate": 4.256966248037677e-05,
      "loss": 0.5949,
      "step": 757300
    },
    {
      "epoch": 11.89010989010989,
      "grad_norm": 4.49389123916626,
      "learning_rate": 4.256868131868132e-05,
      "loss": 0.6543,
      "step": 757400
    },
    {
      "epoch": 11.891679748822606,
      "grad_norm": 4.0544610023498535,
      "learning_rate": 4.256770015698588e-05,
      "loss": 0.6212,
      "step": 757500
    },
    {
      "epoch": 11.893249607535322,
      "grad_norm": 4.057115077972412,
      "learning_rate": 4.256671899529043e-05,
      "loss": 0.6438,
      "step": 757600
    },
    {
      "epoch": 11.894819466248038,
      "grad_norm": 3.483393907546997,
      "learning_rate": 4.256573783359498e-05,
      "loss": 0.6467,
      "step": 757700
    },
    {
      "epoch": 11.896389324960754,
      "grad_norm": 5.00274133682251,
      "learning_rate": 4.256475667189953e-05,
      "loss": 0.6159,
      "step": 757800
    },
    {
      "epoch": 11.89795918367347,
      "grad_norm": 3.985949993133545,
      "learning_rate": 4.256377551020409e-05,
      "loss": 0.6469,
      "step": 757900
    },
    {
      "epoch": 11.899529042386185,
      "grad_norm": 3.834812641143799,
      "learning_rate": 4.256279434850864e-05,
      "loss": 0.6269,
      "step": 758000
    },
    {
      "epoch": 11.901098901098901,
      "grad_norm": 5.421640396118164,
      "learning_rate": 4.256181318681319e-05,
      "loss": 0.616,
      "step": 758100
    },
    {
      "epoch": 11.902668759811617,
      "grad_norm": 4.0448808670043945,
      "learning_rate": 4.256083202511774e-05,
      "loss": 0.6271,
      "step": 758200
    },
    {
      "epoch": 11.904238618524333,
      "grad_norm": 3.8259894847869873,
      "learning_rate": 4.25598508634223e-05,
      "loss": 0.6267,
      "step": 758300
    },
    {
      "epoch": 11.905808477237048,
      "grad_norm": 4.314177513122559,
      "learning_rate": 4.255886970172684e-05,
      "loss": 0.6449,
      "step": 758400
    },
    {
      "epoch": 11.907378335949765,
      "grad_norm": 4.314117908477783,
      "learning_rate": 4.25578885400314e-05,
      "loss": 0.6346,
      "step": 758500
    },
    {
      "epoch": 11.90894819466248,
      "grad_norm": 4.472388744354248,
      "learning_rate": 4.255690737833595e-05,
      "loss": 0.6312,
      "step": 758600
    },
    {
      "epoch": 11.910518053375196,
      "grad_norm": 4.467255115509033,
      "learning_rate": 4.255592621664051e-05,
      "loss": 0.6337,
      "step": 758700
    },
    {
      "epoch": 11.912087912087912,
      "grad_norm": 3.8977410793304443,
      "learning_rate": 4.255494505494505e-05,
      "loss": 0.614,
      "step": 758800
    },
    {
      "epoch": 11.913657770800627,
      "grad_norm": 4.031247615814209,
      "learning_rate": 4.255396389324961e-05,
      "loss": 0.6839,
      "step": 758900
    },
    {
      "epoch": 11.915227629513344,
      "grad_norm": 3.6769485473632812,
      "learning_rate": 4.255298273155416e-05,
      "loss": 0.6126,
      "step": 759000
    },
    {
      "epoch": 11.91679748822606,
      "grad_norm": 5.199347496032715,
      "learning_rate": 4.255200156985871e-05,
      "loss": 0.6303,
      "step": 759100
    },
    {
      "epoch": 11.918367346938776,
      "grad_norm": 5.264276027679443,
      "learning_rate": 4.2551020408163264e-05,
      "loss": 0.6868,
      "step": 759200
    },
    {
      "epoch": 11.919937205651491,
      "grad_norm": 4.409183025360107,
      "learning_rate": 4.255003924646782e-05,
      "loss": 0.597,
      "step": 759300
    },
    {
      "epoch": 11.921507064364206,
      "grad_norm": 2.5769639015197754,
      "learning_rate": 4.254905808477237e-05,
      "loss": 0.6594,
      "step": 759400
    },
    {
      "epoch": 11.923076923076923,
      "grad_norm": 4.731658935546875,
      "learning_rate": 4.2548076923076924e-05,
      "loss": 0.6419,
      "step": 759500
    },
    {
      "epoch": 11.924646781789638,
      "grad_norm": 3.140214204788208,
      "learning_rate": 4.2547095761381475e-05,
      "loss": 0.5815,
      "step": 759600
    },
    {
      "epoch": 11.926216640502355,
      "grad_norm": 3.893843412399292,
      "learning_rate": 4.254611459968603e-05,
      "loss": 0.5996,
      "step": 759700
    },
    {
      "epoch": 11.92778649921507,
      "grad_norm": 4.209290504455566,
      "learning_rate": 4.254513343799058e-05,
      "loss": 0.6428,
      "step": 759800
    },
    {
      "epoch": 11.929356357927787,
      "grad_norm": 4.61837100982666,
      "learning_rate": 4.2544152276295134e-05,
      "loss": 0.6454,
      "step": 759900
    },
    {
      "epoch": 11.930926216640502,
      "grad_norm": 4.352217674255371,
      "learning_rate": 4.254317111459969e-05,
      "loss": 0.6516,
      "step": 760000
    },
    {
      "epoch": 11.932496075353217,
      "grad_norm": 4.277975559234619,
      "learning_rate": 4.254218995290424e-05,
      "loss": 0.6628,
      "step": 760100
    },
    {
      "epoch": 11.934065934065934,
      "grad_norm": 3.4017035961151123,
      "learning_rate": 4.2541208791208794e-05,
      "loss": 0.6375,
      "step": 760200
    },
    {
      "epoch": 11.93563579277865,
      "grad_norm": 3.68565034866333,
      "learning_rate": 4.2540227629513345e-05,
      "loss": 0.6307,
      "step": 760300
    },
    {
      "epoch": 11.937205651491366,
      "grad_norm": 3.236039161682129,
      "learning_rate": 4.25392464678179e-05,
      "loss": 0.6206,
      "step": 760400
    },
    {
      "epoch": 11.938775510204081,
      "grad_norm": 3.8102190494537354,
      "learning_rate": 4.253826530612245e-05,
      "loss": 0.6563,
      "step": 760500
    },
    {
      "epoch": 11.940345368916798,
      "grad_norm": 4.011051654815674,
      "learning_rate": 4.2537284144427005e-05,
      "loss": 0.6321,
      "step": 760600
    },
    {
      "epoch": 11.941915227629513,
      "grad_norm": 4.147353172302246,
      "learning_rate": 4.2536302982731556e-05,
      "loss": 0.624,
      "step": 760700
    },
    {
      "epoch": 11.943485086342228,
      "grad_norm": 3.643051862716675,
      "learning_rate": 4.2535321821036114e-05,
      "loss": 0.6531,
      "step": 760800
    },
    {
      "epoch": 11.945054945054945,
      "grad_norm": 4.409040927886963,
      "learning_rate": 4.253434065934066e-05,
      "loss": 0.6227,
      "step": 760900
    },
    {
      "epoch": 11.94662480376766,
      "grad_norm": 4.225524425506592,
      "learning_rate": 4.2533359497645215e-05,
      "loss": 0.6429,
      "step": 761000
    },
    {
      "epoch": 11.948194662480377,
      "grad_norm": 3.9468934535980225,
      "learning_rate": 4.2532378335949766e-05,
      "loss": 0.6547,
      "step": 761100
    },
    {
      "epoch": 11.949764521193092,
      "grad_norm": 4.142456531524658,
      "learning_rate": 4.253139717425432e-05,
      "loss": 0.6452,
      "step": 761200
    },
    {
      "epoch": 11.95133437990581,
      "grad_norm": 3.4563956260681152,
      "learning_rate": 4.253041601255887e-05,
      "loss": 0.6565,
      "step": 761300
    },
    {
      "epoch": 11.952904238618524,
      "grad_norm": 3.853724241256714,
      "learning_rate": 4.2529434850863426e-05,
      "loss": 0.6503,
      "step": 761400
    },
    {
      "epoch": 11.95447409733124,
      "grad_norm": 3.826810121536255,
      "learning_rate": 4.252845368916798e-05,
      "loss": 0.6045,
      "step": 761500
    },
    {
      "epoch": 11.956043956043956,
      "grad_norm": 4.318597316741943,
      "learning_rate": 4.252747252747253e-05,
      "loss": 0.6069,
      "step": 761600
    },
    {
      "epoch": 11.957613814756671,
      "grad_norm": 3.4892845153808594,
      "learning_rate": 4.252649136577708e-05,
      "loss": 0.6243,
      "step": 761700
    },
    {
      "epoch": 11.959183673469388,
      "grad_norm": 3.3294460773468018,
      "learning_rate": 4.252551020408164e-05,
      "loss": 0.6148,
      "step": 761800
    },
    {
      "epoch": 11.960753532182103,
      "grad_norm": 2.3683950901031494,
      "learning_rate": 4.252452904238619e-05,
      "loss": 0.6669,
      "step": 761900
    },
    {
      "epoch": 11.96232339089482,
      "grad_norm": 3.5740904808044434,
      "learning_rate": 4.252354788069074e-05,
      "loss": 0.6788,
      "step": 762000
    },
    {
      "epoch": 11.963893249607535,
      "grad_norm": 4.0510125160217285,
      "learning_rate": 4.2522566718995297e-05,
      "loss": 0.6452,
      "step": 762100
    },
    {
      "epoch": 11.96546310832025,
      "grad_norm": 3.4640042781829834,
      "learning_rate": 4.252158555729984e-05,
      "loss": 0.6194,
      "step": 762200
    },
    {
      "epoch": 11.967032967032967,
      "grad_norm": 3.4944677352905273,
      "learning_rate": 4.25206043956044e-05,
      "loss": 0.6771,
      "step": 762300
    },
    {
      "epoch": 11.968602825745682,
      "grad_norm": 4.028907775878906,
      "learning_rate": 4.251962323390895e-05,
      "loss": 0.6709,
      "step": 762400
    },
    {
      "epoch": 11.970172684458399,
      "grad_norm": 3.235220193862915,
      "learning_rate": 4.251864207221351e-05,
      "loss": 0.642,
      "step": 762500
    },
    {
      "epoch": 11.971742543171114,
      "grad_norm": 3.1387779712677,
      "learning_rate": 4.251766091051805e-05,
      "loss": 0.686,
      "step": 762600
    },
    {
      "epoch": 11.973312401883831,
      "grad_norm": 4.349429130554199,
      "learning_rate": 4.251667974882261e-05,
      "loss": 0.6385,
      "step": 762700
    },
    {
      "epoch": 11.974882260596546,
      "grad_norm": 4.688923358917236,
      "learning_rate": 4.251569858712716e-05,
      "loss": 0.6568,
      "step": 762800
    },
    {
      "epoch": 11.976452119309261,
      "grad_norm": 5.124492645263672,
      "learning_rate": 4.251471742543171e-05,
      "loss": 0.6476,
      "step": 762900
    },
    {
      "epoch": 11.978021978021978,
      "grad_norm": 2.898493528366089,
      "learning_rate": 4.251373626373626e-05,
      "loss": 0.6479,
      "step": 763000
    },
    {
      "epoch": 11.979591836734693,
      "grad_norm": 4.607435703277588,
      "learning_rate": 4.251275510204082e-05,
      "loss": 0.603,
      "step": 763100
    },
    {
      "epoch": 11.98116169544741,
      "grad_norm": 4.355672359466553,
      "learning_rate": 4.251177394034537e-05,
      "loss": 0.6259,
      "step": 763200
    },
    {
      "epoch": 11.982731554160125,
      "grad_norm": 5.7815351486206055,
      "learning_rate": 4.251079277864992e-05,
      "loss": 0.6611,
      "step": 763300
    },
    {
      "epoch": 11.984301412872842,
      "grad_norm": 3.2806882858276367,
      "learning_rate": 4.250981161695447e-05,
      "loss": 0.6044,
      "step": 763400
    },
    {
      "epoch": 11.985871271585557,
      "grad_norm": 4.073241710662842,
      "learning_rate": 4.250883045525903e-05,
      "loss": 0.6235,
      "step": 763500
    },
    {
      "epoch": 11.987441130298274,
      "grad_norm": 3.7897467613220215,
      "learning_rate": 4.250784929356358e-05,
      "loss": 0.5705,
      "step": 763600
    },
    {
      "epoch": 11.989010989010989,
      "grad_norm": 4.279078960418701,
      "learning_rate": 4.250686813186813e-05,
      "loss": 0.6418,
      "step": 763700
    },
    {
      "epoch": 11.990580847723704,
      "grad_norm": 3.7959256172180176,
      "learning_rate": 4.2505886970172684e-05,
      "loss": 0.6215,
      "step": 763800
    },
    {
      "epoch": 11.992150706436421,
      "grad_norm": 4.3631415367126465,
      "learning_rate": 4.250490580847724e-05,
      "loss": 0.5979,
      "step": 763900
    },
    {
      "epoch": 11.993720565149136,
      "grad_norm": 2.9293501377105713,
      "learning_rate": 4.250392464678179e-05,
      "loss": 0.6633,
      "step": 764000
    },
    {
      "epoch": 11.995290423861853,
      "grad_norm": 2.8123457431793213,
      "learning_rate": 4.250294348508634e-05,
      "loss": 0.6111,
      "step": 764100
    },
    {
      "epoch": 11.996860282574568,
      "grad_norm": 4.195301055908203,
      "learning_rate": 4.25019623233909e-05,
      "loss": 0.7044,
      "step": 764200
    },
    {
      "epoch": 11.998430141287285,
      "grad_norm": 3.758010149002075,
      "learning_rate": 4.2500981161695445e-05,
      "loss": 0.6733,
      "step": 764300
    },
    {
      "epoch": 12.0,
      "grad_norm": 3.9894204139709473,
      "learning_rate": 4.25e-05,
      "loss": 0.603,
      "step": 764400
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.0419069528579712,
      "eval_runtime": 15.0493,
      "eval_samples_per_second": 222.8,
      "eval_steps_per_second": 222.8,
      "step": 764400
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.4910407066345215,
      "eval_runtime": 289.9774,
      "eval_samples_per_second": 219.672,
      "eval_steps_per_second": 219.672,
      "step": 764400
    },
    {
      "epoch": 12.001569858712715,
      "grad_norm": 3.4250481128692627,
      "learning_rate": 4.2499018838304554e-05,
      "loss": 0.635,
      "step": 764500
    },
    {
      "epoch": 12.003139717425432,
      "grad_norm": 4.031712055206299,
      "learning_rate": 4.249803767660911e-05,
      "loss": 0.645,
      "step": 764600
    },
    {
      "epoch": 12.004709576138147,
      "grad_norm": 3.8039803504943848,
      "learning_rate": 4.2497056514913656e-05,
      "loss": 0.5909,
      "step": 764700
    },
    {
      "epoch": 12.006279434850864,
      "grad_norm": 3.5385963916778564,
      "learning_rate": 4.2496075353218214e-05,
      "loss": 0.6574,
      "step": 764800
    },
    {
      "epoch": 12.007849293563579,
      "grad_norm": 3.816493272781372,
      "learning_rate": 4.2495094191522765e-05,
      "loss": 0.613,
      "step": 764900
    },
    {
      "epoch": 12.009419152276296,
      "grad_norm": 3.812723398208618,
      "learning_rate": 4.2494113029827316e-05,
      "loss": 0.6365,
      "step": 765000
    },
    {
      "epoch": 12.010989010989011,
      "grad_norm": 4.342123985290527,
      "learning_rate": 4.249313186813187e-05,
      "loss": 0.6,
      "step": 765100
    },
    {
      "epoch": 12.012558869701726,
      "grad_norm": 4.414202690124512,
      "learning_rate": 4.2492150706436424e-05,
      "loss": 0.6371,
      "step": 765200
    },
    {
      "epoch": 12.014128728414443,
      "grad_norm": 3.7336010932922363,
      "learning_rate": 4.2491169544740975e-05,
      "loss": 0.6133,
      "step": 765300
    },
    {
      "epoch": 12.015698587127158,
      "grad_norm": 4.064803123474121,
      "learning_rate": 4.2490188383045526e-05,
      "loss": 0.6173,
      "step": 765400
    },
    {
      "epoch": 12.017268445839875,
      "grad_norm": 3.3643901348114014,
      "learning_rate": 4.248920722135008e-05,
      "loss": 0.577,
      "step": 765500
    },
    {
      "epoch": 12.01883830455259,
      "grad_norm": 4.731164932250977,
      "learning_rate": 4.2488226059654635e-05,
      "loss": 0.5886,
      "step": 765600
    },
    {
      "epoch": 12.020408163265307,
      "grad_norm": 4.687787055969238,
      "learning_rate": 4.2487244897959186e-05,
      "loss": 0.6151,
      "step": 765700
    },
    {
      "epoch": 12.021978021978022,
      "grad_norm": 2.4979259967803955,
      "learning_rate": 4.248626373626374e-05,
      "loss": 0.5555,
      "step": 765800
    },
    {
      "epoch": 12.023547880690737,
      "grad_norm": 4.563947677612305,
      "learning_rate": 4.248528257456829e-05,
      "loss": 0.6755,
      "step": 765900
    },
    {
      "epoch": 12.025117739403454,
      "grad_norm": 3.750171184539795,
      "learning_rate": 4.2484301412872846e-05,
      "loss": 0.6251,
      "step": 766000
    },
    {
      "epoch": 12.026687598116169,
      "grad_norm": 3.988100051879883,
      "learning_rate": 4.24833202511774e-05,
      "loss": 0.644,
      "step": 766100
    },
    {
      "epoch": 12.028257456828886,
      "grad_norm": 2.9369704723358154,
      "learning_rate": 4.248233908948195e-05,
      "loss": 0.6107,
      "step": 766200
    },
    {
      "epoch": 12.029827315541601,
      "grad_norm": 4.827917098999023,
      "learning_rate": 4.2481357927786506e-05,
      "loss": 0.6322,
      "step": 766300
    },
    {
      "epoch": 12.031397174254318,
      "grad_norm": 3.2234249114990234,
      "learning_rate": 4.248037676609105e-05,
      "loss": 0.6346,
      "step": 766400
    },
    {
      "epoch": 12.032967032967033,
      "grad_norm": 3.8625872135162354,
      "learning_rate": 4.247939560439561e-05,
      "loss": 0.6022,
      "step": 766500
    },
    {
      "epoch": 12.034536891679748,
      "grad_norm": 4.714176654815674,
      "learning_rate": 4.247841444270016e-05,
      "loss": 0.6322,
      "step": 766600
    },
    {
      "epoch": 12.036106750392465,
      "grad_norm": 3.237921953201294,
      "learning_rate": 4.2477433281004716e-05,
      "loss": 0.6098,
      "step": 766700
    },
    {
      "epoch": 12.03767660910518,
      "grad_norm": 4.13721227645874,
      "learning_rate": 4.247645211930926e-05,
      "loss": 0.6073,
      "step": 766800
    },
    {
      "epoch": 12.039246467817897,
      "grad_norm": 4.080921649932861,
      "learning_rate": 4.247547095761382e-05,
      "loss": 0.5605,
      "step": 766900
    },
    {
      "epoch": 12.040816326530612,
      "grad_norm": 4.025861740112305,
      "learning_rate": 4.247448979591837e-05,
      "loss": 0.6373,
      "step": 767000
    },
    {
      "epoch": 12.042386185243329,
      "grad_norm": 3.786072015762329,
      "learning_rate": 4.247350863422292e-05,
      "loss": 0.6296,
      "step": 767100
    },
    {
      "epoch": 12.043956043956044,
      "grad_norm": 3.513472080230713,
      "learning_rate": 4.247252747252747e-05,
      "loss": 0.6158,
      "step": 767200
    },
    {
      "epoch": 12.04552590266876,
      "grad_norm": 4.367823123931885,
      "learning_rate": 4.247154631083203e-05,
      "loss": 0.5856,
      "step": 767300
    },
    {
      "epoch": 12.047095761381476,
      "grad_norm": 2.686903476715088,
      "learning_rate": 4.247056514913658e-05,
      "loss": 0.6454,
      "step": 767400
    },
    {
      "epoch": 12.04866562009419,
      "grad_norm": 3.6335372924804688,
      "learning_rate": 4.246958398744113e-05,
      "loss": 0.6124,
      "step": 767500
    },
    {
      "epoch": 12.050235478806908,
      "grad_norm": 3.227086305618286,
      "learning_rate": 4.246860282574568e-05,
      "loss": 0.565,
      "step": 767600
    },
    {
      "epoch": 12.051805337519623,
      "grad_norm": 4.0053277015686035,
      "learning_rate": 4.246762166405024e-05,
      "loss": 0.6292,
      "step": 767700
    },
    {
      "epoch": 12.05337519623234,
      "grad_norm": 4.305641174316406,
      "learning_rate": 4.246664050235479e-05,
      "loss": 0.6687,
      "step": 767800
    },
    {
      "epoch": 12.054945054945055,
      "grad_norm": 4.3556084632873535,
      "learning_rate": 4.246565934065934e-05,
      "loss": 0.6029,
      "step": 767900
    },
    {
      "epoch": 12.056514913657772,
      "grad_norm": 4.285740852355957,
      "learning_rate": 4.246467817896389e-05,
      "loss": 0.6298,
      "step": 768000
    },
    {
      "epoch": 12.058084772370487,
      "grad_norm": 3.15213680267334,
      "learning_rate": 4.246369701726845e-05,
      "loss": 0.6559,
      "step": 768100
    },
    {
      "epoch": 12.059654631083202,
      "grad_norm": 3.6147024631500244,
      "learning_rate": 4.2462715855573e-05,
      "loss": 0.5957,
      "step": 768200
    },
    {
      "epoch": 12.061224489795919,
      "grad_norm": 3.1540379524230957,
      "learning_rate": 4.246173469387755e-05,
      "loss": 0.6218,
      "step": 768300
    },
    {
      "epoch": 12.062794348508634,
      "grad_norm": 3.0678365230560303,
      "learning_rate": 4.246075353218211e-05,
      "loss": 0.6319,
      "step": 768400
    },
    {
      "epoch": 12.06436420722135,
      "grad_norm": 2.9337589740753174,
      "learning_rate": 4.2459772370486654e-05,
      "loss": 0.5634,
      "step": 768500
    },
    {
      "epoch": 12.065934065934066,
      "grad_norm": 3.726367712020874,
      "learning_rate": 4.245879120879121e-05,
      "loss": 0.6286,
      "step": 768600
    },
    {
      "epoch": 12.067503924646783,
      "grad_norm": 4.856541633605957,
      "learning_rate": 4.245781004709576e-05,
      "loss": 0.6277,
      "step": 768700
    },
    {
      "epoch": 12.069073783359498,
      "grad_norm": 3.9783732891082764,
      "learning_rate": 4.245682888540032e-05,
      "loss": 0.6232,
      "step": 768800
    },
    {
      "epoch": 12.070643642072213,
      "grad_norm": 4.217706680297852,
      "learning_rate": 4.2455847723704865e-05,
      "loss": 0.6377,
      "step": 768900
    },
    {
      "epoch": 12.07221350078493,
      "grad_norm": 3.686821460723877,
      "learning_rate": 4.245486656200942e-05,
      "loss": 0.6481,
      "step": 769000
    },
    {
      "epoch": 12.073783359497645,
      "grad_norm": 4.264686107635498,
      "learning_rate": 4.2453885400313974e-05,
      "loss": 0.6091,
      "step": 769100
    },
    {
      "epoch": 12.075353218210362,
      "grad_norm": 3.4770090579986572,
      "learning_rate": 4.2452904238618525e-05,
      "loss": 0.6316,
      "step": 769200
    },
    {
      "epoch": 12.076923076923077,
      "grad_norm": 5.249335289001465,
      "learning_rate": 4.2451923076923076e-05,
      "loss": 0.6158,
      "step": 769300
    },
    {
      "epoch": 12.078492935635794,
      "grad_norm": 3.313565731048584,
      "learning_rate": 4.2450941915227633e-05,
      "loss": 0.6133,
      "step": 769400
    },
    {
      "epoch": 12.080062794348509,
      "grad_norm": 3.2471823692321777,
      "learning_rate": 4.2449960753532184e-05,
      "loss": 0.6101,
      "step": 769500
    },
    {
      "epoch": 12.081632653061224,
      "grad_norm": 3.2026121616363525,
      "learning_rate": 4.2448979591836735e-05,
      "loss": 0.6136,
      "step": 769600
    },
    {
      "epoch": 12.08320251177394,
      "grad_norm": 3.806466579437256,
      "learning_rate": 4.2447998430141286e-05,
      "loss": 0.664,
      "step": 769700
    },
    {
      "epoch": 12.084772370486656,
      "grad_norm": 2.9351463317871094,
      "learning_rate": 4.2447017268445844e-05,
      "loss": 0.6157,
      "step": 769800
    },
    {
      "epoch": 12.086342229199373,
      "grad_norm": 3.0343892574310303,
      "learning_rate": 4.2446036106750395e-05,
      "loss": 0.593,
      "step": 769900
    },
    {
      "epoch": 12.087912087912088,
      "grad_norm": 3.3367955684661865,
      "learning_rate": 4.2445054945054946e-05,
      "loss": 0.6321,
      "step": 770000
    },
    {
      "epoch": 12.089481946624804,
      "grad_norm": 3.0473265647888184,
      "learning_rate": 4.24440737833595e-05,
      "loss": 0.6037,
      "step": 770100
    },
    {
      "epoch": 12.09105180533752,
      "grad_norm": 3.2757041454315186,
      "learning_rate": 4.2443092621664055e-05,
      "loss": 0.636,
      "step": 770200
    },
    {
      "epoch": 12.092621664050235,
      "grad_norm": 4.0826592445373535,
      "learning_rate": 4.2442111459968606e-05,
      "loss": 0.6139,
      "step": 770300
    },
    {
      "epoch": 12.094191522762952,
      "grad_norm": 4.759267807006836,
      "learning_rate": 4.244113029827316e-05,
      "loss": 0.6685,
      "step": 770400
    },
    {
      "epoch": 12.095761381475667,
      "grad_norm": 4.365513801574707,
      "learning_rate": 4.2440149136577715e-05,
      "loss": 0.6302,
      "step": 770500
    },
    {
      "epoch": 12.097331240188383,
      "grad_norm": 2.8232951164245605,
      "learning_rate": 4.243916797488226e-05,
      "loss": 0.6296,
      "step": 770600
    },
    {
      "epoch": 12.098901098901099,
      "grad_norm": 3.8877995014190674,
      "learning_rate": 4.2438186813186817e-05,
      "loss": 0.6206,
      "step": 770700
    },
    {
      "epoch": 12.100470957613815,
      "grad_norm": 4.493533611297607,
      "learning_rate": 4.243720565149137e-05,
      "loss": 0.6008,
      "step": 770800
    },
    {
      "epoch": 12.10204081632653,
      "grad_norm": 3.7693796157836914,
      "learning_rate": 4.2436224489795925e-05,
      "loss": 0.5839,
      "step": 770900
    },
    {
      "epoch": 12.103610675039246,
      "grad_norm": 2.5673763751983643,
      "learning_rate": 4.243524332810047e-05,
      "loss": 0.6864,
      "step": 771000
    },
    {
      "epoch": 12.105180533751962,
      "grad_norm": 4.025877475738525,
      "learning_rate": 4.243426216640503e-05,
      "loss": 0.5727,
      "step": 771100
    },
    {
      "epoch": 12.106750392464678,
      "grad_norm": 3.3870954513549805,
      "learning_rate": 4.243328100470958e-05,
      "loss": 0.6668,
      "step": 771200
    },
    {
      "epoch": 12.108320251177394,
      "grad_norm": 5.12338924407959,
      "learning_rate": 4.243229984301413e-05,
      "loss": 0.6295,
      "step": 771300
    },
    {
      "epoch": 12.10989010989011,
      "grad_norm": 3.9687018394470215,
      "learning_rate": 4.243131868131868e-05,
      "loss": 0.6485,
      "step": 771400
    },
    {
      "epoch": 12.111459968602826,
      "grad_norm": 4.183666706085205,
      "learning_rate": 4.243033751962324e-05,
      "loss": 0.6474,
      "step": 771500
    },
    {
      "epoch": 12.113029827315541,
      "grad_norm": 4.2286481857299805,
      "learning_rate": 4.242935635792779e-05,
      "loss": 0.6422,
      "step": 771600
    },
    {
      "epoch": 12.114599686028258,
      "grad_norm": 4.099234580993652,
      "learning_rate": 4.242837519623234e-05,
      "loss": 0.6397,
      "step": 771700
    },
    {
      "epoch": 12.116169544740973,
      "grad_norm": 3.0105817317962646,
      "learning_rate": 4.242739403453689e-05,
      "loss": 0.6528,
      "step": 771800
    },
    {
      "epoch": 12.117739403453688,
      "grad_norm": 3.687608003616333,
      "learning_rate": 4.242641287284145e-05,
      "loss": 0.6685,
      "step": 771900
    },
    {
      "epoch": 12.119309262166405,
      "grad_norm": 2.6870381832122803,
      "learning_rate": 4.2425431711146e-05,
      "loss": 0.6337,
      "step": 772000
    },
    {
      "epoch": 12.12087912087912,
      "grad_norm": 4.082512855529785,
      "learning_rate": 4.242445054945055e-05,
      "loss": 0.6159,
      "step": 772100
    },
    {
      "epoch": 12.122448979591837,
      "grad_norm": 2.982109785079956,
      "learning_rate": 4.24234693877551e-05,
      "loss": 0.6602,
      "step": 772200
    },
    {
      "epoch": 12.124018838304552,
      "grad_norm": 3.358858108520508,
      "learning_rate": 4.242248822605966e-05,
      "loss": 0.6193,
      "step": 772300
    },
    {
      "epoch": 12.12558869701727,
      "grad_norm": 2.9735610485076904,
      "learning_rate": 4.242150706436421e-05,
      "loss": 0.6402,
      "step": 772400
    },
    {
      "epoch": 12.127158555729984,
      "grad_norm": 4.044894695281982,
      "learning_rate": 4.242052590266876e-05,
      "loss": 0.6041,
      "step": 772500
    },
    {
      "epoch": 12.1287284144427,
      "grad_norm": 3.24440860748291,
      "learning_rate": 4.241954474097332e-05,
      "loss": 0.6494,
      "step": 772600
    },
    {
      "epoch": 12.130298273155416,
      "grad_norm": 3.632342576980591,
      "learning_rate": 4.241856357927786e-05,
      "loss": 0.5915,
      "step": 772700
    },
    {
      "epoch": 12.131868131868131,
      "grad_norm": 3.4203333854675293,
      "learning_rate": 4.241758241758242e-05,
      "loss": 0.6349,
      "step": 772800
    },
    {
      "epoch": 12.133437990580848,
      "grad_norm": 3.4182376861572266,
      "learning_rate": 4.241660125588697e-05,
      "loss": 0.6005,
      "step": 772900
    },
    {
      "epoch": 12.135007849293563,
      "grad_norm": 3.6940548419952393,
      "learning_rate": 4.241562009419153e-05,
      "loss": 0.6481,
      "step": 773000
    },
    {
      "epoch": 12.13657770800628,
      "grad_norm": 4.355709075927734,
      "learning_rate": 4.2414638932496074e-05,
      "loss": 0.6337,
      "step": 773100
    },
    {
      "epoch": 12.138147566718995,
      "grad_norm": 3.9157466888427734,
      "learning_rate": 4.241365777080063e-05,
      "loss": 0.6148,
      "step": 773200
    },
    {
      "epoch": 12.13971742543171,
      "grad_norm": 3.9493656158447266,
      "learning_rate": 4.241267660910518e-05,
      "loss": 0.6642,
      "step": 773300
    },
    {
      "epoch": 12.141287284144427,
      "grad_norm": 2.498664140701294,
      "learning_rate": 4.2411695447409734e-05,
      "loss": 0.6459,
      "step": 773400
    },
    {
      "epoch": 12.142857142857142,
      "grad_norm": 3.0983076095581055,
      "learning_rate": 4.2410714285714285e-05,
      "loss": 0.6392,
      "step": 773500
    },
    {
      "epoch": 12.14442700156986,
      "grad_norm": 4.483814239501953,
      "learning_rate": 4.240973312401884e-05,
      "loss": 0.6535,
      "step": 773600
    },
    {
      "epoch": 12.145996860282574,
      "grad_norm": 3.9755423069000244,
      "learning_rate": 4.2408751962323393e-05,
      "loss": 0.6474,
      "step": 773700
    },
    {
      "epoch": 12.147566718995291,
      "grad_norm": 3.0126593112945557,
      "learning_rate": 4.2407770800627944e-05,
      "loss": 0.6644,
      "step": 773800
    },
    {
      "epoch": 12.149136577708006,
      "grad_norm": 4.755528926849365,
      "learning_rate": 4.2406789638932495e-05,
      "loss": 0.6608,
      "step": 773900
    },
    {
      "epoch": 12.150706436420721,
      "grad_norm": 3.737882137298584,
      "learning_rate": 4.240580847723705e-05,
      "loss": 0.6221,
      "step": 774000
    },
    {
      "epoch": 12.152276295133438,
      "grad_norm": 3.9898478984832764,
      "learning_rate": 4.2404827315541604e-05,
      "loss": 0.6449,
      "step": 774100
    },
    {
      "epoch": 12.153846153846153,
      "grad_norm": 4.08651876449585,
      "learning_rate": 4.2403846153846155e-05,
      "loss": 0.6297,
      "step": 774200
    },
    {
      "epoch": 12.15541601255887,
      "grad_norm": 4.189028739929199,
      "learning_rate": 4.2402864992150706e-05,
      "loss": 0.5971,
      "step": 774300
    },
    {
      "epoch": 12.156985871271585,
      "grad_norm": 3.9532082080841064,
      "learning_rate": 4.2401883830455264e-05,
      "loss": 0.6089,
      "step": 774400
    },
    {
      "epoch": 12.158555729984302,
      "grad_norm": 4.305517196655273,
      "learning_rate": 4.2400902668759815e-05,
      "loss": 0.6059,
      "step": 774500
    },
    {
      "epoch": 12.160125588697017,
      "grad_norm": 3.360213041305542,
      "learning_rate": 4.2399921507064366e-05,
      "loss": 0.6066,
      "step": 774600
    },
    {
      "epoch": 12.161695447409732,
      "grad_norm": 5.1331257820129395,
      "learning_rate": 4.2398940345368924e-05,
      "loss": 0.6008,
      "step": 774700
    },
    {
      "epoch": 12.16326530612245,
      "grad_norm": 2.766249656677246,
      "learning_rate": 4.239795918367347e-05,
      "loss": 0.5874,
      "step": 774800
    },
    {
      "epoch": 12.164835164835164,
      "grad_norm": 3.4713809490203857,
      "learning_rate": 4.2396978021978026e-05,
      "loss": 0.5923,
      "step": 774900
    },
    {
      "epoch": 12.166405023547881,
      "grad_norm": 3.231607437133789,
      "learning_rate": 4.2395996860282576e-05,
      "loss": 0.5977,
      "step": 775000
    },
    {
      "epoch": 12.167974882260596,
      "grad_norm": 3.487926721572876,
      "learning_rate": 4.2395015698587134e-05,
      "loss": 0.643,
      "step": 775100
    },
    {
      "epoch": 12.169544740973313,
      "grad_norm": 3.1930160522460938,
      "learning_rate": 4.239403453689168e-05,
      "loss": 0.5995,
      "step": 775200
    },
    {
      "epoch": 12.171114599686028,
      "grad_norm": 3.3805556297302246,
      "learning_rate": 4.2393053375196236e-05,
      "loss": 0.6491,
      "step": 775300
    },
    {
      "epoch": 12.172684458398743,
      "grad_norm": 4.418482303619385,
      "learning_rate": 4.239207221350079e-05,
      "loss": 0.62,
      "step": 775400
    },
    {
      "epoch": 12.17425431711146,
      "grad_norm": 4.49406623840332,
      "learning_rate": 4.239109105180534e-05,
      "loss": 0.6324,
      "step": 775500
    },
    {
      "epoch": 12.175824175824175,
      "grad_norm": 4.390663146972656,
      "learning_rate": 4.239010989010989e-05,
      "loss": 0.6341,
      "step": 775600
    },
    {
      "epoch": 12.177394034536892,
      "grad_norm": 3.3839187622070312,
      "learning_rate": 4.238912872841445e-05,
      "loss": 0.6335,
      "step": 775700
    },
    {
      "epoch": 12.178963893249607,
      "grad_norm": 4.320076942443848,
      "learning_rate": 4.2388147566719e-05,
      "loss": 0.604,
      "step": 775800
    },
    {
      "epoch": 12.180533751962324,
      "grad_norm": 4.173666000366211,
      "learning_rate": 4.238716640502355e-05,
      "loss": 0.6032,
      "step": 775900
    },
    {
      "epoch": 12.182103610675039,
      "grad_norm": 4.191792011260986,
      "learning_rate": 4.23861852433281e-05,
      "loss": 0.578,
      "step": 776000
    },
    {
      "epoch": 12.183673469387756,
      "grad_norm": 4.570744037628174,
      "learning_rate": 4.238520408163266e-05,
      "loss": 0.6377,
      "step": 776100
    },
    {
      "epoch": 12.185243328100471,
      "grad_norm": 3.942049503326416,
      "learning_rate": 4.238422291993721e-05,
      "loss": 0.5866,
      "step": 776200
    },
    {
      "epoch": 12.186813186813186,
      "grad_norm": 3.888484477996826,
      "learning_rate": 4.238324175824176e-05,
      "loss": 0.634,
      "step": 776300
    },
    {
      "epoch": 12.188383045525903,
      "grad_norm": 4.156191825866699,
      "learning_rate": 4.238226059654631e-05,
      "loss": 0.5966,
      "step": 776400
    },
    {
      "epoch": 12.189952904238618,
      "grad_norm": 4.011348724365234,
      "learning_rate": 4.238127943485087e-05,
      "loss": 0.6417,
      "step": 776500
    },
    {
      "epoch": 12.191522762951335,
      "grad_norm": 3.4668216705322266,
      "learning_rate": 4.238029827315542e-05,
      "loss": 0.6244,
      "step": 776600
    },
    {
      "epoch": 12.19309262166405,
      "grad_norm": 3.176060438156128,
      "learning_rate": 4.237931711145997e-05,
      "loss": 0.6165,
      "step": 776700
    },
    {
      "epoch": 12.194662480376767,
      "grad_norm": 4.955105781555176,
      "learning_rate": 4.237833594976453e-05,
      "loss": 0.6302,
      "step": 776800
    },
    {
      "epoch": 12.196232339089482,
      "grad_norm": 3.3150711059570312,
      "learning_rate": 4.237735478806907e-05,
      "loss": 0.6548,
      "step": 776900
    },
    {
      "epoch": 12.197802197802197,
      "grad_norm": 3.7097432613372803,
      "learning_rate": 4.237637362637363e-05,
      "loss": 0.653,
      "step": 777000
    },
    {
      "epoch": 12.199372056514914,
      "grad_norm": 4.351600646972656,
      "learning_rate": 4.237539246467818e-05,
      "loss": 0.6756,
      "step": 777100
    },
    {
      "epoch": 12.200941915227629,
      "grad_norm": 4.385390281677246,
      "learning_rate": 4.237441130298274e-05,
      "loss": 0.6413,
      "step": 777200
    },
    {
      "epoch": 12.202511773940346,
      "grad_norm": 2.368070125579834,
      "learning_rate": 4.237343014128728e-05,
      "loss": 0.5865,
      "step": 777300
    },
    {
      "epoch": 12.204081632653061,
      "grad_norm": 3.7024502754211426,
      "learning_rate": 4.237244897959184e-05,
      "loss": 0.5903,
      "step": 777400
    },
    {
      "epoch": 12.205651491365778,
      "grad_norm": 3.734914541244507,
      "learning_rate": 4.237146781789639e-05,
      "loss": 0.6482,
      "step": 777500
    },
    {
      "epoch": 12.207221350078493,
      "grad_norm": 4.45917272567749,
      "learning_rate": 4.237048665620094e-05,
      "loss": 0.6133,
      "step": 777600
    },
    {
      "epoch": 12.208791208791208,
      "grad_norm": 2.2455403804779053,
      "learning_rate": 4.2369505494505494e-05,
      "loss": 0.6253,
      "step": 777700
    },
    {
      "epoch": 12.210361067503925,
      "grad_norm": 4.19310188293457,
      "learning_rate": 4.236852433281005e-05,
      "loss": 0.6104,
      "step": 777800
    },
    {
      "epoch": 12.21193092621664,
      "grad_norm": 3.654493570327759,
      "learning_rate": 4.23675431711146e-05,
      "loss": 0.6171,
      "step": 777900
    },
    {
      "epoch": 12.213500784929357,
      "grad_norm": 3.6322081089019775,
      "learning_rate": 4.236656200941915e-05,
      "loss": 0.6152,
      "step": 778000
    },
    {
      "epoch": 12.215070643642072,
      "grad_norm": 3.2697737216949463,
      "learning_rate": 4.2365580847723704e-05,
      "loss": 0.6545,
      "step": 778100
    },
    {
      "epoch": 12.216640502354789,
      "grad_norm": 3.9933650493621826,
      "learning_rate": 4.236459968602826e-05,
      "loss": 0.5965,
      "step": 778200
    },
    {
      "epoch": 12.218210361067504,
      "grad_norm": 4.163745880126953,
      "learning_rate": 4.236361852433281e-05,
      "loss": 0.6324,
      "step": 778300
    },
    {
      "epoch": 12.219780219780219,
      "grad_norm": 3.7911007404327393,
      "learning_rate": 4.2362637362637364e-05,
      "loss": 0.6151,
      "step": 778400
    },
    {
      "epoch": 12.221350078492936,
      "grad_norm": 5.020460605621338,
      "learning_rate": 4.2361656200941915e-05,
      "loss": 0.5983,
      "step": 778500
    },
    {
      "epoch": 12.222919937205651,
      "grad_norm": 3.4173572063446045,
      "learning_rate": 4.236067503924647e-05,
      "loss": 0.6554,
      "step": 778600
    },
    {
      "epoch": 12.224489795918368,
      "grad_norm": 3.9242753982543945,
      "learning_rate": 4.2359693877551024e-05,
      "loss": 0.6213,
      "step": 778700
    },
    {
      "epoch": 12.226059654631083,
      "grad_norm": 4.384892463684082,
      "learning_rate": 4.2358712715855575e-05,
      "loss": 0.6016,
      "step": 778800
    },
    {
      "epoch": 12.2276295133438,
      "grad_norm": 4.355672359466553,
      "learning_rate": 4.235773155416013e-05,
      "loss": 0.618,
      "step": 778900
    },
    {
      "epoch": 12.229199372056515,
      "grad_norm": 4.208124160766602,
      "learning_rate": 4.235675039246468e-05,
      "loss": 0.5926,
      "step": 779000
    },
    {
      "epoch": 12.23076923076923,
      "grad_norm": 3.249197006225586,
      "learning_rate": 4.2355769230769234e-05,
      "loss": 0.6355,
      "step": 779100
    },
    {
      "epoch": 12.232339089481947,
      "grad_norm": 3.8577723503112793,
      "learning_rate": 4.2354788069073785e-05,
      "loss": 0.6313,
      "step": 779200
    },
    {
      "epoch": 12.233908948194662,
      "grad_norm": 4.109933376312256,
      "learning_rate": 4.235380690737834e-05,
      "loss": 0.6142,
      "step": 779300
    },
    {
      "epoch": 12.235478806907379,
      "grad_norm": 4.554508686065674,
      "learning_rate": 4.235282574568289e-05,
      "loss": 0.6335,
      "step": 779400
    },
    {
      "epoch": 12.237048665620094,
      "grad_norm": 4.824636459350586,
      "learning_rate": 4.2351844583987445e-05,
      "loss": 0.6388,
      "step": 779500
    },
    {
      "epoch": 12.23861852433281,
      "grad_norm": 3.7111611366271973,
      "learning_rate": 4.2350863422291996e-05,
      "loss": 0.6006,
      "step": 779600
    },
    {
      "epoch": 12.240188383045526,
      "grad_norm": 3.043562889099121,
      "learning_rate": 4.234988226059655e-05,
      "loss": 0.6644,
      "step": 779700
    },
    {
      "epoch": 12.241758241758241,
      "grad_norm": 5.184266567230225,
      "learning_rate": 4.23489010989011e-05,
      "loss": 0.6125,
      "step": 779800
    },
    {
      "epoch": 12.243328100470958,
      "grad_norm": 4.404911994934082,
      "learning_rate": 4.2347919937205656e-05,
      "loss": 0.631,
      "step": 779900
    },
    {
      "epoch": 12.244897959183673,
      "grad_norm": 3.958677291870117,
      "learning_rate": 4.234693877551021e-05,
      "loss": 0.6276,
      "step": 780000
    },
    {
      "epoch": 12.24646781789639,
      "grad_norm": 4.1328606605529785,
      "learning_rate": 4.234595761381476e-05,
      "loss": 0.6127,
      "step": 780100
    },
    {
      "epoch": 12.248037676609105,
      "grad_norm": 4.860747337341309,
      "learning_rate": 4.234497645211931e-05,
      "loss": 0.6005,
      "step": 780200
    },
    {
      "epoch": 12.249607535321822,
      "grad_norm": 2.842282772064209,
      "learning_rate": 4.2343995290423867e-05,
      "loss": 0.6186,
      "step": 780300
    },
    {
      "epoch": 12.251177394034537,
      "grad_norm": 3.3017525672912598,
      "learning_rate": 4.234301412872842e-05,
      "loss": 0.6645,
      "step": 780400
    },
    {
      "epoch": 12.252747252747252,
      "grad_norm": 2.675764322280884,
      "learning_rate": 4.234203296703297e-05,
      "loss": 0.6392,
      "step": 780500
    },
    {
      "epoch": 12.254317111459969,
      "grad_norm": 4.253970146179199,
      "learning_rate": 4.234105180533752e-05,
      "loss": 0.6128,
      "step": 780600
    },
    {
      "epoch": 12.255886970172684,
      "grad_norm": 3.2660458087921143,
      "learning_rate": 4.234007064364208e-05,
      "loss": 0.6318,
      "step": 780700
    },
    {
      "epoch": 12.2574568288854,
      "grad_norm": 3.4136579036712646,
      "learning_rate": 4.233908948194663e-05,
      "loss": 0.7159,
      "step": 780800
    },
    {
      "epoch": 12.259026687598116,
      "grad_norm": 3.177974224090576,
      "learning_rate": 4.233810832025118e-05,
      "loss": 0.6128,
      "step": 780900
    },
    {
      "epoch": 12.260596546310833,
      "grad_norm": 4.011801242828369,
      "learning_rate": 4.233712715855574e-05,
      "loss": 0.6313,
      "step": 781000
    },
    {
      "epoch": 12.262166405023548,
      "grad_norm": 4.10152530670166,
      "learning_rate": 4.233614599686028e-05,
      "loss": 0.6394,
      "step": 781100
    },
    {
      "epoch": 12.263736263736265,
      "grad_norm": 3.5376405715942383,
      "learning_rate": 4.233516483516484e-05,
      "loss": 0.5821,
      "step": 781200
    },
    {
      "epoch": 12.26530612244898,
      "grad_norm": 3.292174816131592,
      "learning_rate": 4.233418367346939e-05,
      "loss": 0.6399,
      "step": 781300
    },
    {
      "epoch": 12.266875981161695,
      "grad_norm": 3.049506187438965,
      "learning_rate": 4.233320251177395e-05,
      "loss": 0.6663,
      "step": 781400
    },
    {
      "epoch": 12.268445839874412,
      "grad_norm": 4.027794361114502,
      "learning_rate": 4.233222135007849e-05,
      "loss": 0.6284,
      "step": 781500
    },
    {
      "epoch": 12.270015698587127,
      "grad_norm": 3.7624988555908203,
      "learning_rate": 4.233124018838305e-05,
      "loss": 0.6963,
      "step": 781600
    },
    {
      "epoch": 12.271585557299844,
      "grad_norm": 3.991825580596924,
      "learning_rate": 4.23302590266876e-05,
      "loss": 0.609,
      "step": 781700
    },
    {
      "epoch": 12.273155416012559,
      "grad_norm": 3.570709228515625,
      "learning_rate": 4.232927786499215e-05,
      "loss": 0.6304,
      "step": 781800
    },
    {
      "epoch": 12.274725274725276,
      "grad_norm": 4.200159549713135,
      "learning_rate": 4.23282967032967e-05,
      "loss": 0.5991,
      "step": 781900
    },
    {
      "epoch": 12.27629513343799,
      "grad_norm": 3.9619319438934326,
      "learning_rate": 4.232731554160126e-05,
      "loss": 0.6461,
      "step": 782000
    },
    {
      "epoch": 12.277864992150706,
      "grad_norm": 4.222545146942139,
      "learning_rate": 4.232633437990581e-05,
      "loss": 0.6248,
      "step": 782100
    },
    {
      "epoch": 12.279434850863423,
      "grad_norm": 4.445512771606445,
      "learning_rate": 4.232535321821036e-05,
      "loss": 0.6881,
      "step": 782200
    },
    {
      "epoch": 12.281004709576138,
      "grad_norm": 2.895012617111206,
      "learning_rate": 4.232437205651491e-05,
      "loss": 0.6654,
      "step": 782300
    },
    {
      "epoch": 12.282574568288855,
      "grad_norm": 3.8647074699401855,
      "learning_rate": 4.232339089481947e-05,
      "loss": 0.603,
      "step": 782400
    },
    {
      "epoch": 12.28414442700157,
      "grad_norm": 3.7605996131896973,
      "learning_rate": 4.232240973312402e-05,
      "loss": 0.6503,
      "step": 782500
    },
    {
      "epoch": 12.285714285714286,
      "grad_norm": 3.770655632019043,
      "learning_rate": 4.232142857142857e-05,
      "loss": 0.5957,
      "step": 782600
    },
    {
      "epoch": 12.287284144427002,
      "grad_norm": 3.4644970893859863,
      "learning_rate": 4.2320447409733124e-05,
      "loss": 0.6062,
      "step": 782700
    },
    {
      "epoch": 12.288854003139717,
      "grad_norm": 4.126768112182617,
      "learning_rate": 4.231946624803768e-05,
      "loss": 0.6421,
      "step": 782800
    },
    {
      "epoch": 12.290423861852434,
      "grad_norm": 3.2627720832824707,
      "learning_rate": 4.231848508634223e-05,
      "loss": 0.6149,
      "step": 782900
    },
    {
      "epoch": 12.291993720565149,
      "grad_norm": 3.5660300254821777,
      "learning_rate": 4.2317503924646784e-05,
      "loss": 0.6068,
      "step": 783000
    },
    {
      "epoch": 12.293563579277865,
      "grad_norm": 4.165375709533691,
      "learning_rate": 4.231652276295134e-05,
      "loss": 0.6284,
      "step": 783100
    },
    {
      "epoch": 12.29513343799058,
      "grad_norm": 3.347456216812134,
      "learning_rate": 4.2315541601255886e-05,
      "loss": 0.635,
      "step": 783200
    },
    {
      "epoch": 12.296703296703297,
      "grad_norm": 3.9902849197387695,
      "learning_rate": 4.2314560439560443e-05,
      "loss": 0.6622,
      "step": 783300
    },
    {
      "epoch": 12.298273155416013,
      "grad_norm": 3.56027889251709,
      "learning_rate": 4.2313579277864994e-05,
      "loss": 0.5921,
      "step": 783400
    },
    {
      "epoch": 12.299843014128728,
      "grad_norm": 3.458225727081299,
      "learning_rate": 4.231259811616955e-05,
      "loss": 0.6609,
      "step": 783500
    },
    {
      "epoch": 12.301412872841444,
      "grad_norm": 4.302303314208984,
      "learning_rate": 4.2311616954474096e-05,
      "loss": 0.632,
      "step": 783600
    },
    {
      "epoch": 12.30298273155416,
      "grad_norm": 4.727138996124268,
      "learning_rate": 4.2310635792778654e-05,
      "loss": 0.6239,
      "step": 783700
    },
    {
      "epoch": 12.304552590266876,
      "grad_norm": 3.9448423385620117,
      "learning_rate": 4.2309654631083205e-05,
      "loss": 0.625,
      "step": 783800
    },
    {
      "epoch": 12.306122448979592,
      "grad_norm": 3.4321961402893066,
      "learning_rate": 4.2308673469387756e-05,
      "loss": 0.6074,
      "step": 783900
    },
    {
      "epoch": 12.307692307692308,
      "grad_norm": 3.2850711345672607,
      "learning_rate": 4.230769230769231e-05,
      "loss": 0.6197,
      "step": 784000
    },
    {
      "epoch": 12.309262166405023,
      "grad_norm": 3.6815185546875,
      "learning_rate": 4.2306711145996865e-05,
      "loss": 0.6217,
      "step": 784100
    },
    {
      "epoch": 12.310832025117739,
      "grad_norm": 3.158484935760498,
      "learning_rate": 4.2305729984301416e-05,
      "loss": 0.6017,
      "step": 784200
    },
    {
      "epoch": 12.312401883830455,
      "grad_norm": 4.092389106750488,
      "learning_rate": 4.230474882260597e-05,
      "loss": 0.6297,
      "step": 784300
    },
    {
      "epoch": 12.31397174254317,
      "grad_norm": 4.448975563049316,
      "learning_rate": 4.230376766091052e-05,
      "loss": 0.6047,
      "step": 784400
    },
    {
      "epoch": 12.315541601255887,
      "grad_norm": 4.535863399505615,
      "learning_rate": 4.2302786499215076e-05,
      "loss": 0.6423,
      "step": 784500
    },
    {
      "epoch": 12.317111459968602,
      "grad_norm": 3.165569543838501,
      "learning_rate": 4.2301805337519627e-05,
      "loss": 0.6263,
      "step": 784600
    },
    {
      "epoch": 12.31868131868132,
      "grad_norm": 4.007636070251465,
      "learning_rate": 4.230082417582418e-05,
      "loss": 0.6598,
      "step": 784700
    },
    {
      "epoch": 12.320251177394034,
      "grad_norm": 2.9360172748565674,
      "learning_rate": 4.229984301412873e-05,
      "loss": 0.6102,
      "step": 784800
    },
    {
      "epoch": 12.321821036106751,
      "grad_norm": 4.112991809844971,
      "learning_rate": 4.229886185243328e-05,
      "loss": 0.6078,
      "step": 784900
    },
    {
      "epoch": 12.323390894819466,
      "grad_norm": 1.9726332426071167,
      "learning_rate": 4.229788069073784e-05,
      "loss": 0.595,
      "step": 785000
    },
    {
      "epoch": 12.324960753532181,
      "grad_norm": 3.986198663711548,
      "learning_rate": 4.229689952904239e-05,
      "loss": 0.6132,
      "step": 785100
    },
    {
      "epoch": 12.326530612244898,
      "grad_norm": 3.862248659133911,
      "learning_rate": 4.2295918367346946e-05,
      "loss": 0.6582,
      "step": 785200
    },
    {
      "epoch": 12.328100470957613,
      "grad_norm": 4.242580890655518,
      "learning_rate": 4.229493720565149e-05,
      "loss": 0.6135,
      "step": 785300
    },
    {
      "epoch": 12.32967032967033,
      "grad_norm": 3.858724355697632,
      "learning_rate": 4.229395604395605e-05,
      "loss": 0.6438,
      "step": 785400
    },
    {
      "epoch": 12.331240188383045,
      "grad_norm": 4.063832759857178,
      "learning_rate": 4.22929748822606e-05,
      "loss": 0.5879,
      "step": 785500
    },
    {
      "epoch": 12.332810047095762,
      "grad_norm": 3.028615951538086,
      "learning_rate": 4.229199372056515e-05,
      "loss": 0.6633,
      "step": 785600
    },
    {
      "epoch": 12.334379905808477,
      "grad_norm": 3.3551788330078125,
      "learning_rate": 4.22910125588697e-05,
      "loss": 0.6029,
      "step": 785700
    },
    {
      "epoch": 12.335949764521192,
      "grad_norm": 4.695104122161865,
      "learning_rate": 4.229003139717426e-05,
      "loss": 0.6364,
      "step": 785800
    },
    {
      "epoch": 12.33751962323391,
      "grad_norm": 3.702347993850708,
      "learning_rate": 4.228905023547881e-05,
      "loss": 0.6376,
      "step": 785900
    },
    {
      "epoch": 12.339089481946624,
      "grad_norm": 3.6073694229125977,
      "learning_rate": 4.228806907378336e-05,
      "loss": 0.6381,
      "step": 786000
    },
    {
      "epoch": 12.340659340659341,
      "grad_norm": 3.4876022338867188,
      "learning_rate": 4.228708791208791e-05,
      "loss": 0.5971,
      "step": 786100
    },
    {
      "epoch": 12.342229199372056,
      "grad_norm": 4.183498382568359,
      "learning_rate": 4.228610675039247e-05,
      "loss": 0.6411,
      "step": 786200
    },
    {
      "epoch": 12.343799058084773,
      "grad_norm": 4.778512477874756,
      "learning_rate": 4.2285125588697014e-05,
      "loss": 0.6425,
      "step": 786300
    },
    {
      "epoch": 12.345368916797488,
      "grad_norm": 4.251036643981934,
      "learning_rate": 4.228414442700157e-05,
      "loss": 0.6253,
      "step": 786400
    },
    {
      "epoch": 12.346938775510203,
      "grad_norm": 3.3223941326141357,
      "learning_rate": 4.228316326530612e-05,
      "loss": 0.6443,
      "step": 786500
    },
    {
      "epoch": 12.34850863422292,
      "grad_norm": 3.8357670307159424,
      "learning_rate": 4.228218210361068e-05,
      "loss": 0.6267,
      "step": 786600
    },
    {
      "epoch": 12.350078492935635,
      "grad_norm": 4.219786167144775,
      "learning_rate": 4.228120094191523e-05,
      "loss": 0.6147,
      "step": 786700
    },
    {
      "epoch": 12.351648351648352,
      "grad_norm": 4.000993728637695,
      "learning_rate": 4.228021978021978e-05,
      "loss": 0.6102,
      "step": 786800
    },
    {
      "epoch": 12.353218210361067,
      "grad_norm": 4.310503959655762,
      "learning_rate": 4.227923861852433e-05,
      "loss": 0.6816,
      "step": 786900
    },
    {
      "epoch": 12.354788069073784,
      "grad_norm": 3.6042063236236572,
      "learning_rate": 4.2278257456828884e-05,
      "loss": 0.6253,
      "step": 787000
    },
    {
      "epoch": 12.3563579277865,
      "grad_norm": 3.94262957572937,
      "learning_rate": 4.227727629513344e-05,
      "loss": 0.6142,
      "step": 787100
    },
    {
      "epoch": 12.357927786499214,
      "grad_norm": 4.114116668701172,
      "learning_rate": 4.227629513343799e-05,
      "loss": 0.6279,
      "step": 787200
    },
    {
      "epoch": 12.359497645211931,
      "grad_norm": 4.777519226074219,
      "learning_rate": 4.227531397174255e-05,
      "loss": 0.6118,
      "step": 787300
    },
    {
      "epoch": 12.361067503924646,
      "grad_norm": 3.465331792831421,
      "learning_rate": 4.2274332810047095e-05,
      "loss": 0.6216,
      "step": 787400
    },
    {
      "epoch": 12.362637362637363,
      "grad_norm": 3.3226981163024902,
      "learning_rate": 4.227335164835165e-05,
      "loss": 0.6575,
      "step": 787500
    },
    {
      "epoch": 12.364207221350078,
      "grad_norm": 4.290096759796143,
      "learning_rate": 4.2272370486656203e-05,
      "loss": 0.6165,
      "step": 787600
    },
    {
      "epoch": 12.365777080062795,
      "grad_norm": 3.774895668029785,
      "learning_rate": 4.2271389324960754e-05,
      "loss": 0.6029,
      "step": 787700
    },
    {
      "epoch": 12.36734693877551,
      "grad_norm": 3.2773828506469727,
      "learning_rate": 4.2270408163265305e-05,
      "loss": 0.6337,
      "step": 787800
    },
    {
      "epoch": 12.368916797488225,
      "grad_norm": 2.622293710708618,
      "learning_rate": 4.226942700156986e-05,
      "loss": 0.6215,
      "step": 787900
    },
    {
      "epoch": 12.370486656200942,
      "grad_norm": 3.803020477294922,
      "learning_rate": 4.2268445839874414e-05,
      "loss": 0.6339,
      "step": 788000
    },
    {
      "epoch": 12.372056514913657,
      "grad_norm": 4.067740440368652,
      "learning_rate": 4.2267464678178965e-05,
      "loss": 0.6579,
      "step": 788100
    },
    {
      "epoch": 12.373626373626374,
      "grad_norm": 4.627589225769043,
      "learning_rate": 4.2266483516483516e-05,
      "loss": 0.6362,
      "step": 788200
    },
    {
      "epoch": 12.37519623233909,
      "grad_norm": 3.5494894981384277,
      "learning_rate": 4.2265502354788074e-05,
      "loss": 0.594,
      "step": 788300
    },
    {
      "epoch": 12.376766091051806,
      "grad_norm": 4.634501934051514,
      "learning_rate": 4.226452119309262e-05,
      "loss": 0.6619,
      "step": 788400
    },
    {
      "epoch": 12.378335949764521,
      "grad_norm": 4.350201606750488,
      "learning_rate": 4.2263540031397176e-05,
      "loss": 0.5844,
      "step": 788500
    },
    {
      "epoch": 12.379905808477236,
      "grad_norm": 3.428563117980957,
      "learning_rate": 4.226255886970173e-05,
      "loss": 0.6426,
      "step": 788600
    },
    {
      "epoch": 12.381475667189953,
      "grad_norm": 4.133505821228027,
      "learning_rate": 4.2261577708006285e-05,
      "loss": 0.637,
      "step": 788700
    },
    {
      "epoch": 12.383045525902668,
      "grad_norm": 3.245699405670166,
      "learning_rate": 4.2260596546310836e-05,
      "loss": 0.5999,
      "step": 788800
    },
    {
      "epoch": 12.384615384615385,
      "grad_norm": 3.7832753658294678,
      "learning_rate": 4.2259615384615387e-05,
      "loss": 0.658,
      "step": 788900
    },
    {
      "epoch": 12.3861852433281,
      "grad_norm": 3.6685471534729004,
      "learning_rate": 4.225863422291994e-05,
      "loss": 0.624,
      "step": 789000
    },
    {
      "epoch": 12.387755102040817,
      "grad_norm": 4.564136981964111,
      "learning_rate": 4.225765306122449e-05,
      "loss": 0.6405,
      "step": 789100
    },
    {
      "epoch": 12.389324960753532,
      "grad_norm": 4.035539627075195,
      "learning_rate": 4.2256671899529046e-05,
      "loss": 0.6089,
      "step": 789200
    },
    {
      "epoch": 12.390894819466247,
      "grad_norm": 4.4216628074646,
      "learning_rate": 4.22556907378336e-05,
      "loss": 0.6275,
      "step": 789300
    },
    {
      "epoch": 12.392464678178964,
      "grad_norm": 2.8439180850982666,
      "learning_rate": 4.2254709576138155e-05,
      "loss": 0.6558,
      "step": 789400
    },
    {
      "epoch": 12.394034536891679,
      "grad_norm": 3.214733362197876,
      "learning_rate": 4.22537284144427e-05,
      "loss": 0.6384,
      "step": 789500
    },
    {
      "epoch": 12.395604395604396,
      "grad_norm": 3.8341662883758545,
      "learning_rate": 4.225274725274726e-05,
      "loss": 0.6212,
      "step": 789600
    },
    {
      "epoch": 12.397174254317111,
      "grad_norm": 3.1520233154296875,
      "learning_rate": 4.225176609105181e-05,
      "loss": 0.6347,
      "step": 789700
    },
    {
      "epoch": 12.398744113029828,
      "grad_norm": 4.041660785675049,
      "learning_rate": 4.225078492935636e-05,
      "loss": 0.6563,
      "step": 789800
    },
    {
      "epoch": 12.400313971742543,
      "grad_norm": 2.983123779296875,
      "learning_rate": 4.224980376766091e-05,
      "loss": 0.6204,
      "step": 789900
    },
    {
      "epoch": 12.40188383045526,
      "grad_norm": 3.0575196743011475,
      "learning_rate": 4.224882260596547e-05,
      "loss": 0.6361,
      "step": 790000
    },
    {
      "epoch": 12.403453689167975,
      "grad_norm": 3.4439408779144287,
      "learning_rate": 4.224784144427002e-05,
      "loss": 0.6369,
      "step": 790100
    },
    {
      "epoch": 12.40502354788069,
      "grad_norm": 4.137618064880371,
      "learning_rate": 4.224686028257457e-05,
      "loss": 0.6227,
      "step": 790200
    },
    {
      "epoch": 12.406593406593407,
      "grad_norm": 2.911646604537964,
      "learning_rate": 4.224587912087912e-05,
      "loss": 0.626,
      "step": 790300
    },
    {
      "epoch": 12.408163265306122,
      "grad_norm": 3.332505941390991,
      "learning_rate": 4.224489795918368e-05,
      "loss": 0.6653,
      "step": 790400
    },
    {
      "epoch": 12.409733124018839,
      "grad_norm": 3.8347818851470947,
      "learning_rate": 4.224391679748822e-05,
      "loss": 0.6218,
      "step": 790500
    },
    {
      "epoch": 12.411302982731554,
      "grad_norm": 3.6172361373901367,
      "learning_rate": 4.224293563579278e-05,
      "loss": 0.6254,
      "step": 790600
    },
    {
      "epoch": 12.41287284144427,
      "grad_norm": 3.726353168487549,
      "learning_rate": 4.224195447409733e-05,
      "loss": 0.6603,
      "step": 790700
    },
    {
      "epoch": 12.414442700156986,
      "grad_norm": 5.027165412902832,
      "learning_rate": 4.224097331240189e-05,
      "loss": 0.6514,
      "step": 790800
    },
    {
      "epoch": 12.416012558869701,
      "grad_norm": 4.471487522125244,
      "learning_rate": 4.223999215070644e-05,
      "loss": 0.6445,
      "step": 790900
    },
    {
      "epoch": 12.417582417582418,
      "grad_norm": 3.6502087116241455,
      "learning_rate": 4.223901098901099e-05,
      "loss": 0.615,
      "step": 791000
    },
    {
      "epoch": 12.419152276295133,
      "grad_norm": 3.8959176540374756,
      "learning_rate": 4.223802982731554e-05,
      "loss": 0.633,
      "step": 791100
    },
    {
      "epoch": 12.42072213500785,
      "grad_norm": 4.095008850097656,
      "learning_rate": 4.223704866562009e-05,
      "loss": 0.6732,
      "step": 791200
    },
    {
      "epoch": 12.422291993720565,
      "grad_norm": 3.7114217281341553,
      "learning_rate": 4.223606750392465e-05,
      "loss": 0.6018,
      "step": 791300
    },
    {
      "epoch": 12.423861852433282,
      "grad_norm": 4.623677730560303,
      "learning_rate": 4.22350863422292e-05,
      "loss": 0.6497,
      "step": 791400
    },
    {
      "epoch": 12.425431711145997,
      "grad_norm": 3.641906976699829,
      "learning_rate": 4.223410518053376e-05,
      "loss": 0.6206,
      "step": 791500
    },
    {
      "epoch": 12.427001569858712,
      "grad_norm": 4.3312153816223145,
      "learning_rate": 4.2233124018838304e-05,
      "loss": 0.6199,
      "step": 791600
    },
    {
      "epoch": 12.428571428571429,
      "grad_norm": 4.154832363128662,
      "learning_rate": 4.223214285714286e-05,
      "loss": 0.6514,
      "step": 791700
    },
    {
      "epoch": 12.430141287284144,
      "grad_norm": 4.415135860443115,
      "learning_rate": 4.223116169544741e-05,
      "loss": 0.643,
      "step": 791800
    },
    {
      "epoch": 12.43171114599686,
      "grad_norm": 4.458327770233154,
      "learning_rate": 4.2230180533751963e-05,
      "loss": 0.6239,
      "step": 791900
    },
    {
      "epoch": 12.433281004709576,
      "grad_norm": 3.9557883739471436,
      "learning_rate": 4.2229199372056514e-05,
      "loss": 0.6096,
      "step": 792000
    },
    {
      "epoch": 12.434850863422293,
      "grad_norm": 3.282036304473877,
      "learning_rate": 4.222821821036107e-05,
      "loss": 0.6041,
      "step": 792100
    },
    {
      "epoch": 12.436420722135008,
      "grad_norm": 4.077369689941406,
      "learning_rate": 4.222723704866562e-05,
      "loss": 0.6272,
      "step": 792200
    },
    {
      "epoch": 12.437990580847723,
      "grad_norm": 4.934089183807373,
      "learning_rate": 4.2226255886970174e-05,
      "loss": 0.6597,
      "step": 792300
    },
    {
      "epoch": 12.43956043956044,
      "grad_norm": 3.782203197479248,
      "learning_rate": 4.2225274725274725e-05,
      "loss": 0.6156,
      "step": 792400
    },
    {
      "epoch": 12.441130298273155,
      "grad_norm": 4.02534818649292,
      "learning_rate": 4.222429356357928e-05,
      "loss": 0.6312,
      "step": 792500
    },
    {
      "epoch": 12.442700156985872,
      "grad_norm": 4.253499507904053,
      "learning_rate": 4.222331240188383e-05,
      "loss": 0.6335,
      "step": 792600
    },
    {
      "epoch": 12.444270015698587,
      "grad_norm": 4.238421440124512,
      "learning_rate": 4.2222331240188385e-05,
      "loss": 0.5942,
      "step": 792700
    },
    {
      "epoch": 12.445839874411304,
      "grad_norm": 4.286977291107178,
      "learning_rate": 4.2221350078492936e-05,
      "loss": 0.5996,
      "step": 792800
    },
    {
      "epoch": 12.447409733124019,
      "grad_norm": 3.592291831970215,
      "learning_rate": 4.2220368916797494e-05,
      "loss": 0.6643,
      "step": 792900
    },
    {
      "epoch": 12.448979591836734,
      "grad_norm": 4.5219292640686035,
      "learning_rate": 4.2219387755102045e-05,
      "loss": 0.625,
      "step": 793000
    },
    {
      "epoch": 12.45054945054945,
      "grad_norm": 4.335308074951172,
      "learning_rate": 4.2218406593406595e-05,
      "loss": 0.6451,
      "step": 793100
    },
    {
      "epoch": 12.452119309262166,
      "grad_norm": 4.55123233795166,
      "learning_rate": 4.2217425431711146e-05,
      "loss": 0.6084,
      "step": 793200
    },
    {
      "epoch": 12.453689167974883,
      "grad_norm": 2.881295680999756,
      "learning_rate": 4.22164442700157e-05,
      "loss": 0.6586,
      "step": 793300
    },
    {
      "epoch": 12.455259026687598,
      "grad_norm": 3.2559423446655273,
      "learning_rate": 4.2215463108320255e-05,
      "loss": 0.6392,
      "step": 793400
    },
    {
      "epoch": 12.456828885400315,
      "grad_norm": 4.304422855377197,
      "learning_rate": 4.2214481946624806e-05,
      "loss": 0.6089,
      "step": 793500
    },
    {
      "epoch": 12.45839874411303,
      "grad_norm": 3.5646812915802,
      "learning_rate": 4.2213500784929364e-05,
      "loss": 0.6161,
      "step": 793600
    },
    {
      "epoch": 12.459968602825747,
      "grad_norm": 3.7764742374420166,
      "learning_rate": 4.221251962323391e-05,
      "loss": 0.6501,
      "step": 793700
    },
    {
      "epoch": 12.461538461538462,
      "grad_norm": 4.084961891174316,
      "learning_rate": 4.2211538461538466e-05,
      "loss": 0.6644,
      "step": 793800
    },
    {
      "epoch": 12.463108320251177,
      "grad_norm": 4.725725173950195,
      "learning_rate": 4.221055729984302e-05,
      "loss": 0.641,
      "step": 793900
    },
    {
      "epoch": 12.464678178963894,
      "grad_norm": 3.8651843070983887,
      "learning_rate": 4.220957613814757e-05,
      "loss": 0.6174,
      "step": 794000
    },
    {
      "epoch": 12.466248037676609,
      "grad_norm": 1.7999773025512695,
      "learning_rate": 4.220859497645212e-05,
      "loss": 0.6345,
      "step": 794100
    },
    {
      "epoch": 12.467817896389326,
      "grad_norm": 3.236213445663452,
      "learning_rate": 4.220761381475668e-05,
      "loss": 0.5793,
      "step": 794200
    },
    {
      "epoch": 12.46938775510204,
      "grad_norm": 2.7091002464294434,
      "learning_rate": 4.220663265306123e-05,
      "loss": 0.575,
      "step": 794300
    },
    {
      "epoch": 12.470957613814758,
      "grad_norm": 3.8328018188476562,
      "learning_rate": 4.220565149136578e-05,
      "loss": 0.6246,
      "step": 794400
    },
    {
      "epoch": 12.472527472527473,
      "grad_norm": 4.355989933013916,
      "learning_rate": 4.220467032967033e-05,
      "loss": 0.6015,
      "step": 794500
    },
    {
      "epoch": 12.474097331240188,
      "grad_norm": 4.084927082061768,
      "learning_rate": 4.220368916797489e-05,
      "loss": 0.592,
      "step": 794600
    },
    {
      "epoch": 12.475667189952905,
      "grad_norm": 3.757972240447998,
      "learning_rate": 4.220270800627943e-05,
      "loss": 0.6272,
      "step": 794700
    },
    {
      "epoch": 12.47723704866562,
      "grad_norm": 3.9751882553100586,
      "learning_rate": 4.220172684458399e-05,
      "loss": 0.6542,
      "step": 794800
    },
    {
      "epoch": 12.478806907378337,
      "grad_norm": 2.9156346321105957,
      "learning_rate": 4.220074568288854e-05,
      "loss": 0.6228,
      "step": 794900
    },
    {
      "epoch": 12.480376766091052,
      "grad_norm": 3.7531158924102783,
      "learning_rate": 4.21997645211931e-05,
      "loss": 0.6399,
      "step": 795000
    },
    {
      "epoch": 12.481946624803768,
      "grad_norm": 3.870173215866089,
      "learning_rate": 4.219878335949764e-05,
      "loss": 0.6059,
      "step": 795100
    },
    {
      "epoch": 12.483516483516484,
      "grad_norm": 4.177516937255859,
      "learning_rate": 4.21978021978022e-05,
      "loss": 0.6095,
      "step": 795200
    },
    {
      "epoch": 12.485086342229199,
      "grad_norm": 4.484286785125732,
      "learning_rate": 4.219682103610675e-05,
      "loss": 0.6097,
      "step": 795300
    },
    {
      "epoch": 12.486656200941916,
      "grad_norm": 3.280092716217041,
      "learning_rate": 4.21958398744113e-05,
      "loss": 0.6177,
      "step": 795400
    },
    {
      "epoch": 12.48822605965463,
      "grad_norm": 3.1607506275177,
      "learning_rate": 4.219485871271586e-05,
      "loss": 0.6153,
      "step": 795500
    },
    {
      "epoch": 12.489795918367347,
      "grad_norm": 3.3099019527435303,
      "learning_rate": 4.219387755102041e-05,
      "loss": 0.6295,
      "step": 795600
    },
    {
      "epoch": 12.491365777080063,
      "grad_norm": 2.788614511489868,
      "learning_rate": 4.219289638932497e-05,
      "loss": 0.6273,
      "step": 795700
    },
    {
      "epoch": 12.49293563579278,
      "grad_norm": 2.5509955883026123,
      "learning_rate": 4.219191522762951e-05,
      "loss": 0.6101,
      "step": 795800
    },
    {
      "epoch": 12.494505494505495,
      "grad_norm": 4.0733819007873535,
      "learning_rate": 4.219093406593407e-05,
      "loss": 0.6284,
      "step": 795900
    },
    {
      "epoch": 12.49607535321821,
      "grad_norm": 3.2076759338378906,
      "learning_rate": 4.218995290423862e-05,
      "loss": 0.6176,
      "step": 796000
    },
    {
      "epoch": 12.497645211930926,
      "grad_norm": 3.812269926071167,
      "learning_rate": 4.218897174254317e-05,
      "loss": 0.6107,
      "step": 796100
    },
    {
      "epoch": 12.499215070643642,
      "grad_norm": 3.7243854999542236,
      "learning_rate": 4.218799058084772e-05,
      "loss": 0.6037,
      "step": 796200
    },
    {
      "epoch": 12.500784929356358,
      "grad_norm": 3.653822183609009,
      "learning_rate": 4.218700941915228e-05,
      "loss": 0.6087,
      "step": 796300
    },
    {
      "epoch": 12.502354788069074,
      "grad_norm": 2.839221954345703,
      "learning_rate": 4.218602825745683e-05,
      "loss": 0.5842,
      "step": 796400
    },
    {
      "epoch": 12.50392464678179,
      "grad_norm": 4.055305480957031,
      "learning_rate": 4.218504709576138e-05,
      "loss": 0.6651,
      "step": 796500
    },
    {
      "epoch": 12.505494505494505,
      "grad_norm": 4.175410747528076,
      "learning_rate": 4.2184065934065934e-05,
      "loss": 0.6155,
      "step": 796600
    },
    {
      "epoch": 12.50706436420722,
      "grad_norm": 4.381672382354736,
      "learning_rate": 4.218308477237049e-05,
      "loss": 0.6308,
      "step": 796700
    },
    {
      "epoch": 12.508634222919937,
      "grad_norm": 4.212143898010254,
      "learning_rate": 4.2182103610675036e-05,
      "loss": 0.629,
      "step": 796800
    },
    {
      "epoch": 12.510204081632653,
      "grad_norm": 3.2491211891174316,
      "learning_rate": 4.2181122448979594e-05,
      "loss": 0.6558,
      "step": 796900
    },
    {
      "epoch": 12.51177394034537,
      "grad_norm": 3.6360573768615723,
      "learning_rate": 4.2180141287284145e-05,
      "loss": 0.5847,
      "step": 797000
    },
    {
      "epoch": 12.513343799058084,
      "grad_norm": 4.212645530700684,
      "learning_rate": 4.21791601255887e-05,
      "loss": 0.6187,
      "step": 797100
    },
    {
      "epoch": 12.514913657770801,
      "grad_norm": 2.84560227394104,
      "learning_rate": 4.217817896389325e-05,
      "loss": 0.6124,
      "step": 797200
    },
    {
      "epoch": 12.516483516483516,
      "grad_norm": 3.349961042404175,
      "learning_rate": 4.2177197802197804e-05,
      "loss": 0.5922,
      "step": 797300
    },
    {
      "epoch": 12.518053375196232,
      "grad_norm": 2.9511122703552246,
      "learning_rate": 4.2176216640502355e-05,
      "loss": 0.6167,
      "step": 797400
    },
    {
      "epoch": 12.519623233908948,
      "grad_norm": 4.0914459228515625,
      "learning_rate": 4.2175235478806906e-05,
      "loss": 0.6228,
      "step": 797500
    },
    {
      "epoch": 12.521193092621663,
      "grad_norm": 4.157741546630859,
      "learning_rate": 4.2174254317111464e-05,
      "loss": 0.681,
      "step": 797600
    },
    {
      "epoch": 12.52276295133438,
      "grad_norm": 3.273798704147339,
      "learning_rate": 4.2173273155416015e-05,
      "loss": 0.6255,
      "step": 797700
    },
    {
      "epoch": 12.524332810047095,
      "grad_norm": 3.5338685512542725,
      "learning_rate": 4.217229199372057e-05,
      "loss": 0.598,
      "step": 797800
    },
    {
      "epoch": 12.525902668759812,
      "grad_norm": 6.298757076263428,
      "learning_rate": 4.217131083202512e-05,
      "loss": 0.6249,
      "step": 797900
    },
    {
      "epoch": 12.527472527472527,
      "grad_norm": 2.9707419872283936,
      "learning_rate": 4.2170329670329675e-05,
      "loss": 0.6121,
      "step": 798000
    },
    {
      "epoch": 12.529042386185242,
      "grad_norm": 3.369868516921997,
      "learning_rate": 4.2169348508634226e-05,
      "loss": 0.6575,
      "step": 798100
    },
    {
      "epoch": 12.53061224489796,
      "grad_norm": 3.7299914360046387,
      "learning_rate": 4.216836734693878e-05,
      "loss": 0.59,
      "step": 798200
    },
    {
      "epoch": 12.532182103610674,
      "grad_norm": 2.6800708770751953,
      "learning_rate": 4.216738618524333e-05,
      "loss": 0.5997,
      "step": 798300
    },
    {
      "epoch": 12.533751962323391,
      "grad_norm": 4.365311622619629,
      "learning_rate": 4.2166405023547886e-05,
      "loss": 0.633,
      "step": 798400
    },
    {
      "epoch": 12.535321821036106,
      "grad_norm": 3.590278387069702,
      "learning_rate": 4.2165423861852437e-05,
      "loss": 0.6193,
      "step": 798500
    },
    {
      "epoch": 12.536891679748823,
      "grad_norm": 5.451648235321045,
      "learning_rate": 4.216444270015699e-05,
      "loss": 0.6457,
      "step": 798600
    },
    {
      "epoch": 12.538461538461538,
      "grad_norm": 3.995455265045166,
      "learning_rate": 4.216346153846154e-05,
      "loss": 0.6208,
      "step": 798700
    },
    {
      "epoch": 12.540031397174253,
      "grad_norm": 4.582334518432617,
      "learning_rate": 4.2162480376766096e-05,
      "loss": 0.6365,
      "step": 798800
    },
    {
      "epoch": 12.54160125588697,
      "grad_norm": 3.270146369934082,
      "learning_rate": 4.216149921507064e-05,
      "loss": 0.6401,
      "step": 798900
    },
    {
      "epoch": 12.543171114599685,
      "grad_norm": 3.266226053237915,
      "learning_rate": 4.21605180533752e-05,
      "loss": 0.6131,
      "step": 799000
    },
    {
      "epoch": 12.544740973312402,
      "grad_norm": 3.589348316192627,
      "learning_rate": 4.215953689167975e-05,
      "loss": 0.6485,
      "step": 799100
    },
    {
      "epoch": 12.546310832025117,
      "grad_norm": 3.7941174507141113,
      "learning_rate": 4.215855572998431e-05,
      "loss": 0.6327,
      "step": 799200
    },
    {
      "epoch": 12.547880690737834,
      "grad_norm": 3.5635604858398438,
      "learning_rate": 4.215757456828885e-05,
      "loss": 0.6287,
      "step": 799300
    },
    {
      "epoch": 12.54945054945055,
      "grad_norm": 4.333399772644043,
      "learning_rate": 4.215659340659341e-05,
      "loss": 0.5746,
      "step": 799400
    },
    {
      "epoch": 12.551020408163264,
      "grad_norm": 4.125611782073975,
      "learning_rate": 4.215561224489796e-05,
      "loss": 0.5885,
      "step": 799500
    },
    {
      "epoch": 12.552590266875981,
      "grad_norm": 3.969141721725464,
      "learning_rate": 4.215463108320251e-05,
      "loss": 0.6365,
      "step": 799600
    },
    {
      "epoch": 12.554160125588696,
      "grad_norm": 3.7566187381744385,
      "learning_rate": 4.215364992150707e-05,
      "loss": 0.6439,
      "step": 799700
    },
    {
      "epoch": 12.555729984301413,
      "grad_norm": 3.9991977214813232,
      "learning_rate": 4.215266875981162e-05,
      "loss": 0.633,
      "step": 799800
    },
    {
      "epoch": 12.557299843014128,
      "grad_norm": 3.141371011734009,
      "learning_rate": 4.215168759811618e-05,
      "loss": 0.6427,
      "step": 799900
    },
    {
      "epoch": 12.558869701726845,
      "grad_norm": 3.7523834705352783,
      "learning_rate": 4.215070643642072e-05,
      "loss": 0.5973,
      "step": 800000
    },
    {
      "epoch": 12.56043956043956,
      "grad_norm": 4.486673355102539,
      "learning_rate": 4.214972527472528e-05,
      "loss": 0.638,
      "step": 800100
    },
    {
      "epoch": 12.562009419152277,
      "grad_norm": 2.612943410873413,
      "learning_rate": 4.214874411302983e-05,
      "loss": 0.6632,
      "step": 800200
    },
    {
      "epoch": 12.563579277864992,
      "grad_norm": 3.5637576580047607,
      "learning_rate": 4.214776295133438e-05,
      "loss": 0.6395,
      "step": 800300
    },
    {
      "epoch": 12.565149136577707,
      "grad_norm": 3.9587209224700928,
      "learning_rate": 4.214678178963893e-05,
      "loss": 0.6122,
      "step": 800400
    },
    {
      "epoch": 12.566718995290424,
      "grad_norm": 3.0621590614318848,
      "learning_rate": 4.214580062794349e-05,
      "loss": 0.6464,
      "step": 800500
    },
    {
      "epoch": 12.56828885400314,
      "grad_norm": 3.275621175765991,
      "learning_rate": 4.214481946624804e-05,
      "loss": 0.6233,
      "step": 800600
    },
    {
      "epoch": 12.569858712715856,
      "grad_norm": 4.007518291473389,
      "learning_rate": 4.214383830455259e-05,
      "loss": 0.6287,
      "step": 800700
    },
    {
      "epoch": 12.571428571428571,
      "grad_norm": 4.3547163009643555,
      "learning_rate": 4.214285714285714e-05,
      "loss": 0.6209,
      "step": 800800
    },
    {
      "epoch": 12.572998430141288,
      "grad_norm": 2.6622540950775146,
      "learning_rate": 4.21418759811617e-05,
      "loss": 0.6452,
      "step": 800900
    },
    {
      "epoch": 12.574568288854003,
      "grad_norm": 4.204038143157959,
      "learning_rate": 4.2140894819466245e-05,
      "loss": 0.6126,
      "step": 801000
    },
    {
      "epoch": 12.576138147566718,
      "grad_norm": 3.632105827331543,
      "learning_rate": 4.21399136577708e-05,
      "loss": 0.6511,
      "step": 801100
    },
    {
      "epoch": 12.577708006279435,
      "grad_norm": 3.7525556087493896,
      "learning_rate": 4.2138932496075354e-05,
      "loss": 0.6021,
      "step": 801200
    },
    {
      "epoch": 12.57927786499215,
      "grad_norm": 4.165027141571045,
      "learning_rate": 4.213795133437991e-05,
      "loss": 0.6202,
      "step": 801300
    },
    {
      "epoch": 12.580847723704867,
      "grad_norm": 3.5065689086914062,
      "learning_rate": 4.2136970172684456e-05,
      "loss": 0.6388,
      "step": 801400
    },
    {
      "epoch": 12.582417582417582,
      "grad_norm": 5.129837512969971,
      "learning_rate": 4.2135989010989013e-05,
      "loss": 0.6226,
      "step": 801500
    },
    {
      "epoch": 12.583987441130299,
      "grad_norm": 3.567753553390503,
      "learning_rate": 4.2135007849293564e-05,
      "loss": 0.6388,
      "step": 801600
    },
    {
      "epoch": 12.585557299843014,
      "grad_norm": 3.6385200023651123,
      "learning_rate": 4.2134026687598115e-05,
      "loss": 0.6549,
      "step": 801700
    },
    {
      "epoch": 12.58712715855573,
      "grad_norm": 3.7500061988830566,
      "learning_rate": 4.213304552590267e-05,
      "loss": 0.6112,
      "step": 801800
    },
    {
      "epoch": 12.588697017268446,
      "grad_norm": 4.857316970825195,
      "learning_rate": 4.2132064364207224e-05,
      "loss": 0.587,
      "step": 801900
    },
    {
      "epoch": 12.590266875981161,
      "grad_norm": 4.190154075622559,
      "learning_rate": 4.213108320251178e-05,
      "loss": 0.644,
      "step": 802000
    },
    {
      "epoch": 12.591836734693878,
      "grad_norm": 4.140084743499756,
      "learning_rate": 4.2130102040816326e-05,
      "loss": 0.6321,
      "step": 802100
    },
    {
      "epoch": 12.593406593406593,
      "grad_norm": 3.8916454315185547,
      "learning_rate": 4.2129120879120884e-05,
      "loss": 0.662,
      "step": 802200
    },
    {
      "epoch": 12.59497645211931,
      "grad_norm": 2.0212879180908203,
      "learning_rate": 4.2128139717425435e-05,
      "loss": 0.6369,
      "step": 802300
    },
    {
      "epoch": 12.596546310832025,
      "grad_norm": 4.406124591827393,
      "learning_rate": 4.2127158555729986e-05,
      "loss": 0.626,
      "step": 802400
    },
    {
      "epoch": 12.598116169544742,
      "grad_norm": 3.051790475845337,
      "learning_rate": 4.212617739403454e-05,
      "loss": 0.6033,
      "step": 802500
    },
    {
      "epoch": 12.599686028257457,
      "grad_norm": 4.307604789733887,
      "learning_rate": 4.2125196232339095e-05,
      "loss": 0.6261,
      "step": 802600
    },
    {
      "epoch": 12.601255886970172,
      "grad_norm": 4.931082725524902,
      "learning_rate": 4.2124215070643646e-05,
      "loss": 0.6238,
      "step": 802700
    },
    {
      "epoch": 12.602825745682889,
      "grad_norm": 4.606569290161133,
      "learning_rate": 4.2123233908948197e-05,
      "loss": 0.6686,
      "step": 802800
    },
    {
      "epoch": 12.604395604395604,
      "grad_norm": 3.932652473449707,
      "learning_rate": 4.212225274725275e-05,
      "loss": 0.6104,
      "step": 802900
    },
    {
      "epoch": 12.605965463108321,
      "grad_norm": 4.209753513336182,
      "learning_rate": 4.2121271585557305e-05,
      "loss": 0.6717,
      "step": 803000
    },
    {
      "epoch": 12.607535321821036,
      "grad_norm": 3.4570765495300293,
      "learning_rate": 4.212029042386185e-05,
      "loss": 0.6724,
      "step": 803100
    },
    {
      "epoch": 12.609105180533753,
      "grad_norm": 3.858661651611328,
      "learning_rate": 4.211930926216641e-05,
      "loss": 0.6091,
      "step": 803200
    },
    {
      "epoch": 12.610675039246468,
      "grad_norm": 4.145271301269531,
      "learning_rate": 4.211832810047096e-05,
      "loss": 0.5844,
      "step": 803300
    },
    {
      "epoch": 12.612244897959183,
      "grad_norm": 3.513643264770508,
      "learning_rate": 4.2117346938775516e-05,
      "loss": 0.6254,
      "step": 803400
    },
    {
      "epoch": 12.6138147566719,
      "grad_norm": 4.8216729164123535,
      "learning_rate": 4.211636577708006e-05,
      "loss": 0.6105,
      "step": 803500
    },
    {
      "epoch": 12.615384615384615,
      "grad_norm": 2.5689098834991455,
      "learning_rate": 4.211538461538462e-05,
      "loss": 0.6166,
      "step": 803600
    },
    {
      "epoch": 12.616954474097332,
      "grad_norm": 3.880978584289551,
      "learning_rate": 4.211440345368917e-05,
      "loss": 0.6338,
      "step": 803700
    },
    {
      "epoch": 12.618524332810047,
      "grad_norm": 3.4605703353881836,
      "learning_rate": 4.211342229199372e-05,
      "loss": 0.6528,
      "step": 803800
    },
    {
      "epoch": 12.620094191522764,
      "grad_norm": 3.1942625045776367,
      "learning_rate": 4.211244113029828e-05,
      "loss": 0.6366,
      "step": 803900
    },
    {
      "epoch": 12.621664050235479,
      "grad_norm": 4.2551140785217285,
      "learning_rate": 4.211145996860283e-05,
      "loss": 0.6062,
      "step": 804000
    },
    {
      "epoch": 12.623233908948194,
      "grad_norm": 4.50564432144165,
      "learning_rate": 4.211047880690738e-05,
      "loss": 0.6323,
      "step": 804100
    },
    {
      "epoch": 12.62480376766091,
      "grad_norm": 4.441520690917969,
      "learning_rate": 4.210949764521193e-05,
      "loss": 0.5881,
      "step": 804200
    },
    {
      "epoch": 12.626373626373626,
      "grad_norm": 4.609157562255859,
      "learning_rate": 4.210851648351649e-05,
      "loss": 0.6456,
      "step": 804300
    },
    {
      "epoch": 12.627943485086343,
      "grad_norm": 3.7416164875030518,
      "learning_rate": 4.210753532182104e-05,
      "loss": 0.6378,
      "step": 804400
    },
    {
      "epoch": 12.629513343799058,
      "grad_norm": 4.024802207946777,
      "learning_rate": 4.210655416012559e-05,
      "loss": 0.6134,
      "step": 804500
    },
    {
      "epoch": 12.631083202511775,
      "grad_norm": 3.5615077018737793,
      "learning_rate": 4.210557299843014e-05,
      "loss": 0.6681,
      "step": 804600
    },
    {
      "epoch": 12.63265306122449,
      "grad_norm": 3.952497959136963,
      "learning_rate": 4.21045918367347e-05,
      "loss": 0.6249,
      "step": 804700
    },
    {
      "epoch": 12.634222919937205,
      "grad_norm": 2.8526415824890137,
      "learning_rate": 4.210361067503925e-05,
      "loss": 0.6254,
      "step": 804800
    },
    {
      "epoch": 12.635792778649922,
      "grad_norm": 3.9145891666412354,
      "learning_rate": 4.21026295133438e-05,
      "loss": 0.6315,
      "step": 804900
    },
    {
      "epoch": 12.637362637362637,
      "grad_norm": 4.03404426574707,
      "learning_rate": 4.210164835164835e-05,
      "loss": 0.6322,
      "step": 805000
    },
    {
      "epoch": 12.638932496075354,
      "grad_norm": 2.9405343532562256,
      "learning_rate": 4.210066718995291e-05,
      "loss": 0.6263,
      "step": 805100
    },
    {
      "epoch": 12.640502354788069,
      "grad_norm": 3.1553571224212646,
      "learning_rate": 4.2099686028257454e-05,
      "loss": 0.5934,
      "step": 805200
    },
    {
      "epoch": 12.642072213500786,
      "grad_norm": 5.158633708953857,
      "learning_rate": 4.209870486656201e-05,
      "loss": 0.6536,
      "step": 805300
    },
    {
      "epoch": 12.6436420722135,
      "grad_norm": 4.424692630767822,
      "learning_rate": 4.209772370486656e-05,
      "loss": 0.6392,
      "step": 805400
    },
    {
      "epoch": 12.645211930926216,
      "grad_norm": 3.2669458389282227,
      "learning_rate": 4.209674254317112e-05,
      "loss": 0.6343,
      "step": 805500
    },
    {
      "epoch": 12.646781789638933,
      "grad_norm": 3.141500234603882,
      "learning_rate": 4.2095761381475665e-05,
      "loss": 0.609,
      "step": 805600
    },
    {
      "epoch": 12.648351648351648,
      "grad_norm": 3.3652894496917725,
      "learning_rate": 4.209478021978022e-05,
      "loss": 0.6281,
      "step": 805700
    },
    {
      "epoch": 12.649921507064365,
      "grad_norm": 4.177090167999268,
      "learning_rate": 4.2093799058084773e-05,
      "loss": 0.607,
      "step": 805800
    },
    {
      "epoch": 12.65149136577708,
      "grad_norm": 3.6237785816192627,
      "learning_rate": 4.2092817896389324e-05,
      "loss": 0.6383,
      "step": 805900
    },
    {
      "epoch": 12.653061224489797,
      "grad_norm": 4.156545639038086,
      "learning_rate": 4.209183673469388e-05,
      "loss": 0.6608,
      "step": 806000
    },
    {
      "epoch": 12.654631083202512,
      "grad_norm": 4.273565292358398,
      "learning_rate": 4.209085557299843e-05,
      "loss": 0.6062,
      "step": 806100
    },
    {
      "epoch": 12.656200941915227,
      "grad_norm": 4.384171962738037,
      "learning_rate": 4.2089874411302984e-05,
      "loss": 0.6592,
      "step": 806200
    },
    {
      "epoch": 12.657770800627944,
      "grad_norm": 2.9677343368530273,
      "learning_rate": 4.2088893249607535e-05,
      "loss": 0.6398,
      "step": 806300
    },
    {
      "epoch": 12.659340659340659,
      "grad_norm": 3.8399853706359863,
      "learning_rate": 4.208791208791209e-05,
      "loss": 0.6521,
      "step": 806400
    },
    {
      "epoch": 12.660910518053376,
      "grad_norm": 2.9518556594848633,
      "learning_rate": 4.2086930926216644e-05,
      "loss": 0.6432,
      "step": 806500
    },
    {
      "epoch": 12.66248037676609,
      "grad_norm": 3.6542913913726807,
      "learning_rate": 4.2085949764521195e-05,
      "loss": 0.6354,
      "step": 806600
    },
    {
      "epoch": 12.664050235478808,
      "grad_norm": 3.5150346755981445,
      "learning_rate": 4.2084968602825746e-05,
      "loss": 0.5826,
      "step": 806700
    },
    {
      "epoch": 12.665620094191523,
      "grad_norm": 2.788644313812256,
      "learning_rate": 4.2083987441130304e-05,
      "loss": 0.6243,
      "step": 806800
    },
    {
      "epoch": 12.667189952904238,
      "grad_norm": 4.290541172027588,
      "learning_rate": 4.2083006279434855e-05,
      "loss": 0.6541,
      "step": 806900
    },
    {
      "epoch": 12.668759811616955,
      "grad_norm": 3.3078765869140625,
      "learning_rate": 4.2082025117739406e-05,
      "loss": 0.6538,
      "step": 807000
    },
    {
      "epoch": 12.67032967032967,
      "grad_norm": 4.548982620239258,
      "learning_rate": 4.2081043956043957e-05,
      "loss": 0.591,
      "step": 807100
    },
    {
      "epoch": 12.671899529042387,
      "grad_norm": 2.907996416091919,
      "learning_rate": 4.2080062794348514e-05,
      "loss": 0.5943,
      "step": 807200
    },
    {
      "epoch": 12.673469387755102,
      "grad_norm": 3.5510735511779785,
      "learning_rate": 4.207908163265306e-05,
      "loss": 0.6249,
      "step": 807300
    },
    {
      "epoch": 12.675039246467819,
      "grad_norm": 3.989591121673584,
      "learning_rate": 4.2078100470957616e-05,
      "loss": 0.6746,
      "step": 807400
    },
    {
      "epoch": 12.676609105180534,
      "grad_norm": 3.5780723094940186,
      "learning_rate": 4.207711930926217e-05,
      "loss": 0.6432,
      "step": 807500
    },
    {
      "epoch": 12.678178963893249,
      "grad_norm": 3.5973854064941406,
      "learning_rate": 4.2076138147566725e-05,
      "loss": 0.6064,
      "step": 807600
    },
    {
      "epoch": 12.679748822605966,
      "grad_norm": 3.427947759628296,
      "learning_rate": 4.207515698587127e-05,
      "loss": 0.6033,
      "step": 807700
    },
    {
      "epoch": 12.68131868131868,
      "grad_norm": 3.0483908653259277,
      "learning_rate": 4.207417582417583e-05,
      "loss": 0.6409,
      "step": 807800
    },
    {
      "epoch": 12.682888540031398,
      "grad_norm": 3.4353840351104736,
      "learning_rate": 4.207319466248038e-05,
      "loss": 0.6466,
      "step": 807900
    },
    {
      "epoch": 12.684458398744113,
      "grad_norm": 3.2462096214294434,
      "learning_rate": 4.207221350078493e-05,
      "loss": 0.6096,
      "step": 808000
    },
    {
      "epoch": 12.68602825745683,
      "grad_norm": 4.097718238830566,
      "learning_rate": 4.207123233908949e-05,
      "loss": 0.6028,
      "step": 808100
    },
    {
      "epoch": 12.687598116169545,
      "grad_norm": 4.241965293884277,
      "learning_rate": 4.207025117739404e-05,
      "loss": 0.6284,
      "step": 808200
    },
    {
      "epoch": 12.68916797488226,
      "grad_norm": 3.52114200592041,
      "learning_rate": 4.206927001569859e-05,
      "loss": 0.6035,
      "step": 808300
    },
    {
      "epoch": 12.690737833594977,
      "grad_norm": 3.0490336418151855,
      "learning_rate": 4.206828885400314e-05,
      "loss": 0.637,
      "step": 808400
    },
    {
      "epoch": 12.692307692307692,
      "grad_norm": 4.373931884765625,
      "learning_rate": 4.20673076923077e-05,
      "loss": 0.6144,
      "step": 808500
    },
    {
      "epoch": 12.693877551020408,
      "grad_norm": 3.978637933731079,
      "learning_rate": 4.206632653061225e-05,
      "loss": 0.6383,
      "step": 808600
    },
    {
      "epoch": 12.695447409733124,
      "grad_norm": 3.946857452392578,
      "learning_rate": 4.20653453689168e-05,
      "loss": 0.5799,
      "step": 808700
    },
    {
      "epoch": 12.69701726844584,
      "grad_norm": 2.856184720993042,
      "learning_rate": 4.206436420722135e-05,
      "loss": 0.6737,
      "step": 808800
    },
    {
      "epoch": 12.698587127158556,
      "grad_norm": 3.3689143657684326,
      "learning_rate": 4.206338304552591e-05,
      "loss": 0.6275,
      "step": 808900
    },
    {
      "epoch": 12.700156985871272,
      "grad_norm": 4.507729530334473,
      "learning_rate": 4.206240188383045e-05,
      "loss": 0.6397,
      "step": 809000
    },
    {
      "epoch": 12.701726844583987,
      "grad_norm": 4.1284284591674805,
      "learning_rate": 4.206142072213501e-05,
      "loss": 0.626,
      "step": 809100
    },
    {
      "epoch": 12.703296703296703,
      "grad_norm": 3.3464126586914062,
      "learning_rate": 4.206043956043956e-05,
      "loss": 0.5994,
      "step": 809200
    },
    {
      "epoch": 12.70486656200942,
      "grad_norm": 3.8896491527557373,
      "learning_rate": 4.205945839874412e-05,
      "loss": 0.6351,
      "step": 809300
    },
    {
      "epoch": 12.706436420722135,
      "grad_norm": 3.722440242767334,
      "learning_rate": 4.205847723704866e-05,
      "loss": 0.6395,
      "step": 809400
    },
    {
      "epoch": 12.708006279434851,
      "grad_norm": 3.9099771976470947,
      "learning_rate": 4.205749607535322e-05,
      "loss": 0.6643,
      "step": 809500
    },
    {
      "epoch": 12.709576138147566,
      "grad_norm": 4.266352653503418,
      "learning_rate": 4.205651491365777e-05,
      "loss": 0.6413,
      "step": 809600
    },
    {
      "epoch": 12.711145996860283,
      "grad_norm": 4.706737041473389,
      "learning_rate": 4.205553375196232e-05,
      "loss": 0.6242,
      "step": 809700
    },
    {
      "epoch": 12.712715855572998,
      "grad_norm": 3.419877290725708,
      "learning_rate": 4.2054552590266874e-05,
      "loss": 0.6069,
      "step": 809800
    },
    {
      "epoch": 12.714285714285714,
      "grad_norm": 3.878570079803467,
      "learning_rate": 4.205357142857143e-05,
      "loss": 0.6157,
      "step": 809900
    },
    {
      "epoch": 12.71585557299843,
      "grad_norm": 4.520983695983887,
      "learning_rate": 4.205259026687598e-05,
      "loss": 0.5818,
      "step": 810000
    },
    {
      "epoch": 12.717425431711145,
      "grad_norm": 4.814502239227295,
      "learning_rate": 4.205160910518053e-05,
      "loss": 0.6208,
      "step": 810100
    },
    {
      "epoch": 12.718995290423862,
      "grad_norm": 4.70145845413208,
      "learning_rate": 4.205062794348509e-05,
      "loss": 0.6434,
      "step": 810200
    },
    {
      "epoch": 12.720565149136577,
      "grad_norm": 3.2143681049346924,
      "learning_rate": 4.204964678178964e-05,
      "loss": 0.6144,
      "step": 810300
    },
    {
      "epoch": 12.722135007849294,
      "grad_norm": 4.176303386688232,
      "learning_rate": 4.204866562009419e-05,
      "loss": 0.57,
      "step": 810400
    },
    {
      "epoch": 12.72370486656201,
      "grad_norm": 4.483506202697754,
      "learning_rate": 4.2047684458398744e-05,
      "loss": 0.6218,
      "step": 810500
    },
    {
      "epoch": 12.725274725274724,
      "grad_norm": 3.780158281326294,
      "learning_rate": 4.20467032967033e-05,
      "loss": 0.6174,
      "step": 810600
    },
    {
      "epoch": 12.726844583987441,
      "grad_norm": 4.527712345123291,
      "learning_rate": 4.204572213500785e-05,
      "loss": 0.6522,
      "step": 810700
    },
    {
      "epoch": 12.728414442700156,
      "grad_norm": 4.414273738861084,
      "learning_rate": 4.2044740973312404e-05,
      "loss": 0.6503,
      "step": 810800
    },
    {
      "epoch": 12.729984301412873,
      "grad_norm": 2.8385605812072754,
      "learning_rate": 4.2043759811616955e-05,
      "loss": 0.6273,
      "step": 810900
    },
    {
      "epoch": 12.731554160125588,
      "grad_norm": 3.1700072288513184,
      "learning_rate": 4.204277864992151e-05,
      "loss": 0.6096,
      "step": 811000
    },
    {
      "epoch": 12.733124018838305,
      "grad_norm": 3.9762954711914062,
      "learning_rate": 4.204179748822606e-05,
      "loss": 0.665,
      "step": 811100
    },
    {
      "epoch": 12.73469387755102,
      "grad_norm": 4.49015474319458,
      "learning_rate": 4.2040816326530615e-05,
      "loss": 0.5951,
      "step": 811200
    },
    {
      "epoch": 12.736263736263737,
      "grad_norm": 4.168436527252197,
      "learning_rate": 4.2039835164835165e-05,
      "loss": 0.6276,
      "step": 811300
    },
    {
      "epoch": 12.737833594976452,
      "grad_norm": 4.0038676261901855,
      "learning_rate": 4.203885400313972e-05,
      "loss": 0.6309,
      "step": 811400
    },
    {
      "epoch": 12.739403453689167,
      "grad_norm": 4.635427951812744,
      "learning_rate": 4.203787284144427e-05,
      "loss": 0.6331,
      "step": 811500
    },
    {
      "epoch": 12.740973312401884,
      "grad_norm": 4.469442367553711,
      "learning_rate": 4.2036891679748825e-05,
      "loss": 0.6322,
      "step": 811600
    },
    {
      "epoch": 12.7425431711146,
      "grad_norm": 3.1430373191833496,
      "learning_rate": 4.2035910518053376e-05,
      "loss": 0.6298,
      "step": 811700
    },
    {
      "epoch": 12.744113029827316,
      "grad_norm": 3.4249916076660156,
      "learning_rate": 4.203492935635793e-05,
      "loss": 0.6303,
      "step": 811800
    },
    {
      "epoch": 12.745682888540031,
      "grad_norm": 4.65922737121582,
      "learning_rate": 4.203394819466248e-05,
      "loss": 0.6439,
      "step": 811900
    },
    {
      "epoch": 12.747252747252748,
      "grad_norm": 3.417222499847412,
      "learning_rate": 4.2032967032967036e-05,
      "loss": 0.6626,
      "step": 812000
    },
    {
      "epoch": 12.748822605965463,
      "grad_norm": 6.6956467628479,
      "learning_rate": 4.203198587127159e-05,
      "loss": 0.6141,
      "step": 812100
    },
    {
      "epoch": 12.750392464678178,
      "grad_norm": 2.9738969802856445,
      "learning_rate": 4.203100470957614e-05,
      "loss": 0.6091,
      "step": 812200
    },
    {
      "epoch": 12.751962323390895,
      "grad_norm": 4.8910298347473145,
      "learning_rate": 4.2030023547880696e-05,
      "loss": 0.6182,
      "step": 812300
    },
    {
      "epoch": 12.75353218210361,
      "grad_norm": 3.3699934482574463,
      "learning_rate": 4.202904238618525e-05,
      "loss": 0.6368,
      "step": 812400
    },
    {
      "epoch": 12.755102040816327,
      "grad_norm": 3.9286415576934814,
      "learning_rate": 4.20280612244898e-05,
      "loss": 0.609,
      "step": 812500
    },
    {
      "epoch": 12.756671899529042,
      "grad_norm": 3.222731828689575,
      "learning_rate": 4.202708006279435e-05,
      "loss": 0.6246,
      "step": 812600
    },
    {
      "epoch": 12.758241758241759,
      "grad_norm": 2.391284942626953,
      "learning_rate": 4.2026098901098906e-05,
      "loss": 0.6047,
      "step": 812700
    },
    {
      "epoch": 12.759811616954474,
      "grad_norm": 3.4859073162078857,
      "learning_rate": 4.202511773940346e-05,
      "loss": 0.6794,
      "step": 812800
    },
    {
      "epoch": 12.76138147566719,
      "grad_norm": 2.3026063442230225,
      "learning_rate": 4.202413657770801e-05,
      "loss": 0.6329,
      "step": 812900
    },
    {
      "epoch": 12.762951334379906,
      "grad_norm": 4.248362064361572,
      "learning_rate": 4.202315541601256e-05,
      "loss": 0.6299,
      "step": 813000
    },
    {
      "epoch": 12.764521193092621,
      "grad_norm": 4.050731658935547,
      "learning_rate": 4.202217425431712e-05,
      "loss": 0.6278,
      "step": 813100
    },
    {
      "epoch": 12.766091051805338,
      "grad_norm": 4.53990364074707,
      "learning_rate": 4.202119309262166e-05,
      "loss": 0.6242,
      "step": 813200
    },
    {
      "epoch": 12.767660910518053,
      "grad_norm": 3.4455947875976562,
      "learning_rate": 4.202021193092622e-05,
      "loss": 0.6168,
      "step": 813300
    },
    {
      "epoch": 12.76923076923077,
      "grad_norm": 3.4419360160827637,
      "learning_rate": 4.201923076923077e-05,
      "loss": 0.5955,
      "step": 813400
    },
    {
      "epoch": 12.770800627943485,
      "grad_norm": 3.5355312824249268,
      "learning_rate": 4.201824960753533e-05,
      "loss": 0.6366,
      "step": 813500
    },
    {
      "epoch": 12.7723704866562,
      "grad_norm": 3.143467664718628,
      "learning_rate": 4.201726844583987e-05,
      "loss": 0.6092,
      "step": 813600
    },
    {
      "epoch": 12.773940345368917,
      "grad_norm": 5.447610378265381,
      "learning_rate": 4.201628728414443e-05,
      "loss": 0.6052,
      "step": 813700
    },
    {
      "epoch": 12.775510204081632,
      "grad_norm": 3.8439505100250244,
      "learning_rate": 4.201530612244898e-05,
      "loss": 0.632,
      "step": 813800
    },
    {
      "epoch": 12.777080062794349,
      "grad_norm": 2.741603374481201,
      "learning_rate": 4.201432496075353e-05,
      "loss": 0.6495,
      "step": 813900
    },
    {
      "epoch": 12.778649921507064,
      "grad_norm": 3.1008191108703613,
      "learning_rate": 4.201334379905808e-05,
      "loss": 0.6701,
      "step": 814000
    },
    {
      "epoch": 12.780219780219781,
      "grad_norm": 2.7902519702911377,
      "learning_rate": 4.201236263736264e-05,
      "loss": 0.6129,
      "step": 814100
    },
    {
      "epoch": 12.781789638932496,
      "grad_norm": 3.528620958328247,
      "learning_rate": 4.201138147566719e-05,
      "loss": 0.6386,
      "step": 814200
    },
    {
      "epoch": 12.783359497645211,
      "grad_norm": 4.622072219848633,
      "learning_rate": 4.201040031397174e-05,
      "loss": 0.6509,
      "step": 814300
    },
    {
      "epoch": 12.784929356357928,
      "grad_norm": 3.5619261264801025,
      "learning_rate": 4.20094191522763e-05,
      "loss": 0.6624,
      "step": 814400
    },
    {
      "epoch": 12.786499215070643,
      "grad_norm": 4.183384418487549,
      "learning_rate": 4.200843799058085e-05,
      "loss": 0.6184,
      "step": 814500
    },
    {
      "epoch": 12.78806907378336,
      "grad_norm": 3.583117961883545,
      "learning_rate": 4.20074568288854e-05,
      "loss": 0.6652,
      "step": 814600
    },
    {
      "epoch": 12.789638932496075,
      "grad_norm": 3.8985235691070557,
      "learning_rate": 4.200647566718995e-05,
      "loss": 0.6372,
      "step": 814700
    },
    {
      "epoch": 12.791208791208792,
      "grad_norm": 4.882711410522461,
      "learning_rate": 4.200549450549451e-05,
      "loss": 0.6592,
      "step": 814800
    },
    {
      "epoch": 12.792778649921507,
      "grad_norm": 3.9294607639312744,
      "learning_rate": 4.200451334379906e-05,
      "loss": 0.6395,
      "step": 814900
    },
    {
      "epoch": 12.794348508634222,
      "grad_norm": 3.6148650646209717,
      "learning_rate": 4.200353218210361e-05,
      "loss": 0.5921,
      "step": 815000
    },
    {
      "epoch": 12.795918367346939,
      "grad_norm": 4.150767803192139,
      "learning_rate": 4.2002551020408164e-05,
      "loss": 0.6344,
      "step": 815100
    },
    {
      "epoch": 12.797488226059654,
      "grad_norm": 4.516966819763184,
      "learning_rate": 4.200156985871272e-05,
      "loss": 0.64,
      "step": 815200
    },
    {
      "epoch": 12.799058084772371,
      "grad_norm": 3.6303770542144775,
      "learning_rate": 4.2000588697017266e-05,
      "loss": 0.6213,
      "step": 815300
    },
    {
      "epoch": 12.800627943485086,
      "grad_norm": 4.300530910491943,
      "learning_rate": 4.1999607535321823e-05,
      "loss": 0.5821,
      "step": 815400
    },
    {
      "epoch": 12.802197802197803,
      "grad_norm": 3.234604597091675,
      "learning_rate": 4.1998626373626374e-05,
      "loss": 0.614,
      "step": 815500
    },
    {
      "epoch": 12.803767660910518,
      "grad_norm": 4.237515449523926,
      "learning_rate": 4.199764521193093e-05,
      "loss": 0.6205,
      "step": 815600
    },
    {
      "epoch": 12.805337519623233,
      "grad_norm": 4.870228290557861,
      "learning_rate": 4.1996664050235476e-05,
      "loss": 0.6284,
      "step": 815700
    },
    {
      "epoch": 12.80690737833595,
      "grad_norm": 2.1063647270202637,
      "learning_rate": 4.1995682888540034e-05,
      "loss": 0.615,
      "step": 815800
    },
    {
      "epoch": 12.808477237048665,
      "grad_norm": 3.086172580718994,
      "learning_rate": 4.1994701726844585e-05,
      "loss": 0.583,
      "step": 815900
    },
    {
      "epoch": 12.810047095761382,
      "grad_norm": 4.283267021179199,
      "learning_rate": 4.1993720565149136e-05,
      "loss": 0.6123,
      "step": 816000
    },
    {
      "epoch": 12.811616954474097,
      "grad_norm": 3.6186442375183105,
      "learning_rate": 4.199273940345369e-05,
      "loss": 0.5898,
      "step": 816100
    },
    {
      "epoch": 12.813186813186814,
      "grad_norm": 3.7133569717407227,
      "learning_rate": 4.1991758241758245e-05,
      "loss": 0.6229,
      "step": 816200
    },
    {
      "epoch": 12.814756671899529,
      "grad_norm": 4.085203170776367,
      "learning_rate": 4.1990777080062796e-05,
      "loss": 0.6268,
      "step": 816300
    },
    {
      "epoch": 12.816326530612244,
      "grad_norm": 2.830994129180908,
      "learning_rate": 4.198979591836735e-05,
      "loss": 0.6559,
      "step": 816400
    },
    {
      "epoch": 12.817896389324961,
      "grad_norm": 3.280031442642212,
      "learning_rate": 4.1988814756671905e-05,
      "loss": 0.6215,
      "step": 816500
    },
    {
      "epoch": 12.819466248037676,
      "grad_norm": 4.829344272613525,
      "learning_rate": 4.1987833594976456e-05,
      "loss": 0.598,
      "step": 816600
    },
    {
      "epoch": 12.821036106750393,
      "grad_norm": 3.031695604324341,
      "learning_rate": 4.1986852433281007e-05,
      "loss": 0.63,
      "step": 816700
    },
    {
      "epoch": 12.822605965463108,
      "grad_norm": 4.139209270477295,
      "learning_rate": 4.198587127158556e-05,
      "loss": 0.6461,
      "step": 816800
    },
    {
      "epoch": 12.824175824175825,
      "grad_norm": 4.3533759117126465,
      "learning_rate": 4.1984890109890115e-05,
      "loss": 0.6097,
      "step": 816900
    },
    {
      "epoch": 12.82574568288854,
      "grad_norm": 4.339237213134766,
      "learning_rate": 4.1983908948194666e-05,
      "loss": 0.5959,
      "step": 817000
    },
    {
      "epoch": 12.827315541601255,
      "grad_norm": 4.322261810302734,
      "learning_rate": 4.198292778649922e-05,
      "loss": 0.6223,
      "step": 817100
    },
    {
      "epoch": 12.828885400313972,
      "grad_norm": 3.4943816661834717,
      "learning_rate": 4.198194662480377e-05,
      "loss": 0.5991,
      "step": 817200
    },
    {
      "epoch": 12.830455259026687,
      "grad_norm": 4.19896125793457,
      "learning_rate": 4.1980965463108326e-05,
      "loss": 0.5847,
      "step": 817300
    },
    {
      "epoch": 12.832025117739404,
      "grad_norm": 3.1270952224731445,
      "learning_rate": 4.197998430141287e-05,
      "loss": 0.6444,
      "step": 817400
    },
    {
      "epoch": 12.833594976452119,
      "grad_norm": 3.349215507507324,
      "learning_rate": 4.197900313971743e-05,
      "loss": 0.5903,
      "step": 817500
    },
    {
      "epoch": 12.835164835164836,
      "grad_norm": 3.4041635990142822,
      "learning_rate": 4.197802197802198e-05,
      "loss": 0.6078,
      "step": 817600
    },
    {
      "epoch": 12.83673469387755,
      "grad_norm": 5.241985321044922,
      "learning_rate": 4.197704081632654e-05,
      "loss": 0.6391,
      "step": 817700
    },
    {
      "epoch": 12.838304552590268,
      "grad_norm": 3.241238832473755,
      "learning_rate": 4.197605965463108e-05,
      "loss": 0.6187,
      "step": 817800
    },
    {
      "epoch": 12.839874411302983,
      "grad_norm": 3.6752209663391113,
      "learning_rate": 4.197507849293564e-05,
      "loss": 0.5992,
      "step": 817900
    },
    {
      "epoch": 12.841444270015698,
      "grad_norm": 4.769993782043457,
      "learning_rate": 4.197409733124019e-05,
      "loss": 0.6387,
      "step": 818000
    },
    {
      "epoch": 12.843014128728415,
      "grad_norm": 4.379456043243408,
      "learning_rate": 4.197311616954474e-05,
      "loss": 0.6528,
      "step": 818100
    },
    {
      "epoch": 12.84458398744113,
      "grad_norm": 3.746685743331909,
      "learning_rate": 4.197213500784929e-05,
      "loss": 0.6316,
      "step": 818200
    },
    {
      "epoch": 12.846153846153847,
      "grad_norm": 3.100087881088257,
      "learning_rate": 4.197115384615385e-05,
      "loss": 0.6098,
      "step": 818300
    },
    {
      "epoch": 12.847723704866562,
      "grad_norm": 3.3968262672424316,
      "learning_rate": 4.19701726844584e-05,
      "loss": 0.6168,
      "step": 818400
    },
    {
      "epoch": 12.849293563579279,
      "grad_norm": 3.576215982437134,
      "learning_rate": 4.196919152276295e-05,
      "loss": 0.6391,
      "step": 818500
    },
    {
      "epoch": 12.850863422291994,
      "grad_norm": 3.285252094268799,
      "learning_rate": 4.196821036106751e-05,
      "loss": 0.6305,
      "step": 818600
    },
    {
      "epoch": 12.852433281004709,
      "grad_norm": 3.673387289047241,
      "learning_rate": 4.196722919937206e-05,
      "loss": 0.619,
      "step": 818700
    },
    {
      "epoch": 12.854003139717426,
      "grad_norm": 3.2900731563568115,
      "learning_rate": 4.196624803767661e-05,
      "loss": 0.616,
      "step": 818800
    },
    {
      "epoch": 12.85557299843014,
      "grad_norm": 3.747626543045044,
      "learning_rate": 4.196526687598116e-05,
      "loss": 0.6053,
      "step": 818900
    },
    {
      "epoch": 12.857142857142858,
      "grad_norm": 4.2514495849609375,
      "learning_rate": 4.196428571428572e-05,
      "loss": 0.6129,
      "step": 819000
    },
    {
      "epoch": 12.858712715855573,
      "grad_norm": 3.8034541606903076,
      "learning_rate": 4.196330455259027e-05,
      "loss": 0.6653,
      "step": 819100
    },
    {
      "epoch": 12.86028257456829,
      "grad_norm": 4.0397562980651855,
      "learning_rate": 4.196232339089482e-05,
      "loss": 0.6314,
      "step": 819200
    },
    {
      "epoch": 12.861852433281005,
      "grad_norm": 4.854918956756592,
      "learning_rate": 4.196134222919937e-05,
      "loss": 0.5967,
      "step": 819300
    },
    {
      "epoch": 12.86342229199372,
      "grad_norm": 4.081678867340088,
      "learning_rate": 4.196036106750393e-05,
      "loss": 0.6135,
      "step": 819400
    },
    {
      "epoch": 12.864992150706437,
      "grad_norm": 4.08319091796875,
      "learning_rate": 4.1959379905808475e-05,
      "loss": 0.5949,
      "step": 819500
    },
    {
      "epoch": 12.866562009419152,
      "grad_norm": 3.5806562900543213,
      "learning_rate": 4.195839874411303e-05,
      "loss": 0.63,
      "step": 819600
    },
    {
      "epoch": 12.868131868131869,
      "grad_norm": 3.3579580783843994,
      "learning_rate": 4.1957417582417583e-05,
      "loss": 0.6174,
      "step": 819700
    },
    {
      "epoch": 12.869701726844584,
      "grad_norm": 5.112721920013428,
      "learning_rate": 4.195643642072214e-05,
      "loss": 0.6077,
      "step": 819800
    },
    {
      "epoch": 12.8712715855573,
      "grad_norm": 2.9723474979400635,
      "learning_rate": 4.1955455259026685e-05,
      "loss": 0.6505,
      "step": 819900
    },
    {
      "epoch": 12.872841444270016,
      "grad_norm": 4.087774753570557,
      "learning_rate": 4.195447409733124e-05,
      "loss": 0.6244,
      "step": 820000
    },
    {
      "epoch": 12.87441130298273,
      "grad_norm": 3.9977004528045654,
      "learning_rate": 4.1953492935635794e-05,
      "loss": 0.6253,
      "step": 820100
    },
    {
      "epoch": 12.875981161695448,
      "grad_norm": 5.006411552429199,
      "learning_rate": 4.1952511773940345e-05,
      "loss": 0.6436,
      "step": 820200
    },
    {
      "epoch": 12.877551020408163,
      "grad_norm": 3.992384195327759,
      "learning_rate": 4.1951530612244896e-05,
      "loss": 0.6401,
      "step": 820300
    },
    {
      "epoch": 12.87912087912088,
      "grad_norm": 4.060144424438477,
      "learning_rate": 4.1950549450549454e-05,
      "loss": 0.6349,
      "step": 820400
    },
    {
      "epoch": 12.880690737833595,
      "grad_norm": 3.500033140182495,
      "learning_rate": 4.1949568288854005e-05,
      "loss": 0.6229,
      "step": 820500
    },
    {
      "epoch": 12.882260596546312,
      "grad_norm": 3.845512628555298,
      "learning_rate": 4.1948587127158556e-05,
      "loss": 0.6605,
      "step": 820600
    },
    {
      "epoch": 12.883830455259027,
      "grad_norm": 3.973881959915161,
      "learning_rate": 4.1947605965463114e-05,
      "loss": 0.6131,
      "step": 820700
    },
    {
      "epoch": 12.885400313971743,
      "grad_norm": 3.855565071105957,
      "learning_rate": 4.1946624803767665e-05,
      "loss": 0.623,
      "step": 820800
    },
    {
      "epoch": 12.886970172684459,
      "grad_norm": 4.325288772583008,
      "learning_rate": 4.1945643642072216e-05,
      "loss": 0.5688,
      "step": 820900
    },
    {
      "epoch": 12.888540031397174,
      "grad_norm": 4.064356327056885,
      "learning_rate": 4.1944662480376767e-05,
      "loss": 0.5848,
      "step": 821000
    },
    {
      "epoch": 12.89010989010989,
      "grad_norm": 4.135031700134277,
      "learning_rate": 4.1943681318681324e-05,
      "loss": 0.6252,
      "step": 821100
    },
    {
      "epoch": 12.891679748822606,
      "grad_norm": 4.22635555267334,
      "learning_rate": 4.1942700156985875e-05,
      "loss": 0.6602,
      "step": 821200
    },
    {
      "epoch": 12.893249607535322,
      "grad_norm": 3.4593729972839355,
      "learning_rate": 4.1941718995290426e-05,
      "loss": 0.652,
      "step": 821300
    },
    {
      "epoch": 12.894819466248038,
      "grad_norm": 4.270310401916504,
      "learning_rate": 4.194073783359498e-05,
      "loss": 0.6103,
      "step": 821400
    },
    {
      "epoch": 12.896389324960754,
      "grad_norm": 3.381503105163574,
      "learning_rate": 4.1939756671899535e-05,
      "loss": 0.6435,
      "step": 821500
    },
    {
      "epoch": 12.89795918367347,
      "grad_norm": 3.8139102458953857,
      "learning_rate": 4.193877551020408e-05,
      "loss": 0.6243,
      "step": 821600
    },
    {
      "epoch": 12.899529042386185,
      "grad_norm": 4.012135028839111,
      "learning_rate": 4.193779434850864e-05,
      "loss": 0.6462,
      "step": 821700
    },
    {
      "epoch": 12.901098901098901,
      "grad_norm": 3.9411532878875732,
      "learning_rate": 4.193681318681319e-05,
      "loss": 0.6291,
      "step": 821800
    },
    {
      "epoch": 12.902668759811617,
      "grad_norm": 4.999231338500977,
      "learning_rate": 4.1935832025117746e-05,
      "loss": 0.6187,
      "step": 821900
    },
    {
      "epoch": 12.904238618524333,
      "grad_norm": 3.87479305267334,
      "learning_rate": 4.193485086342229e-05,
      "loss": 0.606,
      "step": 822000
    },
    {
      "epoch": 12.905808477237048,
      "grad_norm": 3.1963448524475098,
      "learning_rate": 4.193386970172685e-05,
      "loss": 0.6179,
      "step": 822100
    },
    {
      "epoch": 12.907378335949765,
      "grad_norm": 2.8861539363861084,
      "learning_rate": 4.19328885400314e-05,
      "loss": 0.6518,
      "step": 822200
    },
    {
      "epoch": 12.90894819466248,
      "grad_norm": 3.9097893238067627,
      "learning_rate": 4.193190737833595e-05,
      "loss": 0.6619,
      "step": 822300
    },
    {
      "epoch": 12.910518053375196,
      "grad_norm": 4.246618270874023,
      "learning_rate": 4.19309262166405e-05,
      "loss": 0.6183,
      "step": 822400
    },
    {
      "epoch": 12.912087912087912,
      "grad_norm": 4.7550554275512695,
      "learning_rate": 4.192994505494506e-05,
      "loss": 0.6544,
      "step": 822500
    },
    {
      "epoch": 12.913657770800627,
      "grad_norm": 3.790661096572876,
      "learning_rate": 4.192896389324961e-05,
      "loss": 0.6351,
      "step": 822600
    },
    {
      "epoch": 12.915227629513344,
      "grad_norm": 3.520068883895874,
      "learning_rate": 4.192798273155416e-05,
      "loss": 0.6076,
      "step": 822700
    },
    {
      "epoch": 12.91679748822606,
      "grad_norm": 4.394237041473389,
      "learning_rate": 4.192700156985872e-05,
      "loss": 0.6275,
      "step": 822800
    },
    {
      "epoch": 12.918367346938776,
      "grad_norm": 4.185754776000977,
      "learning_rate": 4.192602040816327e-05,
      "loss": 0.6496,
      "step": 822900
    },
    {
      "epoch": 12.919937205651491,
      "grad_norm": 3.3204762935638428,
      "learning_rate": 4.192503924646782e-05,
      "loss": 0.6232,
      "step": 823000
    },
    {
      "epoch": 12.921507064364206,
      "grad_norm": 3.943835973739624,
      "learning_rate": 4.192405808477237e-05,
      "loss": 0.624,
      "step": 823100
    },
    {
      "epoch": 12.923076923076923,
      "grad_norm": 3.951336145401001,
      "learning_rate": 4.192307692307693e-05,
      "loss": 0.609,
      "step": 823200
    },
    {
      "epoch": 12.924646781789638,
      "grad_norm": 3.697082757949829,
      "learning_rate": 4.192209576138148e-05,
      "loss": 0.6495,
      "step": 823300
    },
    {
      "epoch": 12.926216640502355,
      "grad_norm": 3.977022886276245,
      "learning_rate": 4.192111459968603e-05,
      "loss": 0.62,
      "step": 823400
    },
    {
      "epoch": 12.92778649921507,
      "grad_norm": 3.3971359729766846,
      "learning_rate": 4.192013343799058e-05,
      "loss": 0.6339,
      "step": 823500
    },
    {
      "epoch": 12.929356357927787,
      "grad_norm": 2.7982099056243896,
      "learning_rate": 4.191915227629514e-05,
      "loss": 0.6012,
      "step": 823600
    },
    {
      "epoch": 12.930926216640502,
      "grad_norm": 3.890146017074585,
      "learning_rate": 4.1918171114599684e-05,
      "loss": 0.6419,
      "step": 823700
    },
    {
      "epoch": 12.932496075353217,
      "grad_norm": 3.96402645111084,
      "learning_rate": 4.191718995290424e-05,
      "loss": 0.6168,
      "step": 823800
    },
    {
      "epoch": 12.934065934065934,
      "grad_norm": 3.42765212059021,
      "learning_rate": 4.191620879120879e-05,
      "loss": 0.6181,
      "step": 823900
    },
    {
      "epoch": 12.93563579277865,
      "grad_norm": 3.411200523376465,
      "learning_rate": 4.191522762951335e-05,
      "loss": 0.6268,
      "step": 824000
    },
    {
      "epoch": 12.937205651491366,
      "grad_norm": 3.1885826587677,
      "learning_rate": 4.1914246467817894e-05,
      "loss": 0.618,
      "step": 824100
    },
    {
      "epoch": 12.938775510204081,
      "grad_norm": 4.171711444854736,
      "learning_rate": 4.191326530612245e-05,
      "loss": 0.6673,
      "step": 824200
    },
    {
      "epoch": 12.940345368916798,
      "grad_norm": 3.417942523956299,
      "learning_rate": 4.1912284144427e-05,
      "loss": 0.636,
      "step": 824300
    },
    {
      "epoch": 12.941915227629513,
      "grad_norm": 4.484140872955322,
      "learning_rate": 4.1911302982731554e-05,
      "loss": 0.644,
      "step": 824400
    },
    {
      "epoch": 12.943485086342228,
      "grad_norm": 4.101253986358643,
      "learning_rate": 4.1910321821036105e-05,
      "loss": 0.6057,
      "step": 824500
    },
    {
      "epoch": 12.945054945054945,
      "grad_norm": 4.214594841003418,
      "learning_rate": 4.190934065934066e-05,
      "loss": 0.6412,
      "step": 824600
    },
    {
      "epoch": 12.94662480376766,
      "grad_norm": 3.8565621376037598,
      "learning_rate": 4.1908359497645214e-05,
      "loss": 0.6483,
      "step": 824700
    },
    {
      "epoch": 12.948194662480377,
      "grad_norm": 3.6286892890930176,
      "learning_rate": 4.1907378335949765e-05,
      "loss": 0.615,
      "step": 824800
    },
    {
      "epoch": 12.949764521193092,
      "grad_norm": 4.035490989685059,
      "learning_rate": 4.190639717425432e-05,
      "loss": 0.6583,
      "step": 824900
    },
    {
      "epoch": 12.95133437990581,
      "grad_norm": 3.839521646499634,
      "learning_rate": 4.1905416012558874e-05,
      "loss": 0.6595,
      "step": 825000
    },
    {
      "epoch": 12.952904238618524,
      "grad_norm": 3.450634479522705,
      "learning_rate": 4.1904434850863425e-05,
      "loss": 0.6793,
      "step": 825100
    },
    {
      "epoch": 12.95447409733124,
      "grad_norm": 3.43567156791687,
      "learning_rate": 4.1903453689167976e-05,
      "loss": 0.6105,
      "step": 825200
    },
    {
      "epoch": 12.956043956043956,
      "grad_norm": 4.03566312789917,
      "learning_rate": 4.190247252747253e-05,
      "loss": 0.6147,
      "step": 825300
    },
    {
      "epoch": 12.957613814756671,
      "grad_norm": 3.0542662143707275,
      "learning_rate": 4.1901491365777084e-05,
      "loss": 0.6029,
      "step": 825400
    },
    {
      "epoch": 12.959183673469388,
      "grad_norm": 3.6846535205841064,
      "learning_rate": 4.1900510204081635e-05,
      "loss": 0.6319,
      "step": 825500
    },
    {
      "epoch": 12.960753532182103,
      "grad_norm": 3.784607172012329,
      "learning_rate": 4.1899529042386186e-05,
      "loss": 0.6266,
      "step": 825600
    },
    {
      "epoch": 12.96232339089482,
      "grad_norm": 3.588197946548462,
      "learning_rate": 4.1898547880690744e-05,
      "loss": 0.6257,
      "step": 825700
    },
    {
      "epoch": 12.963893249607535,
      "grad_norm": 3.972378969192505,
      "learning_rate": 4.189756671899529e-05,
      "loss": 0.6402,
      "step": 825800
    },
    {
      "epoch": 12.96546310832025,
      "grad_norm": 2.7673656940460205,
      "learning_rate": 4.1896585557299846e-05,
      "loss": 0.6422,
      "step": 825900
    },
    {
      "epoch": 12.967032967032967,
      "grad_norm": 3.570601224899292,
      "learning_rate": 4.18956043956044e-05,
      "loss": 0.584,
      "step": 826000
    },
    {
      "epoch": 12.968602825745682,
      "grad_norm": 3.9320435523986816,
      "learning_rate": 4.1894623233908955e-05,
      "loss": 0.5965,
      "step": 826100
    },
    {
      "epoch": 12.970172684458399,
      "grad_norm": 4.21999979019165,
      "learning_rate": 4.18936420722135e-05,
      "loss": 0.6361,
      "step": 826200
    },
    {
      "epoch": 12.971742543171114,
      "grad_norm": 3.9102981090545654,
      "learning_rate": 4.189266091051806e-05,
      "loss": 0.6182,
      "step": 826300
    },
    {
      "epoch": 12.973312401883831,
      "grad_norm": 4.023472309112549,
      "learning_rate": 4.189167974882261e-05,
      "loss": 0.614,
      "step": 826400
    },
    {
      "epoch": 12.974882260596546,
      "grad_norm": 3.6105804443359375,
      "learning_rate": 4.189069858712716e-05,
      "loss": 0.5911,
      "step": 826500
    },
    {
      "epoch": 12.976452119309261,
      "grad_norm": 4.23799991607666,
      "learning_rate": 4.188971742543171e-05,
      "loss": 0.61,
      "step": 826600
    },
    {
      "epoch": 12.978021978021978,
      "grad_norm": 4.294661998748779,
      "learning_rate": 4.188873626373627e-05,
      "loss": 0.6134,
      "step": 826700
    },
    {
      "epoch": 12.979591836734693,
      "grad_norm": 3.2664482593536377,
      "learning_rate": 4.188775510204082e-05,
      "loss": 0.691,
      "step": 826800
    },
    {
      "epoch": 12.98116169544741,
      "grad_norm": 4.398392200469971,
      "learning_rate": 4.188677394034537e-05,
      "loss": 0.6476,
      "step": 826900
    },
    {
      "epoch": 12.982731554160125,
      "grad_norm": 3.8591363430023193,
      "learning_rate": 4.188579277864993e-05,
      "loss": 0.6146,
      "step": 827000
    },
    {
      "epoch": 12.984301412872842,
      "grad_norm": 3.7431650161743164,
      "learning_rate": 4.188481161695448e-05,
      "loss": 0.6088,
      "step": 827100
    },
    {
      "epoch": 12.985871271585557,
      "grad_norm": 4.379288196563721,
      "learning_rate": 4.188383045525903e-05,
      "loss": 0.6577,
      "step": 827200
    },
    {
      "epoch": 12.987441130298274,
      "grad_norm": 3.2201106548309326,
      "learning_rate": 4.188284929356358e-05,
      "loss": 0.6152,
      "step": 827300
    },
    {
      "epoch": 12.989010989010989,
      "grad_norm": 3.474720001220703,
      "learning_rate": 4.188186813186814e-05,
      "loss": 0.6203,
      "step": 827400
    },
    {
      "epoch": 12.990580847723704,
      "grad_norm": 3.3345203399658203,
      "learning_rate": 4.188088697017269e-05,
      "loss": 0.6175,
      "step": 827500
    },
    {
      "epoch": 12.992150706436421,
      "grad_norm": 2.8300273418426514,
      "learning_rate": 4.187990580847724e-05,
      "loss": 0.6009,
      "step": 827600
    },
    {
      "epoch": 12.993720565149136,
      "grad_norm": 5.1985321044921875,
      "learning_rate": 4.187892464678179e-05,
      "loss": 0.6099,
      "step": 827700
    },
    {
      "epoch": 12.995290423861853,
      "grad_norm": 4.307958602905273,
      "learning_rate": 4.187794348508635e-05,
      "loss": 0.5882,
      "step": 827800
    },
    {
      "epoch": 12.996860282574568,
      "grad_norm": 3.694413661956787,
      "learning_rate": 4.187696232339089e-05,
      "loss": 0.6023,
      "step": 827900
    },
    {
      "epoch": 12.998430141287285,
      "grad_norm": 4.339383125305176,
      "learning_rate": 4.187598116169545e-05,
      "loss": 0.6455,
      "step": 828000
    },
    {
      "epoch": 13.0,
      "grad_norm": 4.813668251037598,
      "learning_rate": 4.1875e-05,
      "loss": 0.6317,
      "step": 828100
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.0435354709625244,
      "eval_runtime": 14.9484,
      "eval_samples_per_second": 224.305,
      "eval_steps_per_second": 224.305,
      "step": 828100
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.48646652698516846,
      "eval_runtime": 287.9769,
      "eval_samples_per_second": 221.198,
      "eval_steps_per_second": 221.198,
      "step": 828100
    },
    {
      "epoch": 13.001569858712715,
      "grad_norm": 3.9297807216644287,
      "learning_rate": 4.187401883830456e-05,
      "loss": 0.5775,
      "step": 828200
    },
    {
      "epoch": 13.003139717425432,
      "grad_norm": 4.3237528800964355,
      "learning_rate": 4.18730376766091e-05,
      "loss": 0.6437,
      "step": 828300
    },
    {
      "epoch": 13.004709576138147,
      "grad_norm": 4.5176591873168945,
      "learning_rate": 4.187205651491366e-05,
      "loss": 0.6426,
      "step": 828400
    },
    {
      "epoch": 13.006279434850864,
      "grad_norm": 3.351874589920044,
      "learning_rate": 4.187107535321821e-05,
      "loss": 0.5817,
      "step": 828500
    },
    {
      "epoch": 13.007849293563579,
      "grad_norm": 4.360341548919678,
      "learning_rate": 4.187009419152276e-05,
      "loss": 0.6175,
      "step": 828600
    },
    {
      "epoch": 13.009419152276296,
      "grad_norm": 4.025101184844971,
      "learning_rate": 4.1869113029827314e-05,
      "loss": 0.6194,
      "step": 828700
    },
    {
      "epoch": 13.010989010989011,
      "grad_norm": 4.206018924713135,
      "learning_rate": 4.186813186813187e-05,
      "loss": 0.6005,
      "step": 828800
    },
    {
      "epoch": 13.012558869701726,
      "grad_norm": 3.5158932209014893,
      "learning_rate": 4.186715070643642e-05,
      "loss": 0.6094,
      "step": 828900
    },
    {
      "epoch": 13.014128728414443,
      "grad_norm": 3.1902453899383545,
      "learning_rate": 4.1866169544740974e-05,
      "loss": 0.5987,
      "step": 829000
    },
    {
      "epoch": 13.015698587127158,
      "grad_norm": 2.7239179611206055,
      "learning_rate": 4.186518838304553e-05,
      "loss": 0.6275,
      "step": 829100
    },
    {
      "epoch": 13.017268445839875,
      "grad_norm": 3.547248363494873,
      "learning_rate": 4.186420722135008e-05,
      "loss": 0.58,
      "step": 829200
    },
    {
      "epoch": 13.01883830455259,
      "grad_norm": 4.133412837982178,
      "learning_rate": 4.1863226059654634e-05,
      "loss": 0.6399,
      "step": 829300
    },
    {
      "epoch": 13.020408163265307,
      "grad_norm": 3.396286725997925,
      "learning_rate": 4.1862244897959185e-05,
      "loss": 0.6161,
      "step": 829400
    },
    {
      "epoch": 13.021978021978022,
      "grad_norm": 4.198385715484619,
      "learning_rate": 4.186126373626374e-05,
      "loss": 0.6466,
      "step": 829500
    },
    {
      "epoch": 13.023547880690737,
      "grad_norm": 4.426564693450928,
      "learning_rate": 4.186028257456829e-05,
      "loss": 0.615,
      "step": 829600
    },
    {
      "epoch": 13.025117739403454,
      "grad_norm": 3.1537671089172363,
      "learning_rate": 4.1859301412872844e-05,
      "loss": 0.6363,
      "step": 829700
    },
    {
      "epoch": 13.026687598116169,
      "grad_norm": 2.97609543800354,
      "learning_rate": 4.1858320251177395e-05,
      "loss": 0.6139,
      "step": 829800
    },
    {
      "epoch": 13.028257456828886,
      "grad_norm": 3.81009840965271,
      "learning_rate": 4.185733908948195e-05,
      "loss": 0.5805,
      "step": 829900
    },
    {
      "epoch": 13.029827315541601,
      "grad_norm": 4.237087726593018,
      "learning_rate": 4.18563579277865e-05,
      "loss": 0.5734,
      "step": 830000
    },
    {
      "epoch": 13.031397174254318,
      "grad_norm": 4.487766265869141,
      "learning_rate": 4.1855376766091055e-05,
      "loss": 0.5704,
      "step": 830100
    },
    {
      "epoch": 13.032967032967033,
      "grad_norm": 3.976501941680908,
      "learning_rate": 4.1854395604395606e-05,
      "loss": 0.6295,
      "step": 830200
    },
    {
      "epoch": 13.034536891679748,
      "grad_norm": 4.0123114585876465,
      "learning_rate": 4.1853414442700164e-05,
      "loss": 0.6311,
      "step": 830300
    },
    {
      "epoch": 13.036106750392465,
      "grad_norm": 4.465720176696777,
      "learning_rate": 4.185243328100471e-05,
      "loss": 0.6336,
      "step": 830400
    },
    {
      "epoch": 13.03767660910518,
      "grad_norm": 3.2986860275268555,
      "learning_rate": 4.1851452119309266e-05,
      "loss": 0.6004,
      "step": 830500
    },
    {
      "epoch": 13.039246467817897,
      "grad_norm": 4.326062202453613,
      "learning_rate": 4.1850470957613817e-05,
      "loss": 0.636,
      "step": 830600
    },
    {
      "epoch": 13.040816326530612,
      "grad_norm": 4.438651084899902,
      "learning_rate": 4.184948979591837e-05,
      "loss": 0.6322,
      "step": 830700
    },
    {
      "epoch": 13.042386185243329,
      "grad_norm": 3.893104076385498,
      "learning_rate": 4.184850863422292e-05,
      "loss": 0.6656,
      "step": 830800
    },
    {
      "epoch": 13.043956043956044,
      "grad_norm": 3.404947519302368,
      "learning_rate": 4.1847527472527476e-05,
      "loss": 0.6033,
      "step": 830900
    },
    {
      "epoch": 13.04552590266876,
      "grad_norm": 3.2235724925994873,
      "learning_rate": 4.184654631083203e-05,
      "loss": 0.6493,
      "step": 831000
    },
    {
      "epoch": 13.047095761381476,
      "grad_norm": 3.824195384979248,
      "learning_rate": 4.184556514913658e-05,
      "loss": 0.6086,
      "step": 831100
    },
    {
      "epoch": 13.04866562009419,
      "grad_norm": 2.5794074535369873,
      "learning_rate": 4.1844583987441136e-05,
      "loss": 0.6222,
      "step": 831200
    },
    {
      "epoch": 13.050235478806908,
      "grad_norm": 3.7078306674957275,
      "learning_rate": 4.184360282574569e-05,
      "loss": 0.6656,
      "step": 831300
    },
    {
      "epoch": 13.051805337519623,
      "grad_norm": 3.5976052284240723,
      "learning_rate": 4.184262166405024e-05,
      "loss": 0.6521,
      "step": 831400
    },
    {
      "epoch": 13.05337519623234,
      "grad_norm": 4.690183162689209,
      "learning_rate": 4.184164050235479e-05,
      "loss": 0.5827,
      "step": 831500
    },
    {
      "epoch": 13.054945054945055,
      "grad_norm": 4.119332313537598,
      "learning_rate": 4.184065934065935e-05,
      "loss": 0.5908,
      "step": 831600
    },
    {
      "epoch": 13.056514913657772,
      "grad_norm": 3.5861048698425293,
      "learning_rate": 4.183967817896389e-05,
      "loss": 0.5955,
      "step": 831700
    },
    {
      "epoch": 13.058084772370487,
      "grad_norm": 4.469001293182373,
      "learning_rate": 4.183869701726845e-05,
      "loss": 0.6276,
      "step": 831800
    },
    {
      "epoch": 13.059654631083202,
      "grad_norm": 4.228382587432861,
      "learning_rate": 4.1837715855573e-05,
      "loss": 0.5956,
      "step": 831900
    },
    {
      "epoch": 13.061224489795919,
      "grad_norm": 4.505558967590332,
      "learning_rate": 4.183673469387756e-05,
      "loss": 0.6149,
      "step": 832000
    },
    {
      "epoch": 13.062794348508634,
      "grad_norm": 4.020728588104248,
      "learning_rate": 4.18357535321821e-05,
      "loss": 0.6336,
      "step": 832100
    },
    {
      "epoch": 13.06436420722135,
      "grad_norm": 4.541677474975586,
      "learning_rate": 4.183477237048666e-05,
      "loss": 0.6558,
      "step": 832200
    },
    {
      "epoch": 13.065934065934066,
      "grad_norm": 4.182307243347168,
      "learning_rate": 4.183379120879121e-05,
      "loss": 0.6439,
      "step": 832300
    },
    {
      "epoch": 13.067503924646783,
      "grad_norm": 4.070850372314453,
      "learning_rate": 4.183281004709576e-05,
      "loss": 0.5724,
      "step": 832400
    },
    {
      "epoch": 13.069073783359498,
      "grad_norm": 3.0172600746154785,
      "learning_rate": 4.183182888540031e-05,
      "loss": 0.6459,
      "step": 832500
    },
    {
      "epoch": 13.070643642072213,
      "grad_norm": 4.287332534790039,
      "learning_rate": 4.183084772370487e-05,
      "loss": 0.6137,
      "step": 832600
    },
    {
      "epoch": 13.07221350078493,
      "grad_norm": 2.8774755001068115,
      "learning_rate": 4.182986656200942e-05,
      "loss": 0.6164,
      "step": 832700
    },
    {
      "epoch": 13.073783359497645,
      "grad_norm": 2.1897122859954834,
      "learning_rate": 4.182888540031397e-05,
      "loss": 0.6014,
      "step": 832800
    },
    {
      "epoch": 13.075353218210362,
      "grad_norm": 4.574592590332031,
      "learning_rate": 4.182790423861852e-05,
      "loss": 0.6441,
      "step": 832900
    },
    {
      "epoch": 13.076923076923077,
      "grad_norm": 2.853410482406616,
      "learning_rate": 4.182692307692308e-05,
      "loss": 0.616,
      "step": 833000
    },
    {
      "epoch": 13.078492935635794,
      "grad_norm": 4.804084777832031,
      "learning_rate": 4.182594191522763e-05,
      "loss": 0.6226,
      "step": 833100
    },
    {
      "epoch": 13.080062794348509,
      "grad_norm": 3.4783523082733154,
      "learning_rate": 4.182496075353218e-05,
      "loss": 0.613,
      "step": 833200
    },
    {
      "epoch": 13.081632653061224,
      "grad_norm": 3.6184442043304443,
      "learning_rate": 4.182397959183674e-05,
      "loss": 0.6121,
      "step": 833300
    },
    {
      "epoch": 13.08320251177394,
      "grad_norm": 3.152366876602173,
      "learning_rate": 4.182299843014129e-05,
      "loss": 0.5964,
      "step": 833400
    },
    {
      "epoch": 13.084772370486656,
      "grad_norm": 3.898664712905884,
      "learning_rate": 4.182201726844584e-05,
      "loss": 0.5866,
      "step": 833500
    },
    {
      "epoch": 13.086342229199373,
      "grad_norm": 4.128017425537109,
      "learning_rate": 4.1821036106750393e-05,
      "loss": 0.6148,
      "step": 833600
    },
    {
      "epoch": 13.087912087912088,
      "grad_norm": 4.557583808898926,
      "learning_rate": 4.182005494505495e-05,
      "loss": 0.5982,
      "step": 833700
    },
    {
      "epoch": 13.089481946624804,
      "grad_norm": 3.67946195602417,
      "learning_rate": 4.1819073783359495e-05,
      "loss": 0.5793,
      "step": 833800
    },
    {
      "epoch": 13.09105180533752,
      "grad_norm": 3.606201648712158,
      "learning_rate": 4.181809262166405e-05,
      "loss": 0.6057,
      "step": 833900
    },
    {
      "epoch": 13.092621664050235,
      "grad_norm": 3.574683904647827,
      "learning_rate": 4.1817111459968604e-05,
      "loss": 0.6133,
      "step": 834000
    },
    {
      "epoch": 13.094191522762952,
      "grad_norm": 3.873389959335327,
      "learning_rate": 4.181613029827316e-05,
      "loss": 0.6173,
      "step": 834100
    },
    {
      "epoch": 13.095761381475667,
      "grad_norm": 2.4629249572753906,
      "learning_rate": 4.1815149136577706e-05,
      "loss": 0.6085,
      "step": 834200
    },
    {
      "epoch": 13.097331240188383,
      "grad_norm": 3.0892081260681152,
      "learning_rate": 4.1814167974882264e-05,
      "loss": 0.6181,
      "step": 834300
    },
    {
      "epoch": 13.098901098901099,
      "grad_norm": 2.1943445205688477,
      "learning_rate": 4.1813186813186815e-05,
      "loss": 0.6321,
      "step": 834400
    },
    {
      "epoch": 13.100470957613815,
      "grad_norm": 3.9870362281799316,
      "learning_rate": 4.1812205651491366e-05,
      "loss": 0.6092,
      "step": 834500
    },
    {
      "epoch": 13.10204081632653,
      "grad_norm": 3.656816244125366,
      "learning_rate": 4.181122448979592e-05,
      "loss": 0.6237,
      "step": 834600
    },
    {
      "epoch": 13.103610675039246,
      "grad_norm": 3.871934413909912,
      "learning_rate": 4.1810243328100475e-05,
      "loss": 0.599,
      "step": 834700
    },
    {
      "epoch": 13.105180533751962,
      "grad_norm": 4.18734073638916,
      "learning_rate": 4.1809262166405026e-05,
      "loss": 0.6145,
      "step": 834800
    },
    {
      "epoch": 13.106750392464678,
      "grad_norm": 3.1253981590270996,
      "learning_rate": 4.1808281004709577e-05,
      "loss": 0.6207,
      "step": 834900
    },
    {
      "epoch": 13.108320251177394,
      "grad_norm": 3.2176132202148438,
      "learning_rate": 4.180729984301413e-05,
      "loss": 0.6104,
      "step": 835000
    },
    {
      "epoch": 13.10989010989011,
      "grad_norm": 4.039741039276123,
      "learning_rate": 4.1806318681318685e-05,
      "loss": 0.6066,
      "step": 835100
    },
    {
      "epoch": 13.111459968602826,
      "grad_norm": 3.371053457260132,
      "learning_rate": 4.1805337519623236e-05,
      "loss": 0.5902,
      "step": 835200
    },
    {
      "epoch": 13.113029827315541,
      "grad_norm": 2.568418025970459,
      "learning_rate": 4.180435635792779e-05,
      "loss": 0.5953,
      "step": 835300
    },
    {
      "epoch": 13.114599686028258,
      "grad_norm": 3.8429253101348877,
      "learning_rate": 4.1803375196232345e-05,
      "loss": 0.5982,
      "step": 835400
    },
    {
      "epoch": 13.116169544740973,
      "grad_norm": 2.9685888290405273,
      "learning_rate": 4.1802394034536896e-05,
      "loss": 0.616,
      "step": 835500
    },
    {
      "epoch": 13.117739403453688,
      "grad_norm": 3.0434250831604004,
      "learning_rate": 4.180141287284145e-05,
      "loss": 0.6131,
      "step": 835600
    },
    {
      "epoch": 13.119309262166405,
      "grad_norm": 3.518688201904297,
      "learning_rate": 4.1800431711146e-05,
      "loss": 0.6127,
      "step": 835700
    },
    {
      "epoch": 13.12087912087912,
      "grad_norm": 3.722547769546509,
      "learning_rate": 4.1799450549450556e-05,
      "loss": 0.6575,
      "step": 835800
    },
    {
      "epoch": 13.122448979591837,
      "grad_norm": 4.9787445068359375,
      "learning_rate": 4.17984693877551e-05,
      "loss": 0.6156,
      "step": 835900
    },
    {
      "epoch": 13.124018838304552,
      "grad_norm": 3.663116216659546,
      "learning_rate": 4.179748822605966e-05,
      "loss": 0.5844,
      "step": 836000
    },
    {
      "epoch": 13.12558869701727,
      "grad_norm": 3.3358659744262695,
      "learning_rate": 4.179650706436421e-05,
      "loss": 0.6311,
      "step": 836100
    },
    {
      "epoch": 13.127158555729984,
      "grad_norm": 3.814206123352051,
      "learning_rate": 4.1795525902668766e-05,
      "loss": 0.633,
      "step": 836200
    },
    {
      "epoch": 13.1287284144427,
      "grad_norm": 2.9704926013946533,
      "learning_rate": 4.179454474097331e-05,
      "loss": 0.6294,
      "step": 836300
    },
    {
      "epoch": 13.130298273155416,
      "grad_norm": 4.429142951965332,
      "learning_rate": 4.179356357927787e-05,
      "loss": 0.6377,
      "step": 836400
    },
    {
      "epoch": 13.131868131868131,
      "grad_norm": 3.6261298656463623,
      "learning_rate": 4.179258241758242e-05,
      "loss": 0.6201,
      "step": 836500
    },
    {
      "epoch": 13.133437990580848,
      "grad_norm": 4.485313415527344,
      "learning_rate": 4.179160125588697e-05,
      "loss": 0.6015,
      "step": 836600
    },
    {
      "epoch": 13.135007849293563,
      "grad_norm": 4.260100364685059,
      "learning_rate": 4.179062009419152e-05,
      "loss": 0.6228,
      "step": 836700
    },
    {
      "epoch": 13.13657770800628,
      "grad_norm": 3.346365451812744,
      "learning_rate": 4.178963893249608e-05,
      "loss": 0.5967,
      "step": 836800
    },
    {
      "epoch": 13.138147566718995,
      "grad_norm": 3.5901129245758057,
      "learning_rate": 4.178865777080063e-05,
      "loss": 0.6082,
      "step": 836900
    },
    {
      "epoch": 13.13971742543171,
      "grad_norm": 4.212651252746582,
      "learning_rate": 4.178767660910518e-05,
      "loss": 0.5985,
      "step": 837000
    },
    {
      "epoch": 13.141287284144427,
      "grad_norm": 4.099793434143066,
      "learning_rate": 4.178669544740973e-05,
      "loss": 0.622,
      "step": 837100
    },
    {
      "epoch": 13.142857142857142,
      "grad_norm": 3.2103962898254395,
      "learning_rate": 4.178571428571429e-05,
      "loss": 0.6215,
      "step": 837200
    },
    {
      "epoch": 13.14442700156986,
      "grad_norm": 2.9421586990356445,
      "learning_rate": 4.178473312401884e-05,
      "loss": 0.6558,
      "step": 837300
    },
    {
      "epoch": 13.145996860282574,
      "grad_norm": 4.2141804695129395,
      "learning_rate": 4.178375196232339e-05,
      "loss": 0.6253,
      "step": 837400
    },
    {
      "epoch": 13.147566718995291,
      "grad_norm": 2.8174667358398438,
      "learning_rate": 4.178277080062795e-05,
      "loss": 0.6604,
      "step": 837500
    },
    {
      "epoch": 13.149136577708006,
      "grad_norm": 3.110105276107788,
      "learning_rate": 4.17817896389325e-05,
      "loss": 0.6047,
      "step": 837600
    },
    {
      "epoch": 13.150706436420721,
      "grad_norm": 3.182651996612549,
      "learning_rate": 4.178080847723705e-05,
      "loss": 0.6027,
      "step": 837700
    },
    {
      "epoch": 13.152276295133438,
      "grad_norm": 2.434187173843384,
      "learning_rate": 4.17798273155416e-05,
      "loss": 0.6016,
      "step": 837800
    },
    {
      "epoch": 13.153846153846153,
      "grad_norm": 4.260429382324219,
      "learning_rate": 4.177884615384616e-05,
      "loss": 0.6205,
      "step": 837900
    },
    {
      "epoch": 13.15541601255887,
      "grad_norm": 3.2612366676330566,
      "learning_rate": 4.1777864992150704e-05,
      "loss": 0.6406,
      "step": 838000
    },
    {
      "epoch": 13.156985871271585,
      "grad_norm": 3.5587005615234375,
      "learning_rate": 4.177688383045526e-05,
      "loss": 0.6604,
      "step": 838100
    },
    {
      "epoch": 13.158555729984302,
      "grad_norm": 4.2829437255859375,
      "learning_rate": 4.177590266875981e-05,
      "loss": 0.6414,
      "step": 838200
    },
    {
      "epoch": 13.160125588697017,
      "grad_norm": 4.500834941864014,
      "learning_rate": 4.177492150706437e-05,
      "loss": 0.6411,
      "step": 838300
    },
    {
      "epoch": 13.161695447409732,
      "grad_norm": 4.174737453460693,
      "learning_rate": 4.1773940345368915e-05,
      "loss": 0.6058,
      "step": 838400
    },
    {
      "epoch": 13.16326530612245,
      "grad_norm": 4.484005451202393,
      "learning_rate": 4.177295918367347e-05,
      "loss": 0.6138,
      "step": 838500
    },
    {
      "epoch": 13.164835164835164,
      "grad_norm": 3.961062431335449,
      "learning_rate": 4.1771978021978024e-05,
      "loss": 0.6293,
      "step": 838600
    },
    {
      "epoch": 13.166405023547881,
      "grad_norm": 3.1704957485198975,
      "learning_rate": 4.1770996860282575e-05,
      "loss": 0.64,
      "step": 838700
    },
    {
      "epoch": 13.167974882260596,
      "grad_norm": 4.128977298736572,
      "learning_rate": 4.1770015698587126e-05,
      "loss": 0.6198,
      "step": 838800
    },
    {
      "epoch": 13.169544740973313,
      "grad_norm": 2.5595967769622803,
      "learning_rate": 4.1769034536891684e-05,
      "loss": 0.5913,
      "step": 838900
    },
    {
      "epoch": 13.171114599686028,
      "grad_norm": 4.080637454986572,
      "learning_rate": 4.1768053375196235e-05,
      "loss": 0.6427,
      "step": 839000
    },
    {
      "epoch": 13.172684458398743,
      "grad_norm": 3.7776498794555664,
      "learning_rate": 4.1767072213500786e-05,
      "loss": 0.5948,
      "step": 839100
    },
    {
      "epoch": 13.17425431711146,
      "grad_norm": 3.129957675933838,
      "learning_rate": 4.1766091051805337e-05,
      "loss": 0.6321,
      "step": 839200
    },
    {
      "epoch": 13.175824175824175,
      "grad_norm": 3.4076857566833496,
      "learning_rate": 4.1765109890109894e-05,
      "loss": 0.6169,
      "step": 839300
    },
    {
      "epoch": 13.177394034536892,
      "grad_norm": 3.40299129486084,
      "learning_rate": 4.1764128728414445e-05,
      "loss": 0.5777,
      "step": 839400
    },
    {
      "epoch": 13.178963893249607,
      "grad_norm": 4.452070236206055,
      "learning_rate": 4.1763147566718996e-05,
      "loss": 0.6413,
      "step": 839500
    },
    {
      "epoch": 13.180533751962324,
      "grad_norm": 4.809220314025879,
      "learning_rate": 4.176216640502355e-05,
      "loss": 0.622,
      "step": 839600
    },
    {
      "epoch": 13.182103610675039,
      "grad_norm": 3.429921865463257,
      "learning_rate": 4.1761185243328105e-05,
      "loss": 0.687,
      "step": 839700
    },
    {
      "epoch": 13.183673469387756,
      "grad_norm": 4.409659385681152,
      "learning_rate": 4.1760204081632656e-05,
      "loss": 0.638,
      "step": 839800
    },
    {
      "epoch": 13.185243328100471,
      "grad_norm": 3.9429333209991455,
      "learning_rate": 4.175922291993721e-05,
      "loss": 0.6278,
      "step": 839900
    },
    {
      "epoch": 13.186813186813186,
      "grad_norm": 4.574337005615234,
      "learning_rate": 4.1758241758241765e-05,
      "loss": 0.6366,
      "step": 840000
    },
    {
      "epoch": 13.188383045525903,
      "grad_norm": 3.199054002761841,
      "learning_rate": 4.175726059654631e-05,
      "loss": 0.6006,
      "step": 840100
    },
    {
      "epoch": 13.189952904238618,
      "grad_norm": 2.792567729949951,
      "learning_rate": 4.175627943485087e-05,
      "loss": 0.6173,
      "step": 840200
    },
    {
      "epoch": 13.191522762951335,
      "grad_norm": 4.030447483062744,
      "learning_rate": 4.175529827315542e-05,
      "loss": 0.6171,
      "step": 840300
    },
    {
      "epoch": 13.19309262166405,
      "grad_norm": 34.15858840942383,
      "learning_rate": 4.1754317111459975e-05,
      "loss": 0.5979,
      "step": 840400
    },
    {
      "epoch": 13.194662480376767,
      "grad_norm": 3.590883731842041,
      "learning_rate": 4.175333594976452e-05,
      "loss": 0.6767,
      "step": 840500
    },
    {
      "epoch": 13.196232339089482,
      "grad_norm": 5.588531494140625,
      "learning_rate": 4.175235478806908e-05,
      "loss": 0.6415,
      "step": 840600
    },
    {
      "epoch": 13.197802197802197,
      "grad_norm": 3.234907627105713,
      "learning_rate": 4.175137362637363e-05,
      "loss": 0.6306,
      "step": 840700
    },
    {
      "epoch": 13.199372056514914,
      "grad_norm": 4.249197959899902,
      "learning_rate": 4.175039246467818e-05,
      "loss": 0.641,
      "step": 840800
    },
    {
      "epoch": 13.200941915227629,
      "grad_norm": 3.042593479156494,
      "learning_rate": 4.174941130298273e-05,
      "loss": 0.6412,
      "step": 840900
    },
    {
      "epoch": 13.202511773940346,
      "grad_norm": 3.626983404159546,
      "learning_rate": 4.174843014128729e-05,
      "loss": 0.6344,
      "step": 841000
    },
    {
      "epoch": 13.204081632653061,
      "grad_norm": 3.1716601848602295,
      "learning_rate": 4.174744897959184e-05,
      "loss": 0.6032,
      "step": 841100
    },
    {
      "epoch": 13.205651491365778,
      "grad_norm": 3.132267951965332,
      "learning_rate": 4.174646781789639e-05,
      "loss": 0.6187,
      "step": 841200
    },
    {
      "epoch": 13.207221350078493,
      "grad_norm": 4.4104814529418945,
      "learning_rate": 4.174548665620094e-05,
      "loss": 0.5986,
      "step": 841300
    },
    {
      "epoch": 13.208791208791208,
      "grad_norm": 4.996984481811523,
      "learning_rate": 4.17445054945055e-05,
      "loss": 0.6296,
      "step": 841400
    },
    {
      "epoch": 13.210361067503925,
      "grad_norm": 5.195118427276611,
      "learning_rate": 4.174352433281005e-05,
      "loss": 0.6047,
      "step": 841500
    },
    {
      "epoch": 13.21193092621664,
      "grad_norm": 3.8026578426361084,
      "learning_rate": 4.17425431711146e-05,
      "loss": 0.6243,
      "step": 841600
    },
    {
      "epoch": 13.213500784929357,
      "grad_norm": 3.9197232723236084,
      "learning_rate": 4.174156200941915e-05,
      "loss": 0.5932,
      "step": 841700
    },
    {
      "epoch": 13.215070643642072,
      "grad_norm": 4.688037872314453,
      "learning_rate": 4.174058084772371e-05,
      "loss": 0.5684,
      "step": 841800
    },
    {
      "epoch": 13.216640502354789,
      "grad_norm": 2.908435583114624,
      "learning_rate": 4.173959968602826e-05,
      "loss": 0.62,
      "step": 841900
    },
    {
      "epoch": 13.218210361067504,
      "grad_norm": 3.6040468215942383,
      "learning_rate": 4.173861852433281e-05,
      "loss": 0.6071,
      "step": 842000
    },
    {
      "epoch": 13.219780219780219,
      "grad_norm": 4.576955318450928,
      "learning_rate": 4.173763736263737e-05,
      "loss": 0.6001,
      "step": 842100
    },
    {
      "epoch": 13.221350078492936,
      "grad_norm": 2.6733715534210205,
      "learning_rate": 4.1736656200941913e-05,
      "loss": 0.6524,
      "step": 842200
    },
    {
      "epoch": 13.222919937205651,
      "grad_norm": 4.371292591094971,
      "learning_rate": 4.173567503924647e-05,
      "loss": 0.6142,
      "step": 842300
    },
    {
      "epoch": 13.224489795918368,
      "grad_norm": 2.2106614112854004,
      "learning_rate": 4.173469387755102e-05,
      "loss": 0.6213,
      "step": 842400
    },
    {
      "epoch": 13.226059654631083,
      "grad_norm": 3.324272394180298,
      "learning_rate": 4.173371271585558e-05,
      "loss": 0.6175,
      "step": 842500
    },
    {
      "epoch": 13.2276295133438,
      "grad_norm": 5.258948802947998,
      "learning_rate": 4.1732731554160124e-05,
      "loss": 0.6155,
      "step": 842600
    },
    {
      "epoch": 13.229199372056515,
      "grad_norm": 4.704245567321777,
      "learning_rate": 4.173175039246468e-05,
      "loss": 0.667,
      "step": 842700
    },
    {
      "epoch": 13.23076923076923,
      "grad_norm": 2.7609071731567383,
      "learning_rate": 4.173076923076923e-05,
      "loss": 0.6331,
      "step": 842800
    },
    {
      "epoch": 13.232339089481947,
      "grad_norm": 3.96262526512146,
      "learning_rate": 4.1729788069073784e-05,
      "loss": 0.6031,
      "step": 842900
    },
    {
      "epoch": 13.233908948194662,
      "grad_norm": 3.446770668029785,
      "learning_rate": 4.1728806907378335e-05,
      "loss": 0.6065,
      "step": 843000
    },
    {
      "epoch": 13.235478806907379,
      "grad_norm": 4.1164937019348145,
      "learning_rate": 4.172782574568289e-05,
      "loss": 0.6109,
      "step": 843100
    },
    {
      "epoch": 13.237048665620094,
      "grad_norm": 3.1622507572174072,
      "learning_rate": 4.1726844583987444e-05,
      "loss": 0.6293,
      "step": 843200
    },
    {
      "epoch": 13.23861852433281,
      "grad_norm": 2.219534158706665,
      "learning_rate": 4.1725863422291995e-05,
      "loss": 0.6502,
      "step": 843300
    },
    {
      "epoch": 13.240188383045526,
      "grad_norm": 5.070118427276611,
      "learning_rate": 4.1724882260596546e-05,
      "loss": 0.5826,
      "step": 843400
    },
    {
      "epoch": 13.241758241758241,
      "grad_norm": 2.9995310306549072,
      "learning_rate": 4.17239010989011e-05,
      "loss": 0.5984,
      "step": 843500
    },
    {
      "epoch": 13.243328100470958,
      "grad_norm": 4.254751205444336,
      "learning_rate": 4.1722919937205654e-05,
      "loss": 0.653,
      "step": 843600
    },
    {
      "epoch": 13.244897959183673,
      "grad_norm": 3.6794519424438477,
      "learning_rate": 4.1721938775510205e-05,
      "loss": 0.5991,
      "step": 843700
    },
    {
      "epoch": 13.24646781789639,
      "grad_norm": 4.916364669799805,
      "learning_rate": 4.1720957613814756e-05,
      "loss": 0.6147,
      "step": 843800
    },
    {
      "epoch": 13.248037676609105,
      "grad_norm": 3.0301992893218994,
      "learning_rate": 4.1719976452119314e-05,
      "loss": 0.6115,
      "step": 843900
    },
    {
      "epoch": 13.249607535321822,
      "grad_norm": 3.641603946685791,
      "learning_rate": 4.1718995290423865e-05,
      "loss": 0.6417,
      "step": 844000
    },
    {
      "epoch": 13.251177394034537,
      "grad_norm": 4.13350248336792,
      "learning_rate": 4.1718014128728416e-05,
      "loss": 0.6192,
      "step": 844100
    },
    {
      "epoch": 13.252747252747252,
      "grad_norm": 4.965515613555908,
      "learning_rate": 4.1717032967032974e-05,
      "loss": 0.6493,
      "step": 844200
    },
    {
      "epoch": 13.254317111459969,
      "grad_norm": 3.9734745025634766,
      "learning_rate": 4.171605180533752e-05,
      "loss": 0.6282,
      "step": 844300
    },
    {
      "epoch": 13.255886970172684,
      "grad_norm": 4.592144966125488,
      "learning_rate": 4.1715070643642076e-05,
      "loss": 0.6165,
      "step": 844400
    },
    {
      "epoch": 13.2574568288854,
      "grad_norm": 3.3149921894073486,
      "learning_rate": 4.171408948194663e-05,
      "loss": 0.6248,
      "step": 844500
    },
    {
      "epoch": 13.259026687598116,
      "grad_norm": 3.797456741333008,
      "learning_rate": 4.1713108320251184e-05,
      "loss": 0.6336,
      "step": 844600
    },
    {
      "epoch": 13.260596546310833,
      "grad_norm": 3.6847753524780273,
      "learning_rate": 4.171212715855573e-05,
      "loss": 0.6485,
      "step": 844700
    },
    {
      "epoch": 13.262166405023548,
      "grad_norm": 3.94189453125,
      "learning_rate": 4.1711145996860286e-05,
      "loss": 0.6338,
      "step": 844800
    },
    {
      "epoch": 13.263736263736265,
      "grad_norm": 4.680728435516357,
      "learning_rate": 4.171016483516484e-05,
      "loss": 0.5978,
      "step": 844900
    },
    {
      "epoch": 13.26530612244898,
      "grad_norm": 3.660682439804077,
      "learning_rate": 4.170918367346939e-05,
      "loss": 0.6353,
      "step": 845000
    },
    {
      "epoch": 13.266875981161695,
      "grad_norm": 4.6158127784729,
      "learning_rate": 4.170820251177394e-05,
      "loss": 0.6331,
      "step": 845100
    },
    {
      "epoch": 13.268445839874412,
      "grad_norm": 3.9230568408966064,
      "learning_rate": 4.17072213500785e-05,
      "loss": 0.6257,
      "step": 845200
    },
    {
      "epoch": 13.270015698587127,
      "grad_norm": 4.304246425628662,
      "learning_rate": 4.170624018838305e-05,
      "loss": 0.6335,
      "step": 845300
    },
    {
      "epoch": 13.271585557299844,
      "grad_norm": 4.015836238861084,
      "learning_rate": 4.17052590266876e-05,
      "loss": 0.6433,
      "step": 845400
    },
    {
      "epoch": 13.273155416012559,
      "grad_norm": 3.382322311401367,
      "learning_rate": 4.170427786499215e-05,
      "loss": 0.6551,
      "step": 845500
    },
    {
      "epoch": 13.274725274725276,
      "grad_norm": 3.0755655765533447,
      "learning_rate": 4.170329670329671e-05,
      "loss": 0.5876,
      "step": 845600
    },
    {
      "epoch": 13.27629513343799,
      "grad_norm": 3.5832791328430176,
      "learning_rate": 4.170231554160126e-05,
      "loss": 0.6572,
      "step": 845700
    },
    {
      "epoch": 13.277864992150706,
      "grad_norm": 3.8920977115631104,
      "learning_rate": 4.170133437990581e-05,
      "loss": 0.6169,
      "step": 845800
    },
    {
      "epoch": 13.279434850863423,
      "grad_norm": 4.092490196228027,
      "learning_rate": 4.170035321821036e-05,
      "loss": 0.6199,
      "step": 845900
    },
    {
      "epoch": 13.281004709576138,
      "grad_norm": 3.672611951828003,
      "learning_rate": 4.169937205651492e-05,
      "loss": 0.6554,
      "step": 846000
    },
    {
      "epoch": 13.282574568288855,
      "grad_norm": 4.2923054695129395,
      "learning_rate": 4.169839089481947e-05,
      "loss": 0.5887,
      "step": 846100
    },
    {
      "epoch": 13.28414442700157,
      "grad_norm": 4.022356986999512,
      "learning_rate": 4.169740973312402e-05,
      "loss": 0.5704,
      "step": 846200
    },
    {
      "epoch": 13.285714285714286,
      "grad_norm": 3.7913527488708496,
      "learning_rate": 4.169642857142858e-05,
      "loss": 0.6258,
      "step": 846300
    },
    {
      "epoch": 13.287284144427002,
      "grad_norm": 3.2387781143188477,
      "learning_rate": 4.169544740973312e-05,
      "loss": 0.6467,
      "step": 846400
    },
    {
      "epoch": 13.288854003139717,
      "grad_norm": 5.91303825378418,
      "learning_rate": 4.169446624803768e-05,
      "loss": 0.6047,
      "step": 846500
    },
    {
      "epoch": 13.290423861852434,
      "grad_norm": 3.750061511993408,
      "learning_rate": 4.169348508634223e-05,
      "loss": 0.6452,
      "step": 846600
    },
    {
      "epoch": 13.291993720565149,
      "grad_norm": 3.8091769218444824,
      "learning_rate": 4.169250392464679e-05,
      "loss": 0.6543,
      "step": 846700
    },
    {
      "epoch": 13.293563579277865,
      "grad_norm": 3.3402833938598633,
      "learning_rate": 4.169152276295133e-05,
      "loss": 0.6122,
      "step": 846800
    },
    {
      "epoch": 13.29513343799058,
      "grad_norm": 4.17520809173584,
      "learning_rate": 4.169054160125589e-05,
      "loss": 0.599,
      "step": 846900
    },
    {
      "epoch": 13.296703296703297,
      "grad_norm": 4.002958297729492,
      "learning_rate": 4.168956043956044e-05,
      "loss": 0.6347,
      "step": 847000
    },
    {
      "epoch": 13.298273155416013,
      "grad_norm": 3.727494478225708,
      "learning_rate": 4.168857927786499e-05,
      "loss": 0.6285,
      "step": 847100
    },
    {
      "epoch": 13.299843014128728,
      "grad_norm": 4.22696590423584,
      "learning_rate": 4.1687598116169544e-05,
      "loss": 0.6262,
      "step": 847200
    },
    {
      "epoch": 13.301412872841444,
      "grad_norm": 2.951111078262329,
      "learning_rate": 4.16866169544741e-05,
      "loss": 0.5992,
      "step": 847300
    },
    {
      "epoch": 13.30298273155416,
      "grad_norm": 3.937523365020752,
      "learning_rate": 4.168563579277865e-05,
      "loss": 0.6317,
      "step": 847400
    },
    {
      "epoch": 13.304552590266876,
      "grad_norm": 3.106822967529297,
      "learning_rate": 4.1684654631083204e-05,
      "loss": 0.6273,
      "step": 847500
    },
    {
      "epoch": 13.306122448979592,
      "grad_norm": 4.235625743865967,
      "learning_rate": 4.1683673469387754e-05,
      "loss": 0.5926,
      "step": 847600
    },
    {
      "epoch": 13.307692307692308,
      "grad_norm": 5.935668468475342,
      "learning_rate": 4.168269230769231e-05,
      "loss": 0.5938,
      "step": 847700
    },
    {
      "epoch": 13.309262166405023,
      "grad_norm": 3.7392663955688477,
      "learning_rate": 4.168171114599686e-05,
      "loss": 0.6195,
      "step": 847800
    },
    {
      "epoch": 13.310832025117739,
      "grad_norm": 3.7351746559143066,
      "learning_rate": 4.1680729984301414e-05,
      "loss": 0.6378,
      "step": 847900
    },
    {
      "epoch": 13.312401883830455,
      "grad_norm": 3.634904384613037,
      "learning_rate": 4.1679748822605965e-05,
      "loss": 0.5561,
      "step": 848000
    },
    {
      "epoch": 13.31397174254317,
      "grad_norm": 2.76554799079895,
      "learning_rate": 4.167876766091052e-05,
      "loss": 0.6384,
      "step": 848100
    },
    {
      "epoch": 13.315541601255887,
      "grad_norm": 2.958735942840576,
      "learning_rate": 4.1677786499215074e-05,
      "loss": 0.6231,
      "step": 848200
    },
    {
      "epoch": 13.317111459968602,
      "grad_norm": 3.686628818511963,
      "learning_rate": 4.1676805337519625e-05,
      "loss": 0.604,
      "step": 848300
    },
    {
      "epoch": 13.31868131868132,
      "grad_norm": 4.96935510635376,
      "learning_rate": 4.167582417582418e-05,
      "loss": 0.6039,
      "step": 848400
    },
    {
      "epoch": 13.320251177394034,
      "grad_norm": 4.4078569412231445,
      "learning_rate": 4.167484301412873e-05,
      "loss": 0.6178,
      "step": 848500
    },
    {
      "epoch": 13.321821036106751,
      "grad_norm": 3.843538522720337,
      "learning_rate": 4.1673861852433285e-05,
      "loss": 0.6195,
      "step": 848600
    },
    {
      "epoch": 13.323390894819466,
      "grad_norm": 2.975046396255493,
      "learning_rate": 4.1672880690737836e-05,
      "loss": 0.6092,
      "step": 848700
    },
    {
      "epoch": 13.324960753532181,
      "grad_norm": 4.204036235809326,
      "learning_rate": 4.167189952904239e-05,
      "loss": 0.62,
      "step": 848800
    },
    {
      "epoch": 13.326530612244898,
      "grad_norm": 4.179612159729004,
      "learning_rate": 4.167091836734694e-05,
      "loss": 0.6322,
      "step": 848900
    },
    {
      "epoch": 13.328100470957613,
      "grad_norm": 4.029383659362793,
      "learning_rate": 4.1669937205651495e-05,
      "loss": 0.6123,
      "step": 849000
    },
    {
      "epoch": 13.32967032967033,
      "grad_norm": 3.446251392364502,
      "learning_rate": 4.1668956043956046e-05,
      "loss": 0.6545,
      "step": 849100
    },
    {
      "epoch": 13.331240188383045,
      "grad_norm": 4.764030933380127,
      "learning_rate": 4.16679748822606e-05,
      "loss": 0.6578,
      "step": 849200
    },
    {
      "epoch": 13.332810047095762,
      "grad_norm": 3.3927550315856934,
      "learning_rate": 4.166699372056515e-05,
      "loss": 0.6113,
      "step": 849300
    },
    {
      "epoch": 13.334379905808477,
      "grad_norm": 3.145362615585327,
      "learning_rate": 4.1666012558869706e-05,
      "loss": 0.6286,
      "step": 849400
    },
    {
      "epoch": 13.335949764521192,
      "grad_norm": 3.4504003524780273,
      "learning_rate": 4.166503139717426e-05,
      "loss": 0.6102,
      "step": 849500
    },
    {
      "epoch": 13.33751962323391,
      "grad_norm": 4.6297125816345215,
      "learning_rate": 4.166405023547881e-05,
      "loss": 0.6325,
      "step": 849600
    },
    {
      "epoch": 13.339089481946624,
      "grad_norm": 3.505800724029541,
      "learning_rate": 4.166306907378336e-05,
      "loss": 0.5814,
      "step": 849700
    },
    {
      "epoch": 13.340659340659341,
      "grad_norm": 3.3946027755737305,
      "learning_rate": 4.166208791208792e-05,
      "loss": 0.6146,
      "step": 849800
    },
    {
      "epoch": 13.342229199372056,
      "grad_norm": 4.105220317840576,
      "learning_rate": 4.166110675039247e-05,
      "loss": 0.6071,
      "step": 849900
    },
    {
      "epoch": 13.343799058084773,
      "grad_norm": 3.608138084411621,
      "learning_rate": 4.166012558869702e-05,
      "loss": 0.6048,
      "step": 850000
    },
    {
      "epoch": 13.345368916797488,
      "grad_norm": 3.8031704425811768,
      "learning_rate": 4.165914442700157e-05,
      "loss": 0.6709,
      "step": 850100
    },
    {
      "epoch": 13.346938775510203,
      "grad_norm": 2.412421941757202,
      "learning_rate": 4.165816326530613e-05,
      "loss": 0.6333,
      "step": 850200
    },
    {
      "epoch": 13.34850863422292,
      "grad_norm": 4.143848419189453,
      "learning_rate": 4.165718210361068e-05,
      "loss": 0.5911,
      "step": 850300
    },
    {
      "epoch": 13.350078492935635,
      "grad_norm": 2.979118585586548,
      "learning_rate": 4.165620094191523e-05,
      "loss": 0.6241,
      "step": 850400
    },
    {
      "epoch": 13.351648351648352,
      "grad_norm": 3.56728458404541,
      "learning_rate": 4.165521978021979e-05,
      "loss": 0.6386,
      "step": 850500
    },
    {
      "epoch": 13.353218210361067,
      "grad_norm": 4.292600631713867,
      "learning_rate": 4.165423861852433e-05,
      "loss": 0.6313,
      "step": 850600
    },
    {
      "epoch": 13.354788069073784,
      "grad_norm": 4.055825233459473,
      "learning_rate": 4.165325745682889e-05,
      "loss": 0.6332,
      "step": 850700
    },
    {
      "epoch": 13.3563579277865,
      "grad_norm": 3.679889440536499,
      "learning_rate": 4.165227629513344e-05,
      "loss": 0.6652,
      "step": 850800
    },
    {
      "epoch": 13.357927786499214,
      "grad_norm": 3.1375315189361572,
      "learning_rate": 4.1651295133438e-05,
      "loss": 0.6336,
      "step": 850900
    },
    {
      "epoch": 13.359497645211931,
      "grad_norm": 4.866097927093506,
      "learning_rate": 4.165031397174254e-05,
      "loss": 0.6526,
      "step": 851000
    },
    {
      "epoch": 13.361067503924646,
      "grad_norm": 4.591742992401123,
      "learning_rate": 4.16493328100471e-05,
      "loss": 0.6516,
      "step": 851100
    },
    {
      "epoch": 13.362637362637363,
      "grad_norm": 3.8867886066436768,
      "learning_rate": 4.164835164835165e-05,
      "loss": 0.5954,
      "step": 851200
    },
    {
      "epoch": 13.364207221350078,
      "grad_norm": 3.4131245613098145,
      "learning_rate": 4.16473704866562e-05,
      "loss": 0.652,
      "step": 851300
    },
    {
      "epoch": 13.365777080062795,
      "grad_norm": 3.836242198944092,
      "learning_rate": 4.164638932496075e-05,
      "loss": 0.5964,
      "step": 851400
    },
    {
      "epoch": 13.36734693877551,
      "grad_norm": 2.946608066558838,
      "learning_rate": 4.164540816326531e-05,
      "loss": 0.6591,
      "step": 851500
    },
    {
      "epoch": 13.368916797488225,
      "grad_norm": 4.361752510070801,
      "learning_rate": 4.164442700156986e-05,
      "loss": 0.6303,
      "step": 851600
    },
    {
      "epoch": 13.370486656200942,
      "grad_norm": 4.516695499420166,
      "learning_rate": 4.164344583987441e-05,
      "loss": 0.5843,
      "step": 851700
    },
    {
      "epoch": 13.372056514913657,
      "grad_norm": 3.419813632965088,
      "learning_rate": 4.1642464678178963e-05,
      "loss": 0.5958,
      "step": 851800
    },
    {
      "epoch": 13.373626373626374,
      "grad_norm": 4.005824089050293,
      "learning_rate": 4.164148351648352e-05,
      "loss": 0.591,
      "step": 851900
    },
    {
      "epoch": 13.37519623233909,
      "grad_norm": 2.902913808822632,
      "learning_rate": 4.164050235478807e-05,
      "loss": 0.6214,
      "step": 852000
    },
    {
      "epoch": 13.376766091051806,
      "grad_norm": 3.4489331245422363,
      "learning_rate": 4.163952119309262e-05,
      "loss": 0.5955,
      "step": 852100
    },
    {
      "epoch": 13.378335949764521,
      "grad_norm": 3.7391092777252197,
      "learning_rate": 4.1638540031397174e-05,
      "loss": 0.5752,
      "step": 852200
    },
    {
      "epoch": 13.379905808477236,
      "grad_norm": 3.476872205734253,
      "learning_rate": 4.163755886970173e-05,
      "loss": 0.633,
      "step": 852300
    },
    {
      "epoch": 13.381475667189953,
      "grad_norm": 4.572443962097168,
      "learning_rate": 4.163657770800628e-05,
      "loss": 0.5978,
      "step": 852400
    },
    {
      "epoch": 13.383045525902668,
      "grad_norm": 3.665489912033081,
      "learning_rate": 4.1635596546310834e-05,
      "loss": 0.6259,
      "step": 852500
    },
    {
      "epoch": 13.384615384615385,
      "grad_norm": 4.698730945587158,
      "learning_rate": 4.163461538461539e-05,
      "loss": 0.6123,
      "step": 852600
    },
    {
      "epoch": 13.3861852433281,
      "grad_norm": 3.900413990020752,
      "learning_rate": 4.1633634222919936e-05,
      "loss": 0.6569,
      "step": 852700
    },
    {
      "epoch": 13.387755102040817,
      "grad_norm": 3.973595142364502,
      "learning_rate": 4.1632653061224494e-05,
      "loss": 0.6112,
      "step": 852800
    },
    {
      "epoch": 13.389324960753532,
      "grad_norm": 3.8639283180236816,
      "learning_rate": 4.1631671899529045e-05,
      "loss": 0.6184,
      "step": 852900
    },
    {
      "epoch": 13.390894819466247,
      "grad_norm": 3.741755723953247,
      "learning_rate": 4.16306907378336e-05,
      "loss": 0.6057,
      "step": 853000
    },
    {
      "epoch": 13.392464678178964,
      "grad_norm": 3.375260591506958,
      "learning_rate": 4.1629709576138147e-05,
      "loss": 0.6056,
      "step": 853100
    },
    {
      "epoch": 13.394034536891679,
      "grad_norm": 3.7056422233581543,
      "learning_rate": 4.1628728414442704e-05,
      "loss": 0.641,
      "step": 853200
    },
    {
      "epoch": 13.395604395604396,
      "grad_norm": 4.303267955780029,
      "learning_rate": 4.1627747252747255e-05,
      "loss": 0.6447,
      "step": 853300
    },
    {
      "epoch": 13.397174254317111,
      "grad_norm": 3.9456803798675537,
      "learning_rate": 4.1626766091051806e-05,
      "loss": 0.667,
      "step": 853400
    },
    {
      "epoch": 13.398744113029828,
      "grad_norm": 4.04453706741333,
      "learning_rate": 4.162578492935636e-05,
      "loss": 0.6389,
      "step": 853500
    },
    {
      "epoch": 13.400313971742543,
      "grad_norm": 3.8862574100494385,
      "learning_rate": 4.1624803767660915e-05,
      "loss": 0.6064,
      "step": 853600
    },
    {
      "epoch": 13.40188383045526,
      "grad_norm": 4.892858982086182,
      "learning_rate": 4.162382260596546e-05,
      "loss": 0.6391,
      "step": 853700
    },
    {
      "epoch": 13.403453689167975,
      "grad_norm": 4.892134189605713,
      "learning_rate": 4.162284144427002e-05,
      "loss": 0.65,
      "step": 853800
    },
    {
      "epoch": 13.40502354788069,
      "grad_norm": 4.480887413024902,
      "learning_rate": 4.162186028257457e-05,
      "loss": 0.6366,
      "step": 853900
    },
    {
      "epoch": 13.406593406593407,
      "grad_norm": 3.152207374572754,
      "learning_rate": 4.1620879120879126e-05,
      "loss": 0.5988,
      "step": 854000
    },
    {
      "epoch": 13.408163265306122,
      "grad_norm": 4.391473770141602,
      "learning_rate": 4.161989795918368e-05,
      "loss": 0.6273,
      "step": 854100
    },
    {
      "epoch": 13.409733124018839,
      "grad_norm": 3.0302720069885254,
      "learning_rate": 4.161891679748823e-05,
      "loss": 0.5989,
      "step": 854200
    },
    {
      "epoch": 13.411302982731554,
      "grad_norm": 4.47937536239624,
      "learning_rate": 4.161793563579278e-05,
      "loss": 0.5688,
      "step": 854300
    },
    {
      "epoch": 13.41287284144427,
      "grad_norm": 4.221583843231201,
      "learning_rate": 4.161695447409733e-05,
      "loss": 0.657,
      "step": 854400
    },
    {
      "epoch": 13.414442700156986,
      "grad_norm": 4.375484943389893,
      "learning_rate": 4.161597331240189e-05,
      "loss": 0.6515,
      "step": 854500
    },
    {
      "epoch": 13.416012558869701,
      "grad_norm": 3.107837200164795,
      "learning_rate": 4.161499215070644e-05,
      "loss": 0.6462,
      "step": 854600
    },
    {
      "epoch": 13.417582417582418,
      "grad_norm": 2.501084566116333,
      "learning_rate": 4.1614010989010996e-05,
      "loss": 0.66,
      "step": 854700
    },
    {
      "epoch": 13.419152276295133,
      "grad_norm": 4.189516067504883,
      "learning_rate": 4.161302982731554e-05,
      "loss": 0.6136,
      "step": 854800
    },
    {
      "epoch": 13.42072213500785,
      "grad_norm": 5.559978008270264,
      "learning_rate": 4.16120486656201e-05,
      "loss": 0.5957,
      "step": 854900
    },
    {
      "epoch": 13.422291993720565,
      "grad_norm": 3.840531826019287,
      "learning_rate": 4.161106750392465e-05,
      "loss": 0.6505,
      "step": 855000
    },
    {
      "epoch": 13.423861852433282,
      "grad_norm": 5.23068380355835,
      "learning_rate": 4.16100863422292e-05,
      "loss": 0.6502,
      "step": 855100
    },
    {
      "epoch": 13.425431711145997,
      "grad_norm": 2.723572254180908,
      "learning_rate": 4.160910518053375e-05,
      "loss": 0.6019,
      "step": 855200
    },
    {
      "epoch": 13.427001569858712,
      "grad_norm": 4.619724273681641,
      "learning_rate": 4.160812401883831e-05,
      "loss": 0.6321,
      "step": 855300
    },
    {
      "epoch": 13.428571428571429,
      "grad_norm": 3.29585599899292,
      "learning_rate": 4.160714285714286e-05,
      "loss": 0.6157,
      "step": 855400
    },
    {
      "epoch": 13.430141287284144,
      "grad_norm": 4.548160076141357,
      "learning_rate": 4.160616169544741e-05,
      "loss": 0.6356,
      "step": 855500
    },
    {
      "epoch": 13.43171114599686,
      "grad_norm": 3.956852912902832,
      "learning_rate": 4.160518053375196e-05,
      "loss": 0.6095,
      "step": 855600
    },
    {
      "epoch": 13.433281004709576,
      "grad_norm": 3.331972122192383,
      "learning_rate": 4.160419937205652e-05,
      "loss": 0.6213,
      "step": 855700
    },
    {
      "epoch": 13.434850863422293,
      "grad_norm": 4.575313091278076,
      "learning_rate": 4.1603218210361064e-05,
      "loss": 0.6151,
      "step": 855800
    },
    {
      "epoch": 13.436420722135008,
      "grad_norm": 3.2062196731567383,
      "learning_rate": 4.160223704866562e-05,
      "loss": 0.6348,
      "step": 855900
    },
    {
      "epoch": 13.437990580847723,
      "grad_norm": 3.7305116653442383,
      "learning_rate": 4.160125588697017e-05,
      "loss": 0.6591,
      "step": 856000
    },
    {
      "epoch": 13.43956043956044,
      "grad_norm": 3.1859958171844482,
      "learning_rate": 4.160027472527473e-05,
      "loss": 0.6118,
      "step": 856100
    },
    {
      "epoch": 13.441130298273155,
      "grad_norm": 4.064027786254883,
      "learning_rate": 4.159929356357928e-05,
      "loss": 0.6297,
      "step": 856200
    },
    {
      "epoch": 13.442700156985872,
      "grad_norm": 4.275556564331055,
      "learning_rate": 4.159831240188383e-05,
      "loss": 0.6156,
      "step": 856300
    },
    {
      "epoch": 13.444270015698587,
      "grad_norm": 3.5299389362335205,
      "learning_rate": 4.159733124018838e-05,
      "loss": 0.6053,
      "step": 856400
    },
    {
      "epoch": 13.445839874411304,
      "grad_norm": 4.2103495597839355,
      "learning_rate": 4.1596350078492934e-05,
      "loss": 0.6505,
      "step": 856500
    },
    {
      "epoch": 13.447409733124019,
      "grad_norm": 3.15718150138855,
      "learning_rate": 4.159536891679749e-05,
      "loss": 0.6286,
      "step": 856600
    },
    {
      "epoch": 13.448979591836734,
      "grad_norm": 3.5279381275177,
      "learning_rate": 4.159438775510204e-05,
      "loss": 0.7015,
      "step": 856700
    },
    {
      "epoch": 13.45054945054945,
      "grad_norm": 3.7953476905822754,
      "learning_rate": 4.15934065934066e-05,
      "loss": 0.6348,
      "step": 856800
    },
    {
      "epoch": 13.452119309262166,
      "grad_norm": 4.2309112548828125,
      "learning_rate": 4.1592425431711145e-05,
      "loss": 0.6444,
      "step": 856900
    },
    {
      "epoch": 13.453689167974883,
      "grad_norm": 2.646594285964966,
      "learning_rate": 4.15914442700157e-05,
      "loss": 0.5956,
      "step": 857000
    },
    {
      "epoch": 13.455259026687598,
      "grad_norm": 3.1347007751464844,
      "learning_rate": 4.1590463108320254e-05,
      "loss": 0.6226,
      "step": 857100
    },
    {
      "epoch": 13.456828885400315,
      "grad_norm": 4.319182395935059,
      "learning_rate": 4.1589481946624805e-05,
      "loss": 0.6079,
      "step": 857200
    },
    {
      "epoch": 13.45839874411303,
      "grad_norm": 3.5736305713653564,
      "learning_rate": 4.1588500784929356e-05,
      "loss": 0.6212,
      "step": 857300
    },
    {
      "epoch": 13.459968602825747,
      "grad_norm": 4.382922172546387,
      "learning_rate": 4.158751962323391e-05,
      "loss": 0.5914,
      "step": 857400
    },
    {
      "epoch": 13.461538461538462,
      "grad_norm": 3.07999324798584,
      "learning_rate": 4.1586538461538464e-05,
      "loss": 0.596,
      "step": 857500
    },
    {
      "epoch": 13.463108320251177,
      "grad_norm": 3.073941707611084,
      "learning_rate": 4.1585557299843015e-05,
      "loss": 0.6083,
      "step": 857600
    },
    {
      "epoch": 13.464678178963894,
      "grad_norm": 2.9754629135131836,
      "learning_rate": 4.1584576138147566e-05,
      "loss": 0.6555,
      "step": 857700
    },
    {
      "epoch": 13.466248037676609,
      "grad_norm": 4.146650314331055,
      "learning_rate": 4.1583594976452124e-05,
      "loss": 0.6342,
      "step": 857800
    },
    {
      "epoch": 13.467817896389326,
      "grad_norm": 3.195697784423828,
      "learning_rate": 4.158261381475667e-05,
      "loss": 0.5512,
      "step": 857900
    },
    {
      "epoch": 13.46938775510204,
      "grad_norm": 3.956148624420166,
      "learning_rate": 4.1581632653061226e-05,
      "loss": 0.6112,
      "step": 858000
    },
    {
      "epoch": 13.470957613814758,
      "grad_norm": 4.9722490310668945,
      "learning_rate": 4.158065149136578e-05,
      "loss": 0.5896,
      "step": 858100
    },
    {
      "epoch": 13.472527472527473,
      "grad_norm": 4.3535237312316895,
      "learning_rate": 4.1579670329670335e-05,
      "loss": 0.6486,
      "step": 858200
    },
    {
      "epoch": 13.474097331240188,
      "grad_norm": 4.260670185089111,
      "learning_rate": 4.1578689167974886e-05,
      "loss": 0.6423,
      "step": 858300
    },
    {
      "epoch": 13.475667189952905,
      "grad_norm": 4.000558376312256,
      "learning_rate": 4.157770800627944e-05,
      "loss": 0.6133,
      "step": 858400
    },
    {
      "epoch": 13.47723704866562,
      "grad_norm": 3.470094919204712,
      "learning_rate": 4.157672684458399e-05,
      "loss": 0.6589,
      "step": 858500
    },
    {
      "epoch": 13.478806907378337,
      "grad_norm": 5.015347480773926,
      "learning_rate": 4.157574568288854e-05,
      "loss": 0.6332,
      "step": 858600
    },
    {
      "epoch": 13.480376766091052,
      "grad_norm": 3.1459712982177734,
      "learning_rate": 4.1574764521193096e-05,
      "loss": 0.6238,
      "step": 858700
    },
    {
      "epoch": 13.481946624803768,
      "grad_norm": 3.4048712253570557,
      "learning_rate": 4.157378335949765e-05,
      "loss": 0.6501,
      "step": 858800
    },
    {
      "epoch": 13.483516483516484,
      "grad_norm": 3.783890724182129,
      "learning_rate": 4.1572802197802205e-05,
      "loss": 0.6228,
      "step": 858900
    },
    {
      "epoch": 13.485086342229199,
      "grad_norm": 3.4516451358795166,
      "learning_rate": 4.157182103610675e-05,
      "loss": 0.6091,
      "step": 859000
    },
    {
      "epoch": 13.486656200941916,
      "grad_norm": 4.478918552398682,
      "learning_rate": 4.157083987441131e-05,
      "loss": 0.6114,
      "step": 859100
    },
    {
      "epoch": 13.48822605965463,
      "grad_norm": 2.996349573135376,
      "learning_rate": 4.156985871271586e-05,
      "loss": 0.6314,
      "step": 859200
    },
    {
      "epoch": 13.489795918367347,
      "grad_norm": 5.1313605308532715,
      "learning_rate": 4.156887755102041e-05,
      "loss": 0.6197,
      "step": 859300
    },
    {
      "epoch": 13.491365777080063,
      "grad_norm": 3.571415424346924,
      "learning_rate": 4.156789638932496e-05,
      "loss": 0.6083,
      "step": 859400
    },
    {
      "epoch": 13.49293563579278,
      "grad_norm": 3.7733583450317383,
      "learning_rate": 4.156691522762952e-05,
      "loss": 0.6735,
      "step": 859500
    },
    {
      "epoch": 13.494505494505495,
      "grad_norm": 3.32853364944458,
      "learning_rate": 4.156593406593407e-05,
      "loss": 0.6536,
      "step": 859600
    },
    {
      "epoch": 13.49607535321821,
      "grad_norm": 3.0005509853363037,
      "learning_rate": 4.156495290423862e-05,
      "loss": 0.6229,
      "step": 859700
    },
    {
      "epoch": 13.497645211930926,
      "grad_norm": 3.2835628986358643,
      "learning_rate": 4.156397174254317e-05,
      "loss": 0.6768,
      "step": 859800
    },
    {
      "epoch": 13.499215070643642,
      "grad_norm": 4.951879501342773,
      "learning_rate": 4.156299058084773e-05,
      "loss": 0.6077,
      "step": 859900
    },
    {
      "epoch": 13.500784929356358,
      "grad_norm": 3.5466113090515137,
      "learning_rate": 4.156200941915227e-05,
      "loss": 0.6103,
      "step": 860000
    },
    {
      "epoch": 13.502354788069074,
      "grad_norm": 4.862947463989258,
      "learning_rate": 4.156102825745683e-05,
      "loss": 0.6328,
      "step": 860100
    },
    {
      "epoch": 13.50392464678179,
      "grad_norm": 4.472604274749756,
      "learning_rate": 4.156004709576138e-05,
      "loss": 0.6385,
      "step": 860200
    },
    {
      "epoch": 13.505494505494505,
      "grad_norm": 4.302545070648193,
      "learning_rate": 4.155906593406594e-05,
      "loss": 0.6074,
      "step": 860300
    },
    {
      "epoch": 13.50706436420722,
      "grad_norm": 2.737071990966797,
      "learning_rate": 4.155808477237049e-05,
      "loss": 0.5919,
      "step": 860400
    },
    {
      "epoch": 13.508634222919937,
      "grad_norm": 4.246279716491699,
      "learning_rate": 4.155710361067504e-05,
      "loss": 0.6413,
      "step": 860500
    },
    {
      "epoch": 13.510204081632653,
      "grad_norm": 4.655854225158691,
      "learning_rate": 4.155612244897959e-05,
      "loss": 0.6242,
      "step": 860600
    },
    {
      "epoch": 13.51177394034537,
      "grad_norm": 3.4193532466888428,
      "learning_rate": 4.155514128728414e-05,
      "loss": 0.6377,
      "step": 860700
    },
    {
      "epoch": 13.513343799058084,
      "grad_norm": 3.1600329875946045,
      "learning_rate": 4.15541601255887e-05,
      "loss": 0.6149,
      "step": 860800
    },
    {
      "epoch": 13.514913657770801,
      "grad_norm": 3.0719761848449707,
      "learning_rate": 4.155317896389325e-05,
      "loss": 0.6356,
      "step": 860900
    },
    {
      "epoch": 13.516483516483516,
      "grad_norm": 3.700423240661621,
      "learning_rate": 4.155219780219781e-05,
      "loss": 0.585,
      "step": 861000
    },
    {
      "epoch": 13.518053375196232,
      "grad_norm": 3.8136017322540283,
      "learning_rate": 4.1551216640502354e-05,
      "loss": 0.6257,
      "step": 861100
    },
    {
      "epoch": 13.519623233908948,
      "grad_norm": 3.9144198894500732,
      "learning_rate": 4.155023547880691e-05,
      "loss": 0.6846,
      "step": 861200
    },
    {
      "epoch": 13.521193092621663,
      "grad_norm": 4.302231788635254,
      "learning_rate": 4.154925431711146e-05,
      "loss": 0.6098,
      "step": 861300
    },
    {
      "epoch": 13.52276295133438,
      "grad_norm": 2.9392004013061523,
      "learning_rate": 4.1548273155416014e-05,
      "loss": 0.5727,
      "step": 861400
    },
    {
      "epoch": 13.524332810047095,
      "grad_norm": 3.563150405883789,
      "learning_rate": 4.1547291993720565e-05,
      "loss": 0.6502,
      "step": 861500
    },
    {
      "epoch": 13.525902668759812,
      "grad_norm": 3.5055618286132812,
      "learning_rate": 4.154631083202512e-05,
      "loss": 0.6276,
      "step": 861600
    },
    {
      "epoch": 13.527472527472527,
      "grad_norm": 3.633502244949341,
      "learning_rate": 4.154532967032967e-05,
      "loss": 0.597,
      "step": 861700
    },
    {
      "epoch": 13.529042386185242,
      "grad_norm": 3.388976812362671,
      "learning_rate": 4.1544348508634224e-05,
      "loss": 0.6128,
      "step": 861800
    },
    {
      "epoch": 13.53061224489796,
      "grad_norm": 5.012114524841309,
      "learning_rate": 4.1543367346938775e-05,
      "loss": 0.6376,
      "step": 861900
    },
    {
      "epoch": 13.532182103610674,
      "grad_norm": 3.846649408340454,
      "learning_rate": 4.154238618524333e-05,
      "loss": 0.645,
      "step": 862000
    },
    {
      "epoch": 13.533751962323391,
      "grad_norm": 4.20290994644165,
      "learning_rate": 4.154140502354788e-05,
      "loss": 0.6185,
      "step": 862100
    },
    {
      "epoch": 13.535321821036106,
      "grad_norm": 3.372844934463501,
      "learning_rate": 4.1540423861852435e-05,
      "loss": 0.6217,
      "step": 862200
    },
    {
      "epoch": 13.536891679748823,
      "grad_norm": 3.3390111923217773,
      "learning_rate": 4.1539442700156986e-05,
      "loss": 0.6415,
      "step": 862300
    },
    {
      "epoch": 13.538461538461538,
      "grad_norm": 4.068419933319092,
      "learning_rate": 4.1538461538461544e-05,
      "loss": 0.6378,
      "step": 862400
    },
    {
      "epoch": 13.540031397174253,
      "grad_norm": 4.180561542510986,
      "learning_rate": 4.1537480376766095e-05,
      "loss": 0.6027,
      "step": 862500
    },
    {
      "epoch": 13.54160125588697,
      "grad_norm": 3.569674491882324,
      "learning_rate": 4.1536499215070646e-05,
      "loss": 0.6038,
      "step": 862600
    },
    {
      "epoch": 13.543171114599685,
      "grad_norm": 4.200919151306152,
      "learning_rate": 4.15355180533752e-05,
      "loss": 0.6484,
      "step": 862700
    },
    {
      "epoch": 13.544740973312402,
      "grad_norm": 3.846233367919922,
      "learning_rate": 4.153453689167975e-05,
      "loss": 0.576,
      "step": 862800
    },
    {
      "epoch": 13.546310832025117,
      "grad_norm": 3.792008876800537,
      "learning_rate": 4.1533555729984305e-05,
      "loss": 0.6412,
      "step": 862900
    },
    {
      "epoch": 13.547880690737834,
      "grad_norm": 3.7859606742858887,
      "learning_rate": 4.1532574568288856e-05,
      "loss": 0.5979,
      "step": 863000
    },
    {
      "epoch": 13.54945054945055,
      "grad_norm": 4.1264543533325195,
      "learning_rate": 4.1531593406593414e-05,
      "loss": 0.6008,
      "step": 863100
    },
    {
      "epoch": 13.551020408163264,
      "grad_norm": 2.512829065322876,
      "learning_rate": 4.153061224489796e-05,
      "loss": 0.6507,
      "step": 863200
    },
    {
      "epoch": 13.552590266875981,
      "grad_norm": 5.538515090942383,
      "learning_rate": 4.1529631083202516e-05,
      "loss": 0.6247,
      "step": 863300
    },
    {
      "epoch": 13.554160125588696,
      "grad_norm": 5.214864730834961,
      "learning_rate": 4.152864992150707e-05,
      "loss": 0.6255,
      "step": 863400
    },
    {
      "epoch": 13.555729984301413,
      "grad_norm": 3.833925247192383,
      "learning_rate": 4.152766875981162e-05,
      "loss": 0.6205,
      "step": 863500
    },
    {
      "epoch": 13.557299843014128,
      "grad_norm": 3.8050029277801514,
      "learning_rate": 4.152668759811617e-05,
      "loss": 0.6287,
      "step": 863600
    },
    {
      "epoch": 13.558869701726845,
      "grad_norm": 4.622026443481445,
      "learning_rate": 4.152570643642073e-05,
      "loss": 0.6449,
      "step": 863700
    },
    {
      "epoch": 13.56043956043956,
      "grad_norm": 4.512797832489014,
      "learning_rate": 4.152472527472528e-05,
      "loss": 0.6161,
      "step": 863800
    },
    {
      "epoch": 13.562009419152277,
      "grad_norm": 4.277970314025879,
      "learning_rate": 4.152374411302983e-05,
      "loss": 0.6203,
      "step": 863900
    },
    {
      "epoch": 13.563579277864992,
      "grad_norm": 4.251214504241943,
      "learning_rate": 4.152276295133438e-05,
      "loss": 0.6181,
      "step": 864000
    },
    {
      "epoch": 13.565149136577707,
      "grad_norm": 4.3782501220703125,
      "learning_rate": 4.152178178963894e-05,
      "loss": 0.5872,
      "step": 864100
    },
    {
      "epoch": 13.566718995290424,
      "grad_norm": 2.5442521572113037,
      "learning_rate": 4.152080062794348e-05,
      "loss": 0.619,
      "step": 864200
    },
    {
      "epoch": 13.56828885400314,
      "grad_norm": 3.8427093029022217,
      "learning_rate": 4.151981946624804e-05,
      "loss": 0.6093,
      "step": 864300
    },
    {
      "epoch": 13.569858712715856,
      "grad_norm": 3.3048243522644043,
      "learning_rate": 4.151883830455259e-05,
      "loss": 0.6337,
      "step": 864400
    },
    {
      "epoch": 13.571428571428571,
      "grad_norm": 2.6846022605895996,
      "learning_rate": 4.151785714285715e-05,
      "loss": 0.5546,
      "step": 864500
    },
    {
      "epoch": 13.572998430141288,
      "grad_norm": 2.615699052810669,
      "learning_rate": 4.15168759811617e-05,
      "loss": 0.6461,
      "step": 864600
    },
    {
      "epoch": 13.574568288854003,
      "grad_norm": 4.050791263580322,
      "learning_rate": 4.151589481946625e-05,
      "loss": 0.6363,
      "step": 864700
    },
    {
      "epoch": 13.576138147566718,
      "grad_norm": 3.6099469661712646,
      "learning_rate": 4.15149136577708e-05,
      "loss": 0.6256,
      "step": 864800
    },
    {
      "epoch": 13.577708006279435,
      "grad_norm": 3.33478045463562,
      "learning_rate": 4.151393249607535e-05,
      "loss": 0.6274,
      "step": 864900
    },
    {
      "epoch": 13.57927786499215,
      "grad_norm": 3.2018442153930664,
      "learning_rate": 4.151295133437991e-05,
      "loss": 0.6295,
      "step": 865000
    },
    {
      "epoch": 13.580847723704867,
      "grad_norm": 4.958144187927246,
      "learning_rate": 4.151197017268446e-05,
      "loss": 0.635,
      "step": 865100
    },
    {
      "epoch": 13.582417582417582,
      "grad_norm": 3.271909713745117,
      "learning_rate": 4.151098901098902e-05,
      "loss": 0.6173,
      "step": 865200
    },
    {
      "epoch": 13.583987441130299,
      "grad_norm": 3.431452751159668,
      "learning_rate": 4.151000784929356e-05,
      "loss": 0.6582,
      "step": 865300
    },
    {
      "epoch": 13.585557299843014,
      "grad_norm": 2.591456174850464,
      "learning_rate": 4.150902668759812e-05,
      "loss": 0.6327,
      "step": 865400
    },
    {
      "epoch": 13.58712715855573,
      "grad_norm": 4.196068286895752,
      "learning_rate": 4.150804552590267e-05,
      "loss": 0.6033,
      "step": 865500
    },
    {
      "epoch": 13.588697017268446,
      "grad_norm": 3.642702341079712,
      "learning_rate": 4.150706436420722e-05,
      "loss": 0.6265,
      "step": 865600
    },
    {
      "epoch": 13.590266875981161,
      "grad_norm": 4.212253570556641,
      "learning_rate": 4.1506083202511774e-05,
      "loss": 0.5831,
      "step": 865700
    },
    {
      "epoch": 13.591836734693878,
      "grad_norm": 3.8442866802215576,
      "learning_rate": 4.150510204081633e-05,
      "loss": 0.6324,
      "step": 865800
    },
    {
      "epoch": 13.593406593406593,
      "grad_norm": 4.398723602294922,
      "learning_rate": 4.150412087912088e-05,
      "loss": 0.6554,
      "step": 865900
    },
    {
      "epoch": 13.59497645211931,
      "grad_norm": 3.7842347621917725,
      "learning_rate": 4.150313971742543e-05,
      "loss": 0.6431,
      "step": 866000
    },
    {
      "epoch": 13.596546310832025,
      "grad_norm": 3.780672550201416,
      "learning_rate": 4.1502158555729984e-05,
      "loss": 0.5969,
      "step": 866100
    },
    {
      "epoch": 13.598116169544742,
      "grad_norm": 3.3182601928710938,
      "learning_rate": 4.150117739403454e-05,
      "loss": 0.6155,
      "step": 866200
    },
    {
      "epoch": 13.599686028257457,
      "grad_norm": 3.944575309753418,
      "learning_rate": 4.1500196232339086e-05,
      "loss": 0.6379,
      "step": 866300
    },
    {
      "epoch": 13.601255886970172,
      "grad_norm": 4.205270767211914,
      "learning_rate": 4.1499215070643644e-05,
      "loss": 0.6637,
      "step": 866400
    },
    {
      "epoch": 13.602825745682889,
      "grad_norm": 3.7221930027008057,
      "learning_rate": 4.1498233908948195e-05,
      "loss": 0.5769,
      "step": 866500
    },
    {
      "epoch": 13.604395604395604,
      "grad_norm": 3.8364551067352295,
      "learning_rate": 4.149725274725275e-05,
      "loss": 0.6443,
      "step": 866600
    },
    {
      "epoch": 13.605965463108321,
      "grad_norm": 3.48828125,
      "learning_rate": 4.1496271585557304e-05,
      "loss": 0.6215,
      "step": 866700
    },
    {
      "epoch": 13.607535321821036,
      "grad_norm": 3.122852325439453,
      "learning_rate": 4.1495290423861855e-05,
      "loss": 0.6263,
      "step": 866800
    },
    {
      "epoch": 13.609105180533753,
      "grad_norm": 4.127038478851318,
      "learning_rate": 4.1494309262166406e-05,
      "loss": 0.5952,
      "step": 866900
    },
    {
      "epoch": 13.610675039246468,
      "grad_norm": 4.294490814208984,
      "learning_rate": 4.1493328100470957e-05,
      "loss": 0.6364,
      "step": 867000
    },
    {
      "epoch": 13.612244897959183,
      "grad_norm": 3.1562886238098145,
      "learning_rate": 4.1492346938775514e-05,
      "loss": 0.6614,
      "step": 867100
    },
    {
      "epoch": 13.6138147566719,
      "grad_norm": 3.0339057445526123,
      "learning_rate": 4.1491365777080065e-05,
      "loss": 0.6596,
      "step": 867200
    },
    {
      "epoch": 13.615384615384615,
      "grad_norm": 3.6939995288848877,
      "learning_rate": 4.149038461538462e-05,
      "loss": 0.6425,
      "step": 867300
    },
    {
      "epoch": 13.616954474097332,
      "grad_norm": 4.274240493774414,
      "learning_rate": 4.148940345368917e-05,
      "loss": 0.6516,
      "step": 867400
    },
    {
      "epoch": 13.618524332810047,
      "grad_norm": 4.39522647857666,
      "learning_rate": 4.1488422291993725e-05,
      "loss": 0.5792,
      "step": 867500
    },
    {
      "epoch": 13.620094191522764,
      "grad_norm": 4.546288967132568,
      "learning_rate": 4.1487441130298276e-05,
      "loss": 0.6102,
      "step": 867600
    },
    {
      "epoch": 13.621664050235479,
      "grad_norm": 4.60294246673584,
      "learning_rate": 4.148645996860283e-05,
      "loss": 0.6273,
      "step": 867700
    },
    {
      "epoch": 13.623233908948194,
      "grad_norm": 3.7941462993621826,
      "learning_rate": 4.148547880690738e-05,
      "loss": 0.5962,
      "step": 867800
    },
    {
      "epoch": 13.62480376766091,
      "grad_norm": 3.8223137855529785,
      "learning_rate": 4.1484497645211936e-05,
      "loss": 0.6269,
      "step": 867900
    },
    {
      "epoch": 13.626373626373626,
      "grad_norm": 4.4171576499938965,
      "learning_rate": 4.148351648351649e-05,
      "loss": 0.5685,
      "step": 868000
    },
    {
      "epoch": 13.627943485086343,
      "grad_norm": 3.311929225921631,
      "learning_rate": 4.148253532182104e-05,
      "loss": 0.6472,
      "step": 868100
    },
    {
      "epoch": 13.629513343799058,
      "grad_norm": 3.0281689167022705,
      "learning_rate": 4.148155416012559e-05,
      "loss": 0.6304,
      "step": 868200
    },
    {
      "epoch": 13.631083202511775,
      "grad_norm": 2.4941837787628174,
      "learning_rate": 4.1480572998430146e-05,
      "loss": 0.6475,
      "step": 868300
    },
    {
      "epoch": 13.63265306122449,
      "grad_norm": 4.123551368713379,
      "learning_rate": 4.147959183673469e-05,
      "loss": 0.6308,
      "step": 868400
    },
    {
      "epoch": 13.634222919937205,
      "grad_norm": 3.9440927505493164,
      "learning_rate": 4.147861067503925e-05,
      "loss": 0.6181,
      "step": 868500
    },
    {
      "epoch": 13.635792778649922,
      "grad_norm": 4.002208232879639,
      "learning_rate": 4.14776295133438e-05,
      "loss": 0.6226,
      "step": 868600
    },
    {
      "epoch": 13.637362637362637,
      "grad_norm": 4.819285869598389,
      "learning_rate": 4.147664835164836e-05,
      "loss": 0.6452,
      "step": 868700
    },
    {
      "epoch": 13.638932496075354,
      "grad_norm": 4.4442458152771,
      "learning_rate": 4.147566718995291e-05,
      "loss": 0.6031,
      "step": 868800
    },
    {
      "epoch": 13.640502354788069,
      "grad_norm": 2.6708593368530273,
      "learning_rate": 4.147468602825746e-05,
      "loss": 0.6433,
      "step": 868900
    },
    {
      "epoch": 13.642072213500786,
      "grad_norm": 2.5219764709472656,
      "learning_rate": 4.147370486656201e-05,
      "loss": 0.6396,
      "step": 869000
    },
    {
      "epoch": 13.6436420722135,
      "grad_norm": 3.702472448348999,
      "learning_rate": 4.147272370486656e-05,
      "loss": 0.6502,
      "step": 869100
    },
    {
      "epoch": 13.645211930926216,
      "grad_norm": 4.245243549346924,
      "learning_rate": 4.147174254317112e-05,
      "loss": 0.6557,
      "step": 869200
    },
    {
      "epoch": 13.646781789638933,
      "grad_norm": 4.0504150390625,
      "learning_rate": 4.147076138147567e-05,
      "loss": 0.6588,
      "step": 869300
    },
    {
      "epoch": 13.648351648351648,
      "grad_norm": 4.032891273498535,
      "learning_rate": 4.146978021978023e-05,
      "loss": 0.5891,
      "step": 869400
    },
    {
      "epoch": 13.649921507064365,
      "grad_norm": 4.045834064483643,
      "learning_rate": 4.146879905808477e-05,
      "loss": 0.6416,
      "step": 869500
    },
    {
      "epoch": 13.65149136577708,
      "grad_norm": 3.6998558044433594,
      "learning_rate": 4.146781789638933e-05,
      "loss": 0.6459,
      "step": 869600
    },
    {
      "epoch": 13.653061224489797,
      "grad_norm": 4.915085315704346,
      "learning_rate": 4.146683673469388e-05,
      "loss": 0.6028,
      "step": 869700
    },
    {
      "epoch": 13.654631083202512,
      "grad_norm": 4.1118855476379395,
      "learning_rate": 4.146585557299843e-05,
      "loss": 0.6007,
      "step": 869800
    },
    {
      "epoch": 13.656200941915227,
      "grad_norm": 4.206745147705078,
      "learning_rate": 4.146487441130298e-05,
      "loss": 0.5981,
      "step": 869900
    },
    {
      "epoch": 13.657770800627944,
      "grad_norm": 3.2869713306427,
      "learning_rate": 4.146389324960754e-05,
      "loss": 0.6228,
      "step": 870000
    },
    {
      "epoch": 13.659340659340659,
      "grad_norm": 3.754695177078247,
      "learning_rate": 4.146291208791209e-05,
      "loss": 0.6439,
      "step": 870100
    },
    {
      "epoch": 13.660910518053376,
      "grad_norm": 4.928072452545166,
      "learning_rate": 4.146193092621664e-05,
      "loss": 0.6322,
      "step": 870200
    },
    {
      "epoch": 13.66248037676609,
      "grad_norm": 3.305088520050049,
      "learning_rate": 4.146094976452119e-05,
      "loss": 0.6168,
      "step": 870300
    },
    {
      "epoch": 13.664050235478808,
      "grad_norm": 4.415356159210205,
      "learning_rate": 4.145996860282575e-05,
      "loss": 0.6161,
      "step": 870400
    },
    {
      "epoch": 13.665620094191523,
      "grad_norm": 3.416222095489502,
      "learning_rate": 4.1458987441130295e-05,
      "loss": 0.6412,
      "step": 870500
    },
    {
      "epoch": 13.667189952904238,
      "grad_norm": 4.320067882537842,
      "learning_rate": 4.145800627943485e-05,
      "loss": 0.6437,
      "step": 870600
    },
    {
      "epoch": 13.668759811616955,
      "grad_norm": 3.942941665649414,
      "learning_rate": 4.1457025117739404e-05,
      "loss": 0.6132,
      "step": 870700
    },
    {
      "epoch": 13.67032967032967,
      "grad_norm": 5.476624488830566,
      "learning_rate": 4.145604395604396e-05,
      "loss": 0.6493,
      "step": 870800
    },
    {
      "epoch": 13.671899529042387,
      "grad_norm": 5.488574028015137,
      "learning_rate": 4.145506279434851e-05,
      "loss": 0.6257,
      "step": 870900
    },
    {
      "epoch": 13.673469387755102,
      "grad_norm": 3.9325122833251953,
      "learning_rate": 4.1454081632653064e-05,
      "loss": 0.6011,
      "step": 871000
    },
    {
      "epoch": 13.675039246467819,
      "grad_norm": 3.892397403717041,
      "learning_rate": 4.1453100470957615e-05,
      "loss": 0.6276,
      "step": 871100
    },
    {
      "epoch": 13.676609105180534,
      "grad_norm": 3.794485092163086,
      "learning_rate": 4.1452119309262166e-05,
      "loss": 0.6232,
      "step": 871200
    },
    {
      "epoch": 13.678178963893249,
      "grad_norm": 4.075138092041016,
      "learning_rate": 4.145113814756672e-05,
      "loss": 0.6282,
      "step": 871300
    },
    {
      "epoch": 13.679748822605966,
      "grad_norm": 4.137882709503174,
      "learning_rate": 4.1450156985871274e-05,
      "loss": 0.6438,
      "step": 871400
    },
    {
      "epoch": 13.68131868131868,
      "grad_norm": 3.860943555831909,
      "learning_rate": 4.144917582417583e-05,
      "loss": 0.618,
      "step": 871500
    },
    {
      "epoch": 13.682888540031398,
      "grad_norm": 2.6875035762786865,
      "learning_rate": 4.1448194662480376e-05,
      "loss": 0.597,
      "step": 871600
    },
    {
      "epoch": 13.684458398744113,
      "grad_norm": 2.9293882846832275,
      "learning_rate": 4.1447213500784934e-05,
      "loss": 0.6539,
      "step": 871700
    },
    {
      "epoch": 13.68602825745683,
      "grad_norm": 4.816852569580078,
      "learning_rate": 4.1446232339089485e-05,
      "loss": 0.631,
      "step": 871800
    },
    {
      "epoch": 13.687598116169545,
      "grad_norm": 3.3103418350219727,
      "learning_rate": 4.1445251177394036e-05,
      "loss": 0.5948,
      "step": 871900
    },
    {
      "epoch": 13.68916797488226,
      "grad_norm": 3.727691888809204,
      "learning_rate": 4.144427001569859e-05,
      "loss": 0.6513,
      "step": 872000
    },
    {
      "epoch": 13.690737833594977,
      "grad_norm": 4.403890609741211,
      "learning_rate": 4.1443288854003145e-05,
      "loss": 0.6317,
      "step": 872100
    },
    {
      "epoch": 13.692307692307692,
      "grad_norm": 2.8132359981536865,
      "learning_rate": 4.1442307692307696e-05,
      "loss": 0.6154,
      "step": 872200
    },
    {
      "epoch": 13.693877551020408,
      "grad_norm": 3.139519453048706,
      "learning_rate": 4.144132653061225e-05,
      "loss": 0.6217,
      "step": 872300
    },
    {
      "epoch": 13.695447409733124,
      "grad_norm": 4.476614952087402,
      "learning_rate": 4.14403453689168e-05,
      "loss": 0.5936,
      "step": 872400
    },
    {
      "epoch": 13.69701726844584,
      "grad_norm": 4.454391956329346,
      "learning_rate": 4.1439364207221355e-05,
      "loss": 0.5951,
      "step": 872500
    },
    {
      "epoch": 13.698587127158556,
      "grad_norm": 3.939195156097412,
      "learning_rate": 4.14383830455259e-05,
      "loss": 0.6499,
      "step": 872600
    },
    {
      "epoch": 13.700156985871272,
      "grad_norm": 4.204197406768799,
      "learning_rate": 4.143740188383046e-05,
      "loss": 0.6623,
      "step": 872700
    },
    {
      "epoch": 13.701726844583987,
      "grad_norm": 4.138668537139893,
      "learning_rate": 4.143642072213501e-05,
      "loss": 0.6268,
      "step": 872800
    },
    {
      "epoch": 13.703296703296703,
      "grad_norm": 2.024772882461548,
      "learning_rate": 4.1435439560439566e-05,
      "loss": 0.6301,
      "step": 872900
    },
    {
      "epoch": 13.70486656200942,
      "grad_norm": 4.294356822967529,
      "learning_rate": 4.143445839874411e-05,
      "loss": 0.642,
      "step": 873000
    },
    {
      "epoch": 13.706436420722135,
      "grad_norm": 4.166355133056641,
      "learning_rate": 4.143347723704867e-05,
      "loss": 0.6105,
      "step": 873100
    },
    {
      "epoch": 13.708006279434851,
      "grad_norm": 4.240442752838135,
      "learning_rate": 4.143249607535322e-05,
      "loss": 0.6151,
      "step": 873200
    },
    {
      "epoch": 13.709576138147566,
      "grad_norm": 3.574495553970337,
      "learning_rate": 4.143151491365777e-05,
      "loss": 0.6183,
      "step": 873300
    },
    {
      "epoch": 13.711145996860283,
      "grad_norm": 3.808297872543335,
      "learning_rate": 4.143053375196233e-05,
      "loss": 0.5805,
      "step": 873400
    },
    {
      "epoch": 13.712715855572998,
      "grad_norm": 3.791543483734131,
      "learning_rate": 4.142955259026688e-05,
      "loss": 0.6289,
      "step": 873500
    },
    {
      "epoch": 13.714285714285714,
      "grad_norm": 3.6533379554748535,
      "learning_rate": 4.1428571428571437e-05,
      "loss": 0.5779,
      "step": 873600
    },
    {
      "epoch": 13.71585557299843,
      "grad_norm": 2.870908498764038,
      "learning_rate": 4.142759026687598e-05,
      "loss": 0.6135,
      "step": 873700
    },
    {
      "epoch": 13.717425431711145,
      "grad_norm": 4.499111652374268,
      "learning_rate": 4.142660910518054e-05,
      "loss": 0.6329,
      "step": 873800
    },
    {
      "epoch": 13.718995290423862,
      "grad_norm": 4.096336364746094,
      "learning_rate": 4.142562794348509e-05,
      "loss": 0.6477,
      "step": 873900
    },
    {
      "epoch": 13.720565149136577,
      "grad_norm": 3.0537478923797607,
      "learning_rate": 4.142464678178964e-05,
      "loss": 0.602,
      "step": 874000
    },
    {
      "epoch": 13.722135007849294,
      "grad_norm": 3.2272393703460693,
      "learning_rate": 4.142366562009419e-05,
      "loss": 0.6561,
      "step": 874100
    },
    {
      "epoch": 13.72370486656201,
      "grad_norm": 3.9546115398406982,
      "learning_rate": 4.142268445839875e-05,
      "loss": 0.6487,
      "step": 874200
    },
    {
      "epoch": 13.725274725274724,
      "grad_norm": 3.330688238143921,
      "learning_rate": 4.14217032967033e-05,
      "loss": 0.6177,
      "step": 874300
    },
    {
      "epoch": 13.726844583987441,
      "grad_norm": 3.424138069152832,
      "learning_rate": 4.142072213500785e-05,
      "loss": 0.5717,
      "step": 874400
    },
    {
      "epoch": 13.728414442700156,
      "grad_norm": 4.921342849731445,
      "learning_rate": 4.14197409733124e-05,
      "loss": 0.6289,
      "step": 874500
    },
    {
      "epoch": 13.729984301412873,
      "grad_norm": 3.735985279083252,
      "learning_rate": 4.141875981161696e-05,
      "loss": 0.6217,
      "step": 874600
    },
    {
      "epoch": 13.731554160125588,
      "grad_norm": 3.3486523628234863,
      "learning_rate": 4.1417778649921504e-05,
      "loss": 0.6098,
      "step": 874700
    },
    {
      "epoch": 13.733124018838305,
      "grad_norm": 4.248642921447754,
      "learning_rate": 4.141679748822606e-05,
      "loss": 0.5947,
      "step": 874800
    },
    {
      "epoch": 13.73469387755102,
      "grad_norm": 4.392847537994385,
      "learning_rate": 4.141581632653061e-05,
      "loss": 0.6541,
      "step": 874900
    },
    {
      "epoch": 13.736263736263737,
      "grad_norm": 5.04516077041626,
      "learning_rate": 4.141483516483517e-05,
      "loss": 0.6256,
      "step": 875000
    },
    {
      "epoch": 13.737833594976452,
      "grad_norm": 4.133009433746338,
      "learning_rate": 4.1413854003139715e-05,
      "loss": 0.6281,
      "step": 875100
    },
    {
      "epoch": 13.739403453689167,
      "grad_norm": 4.193814277648926,
      "learning_rate": 4.141287284144427e-05,
      "loss": 0.6012,
      "step": 875200
    },
    {
      "epoch": 13.740973312401884,
      "grad_norm": 4.444844722747803,
      "learning_rate": 4.1411891679748824e-05,
      "loss": 0.6599,
      "step": 875300
    },
    {
      "epoch": 13.7425431711146,
      "grad_norm": 3.0414369106292725,
      "learning_rate": 4.1410910518053375e-05,
      "loss": 0.6193,
      "step": 875400
    },
    {
      "epoch": 13.744113029827316,
      "grad_norm": 2.4369184970855713,
      "learning_rate": 4.140992935635793e-05,
      "loss": 0.6052,
      "step": 875500
    },
    {
      "epoch": 13.745682888540031,
      "grad_norm": 3.976290225982666,
      "learning_rate": 4.140894819466248e-05,
      "loss": 0.6081,
      "step": 875600
    },
    {
      "epoch": 13.747252747252748,
      "grad_norm": 4.288083553314209,
      "learning_rate": 4.140796703296704e-05,
      "loss": 0.6171,
      "step": 875700
    },
    {
      "epoch": 13.748822605965463,
      "grad_norm": 3.758220911026001,
      "learning_rate": 4.1406985871271585e-05,
      "loss": 0.6127,
      "step": 875800
    },
    {
      "epoch": 13.750392464678178,
      "grad_norm": 3.6296069622039795,
      "learning_rate": 4.140600470957614e-05,
      "loss": 0.5742,
      "step": 875900
    },
    {
      "epoch": 13.751962323390895,
      "grad_norm": 3.805086135864258,
      "learning_rate": 4.1405023547880694e-05,
      "loss": 0.6503,
      "step": 876000
    },
    {
      "epoch": 13.75353218210361,
      "grad_norm": 2.866075277328491,
      "learning_rate": 4.1404042386185245e-05,
      "loss": 0.6298,
      "step": 876100
    },
    {
      "epoch": 13.755102040816327,
      "grad_norm": 2.6900417804718018,
      "learning_rate": 4.1403061224489796e-05,
      "loss": 0.6063,
      "step": 876200
    },
    {
      "epoch": 13.756671899529042,
      "grad_norm": 2.9949612617492676,
      "learning_rate": 4.1402080062794354e-05,
      "loss": 0.6408,
      "step": 876300
    },
    {
      "epoch": 13.758241758241759,
      "grad_norm": 3.049363374710083,
      "learning_rate": 4.14010989010989e-05,
      "loss": 0.6211,
      "step": 876400
    },
    {
      "epoch": 13.759811616954474,
      "grad_norm": 3.395503520965576,
      "learning_rate": 4.1400117739403456e-05,
      "loss": 0.5839,
      "step": 876500
    },
    {
      "epoch": 13.76138147566719,
      "grad_norm": 3.4269962310791016,
      "learning_rate": 4.139913657770801e-05,
      "loss": 0.6369,
      "step": 876600
    },
    {
      "epoch": 13.762951334379906,
      "grad_norm": 2.985800266265869,
      "learning_rate": 4.1398155416012564e-05,
      "loss": 0.5833,
      "step": 876700
    },
    {
      "epoch": 13.764521193092621,
      "grad_norm": 4.458685874938965,
      "learning_rate": 4.139717425431711e-05,
      "loss": 0.6437,
      "step": 876800
    },
    {
      "epoch": 13.766091051805338,
      "grad_norm": 4.454615116119385,
      "learning_rate": 4.1396193092621666e-05,
      "loss": 0.6108,
      "step": 876900
    },
    {
      "epoch": 13.767660910518053,
      "grad_norm": 3.0964860916137695,
      "learning_rate": 4.139521193092622e-05,
      "loss": 0.6078,
      "step": 877000
    },
    {
      "epoch": 13.76923076923077,
      "grad_norm": 2.7151284217834473,
      "learning_rate": 4.139423076923077e-05,
      "loss": 0.598,
      "step": 877100
    },
    {
      "epoch": 13.770800627943485,
      "grad_norm": 3.9188196659088135,
      "learning_rate": 4.139324960753532e-05,
      "loss": 0.5995,
      "step": 877200
    },
    {
      "epoch": 13.7723704866562,
      "grad_norm": 4.748109817504883,
      "learning_rate": 4.139226844583988e-05,
      "loss": 0.6083,
      "step": 877300
    },
    {
      "epoch": 13.773940345368917,
      "grad_norm": 4.174896717071533,
      "learning_rate": 4.139128728414443e-05,
      "loss": 0.6186,
      "step": 877400
    },
    {
      "epoch": 13.775510204081632,
      "grad_norm": 4.0239973068237305,
      "learning_rate": 4.139030612244898e-05,
      "loss": 0.6508,
      "step": 877500
    },
    {
      "epoch": 13.777080062794349,
      "grad_norm": 4.419790744781494,
      "learning_rate": 4.138932496075354e-05,
      "loss": 0.6181,
      "step": 877600
    },
    {
      "epoch": 13.778649921507064,
      "grad_norm": 3.3522863388061523,
      "learning_rate": 4.138834379905809e-05,
      "loss": 0.6416,
      "step": 877700
    },
    {
      "epoch": 13.780219780219781,
      "grad_norm": 4.599416732788086,
      "learning_rate": 4.138736263736264e-05,
      "loss": 0.6439,
      "step": 877800
    },
    {
      "epoch": 13.781789638932496,
      "grad_norm": 4.1728386878967285,
      "learning_rate": 4.138638147566719e-05,
      "loss": 0.6136,
      "step": 877900
    },
    {
      "epoch": 13.783359497645211,
      "grad_norm": 4.476984977722168,
      "learning_rate": 4.138540031397175e-05,
      "loss": 0.5978,
      "step": 878000
    },
    {
      "epoch": 13.784929356357928,
      "grad_norm": 4.494042873382568,
      "learning_rate": 4.13844191522763e-05,
      "loss": 0.6122,
      "step": 878100
    },
    {
      "epoch": 13.786499215070643,
      "grad_norm": 3.768183946609497,
      "learning_rate": 4.138343799058085e-05,
      "loss": 0.6334,
      "step": 878200
    },
    {
      "epoch": 13.78806907378336,
      "grad_norm": 3.3509702682495117,
      "learning_rate": 4.13824568288854e-05,
      "loss": 0.6486,
      "step": 878300
    },
    {
      "epoch": 13.789638932496075,
      "grad_norm": 4.684186935424805,
      "learning_rate": 4.138147566718996e-05,
      "loss": 0.6656,
      "step": 878400
    },
    {
      "epoch": 13.791208791208792,
      "grad_norm": 3.4712588787078857,
      "learning_rate": 4.13804945054945e-05,
      "loss": 0.6441,
      "step": 878500
    },
    {
      "epoch": 13.792778649921507,
      "grad_norm": 3.6664438247680664,
      "learning_rate": 4.137951334379906e-05,
      "loss": 0.5899,
      "step": 878600
    },
    {
      "epoch": 13.794348508634222,
      "grad_norm": 3.2482030391693115,
      "learning_rate": 4.137853218210361e-05,
      "loss": 0.6435,
      "step": 878700
    },
    {
      "epoch": 13.795918367346939,
      "grad_norm": 3.5678493976593018,
      "learning_rate": 4.137755102040817e-05,
      "loss": 0.6088,
      "step": 878800
    },
    {
      "epoch": 13.797488226059654,
      "grad_norm": 4.158816337585449,
      "learning_rate": 4.137656985871271e-05,
      "loss": 0.6036,
      "step": 878900
    },
    {
      "epoch": 13.799058084772371,
      "grad_norm": 2.5656371116638184,
      "learning_rate": 4.137558869701727e-05,
      "loss": 0.6136,
      "step": 879000
    },
    {
      "epoch": 13.800627943485086,
      "grad_norm": 3.4482126235961914,
      "learning_rate": 4.137460753532182e-05,
      "loss": 0.6255,
      "step": 879100
    },
    {
      "epoch": 13.802197802197803,
      "grad_norm": 3.3815836906433105,
      "learning_rate": 4.137362637362637e-05,
      "loss": 0.5678,
      "step": 879200
    },
    {
      "epoch": 13.803767660910518,
      "grad_norm": 4.279601097106934,
      "learning_rate": 4.1372645211930924e-05,
      "loss": 0.5953,
      "step": 879300
    },
    {
      "epoch": 13.805337519623233,
      "grad_norm": 2.0772902965545654,
      "learning_rate": 4.137166405023548e-05,
      "loss": 0.6001,
      "step": 879400
    },
    {
      "epoch": 13.80690737833595,
      "grad_norm": 3.9584434032440186,
      "learning_rate": 4.137068288854003e-05,
      "loss": 0.6269,
      "step": 879500
    },
    {
      "epoch": 13.808477237048665,
      "grad_norm": 4.486548900604248,
      "learning_rate": 4.1369701726844584e-05,
      "loss": 0.6097,
      "step": 879600
    },
    {
      "epoch": 13.810047095761382,
      "grad_norm": 5.619978427886963,
      "learning_rate": 4.136872056514914e-05,
      "loss": 0.5979,
      "step": 879700
    },
    {
      "epoch": 13.811616954474097,
      "grad_norm": 4.585819244384766,
      "learning_rate": 4.136773940345369e-05,
      "loss": 0.6496,
      "step": 879800
    },
    {
      "epoch": 13.813186813186814,
      "grad_norm": 5.034737586975098,
      "learning_rate": 4.136675824175824e-05,
      "loss": 0.6716,
      "step": 879900
    },
    {
      "epoch": 13.814756671899529,
      "grad_norm": 3.6940066814422607,
      "learning_rate": 4.1365777080062794e-05,
      "loss": 0.6208,
      "step": 880000
    },
    {
      "epoch": 13.816326530612244,
      "grad_norm": 3.7506020069122314,
      "learning_rate": 4.136479591836735e-05,
      "loss": 0.6082,
      "step": 880100
    },
    {
      "epoch": 13.817896389324961,
      "grad_norm": 4.02871561050415,
      "learning_rate": 4.13638147566719e-05,
      "loss": 0.6193,
      "step": 880200
    },
    {
      "epoch": 13.819466248037676,
      "grad_norm": 3.0785276889801025,
      "learning_rate": 4.1362833594976454e-05,
      "loss": 0.6472,
      "step": 880300
    },
    {
      "epoch": 13.821036106750393,
      "grad_norm": 4.048940181732178,
      "learning_rate": 4.1361852433281005e-05,
      "loss": 0.6575,
      "step": 880400
    },
    {
      "epoch": 13.822605965463108,
      "grad_norm": 3.3572328090667725,
      "learning_rate": 4.136087127158556e-05,
      "loss": 0.5885,
      "step": 880500
    },
    {
      "epoch": 13.824175824175825,
      "grad_norm": 3.21403431892395,
      "learning_rate": 4.135989010989011e-05,
      "loss": 0.6377,
      "step": 880600
    },
    {
      "epoch": 13.82574568288854,
      "grad_norm": 3.514629364013672,
      "learning_rate": 4.1358908948194665e-05,
      "loss": 0.5965,
      "step": 880700
    },
    {
      "epoch": 13.827315541601255,
      "grad_norm": 4.112508773803711,
      "learning_rate": 4.1357927786499216e-05,
      "loss": 0.6079,
      "step": 880800
    },
    {
      "epoch": 13.828885400313972,
      "grad_norm": 4.118741512298584,
      "learning_rate": 4.1356946624803773e-05,
      "loss": 0.6073,
      "step": 880900
    },
    {
      "epoch": 13.830455259026687,
      "grad_norm": 3.85386061668396,
      "learning_rate": 4.135596546310832e-05,
      "loss": 0.5822,
      "step": 881000
    },
    {
      "epoch": 13.832025117739404,
      "grad_norm": 3.754106044769287,
      "learning_rate": 4.1354984301412875e-05,
      "loss": 0.6441,
      "step": 881100
    },
    {
      "epoch": 13.833594976452119,
      "grad_norm": 3.702587604522705,
      "learning_rate": 4.1354003139717426e-05,
      "loss": 0.5822,
      "step": 881200
    },
    {
      "epoch": 13.835164835164836,
      "grad_norm": 4.725299835205078,
      "learning_rate": 4.135302197802198e-05,
      "loss": 0.6325,
      "step": 881300
    },
    {
      "epoch": 13.83673469387755,
      "grad_norm": 4.391994476318359,
      "learning_rate": 4.135204081632653e-05,
      "loss": 0.6255,
      "step": 881400
    },
    {
      "epoch": 13.838304552590268,
      "grad_norm": 3.401240587234497,
      "learning_rate": 4.1351059654631086e-05,
      "loss": 0.6703,
      "step": 881500
    },
    {
      "epoch": 13.839874411302983,
      "grad_norm": 3.2265825271606445,
      "learning_rate": 4.135007849293564e-05,
      "loss": 0.6101,
      "step": 881600
    },
    {
      "epoch": 13.841444270015698,
      "grad_norm": 4.275714874267578,
      "learning_rate": 4.134909733124019e-05,
      "loss": 0.6653,
      "step": 881700
    },
    {
      "epoch": 13.843014128728415,
      "grad_norm": 3.905834674835205,
      "learning_rate": 4.1348116169544746e-05,
      "loss": 0.6458,
      "step": 881800
    },
    {
      "epoch": 13.84458398744113,
      "grad_norm": 3.89005446434021,
      "learning_rate": 4.13471350078493e-05,
      "loss": 0.6553,
      "step": 881900
    },
    {
      "epoch": 13.846153846153847,
      "grad_norm": 4.115756034851074,
      "learning_rate": 4.134615384615385e-05,
      "loss": 0.6238,
      "step": 882000
    },
    {
      "epoch": 13.847723704866562,
      "grad_norm": 3.555976629257202,
      "learning_rate": 4.13451726844584e-05,
      "loss": 0.7021,
      "step": 882100
    },
    {
      "epoch": 13.849293563579279,
      "grad_norm": 4.242568492889404,
      "learning_rate": 4.1344191522762956e-05,
      "loss": 0.5991,
      "step": 882200
    },
    {
      "epoch": 13.850863422291994,
      "grad_norm": 3.524611234664917,
      "learning_rate": 4.134321036106751e-05,
      "loss": 0.5762,
      "step": 882300
    },
    {
      "epoch": 13.852433281004709,
      "grad_norm": 3.617267370223999,
      "learning_rate": 4.134222919937206e-05,
      "loss": 0.6355,
      "step": 882400
    },
    {
      "epoch": 13.854003139717426,
      "grad_norm": 4.348846912384033,
      "learning_rate": 4.134124803767661e-05,
      "loss": 0.6189,
      "step": 882500
    },
    {
      "epoch": 13.85557299843014,
      "grad_norm": 4.518646240234375,
      "learning_rate": 4.134026687598117e-05,
      "loss": 0.5912,
      "step": 882600
    },
    {
      "epoch": 13.857142857142858,
      "grad_norm": 2.730898380279541,
      "learning_rate": 4.133928571428571e-05,
      "loss": 0.6526,
      "step": 882700
    },
    {
      "epoch": 13.858712715855573,
      "grad_norm": 2.194533348083496,
      "learning_rate": 4.133830455259027e-05,
      "loss": 0.6155,
      "step": 882800
    },
    {
      "epoch": 13.86028257456829,
      "grad_norm": 4.005429744720459,
      "learning_rate": 4.133732339089482e-05,
      "loss": 0.6207,
      "step": 882900
    },
    {
      "epoch": 13.861852433281005,
      "grad_norm": 3.5941474437713623,
      "learning_rate": 4.133634222919938e-05,
      "loss": 0.6448,
      "step": 883000
    },
    {
      "epoch": 13.86342229199372,
      "grad_norm": 3.2818288803100586,
      "learning_rate": 4.133536106750392e-05,
      "loss": 0.5979,
      "step": 883100
    },
    {
      "epoch": 13.864992150706437,
      "grad_norm": 3.959268093109131,
      "learning_rate": 4.133437990580848e-05,
      "loss": 0.6415,
      "step": 883200
    },
    {
      "epoch": 13.866562009419152,
      "grad_norm": 3.5524253845214844,
      "learning_rate": 4.133339874411303e-05,
      "loss": 0.6326,
      "step": 883300
    },
    {
      "epoch": 13.868131868131869,
      "grad_norm": 4.2102952003479,
      "learning_rate": 4.133241758241758e-05,
      "loss": 0.61,
      "step": 883400
    },
    {
      "epoch": 13.869701726844584,
      "grad_norm": 3.021017551422119,
      "learning_rate": 4.133143642072213e-05,
      "loss": 0.6773,
      "step": 883500
    },
    {
      "epoch": 13.8712715855573,
      "grad_norm": 3.5344595909118652,
      "learning_rate": 4.133045525902669e-05,
      "loss": 0.6288,
      "step": 883600
    },
    {
      "epoch": 13.872841444270016,
      "grad_norm": 3.590520143508911,
      "learning_rate": 4.132947409733124e-05,
      "loss": 0.6291,
      "step": 883700
    },
    {
      "epoch": 13.87441130298273,
      "grad_norm": 3.4606480598449707,
      "learning_rate": 4.132849293563579e-05,
      "loss": 0.577,
      "step": 883800
    },
    {
      "epoch": 13.875981161695448,
      "grad_norm": 3.0466115474700928,
      "learning_rate": 4.132751177394035e-05,
      "loss": 0.6086,
      "step": 883900
    },
    {
      "epoch": 13.877551020408163,
      "grad_norm": 3.8820648193359375,
      "learning_rate": 4.13265306122449e-05,
      "loss": 0.6466,
      "step": 884000
    },
    {
      "epoch": 13.87912087912088,
      "grad_norm": 3.7339870929718018,
      "learning_rate": 4.132554945054945e-05,
      "loss": 0.6083,
      "step": 884100
    },
    {
      "epoch": 13.880690737833595,
      "grad_norm": 5.112161159515381,
      "learning_rate": 4.1324568288854e-05,
      "loss": 0.6472,
      "step": 884200
    },
    {
      "epoch": 13.882260596546312,
      "grad_norm": 3.2094290256500244,
      "learning_rate": 4.132358712715856e-05,
      "loss": 0.6364,
      "step": 884300
    },
    {
      "epoch": 13.883830455259027,
      "grad_norm": 4.46509313583374,
      "learning_rate": 4.132260596546311e-05,
      "loss": 0.6084,
      "step": 884400
    },
    {
      "epoch": 13.885400313971743,
      "grad_norm": 3.8583545684814453,
      "learning_rate": 4.132162480376766e-05,
      "loss": 0.6524,
      "step": 884500
    },
    {
      "epoch": 13.886970172684459,
      "grad_norm": 3.482727289199829,
      "learning_rate": 4.1320643642072214e-05,
      "loss": 0.578,
      "step": 884600
    },
    {
      "epoch": 13.888540031397174,
      "grad_norm": 4.682808876037598,
      "learning_rate": 4.131966248037677e-05,
      "loss": 0.5972,
      "step": 884700
    },
    {
      "epoch": 13.89010989010989,
      "grad_norm": 3.7225680351257324,
      "learning_rate": 4.1318681318681316e-05,
      "loss": 0.608,
      "step": 884800
    },
    {
      "epoch": 13.891679748822606,
      "grad_norm": 4.7845354080200195,
      "learning_rate": 4.1317700156985874e-05,
      "loss": 0.5936,
      "step": 884900
    },
    {
      "epoch": 13.893249607535322,
      "grad_norm": 4.956888198852539,
      "learning_rate": 4.1316718995290425e-05,
      "loss": 0.6129,
      "step": 885000
    },
    {
      "epoch": 13.894819466248038,
      "grad_norm": 4.053058624267578,
      "learning_rate": 4.131573783359498e-05,
      "loss": 0.6295,
      "step": 885100
    },
    {
      "epoch": 13.896389324960754,
      "grad_norm": 3.789750337600708,
      "learning_rate": 4.1314756671899527e-05,
      "loss": 0.6121,
      "step": 885200
    },
    {
      "epoch": 13.89795918367347,
      "grad_norm": 2.8253560066223145,
      "learning_rate": 4.1313775510204084e-05,
      "loss": 0.6004,
      "step": 885300
    },
    {
      "epoch": 13.899529042386185,
      "grad_norm": 4.2285380363464355,
      "learning_rate": 4.1312794348508635e-05,
      "loss": 0.6055,
      "step": 885400
    },
    {
      "epoch": 13.901098901098901,
      "grad_norm": 4.1252121925354,
      "learning_rate": 4.1311813186813186e-05,
      "loss": 0.6008,
      "step": 885500
    },
    {
      "epoch": 13.902668759811617,
      "grad_norm": 3.84574556350708,
      "learning_rate": 4.131083202511774e-05,
      "loss": 0.6069,
      "step": 885600
    },
    {
      "epoch": 13.904238618524333,
      "grad_norm": 3.156850576400757,
      "learning_rate": 4.1309850863422295e-05,
      "loss": 0.6677,
      "step": 885700
    },
    {
      "epoch": 13.905808477237048,
      "grad_norm": 4.521552085876465,
      "learning_rate": 4.1308869701726846e-05,
      "loss": 0.6384,
      "step": 885800
    },
    {
      "epoch": 13.907378335949765,
      "grad_norm": 3.3206818103790283,
      "learning_rate": 4.13078885400314e-05,
      "loss": 0.6681,
      "step": 885900
    },
    {
      "epoch": 13.90894819466248,
      "grad_norm": 3.0781049728393555,
      "learning_rate": 4.1306907378335955e-05,
      "loss": 0.6239,
      "step": 886000
    },
    {
      "epoch": 13.910518053375196,
      "grad_norm": 3.6255435943603516,
      "learning_rate": 4.1305926216640506e-05,
      "loss": 0.6054,
      "step": 886100
    },
    {
      "epoch": 13.912087912087912,
      "grad_norm": 2.5961976051330566,
      "learning_rate": 4.130494505494506e-05,
      "loss": 0.624,
      "step": 886200
    },
    {
      "epoch": 13.913657770800627,
      "grad_norm": 4.445897579193115,
      "learning_rate": 4.130396389324961e-05,
      "loss": 0.6109,
      "step": 886300
    },
    {
      "epoch": 13.915227629513344,
      "grad_norm": 4.127620220184326,
      "learning_rate": 4.1302982731554165e-05,
      "loss": 0.6529,
      "step": 886400
    },
    {
      "epoch": 13.91679748822606,
      "grad_norm": 4.0810675621032715,
      "learning_rate": 4.1302001569858716e-05,
      "loss": 0.6388,
      "step": 886500
    },
    {
      "epoch": 13.918367346938776,
      "grad_norm": 3.8154642581939697,
      "learning_rate": 4.130102040816327e-05,
      "loss": 0.642,
      "step": 886600
    },
    {
      "epoch": 13.919937205651491,
      "grad_norm": 4.131986141204834,
      "learning_rate": 4.130003924646782e-05,
      "loss": 0.6359,
      "step": 886700
    },
    {
      "epoch": 13.921507064364206,
      "grad_norm": 4.624607563018799,
      "learning_rate": 4.1299058084772376e-05,
      "loss": 0.5941,
      "step": 886800
    },
    {
      "epoch": 13.923076923076923,
      "grad_norm": 3.978881597518921,
      "learning_rate": 4.129807692307692e-05,
      "loss": 0.6638,
      "step": 886900
    },
    {
      "epoch": 13.924646781789638,
      "grad_norm": 4.065727710723877,
      "learning_rate": 4.129709576138148e-05,
      "loss": 0.6321,
      "step": 887000
    },
    {
      "epoch": 13.926216640502355,
      "grad_norm": 4.357339382171631,
      "learning_rate": 4.129611459968603e-05,
      "loss": 0.6311,
      "step": 887100
    },
    {
      "epoch": 13.92778649921507,
      "grad_norm": 3.9018778800964355,
      "learning_rate": 4.129513343799059e-05,
      "loss": 0.6195,
      "step": 887200
    },
    {
      "epoch": 13.929356357927787,
      "grad_norm": 5.0872626304626465,
      "learning_rate": 4.129415227629513e-05,
      "loss": 0.6267,
      "step": 887300
    },
    {
      "epoch": 13.930926216640502,
      "grad_norm": 3.512151002883911,
      "learning_rate": 4.129317111459969e-05,
      "loss": 0.6297,
      "step": 887400
    },
    {
      "epoch": 13.932496075353217,
      "grad_norm": 3.3852591514587402,
      "learning_rate": 4.129218995290424e-05,
      "loss": 0.6391,
      "step": 887500
    },
    {
      "epoch": 13.934065934065934,
      "grad_norm": 3.955286741256714,
      "learning_rate": 4.129120879120879e-05,
      "loss": 0.5965,
      "step": 887600
    },
    {
      "epoch": 13.93563579277865,
      "grad_norm": 4.851460933685303,
      "learning_rate": 4.129022762951334e-05,
      "loss": 0.5866,
      "step": 887700
    },
    {
      "epoch": 13.937205651491366,
      "grad_norm": 3.0728158950805664,
      "learning_rate": 4.12892464678179e-05,
      "loss": 0.5951,
      "step": 887800
    },
    {
      "epoch": 13.938775510204081,
      "grad_norm": 3.1247875690460205,
      "learning_rate": 4.128826530612245e-05,
      "loss": 0.61,
      "step": 887900
    },
    {
      "epoch": 13.940345368916798,
      "grad_norm": 4.57983922958374,
      "learning_rate": 4.1287284144427e-05,
      "loss": 0.6655,
      "step": 888000
    },
    {
      "epoch": 13.941915227629513,
      "grad_norm": 3.922427177429199,
      "learning_rate": 4.128630298273156e-05,
      "loss": 0.6302,
      "step": 888100
    },
    {
      "epoch": 13.943485086342228,
      "grad_norm": 4.509331226348877,
      "learning_rate": 4.128532182103611e-05,
      "loss": 0.5928,
      "step": 888200
    },
    {
      "epoch": 13.945054945054945,
      "grad_norm": 3.38510799407959,
      "learning_rate": 4.128434065934066e-05,
      "loss": 0.6349,
      "step": 888300
    },
    {
      "epoch": 13.94662480376766,
      "grad_norm": 3.128849506378174,
      "learning_rate": 4.128335949764521e-05,
      "loss": 0.6205,
      "step": 888400
    },
    {
      "epoch": 13.948194662480377,
      "grad_norm": 4.085224151611328,
      "learning_rate": 4.128237833594977e-05,
      "loss": 0.6209,
      "step": 888500
    },
    {
      "epoch": 13.949764521193092,
      "grad_norm": 4.351040840148926,
      "learning_rate": 4.128139717425432e-05,
      "loss": 0.6836,
      "step": 888600
    },
    {
      "epoch": 13.95133437990581,
      "grad_norm": 4.156578540802002,
      "learning_rate": 4.128041601255887e-05,
      "loss": 0.6264,
      "step": 888700
    },
    {
      "epoch": 13.952904238618524,
      "grad_norm": 2.9283533096313477,
      "learning_rate": 4.127943485086342e-05,
      "loss": 0.6368,
      "step": 888800
    },
    {
      "epoch": 13.95447409733124,
      "grad_norm": 3.5457029342651367,
      "learning_rate": 4.127845368916798e-05,
      "loss": 0.6053,
      "step": 888900
    },
    {
      "epoch": 13.956043956043956,
      "grad_norm": 3.627755880355835,
      "learning_rate": 4.1277472527472525e-05,
      "loss": 0.6157,
      "step": 889000
    },
    {
      "epoch": 13.957613814756671,
      "grad_norm": 3.9737608432769775,
      "learning_rate": 4.127649136577708e-05,
      "loss": 0.6797,
      "step": 889100
    },
    {
      "epoch": 13.959183673469388,
      "grad_norm": 3.647521734237671,
      "learning_rate": 4.1275510204081634e-05,
      "loss": 0.6316,
      "step": 889200
    },
    {
      "epoch": 13.960753532182103,
      "grad_norm": 3.421050548553467,
      "learning_rate": 4.127452904238619e-05,
      "loss": 0.6224,
      "step": 889300
    },
    {
      "epoch": 13.96232339089482,
      "grad_norm": 3.627943277359009,
      "learning_rate": 4.1273547880690736e-05,
      "loss": 0.5835,
      "step": 889400
    },
    {
      "epoch": 13.963893249607535,
      "grad_norm": 4.844555377960205,
      "learning_rate": 4.127256671899529e-05,
      "loss": 0.5923,
      "step": 889500
    },
    {
      "epoch": 13.96546310832025,
      "grad_norm": 4.388727188110352,
      "learning_rate": 4.1271585557299844e-05,
      "loss": 0.631,
      "step": 889600
    },
    {
      "epoch": 13.967032967032967,
      "grad_norm": 3.9825456142425537,
      "learning_rate": 4.1270604395604395e-05,
      "loss": 0.5957,
      "step": 889700
    },
    {
      "epoch": 13.968602825745682,
      "grad_norm": 3.9268009662628174,
      "learning_rate": 4.1269623233908946e-05,
      "loss": 0.6526,
      "step": 889800
    },
    {
      "epoch": 13.970172684458399,
      "grad_norm": 3.5742437839508057,
      "learning_rate": 4.1268642072213504e-05,
      "loss": 0.596,
      "step": 889900
    },
    {
      "epoch": 13.971742543171114,
      "grad_norm": 4.219961166381836,
      "learning_rate": 4.1267660910518055e-05,
      "loss": 0.6083,
      "step": 890000
    },
    {
      "epoch": 13.973312401883831,
      "grad_norm": 4.210166931152344,
      "learning_rate": 4.1266679748822606e-05,
      "loss": 0.6047,
      "step": 890100
    },
    {
      "epoch": 13.974882260596546,
      "grad_norm": 4.154574394226074,
      "learning_rate": 4.1265698587127164e-05,
      "loss": 0.5934,
      "step": 890200
    },
    {
      "epoch": 13.976452119309261,
      "grad_norm": 4.029191970825195,
      "learning_rate": 4.1264717425431715e-05,
      "loss": 0.6106,
      "step": 890300
    },
    {
      "epoch": 13.978021978021978,
      "grad_norm": 3.773784875869751,
      "learning_rate": 4.1263736263736266e-05,
      "loss": 0.6373,
      "step": 890400
    },
    {
      "epoch": 13.979591836734693,
      "grad_norm": 3.721904754638672,
      "learning_rate": 4.126275510204082e-05,
      "loss": 0.6053,
      "step": 890500
    },
    {
      "epoch": 13.98116169544741,
      "grad_norm": 3.586777925491333,
      "learning_rate": 4.1261773940345374e-05,
      "loss": 0.6314,
      "step": 890600
    },
    {
      "epoch": 13.982731554160125,
      "grad_norm": 5.308624744415283,
      "learning_rate": 4.1260792778649925e-05,
      "loss": 0.6079,
      "step": 890700
    },
    {
      "epoch": 13.984301412872842,
      "grad_norm": 3.1782584190368652,
      "learning_rate": 4.1259811616954476e-05,
      "loss": 0.5933,
      "step": 890800
    },
    {
      "epoch": 13.985871271585557,
      "grad_norm": 4.176812171936035,
      "learning_rate": 4.125883045525903e-05,
      "loss": 0.6674,
      "step": 890900
    },
    {
      "epoch": 13.987441130298274,
      "grad_norm": 3.463038444519043,
      "learning_rate": 4.1257849293563585e-05,
      "loss": 0.6192,
      "step": 891000
    },
    {
      "epoch": 13.989010989010989,
      "grad_norm": 3.722048044204712,
      "learning_rate": 4.125686813186813e-05,
      "loss": 0.637,
      "step": 891100
    },
    {
      "epoch": 13.990580847723704,
      "grad_norm": 4.02461576461792,
      "learning_rate": 4.125588697017269e-05,
      "loss": 0.6247,
      "step": 891200
    },
    {
      "epoch": 13.992150706436421,
      "grad_norm": 3.8956384658813477,
      "learning_rate": 4.125490580847724e-05,
      "loss": 0.6151,
      "step": 891300
    },
    {
      "epoch": 13.993720565149136,
      "grad_norm": 2.723362684249878,
      "learning_rate": 4.1253924646781796e-05,
      "loss": 0.6484,
      "step": 891400
    },
    {
      "epoch": 13.995290423861853,
      "grad_norm": 4.170080184936523,
      "learning_rate": 4.125294348508634e-05,
      "loss": 0.6043,
      "step": 891500
    },
    {
      "epoch": 13.996860282574568,
      "grad_norm": 4.081011772155762,
      "learning_rate": 4.12519623233909e-05,
      "loss": 0.6539,
      "step": 891600
    },
    {
      "epoch": 13.998430141287285,
      "grad_norm": 4.010976314544678,
      "learning_rate": 4.125098116169545e-05,
      "loss": 0.6528,
      "step": 891700
    },
    {
      "epoch": 14.0,
      "grad_norm": 3.101231098175049,
      "learning_rate": 4.125e-05,
      "loss": 0.6265,
      "step": 891800
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.034849762916565,
      "eval_runtime": 14.7198,
      "eval_samples_per_second": 227.788,
      "eval_steps_per_second": 227.788,
      "step": 891800
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.4833980202674866,
      "eval_runtime": 280.5428,
      "eval_samples_per_second": 227.06,
      "eval_steps_per_second": 227.06,
      "step": 891800
    },
    {
      "epoch": 14.001569858712715,
      "grad_norm": 3.4305365085601807,
      "learning_rate": 4.124901883830455e-05,
      "loss": 0.5946,
      "step": 891900
    },
    {
      "epoch": 14.003139717425432,
      "grad_norm": 3.1827778816223145,
      "learning_rate": 4.124803767660911e-05,
      "loss": 0.6,
      "step": 892000
    },
    {
      "epoch": 14.004709576138147,
      "grad_norm": 3.0826497077941895,
      "learning_rate": 4.124705651491366e-05,
      "loss": 0.5801,
      "step": 892100
    },
    {
      "epoch": 14.006279434850864,
      "grad_norm": 5.339123725891113,
      "learning_rate": 4.124607535321821e-05,
      "loss": 0.621,
      "step": 892200
    },
    {
      "epoch": 14.007849293563579,
      "grad_norm": 4.623805046081543,
      "learning_rate": 4.124509419152277e-05,
      "loss": 0.5996,
      "step": 892300
    },
    {
      "epoch": 14.009419152276296,
      "grad_norm": 3.213940143585205,
      "learning_rate": 4.124411302982732e-05,
      "loss": 0.6481,
      "step": 892400
    },
    {
      "epoch": 14.010989010989011,
      "grad_norm": 3.1573803424835205,
      "learning_rate": 4.124313186813187e-05,
      "loss": 0.5977,
      "step": 892500
    },
    {
      "epoch": 14.012558869701726,
      "grad_norm": 3.504587411880493,
      "learning_rate": 4.124215070643642e-05,
      "loss": 0.6174,
      "step": 892600
    },
    {
      "epoch": 14.014128728414443,
      "grad_norm": 3.31231689453125,
      "learning_rate": 4.124116954474098e-05,
      "loss": 0.6468,
      "step": 892700
    },
    {
      "epoch": 14.015698587127158,
      "grad_norm": 3.950392246246338,
      "learning_rate": 4.124018838304553e-05,
      "loss": 0.6003,
      "step": 892800
    },
    {
      "epoch": 14.017268445839875,
      "grad_norm": 4.014091968536377,
      "learning_rate": 4.123920722135008e-05,
      "loss": 0.5936,
      "step": 892900
    },
    {
      "epoch": 14.01883830455259,
      "grad_norm": 3.2219302654266357,
      "learning_rate": 4.123822605965463e-05,
      "loss": 0.5946,
      "step": 893000
    },
    {
      "epoch": 14.020408163265307,
      "grad_norm": 3.3795294761657715,
      "learning_rate": 4.123724489795919e-05,
      "loss": 0.6298,
      "step": 893100
    },
    {
      "epoch": 14.021978021978022,
      "grad_norm": 3.219789505004883,
      "learning_rate": 4.1236263736263734e-05,
      "loss": 0.6214,
      "step": 893200
    },
    {
      "epoch": 14.023547880690737,
      "grad_norm": 4.984127044677734,
      "learning_rate": 4.123528257456829e-05,
      "loss": 0.6312,
      "step": 893300
    },
    {
      "epoch": 14.025117739403454,
      "grad_norm": 3.0690011978149414,
      "learning_rate": 4.123430141287284e-05,
      "loss": 0.5689,
      "step": 893400
    },
    {
      "epoch": 14.026687598116169,
      "grad_norm": 3.391077995300293,
      "learning_rate": 4.12333202511774e-05,
      "loss": 0.6078,
      "step": 893500
    },
    {
      "epoch": 14.028257456828886,
      "grad_norm": 4.350713729858398,
      "learning_rate": 4.1232339089481945e-05,
      "loss": 0.6259,
      "step": 893600
    },
    {
      "epoch": 14.029827315541601,
      "grad_norm": 3.596433639526367,
      "learning_rate": 4.12313579277865e-05,
      "loss": 0.5776,
      "step": 893700
    },
    {
      "epoch": 14.031397174254318,
      "grad_norm": 4.208102226257324,
      "learning_rate": 4.123037676609105e-05,
      "loss": 0.6067,
      "step": 893800
    },
    {
      "epoch": 14.032967032967033,
      "grad_norm": 3.885875701904297,
      "learning_rate": 4.1229395604395604e-05,
      "loss": 0.5978,
      "step": 893900
    },
    {
      "epoch": 14.034536891679748,
      "grad_norm": 4.2457990646362305,
      "learning_rate": 4.1228414442700155e-05,
      "loss": 0.6389,
      "step": 894000
    },
    {
      "epoch": 14.036106750392465,
      "grad_norm": 2.33186411857605,
      "learning_rate": 4.122743328100471e-05,
      "loss": 0.6355,
      "step": 894100
    },
    {
      "epoch": 14.03767660910518,
      "grad_norm": 4.482162952423096,
      "learning_rate": 4.1226452119309264e-05,
      "loss": 0.6282,
      "step": 894200
    },
    {
      "epoch": 14.039246467817897,
      "grad_norm": 3.448746919631958,
      "learning_rate": 4.1225470957613815e-05,
      "loss": 0.62,
      "step": 894300
    },
    {
      "epoch": 14.040816326530612,
      "grad_norm": 4.770039081573486,
      "learning_rate": 4.122448979591837e-05,
      "loss": 0.5951,
      "step": 894400
    },
    {
      "epoch": 14.042386185243329,
      "grad_norm": 4.496126174926758,
      "learning_rate": 4.1223508634222924e-05,
      "loss": 0.6521,
      "step": 894500
    },
    {
      "epoch": 14.043956043956044,
      "grad_norm": 3.8565056324005127,
      "learning_rate": 4.1222527472527475e-05,
      "loss": 0.6232,
      "step": 894600
    },
    {
      "epoch": 14.04552590266876,
      "grad_norm": 3.288360834121704,
      "learning_rate": 4.1221546310832026e-05,
      "loss": 0.5828,
      "step": 894700
    },
    {
      "epoch": 14.047095761381476,
      "grad_norm": 4.101823329925537,
      "learning_rate": 4.1220565149136583e-05,
      "loss": 0.6213,
      "step": 894800
    },
    {
      "epoch": 14.04866562009419,
      "grad_norm": 4.1174163818359375,
      "learning_rate": 4.1219583987441134e-05,
      "loss": 0.6146,
      "step": 894900
    },
    {
      "epoch": 14.050235478806908,
      "grad_norm": 3.5536155700683594,
      "learning_rate": 4.1218602825745685e-05,
      "loss": 0.5928,
      "step": 895000
    },
    {
      "epoch": 14.051805337519623,
      "grad_norm": 3.747431755065918,
      "learning_rate": 4.1217621664050236e-05,
      "loss": 0.6112,
      "step": 895100
    },
    {
      "epoch": 14.05337519623234,
      "grad_norm": 2.8824191093444824,
      "learning_rate": 4.1216640502354794e-05,
      "loss": 0.6302,
      "step": 895200
    },
    {
      "epoch": 14.054945054945055,
      "grad_norm": 3.719442844390869,
      "learning_rate": 4.121565934065934e-05,
      "loss": 0.6369,
      "step": 895300
    },
    {
      "epoch": 14.056514913657772,
      "grad_norm": 3.448390007019043,
      "learning_rate": 4.1214678178963896e-05,
      "loss": 0.6555,
      "step": 895400
    },
    {
      "epoch": 14.058084772370487,
      "grad_norm": 3.554858446121216,
      "learning_rate": 4.121369701726845e-05,
      "loss": 0.5978,
      "step": 895500
    },
    {
      "epoch": 14.059654631083202,
      "grad_norm": 4.474425315856934,
      "learning_rate": 4.1212715855573005e-05,
      "loss": 0.6004,
      "step": 895600
    },
    {
      "epoch": 14.061224489795919,
      "grad_norm": 2.9634597301483154,
      "learning_rate": 4.121173469387755e-05,
      "loss": 0.6365,
      "step": 895700
    },
    {
      "epoch": 14.062794348508634,
      "grad_norm": 4.0481743812561035,
      "learning_rate": 4.121075353218211e-05,
      "loss": 0.6167,
      "step": 895800
    },
    {
      "epoch": 14.06436420722135,
      "grad_norm": 3.639559745788574,
      "learning_rate": 4.120977237048666e-05,
      "loss": 0.5996,
      "step": 895900
    },
    {
      "epoch": 14.065934065934066,
      "grad_norm": 4.76080322265625,
      "learning_rate": 4.120879120879121e-05,
      "loss": 0.6223,
      "step": 896000
    },
    {
      "epoch": 14.067503924646783,
      "grad_norm": 4.033660411834717,
      "learning_rate": 4.120781004709576e-05,
      "loss": 0.6575,
      "step": 896100
    },
    {
      "epoch": 14.069073783359498,
      "grad_norm": 2.609304666519165,
      "learning_rate": 4.120682888540032e-05,
      "loss": 0.6187,
      "step": 896200
    },
    {
      "epoch": 14.070643642072213,
      "grad_norm": 3.8082637786865234,
      "learning_rate": 4.120584772370487e-05,
      "loss": 0.5972,
      "step": 896300
    },
    {
      "epoch": 14.07221350078493,
      "grad_norm": 3.794262647628784,
      "learning_rate": 4.120486656200942e-05,
      "loss": 0.6091,
      "step": 896400
    },
    {
      "epoch": 14.073783359497645,
      "grad_norm": 4.201101779937744,
      "learning_rate": 4.120388540031398e-05,
      "loss": 0.61,
      "step": 896500
    },
    {
      "epoch": 14.075353218210362,
      "grad_norm": 3.6984543800354004,
      "learning_rate": 4.120290423861853e-05,
      "loss": 0.6327,
      "step": 896600
    },
    {
      "epoch": 14.076923076923077,
      "grad_norm": 4.185000896453857,
      "learning_rate": 4.120192307692308e-05,
      "loss": 0.5834,
      "step": 896700
    },
    {
      "epoch": 14.078492935635794,
      "grad_norm": 4.162154674530029,
      "learning_rate": 4.120094191522763e-05,
      "loss": 0.6367,
      "step": 896800
    },
    {
      "epoch": 14.080062794348509,
      "grad_norm": 4.459955215454102,
      "learning_rate": 4.119996075353219e-05,
      "loss": 0.6241,
      "step": 896900
    },
    {
      "epoch": 14.081632653061224,
      "grad_norm": 3.5786585807800293,
      "learning_rate": 4.119897959183674e-05,
      "loss": 0.58,
      "step": 897000
    },
    {
      "epoch": 14.08320251177394,
      "grad_norm": 2.5110678672790527,
      "learning_rate": 4.119799843014129e-05,
      "loss": 0.5941,
      "step": 897100
    },
    {
      "epoch": 14.084772370486656,
      "grad_norm": 4.3934125900268555,
      "learning_rate": 4.119701726844584e-05,
      "loss": 0.5923,
      "step": 897200
    },
    {
      "epoch": 14.086342229199373,
      "grad_norm": 3.9160335063934326,
      "learning_rate": 4.11960361067504e-05,
      "loss": 0.6214,
      "step": 897300
    },
    {
      "epoch": 14.087912087912088,
      "grad_norm": 3.3764383792877197,
      "learning_rate": 4.119505494505494e-05,
      "loss": 0.6234,
      "step": 897400
    },
    {
      "epoch": 14.089481946624804,
      "grad_norm": 3.6923935413360596,
      "learning_rate": 4.11940737833595e-05,
      "loss": 0.6152,
      "step": 897500
    },
    {
      "epoch": 14.09105180533752,
      "grad_norm": 4.083817005157471,
      "learning_rate": 4.119309262166405e-05,
      "loss": 0.601,
      "step": 897600
    },
    {
      "epoch": 14.092621664050235,
      "grad_norm": 3.5081703662872314,
      "learning_rate": 4.119211145996861e-05,
      "loss": 0.6232,
      "step": 897700
    },
    {
      "epoch": 14.094191522762952,
      "grad_norm": 3.507733106613159,
      "learning_rate": 4.1191130298273154e-05,
      "loss": 0.6434,
      "step": 897800
    },
    {
      "epoch": 14.095761381475667,
      "grad_norm": 5.051191329956055,
      "learning_rate": 4.119014913657771e-05,
      "loss": 0.5996,
      "step": 897900
    },
    {
      "epoch": 14.097331240188383,
      "grad_norm": 3.9991633892059326,
      "learning_rate": 4.118916797488226e-05,
      "loss": 0.6028,
      "step": 898000
    },
    {
      "epoch": 14.098901098901099,
      "grad_norm": 4.3492751121521,
      "learning_rate": 4.118818681318681e-05,
      "loss": 0.6246,
      "step": 898100
    },
    {
      "epoch": 14.100470957613815,
      "grad_norm": 3.374004602432251,
      "learning_rate": 4.1187205651491364e-05,
      "loss": 0.6181,
      "step": 898200
    },
    {
      "epoch": 14.10204081632653,
      "grad_norm": 3.769085645675659,
      "learning_rate": 4.118622448979592e-05,
      "loss": 0.6354,
      "step": 898300
    },
    {
      "epoch": 14.103610675039246,
      "grad_norm": 2.629990577697754,
      "learning_rate": 4.118524332810047e-05,
      "loss": 0.5698,
      "step": 898400
    },
    {
      "epoch": 14.105180533751962,
      "grad_norm": 3.970123291015625,
      "learning_rate": 4.1184262166405024e-05,
      "loss": 0.6409,
      "step": 898500
    },
    {
      "epoch": 14.106750392464678,
      "grad_norm": 4.427921295166016,
      "learning_rate": 4.118328100470958e-05,
      "loss": 0.6367,
      "step": 898600
    },
    {
      "epoch": 14.108320251177394,
      "grad_norm": 3.511837959289551,
      "learning_rate": 4.118229984301413e-05,
      "loss": 0.6291,
      "step": 898700
    },
    {
      "epoch": 14.10989010989011,
      "grad_norm": 2.6761703491210938,
      "learning_rate": 4.1181318681318684e-05,
      "loss": 0.6226,
      "step": 898800
    },
    {
      "epoch": 14.111459968602826,
      "grad_norm": 4.415300369262695,
      "learning_rate": 4.1180337519623235e-05,
      "loss": 0.5926,
      "step": 898900
    },
    {
      "epoch": 14.113029827315541,
      "grad_norm": 2.7084596157073975,
      "learning_rate": 4.117935635792779e-05,
      "loss": 0.6157,
      "step": 899000
    },
    {
      "epoch": 14.114599686028258,
      "grad_norm": 3.699979543685913,
      "learning_rate": 4.117837519623234e-05,
      "loss": 0.6151,
      "step": 899100
    },
    {
      "epoch": 14.116169544740973,
      "grad_norm": 3.158151149749756,
      "learning_rate": 4.1177394034536894e-05,
      "loss": 0.638,
      "step": 899200
    },
    {
      "epoch": 14.117739403453688,
      "grad_norm": 3.6402587890625,
      "learning_rate": 4.1176412872841445e-05,
      "loss": 0.6123,
      "step": 899300
    },
    {
      "epoch": 14.119309262166405,
      "grad_norm": 2.254021167755127,
      "learning_rate": 4.1175431711146e-05,
      "loss": 0.6289,
      "step": 899400
    },
    {
      "epoch": 14.12087912087912,
      "grad_norm": 3.428976058959961,
      "learning_rate": 4.117445054945055e-05,
      "loss": 0.6198,
      "step": 899500
    },
    {
      "epoch": 14.122448979591837,
      "grad_norm": 5.1031904220581055,
      "learning_rate": 4.1173469387755105e-05,
      "loss": 0.5955,
      "step": 899600
    },
    {
      "epoch": 14.124018838304552,
      "grad_norm": 3.7948787212371826,
      "learning_rate": 4.1172488226059656e-05,
      "loss": 0.6347,
      "step": 899700
    },
    {
      "epoch": 14.12558869701727,
      "grad_norm": 3.650325059890747,
      "learning_rate": 4.117150706436421e-05,
      "loss": 0.5925,
      "step": 899800
    },
    {
      "epoch": 14.127158555729984,
      "grad_norm": 3.344526529312134,
      "learning_rate": 4.117052590266876e-05,
      "loss": 0.5994,
      "step": 899900
    },
    {
      "epoch": 14.1287284144427,
      "grad_norm": 3.002060651779175,
      "learning_rate": 4.1169544740973316e-05,
      "loss": 0.6356,
      "step": 900000
    },
    {
      "epoch": 14.130298273155416,
      "grad_norm": 4.098880290985107,
      "learning_rate": 4.116856357927787e-05,
      "loss": 0.6261,
      "step": 900100
    },
    {
      "epoch": 14.131868131868131,
      "grad_norm": 4.125314712524414,
      "learning_rate": 4.116758241758242e-05,
      "loss": 0.6253,
      "step": 900200
    },
    {
      "epoch": 14.133437990580848,
      "grad_norm": 3.790090560913086,
      "learning_rate": 4.116660125588697e-05,
      "loss": 0.6237,
      "step": 900300
    },
    {
      "epoch": 14.135007849293563,
      "grad_norm": 3.1664555072784424,
      "learning_rate": 4.1165620094191526e-05,
      "loss": 0.6426,
      "step": 900400
    },
    {
      "epoch": 14.13657770800628,
      "grad_norm": 3.8717939853668213,
      "learning_rate": 4.116463893249608e-05,
      "loss": 0.6277,
      "step": 900500
    },
    {
      "epoch": 14.138147566718995,
      "grad_norm": 3.1916658878326416,
      "learning_rate": 4.116365777080063e-05,
      "loss": 0.5997,
      "step": 900600
    },
    {
      "epoch": 14.13971742543171,
      "grad_norm": 3.5324666500091553,
      "learning_rate": 4.1162676609105186e-05,
      "loss": 0.6746,
      "step": 900700
    },
    {
      "epoch": 14.141287284144427,
      "grad_norm": 3.7179388999938965,
      "learning_rate": 4.116169544740974e-05,
      "loss": 0.6055,
      "step": 900800
    },
    {
      "epoch": 14.142857142857142,
      "grad_norm": 3.3993685245513916,
      "learning_rate": 4.116071428571429e-05,
      "loss": 0.6419,
      "step": 900900
    },
    {
      "epoch": 14.14442700156986,
      "grad_norm": 3.1122496128082275,
      "learning_rate": 4.115973312401884e-05,
      "loss": 0.6401,
      "step": 901000
    },
    {
      "epoch": 14.145996860282574,
      "grad_norm": 4.171176910400391,
      "learning_rate": 4.11587519623234e-05,
      "loss": 0.6346,
      "step": 901100
    },
    {
      "epoch": 14.147566718995291,
      "grad_norm": 4.576760292053223,
      "learning_rate": 4.115777080062794e-05,
      "loss": 0.6139,
      "step": 901200
    },
    {
      "epoch": 14.149136577708006,
      "grad_norm": 3.830491781234741,
      "learning_rate": 4.11567896389325e-05,
      "loss": 0.6352,
      "step": 901300
    },
    {
      "epoch": 14.150706436420721,
      "grad_norm": 3.763852834701538,
      "learning_rate": 4.115580847723705e-05,
      "loss": 0.6491,
      "step": 901400
    },
    {
      "epoch": 14.152276295133438,
      "grad_norm": 3.4564642906188965,
      "learning_rate": 4.115482731554161e-05,
      "loss": 0.6332,
      "step": 901500
    },
    {
      "epoch": 14.153846153846153,
      "grad_norm": 3.897141933441162,
      "learning_rate": 4.115384615384615e-05,
      "loss": 0.6419,
      "step": 901600
    },
    {
      "epoch": 14.15541601255887,
      "grad_norm": 3.688917875289917,
      "learning_rate": 4.115286499215071e-05,
      "loss": 0.6473,
      "step": 901700
    },
    {
      "epoch": 14.156985871271585,
      "grad_norm": 4.741372108459473,
      "learning_rate": 4.115188383045526e-05,
      "loss": 0.6255,
      "step": 901800
    },
    {
      "epoch": 14.158555729984302,
      "grad_norm": 3.477748155593872,
      "learning_rate": 4.115090266875981e-05,
      "loss": 0.5961,
      "step": 901900
    },
    {
      "epoch": 14.160125588697017,
      "grad_norm": 3.786378860473633,
      "learning_rate": 4.114992150706436e-05,
      "loss": 0.6227,
      "step": 902000
    },
    {
      "epoch": 14.161695447409732,
      "grad_norm": 4.007858753204346,
      "learning_rate": 4.114894034536892e-05,
      "loss": 0.635,
      "step": 902100
    },
    {
      "epoch": 14.16326530612245,
      "grad_norm": 4.170482158660889,
      "learning_rate": 4.114795918367347e-05,
      "loss": 0.5872,
      "step": 902200
    },
    {
      "epoch": 14.164835164835164,
      "grad_norm": 3.4314656257629395,
      "learning_rate": 4.114697802197802e-05,
      "loss": 0.6356,
      "step": 902300
    },
    {
      "epoch": 14.166405023547881,
      "grad_norm": 4.508997917175293,
      "learning_rate": 4.114599686028257e-05,
      "loss": 0.6281,
      "step": 902400
    },
    {
      "epoch": 14.167974882260596,
      "grad_norm": 3.368143081665039,
      "learning_rate": 4.114501569858713e-05,
      "loss": 0.612,
      "step": 902500
    },
    {
      "epoch": 14.169544740973313,
      "grad_norm": 3.8579509258270264,
      "learning_rate": 4.114403453689168e-05,
      "loss": 0.5768,
      "step": 902600
    },
    {
      "epoch": 14.171114599686028,
      "grad_norm": 4.2180047035217285,
      "learning_rate": 4.114305337519623e-05,
      "loss": 0.6364,
      "step": 902700
    },
    {
      "epoch": 14.172684458398743,
      "grad_norm": 4.471648216247559,
      "learning_rate": 4.114207221350079e-05,
      "loss": 0.6158,
      "step": 902800
    },
    {
      "epoch": 14.17425431711146,
      "grad_norm": 4.538547039031982,
      "learning_rate": 4.114109105180534e-05,
      "loss": 0.6297,
      "step": 902900
    },
    {
      "epoch": 14.175824175824175,
      "grad_norm": 3.0772321224212646,
      "learning_rate": 4.114010989010989e-05,
      "loss": 0.6074,
      "step": 903000
    },
    {
      "epoch": 14.177394034536892,
      "grad_norm": 4.278176307678223,
      "learning_rate": 4.1139128728414444e-05,
      "loss": 0.6243,
      "step": 903100
    },
    {
      "epoch": 14.178963893249607,
      "grad_norm": 3.0340490341186523,
      "learning_rate": 4.1138147566719e-05,
      "loss": 0.6126,
      "step": 903200
    },
    {
      "epoch": 14.180533751962324,
      "grad_norm": 4.528943061828613,
      "learning_rate": 4.1137166405023546e-05,
      "loss": 0.6718,
      "step": 903300
    },
    {
      "epoch": 14.182103610675039,
      "grad_norm": 4.495272159576416,
      "learning_rate": 4.11361852433281e-05,
      "loss": 0.6384,
      "step": 903400
    },
    {
      "epoch": 14.183673469387756,
      "grad_norm": 4.150025367736816,
      "learning_rate": 4.1135204081632654e-05,
      "loss": 0.5896,
      "step": 903500
    },
    {
      "epoch": 14.185243328100471,
      "grad_norm": 4.870229721069336,
      "learning_rate": 4.113422291993721e-05,
      "loss": 0.6705,
      "step": 903600
    },
    {
      "epoch": 14.186813186813186,
      "grad_norm": 4.19464635848999,
      "learning_rate": 4.1133241758241756e-05,
      "loss": 0.619,
      "step": 903700
    },
    {
      "epoch": 14.188383045525903,
      "grad_norm": 3.1429660320281982,
      "learning_rate": 4.1132260596546314e-05,
      "loss": 0.6242,
      "step": 903800
    },
    {
      "epoch": 14.189952904238618,
      "grad_norm": 4.697052955627441,
      "learning_rate": 4.1131279434850865e-05,
      "loss": 0.6311,
      "step": 903900
    },
    {
      "epoch": 14.191522762951335,
      "grad_norm": 3.6021671295166016,
      "learning_rate": 4.1130298273155416e-05,
      "loss": 0.5972,
      "step": 904000
    },
    {
      "epoch": 14.19309262166405,
      "grad_norm": 2.6201655864715576,
      "learning_rate": 4.112931711145997e-05,
      "loss": 0.6458,
      "step": 904100
    },
    {
      "epoch": 14.194662480376767,
      "grad_norm": 4.501588344573975,
      "learning_rate": 4.1128335949764525e-05,
      "loss": 0.6154,
      "step": 904200
    },
    {
      "epoch": 14.196232339089482,
      "grad_norm": 4.386496543884277,
      "learning_rate": 4.1127354788069076e-05,
      "loss": 0.609,
      "step": 904300
    },
    {
      "epoch": 14.197802197802197,
      "grad_norm": 2.8840811252593994,
      "learning_rate": 4.112637362637363e-05,
      "loss": 0.6306,
      "step": 904400
    },
    {
      "epoch": 14.199372056514914,
      "grad_norm": 4.691469669342041,
      "learning_rate": 4.112539246467818e-05,
      "loss": 0.6623,
      "step": 904500
    },
    {
      "epoch": 14.200941915227629,
      "grad_norm": 4.398937702178955,
      "learning_rate": 4.1124411302982735e-05,
      "loss": 0.6204,
      "step": 904600
    },
    {
      "epoch": 14.202511773940346,
      "grad_norm": 3.7565805912017822,
      "learning_rate": 4.1123430141287286e-05,
      "loss": 0.5924,
      "step": 904700
    },
    {
      "epoch": 14.204081632653061,
      "grad_norm": 3.6732676029205322,
      "learning_rate": 4.112244897959184e-05,
      "loss": 0.6327,
      "step": 904800
    },
    {
      "epoch": 14.205651491365778,
      "grad_norm": 4.362155914306641,
      "learning_rate": 4.1121467817896395e-05,
      "loss": 0.6104,
      "step": 904900
    },
    {
      "epoch": 14.207221350078493,
      "grad_norm": 3.7567882537841797,
      "learning_rate": 4.1120486656200946e-05,
      "loss": 0.6288,
      "step": 905000
    },
    {
      "epoch": 14.208791208791208,
      "grad_norm": 3.9295921325683594,
      "learning_rate": 4.11195054945055e-05,
      "loss": 0.5943,
      "step": 905100
    },
    {
      "epoch": 14.210361067503925,
      "grad_norm": 3.916856288909912,
      "learning_rate": 4.111852433281005e-05,
      "loss": 0.6792,
      "step": 905200
    },
    {
      "epoch": 14.21193092621664,
      "grad_norm": 2.5455362796783447,
      "learning_rate": 4.1117543171114606e-05,
      "loss": 0.6346,
      "step": 905300
    },
    {
      "epoch": 14.213500784929357,
      "grad_norm": 3.714785099029541,
      "learning_rate": 4.111656200941915e-05,
      "loss": 0.6397,
      "step": 905400
    },
    {
      "epoch": 14.215070643642072,
      "grad_norm": 3.5242350101470947,
      "learning_rate": 4.111558084772371e-05,
      "loss": 0.6409,
      "step": 905500
    },
    {
      "epoch": 14.216640502354789,
      "grad_norm": 3.2745542526245117,
      "learning_rate": 4.111459968602826e-05,
      "loss": 0.649,
      "step": 905600
    },
    {
      "epoch": 14.218210361067504,
      "grad_norm": 3.1608757972717285,
      "learning_rate": 4.1113618524332817e-05,
      "loss": 0.6349,
      "step": 905700
    },
    {
      "epoch": 14.219780219780219,
      "grad_norm": 3.4262752532958984,
      "learning_rate": 4.111263736263736e-05,
      "loss": 0.5957,
      "step": 905800
    },
    {
      "epoch": 14.221350078492936,
      "grad_norm": 5.563179016113281,
      "learning_rate": 4.111165620094192e-05,
      "loss": 0.6604,
      "step": 905900
    },
    {
      "epoch": 14.222919937205651,
      "grad_norm": 3.102843761444092,
      "learning_rate": 4.111067503924647e-05,
      "loss": 0.6449,
      "step": 906000
    },
    {
      "epoch": 14.224489795918368,
      "grad_norm": 3.5374696254730225,
      "learning_rate": 4.110969387755102e-05,
      "loss": 0.5941,
      "step": 906100
    },
    {
      "epoch": 14.226059654631083,
      "grad_norm": 3.4093174934387207,
      "learning_rate": 4.110871271585557e-05,
      "loss": 0.6287,
      "step": 906200
    },
    {
      "epoch": 14.2276295133438,
      "grad_norm": 3.7312610149383545,
      "learning_rate": 4.110773155416013e-05,
      "loss": 0.6227,
      "step": 906300
    },
    {
      "epoch": 14.229199372056515,
      "grad_norm": 4.222445011138916,
      "learning_rate": 4.110675039246468e-05,
      "loss": 0.6124,
      "step": 906400
    },
    {
      "epoch": 14.23076923076923,
      "grad_norm": 3.9734394550323486,
      "learning_rate": 4.110576923076923e-05,
      "loss": 0.5867,
      "step": 906500
    },
    {
      "epoch": 14.232339089481947,
      "grad_norm": 4.200252532958984,
      "learning_rate": 4.110478806907378e-05,
      "loss": 0.6497,
      "step": 906600
    },
    {
      "epoch": 14.233908948194662,
      "grad_norm": 2.933901309967041,
      "learning_rate": 4.110380690737834e-05,
      "loss": 0.6276,
      "step": 906700
    },
    {
      "epoch": 14.235478806907379,
      "grad_norm": 3.937488555908203,
      "learning_rate": 4.110282574568289e-05,
      "loss": 0.6237,
      "step": 906800
    },
    {
      "epoch": 14.237048665620094,
      "grad_norm": 3.441479444503784,
      "learning_rate": 4.110184458398744e-05,
      "loss": 0.5957,
      "step": 906900
    },
    {
      "epoch": 14.23861852433281,
      "grad_norm": 3.825221061706543,
      "learning_rate": 4.1100863422292e-05,
      "loss": 0.6277,
      "step": 907000
    },
    {
      "epoch": 14.240188383045526,
      "grad_norm": 4.175999641418457,
      "learning_rate": 4.109988226059655e-05,
      "loss": 0.6213,
      "step": 907100
    },
    {
      "epoch": 14.241758241758241,
      "grad_norm": 3.7804172039031982,
      "learning_rate": 4.10989010989011e-05,
      "loss": 0.5815,
      "step": 907200
    },
    {
      "epoch": 14.243328100470958,
      "grad_norm": 3.642454147338867,
      "learning_rate": 4.109791993720565e-05,
      "loss": 0.5697,
      "step": 907300
    },
    {
      "epoch": 14.244897959183673,
      "grad_norm": 3.369908571243286,
      "learning_rate": 4.109693877551021e-05,
      "loss": 0.627,
      "step": 907400
    },
    {
      "epoch": 14.24646781789639,
      "grad_norm": 4.671459197998047,
      "learning_rate": 4.1095957613814755e-05,
      "loss": 0.6239,
      "step": 907500
    },
    {
      "epoch": 14.248037676609105,
      "grad_norm": 3.225642442703247,
      "learning_rate": 4.109497645211931e-05,
      "loss": 0.6302,
      "step": 907600
    },
    {
      "epoch": 14.249607535321822,
      "grad_norm": 3.2752909660339355,
      "learning_rate": 4.109399529042386e-05,
      "loss": 0.5958,
      "step": 907700
    },
    {
      "epoch": 14.251177394034537,
      "grad_norm": 2.847175359725952,
      "learning_rate": 4.109301412872842e-05,
      "loss": 0.646,
      "step": 907800
    },
    {
      "epoch": 14.252747252747252,
      "grad_norm": 4.174875736236572,
      "learning_rate": 4.1092032967032965e-05,
      "loss": 0.6205,
      "step": 907900
    },
    {
      "epoch": 14.254317111459969,
      "grad_norm": 3.3114705085754395,
      "learning_rate": 4.109105180533752e-05,
      "loss": 0.6165,
      "step": 908000
    },
    {
      "epoch": 14.255886970172684,
      "grad_norm": 4.658018589019775,
      "learning_rate": 4.1090070643642074e-05,
      "loss": 0.6419,
      "step": 908100
    },
    {
      "epoch": 14.2574568288854,
      "grad_norm": 3.501970052719116,
      "learning_rate": 4.1089089481946625e-05,
      "loss": 0.653,
      "step": 908200
    },
    {
      "epoch": 14.259026687598116,
      "grad_norm": 4.036932468414307,
      "learning_rate": 4.1088108320251176e-05,
      "loss": 0.6342,
      "step": 908300
    },
    {
      "epoch": 14.260596546310833,
      "grad_norm": 3.897345781326294,
      "learning_rate": 4.1087127158555734e-05,
      "loss": 0.5908,
      "step": 908400
    },
    {
      "epoch": 14.262166405023548,
      "grad_norm": 3.937098741531372,
      "learning_rate": 4.1086145996860285e-05,
      "loss": 0.6145,
      "step": 908500
    },
    {
      "epoch": 14.263736263736265,
      "grad_norm": 3.8874545097351074,
      "learning_rate": 4.1085164835164836e-05,
      "loss": 0.6163,
      "step": 908600
    },
    {
      "epoch": 14.26530612244898,
      "grad_norm": 3.387335777282715,
      "learning_rate": 4.108418367346939e-05,
      "loss": 0.6249,
      "step": 908700
    },
    {
      "epoch": 14.266875981161695,
      "grad_norm": 4.412428379058838,
      "learning_rate": 4.1083202511773944e-05,
      "loss": 0.5761,
      "step": 908800
    },
    {
      "epoch": 14.268445839874412,
      "grad_norm": 3.4197330474853516,
      "learning_rate": 4.1082221350078495e-05,
      "loss": 0.6373,
      "step": 908900
    },
    {
      "epoch": 14.270015698587127,
      "grad_norm": 3.4299733638763428,
      "learning_rate": 4.1081240188383046e-05,
      "loss": 0.613,
      "step": 909000
    },
    {
      "epoch": 14.271585557299844,
      "grad_norm": 3.5780320167541504,
      "learning_rate": 4.1080259026687604e-05,
      "loss": 0.5866,
      "step": 909100
    },
    {
      "epoch": 14.273155416012559,
      "grad_norm": 5.529025554656982,
      "learning_rate": 4.1079277864992155e-05,
      "loss": 0.6321,
      "step": 909200
    },
    {
      "epoch": 14.274725274725276,
      "grad_norm": 4.175902843475342,
      "learning_rate": 4.1078296703296706e-05,
      "loss": 0.5941,
      "step": 909300
    },
    {
      "epoch": 14.27629513343799,
      "grad_norm": 4.31833553314209,
      "learning_rate": 4.107731554160126e-05,
      "loss": 0.6314,
      "step": 909400
    },
    {
      "epoch": 14.277864992150706,
      "grad_norm": 5.342425346374512,
      "learning_rate": 4.1076334379905815e-05,
      "loss": 0.6275,
      "step": 909500
    },
    {
      "epoch": 14.279434850863423,
      "grad_norm": 3.3479745388031006,
      "learning_rate": 4.107535321821036e-05,
      "loss": 0.6102,
      "step": 909600
    },
    {
      "epoch": 14.281004709576138,
      "grad_norm": 3.6520261764526367,
      "learning_rate": 4.107437205651492e-05,
      "loss": 0.6371,
      "step": 909700
    },
    {
      "epoch": 14.282574568288855,
      "grad_norm": 3.5515482425689697,
      "learning_rate": 4.107339089481947e-05,
      "loss": 0.643,
      "step": 909800
    },
    {
      "epoch": 14.28414442700157,
      "grad_norm": 3.820269823074341,
      "learning_rate": 4.1072409733124026e-05,
      "loss": 0.6359,
      "step": 909900
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 4.76835823059082,
      "learning_rate": 4.107142857142857e-05,
      "loss": 0.6285,
      "step": 910000
    },
    {
      "epoch": 14.287284144427002,
      "grad_norm": 3.501917839050293,
      "learning_rate": 4.107044740973313e-05,
      "loss": 0.6359,
      "step": 910100
    },
    {
      "epoch": 14.288854003139717,
      "grad_norm": 3.877478837966919,
      "learning_rate": 4.106946624803768e-05,
      "loss": 0.6231,
      "step": 910200
    },
    {
      "epoch": 14.290423861852434,
      "grad_norm": 3.2045884132385254,
      "learning_rate": 4.106848508634223e-05,
      "loss": 0.6531,
      "step": 910300
    },
    {
      "epoch": 14.291993720565149,
      "grad_norm": 3.251569986343384,
      "learning_rate": 4.106750392464678e-05,
      "loss": 0.6237,
      "step": 910400
    },
    {
      "epoch": 14.293563579277865,
      "grad_norm": 3.7154109477996826,
      "learning_rate": 4.106652276295134e-05,
      "loss": 0.6203,
      "step": 910500
    },
    {
      "epoch": 14.29513343799058,
      "grad_norm": 3.308074712753296,
      "learning_rate": 4.106554160125589e-05,
      "loss": 0.6088,
      "step": 910600
    },
    {
      "epoch": 14.296703296703297,
      "grad_norm": 3.8614611625671387,
      "learning_rate": 4.106456043956044e-05,
      "loss": 0.559,
      "step": 910700
    },
    {
      "epoch": 14.298273155416013,
      "grad_norm": 2.897073984146118,
      "learning_rate": 4.106357927786499e-05,
      "loss": 0.6408,
      "step": 910800
    },
    {
      "epoch": 14.299843014128728,
      "grad_norm": 4.29339599609375,
      "learning_rate": 4.106259811616955e-05,
      "loss": 0.6042,
      "step": 910900
    },
    {
      "epoch": 14.301412872841444,
      "grad_norm": 3.7485191822052,
      "learning_rate": 4.10616169544741e-05,
      "loss": 0.6351,
      "step": 911000
    },
    {
      "epoch": 14.30298273155416,
      "grad_norm": 3.592240333557129,
      "learning_rate": 4.106063579277865e-05,
      "loss": 0.6363,
      "step": 911100
    },
    {
      "epoch": 14.304552590266876,
      "grad_norm": 4.082974910736084,
      "learning_rate": 4.105965463108321e-05,
      "loss": 0.6291,
      "step": 911200
    },
    {
      "epoch": 14.306122448979592,
      "grad_norm": 4.352952480316162,
      "learning_rate": 4.105867346938776e-05,
      "loss": 0.5939,
      "step": 911300
    },
    {
      "epoch": 14.307692307692308,
      "grad_norm": 4.113898277282715,
      "learning_rate": 4.105769230769231e-05,
      "loss": 0.572,
      "step": 911400
    },
    {
      "epoch": 14.309262166405023,
      "grad_norm": 4.400912761688232,
      "learning_rate": 4.105671114599686e-05,
      "loss": 0.621,
      "step": 911500
    },
    {
      "epoch": 14.310832025117739,
      "grad_norm": 3.7047955989837646,
      "learning_rate": 4.105572998430142e-05,
      "loss": 0.6247,
      "step": 911600
    },
    {
      "epoch": 14.312401883830455,
      "grad_norm": 3.1797678470611572,
      "learning_rate": 4.1054748822605964e-05,
      "loss": 0.6258,
      "step": 911700
    },
    {
      "epoch": 14.31397174254317,
      "grad_norm": 4.4597487449646,
      "learning_rate": 4.105376766091052e-05,
      "loss": 0.6056,
      "step": 911800
    },
    {
      "epoch": 14.315541601255887,
      "grad_norm": 2.991602659225464,
      "learning_rate": 4.105278649921507e-05,
      "loss": 0.622,
      "step": 911900
    },
    {
      "epoch": 14.317111459968602,
      "grad_norm": 4.32965087890625,
      "learning_rate": 4.105180533751963e-05,
      "loss": 0.6571,
      "step": 912000
    },
    {
      "epoch": 14.31868131868132,
      "grad_norm": 3.600036144256592,
      "learning_rate": 4.1050824175824174e-05,
      "loss": 0.6324,
      "step": 912100
    },
    {
      "epoch": 14.320251177394034,
      "grad_norm": 4.555585861206055,
      "learning_rate": 4.104984301412873e-05,
      "loss": 0.6192,
      "step": 912200
    },
    {
      "epoch": 14.321821036106751,
      "grad_norm": 4.817983150482178,
      "learning_rate": 4.104886185243328e-05,
      "loss": 0.6039,
      "step": 912300
    },
    {
      "epoch": 14.323390894819466,
      "grad_norm": 3.0813491344451904,
      "learning_rate": 4.1047880690737834e-05,
      "loss": 0.6507,
      "step": 912400
    },
    {
      "epoch": 14.324960753532181,
      "grad_norm": 4.114577293395996,
      "learning_rate": 4.1046899529042385e-05,
      "loss": 0.6129,
      "step": 912500
    },
    {
      "epoch": 14.326530612244898,
      "grad_norm": 7.66464900970459,
      "learning_rate": 4.104591836734694e-05,
      "loss": 0.6518,
      "step": 912600
    },
    {
      "epoch": 14.328100470957613,
      "grad_norm": 4.799047946929932,
      "learning_rate": 4.1044937205651494e-05,
      "loss": 0.6521,
      "step": 912700
    },
    {
      "epoch": 14.32967032967033,
      "grad_norm": 4.084012508392334,
      "learning_rate": 4.1043956043956045e-05,
      "loss": 0.6431,
      "step": 912800
    },
    {
      "epoch": 14.331240188383045,
      "grad_norm": 4.230668067932129,
      "learning_rate": 4.1042974882260596e-05,
      "loss": 0.6039,
      "step": 912900
    },
    {
      "epoch": 14.332810047095762,
      "grad_norm": 4.893820762634277,
      "learning_rate": 4.1041993720565153e-05,
      "loss": 0.6114,
      "step": 913000
    },
    {
      "epoch": 14.334379905808477,
      "grad_norm": 4.013307094573975,
      "learning_rate": 4.1041012558869704e-05,
      "loss": 0.5985,
      "step": 913100
    },
    {
      "epoch": 14.335949764521192,
      "grad_norm": 4.790902614593506,
      "learning_rate": 4.1040031397174255e-05,
      "loss": 0.6353,
      "step": 913200
    },
    {
      "epoch": 14.33751962323391,
      "grad_norm": 3.4830243587493896,
      "learning_rate": 4.103905023547881e-05,
      "loss": 0.6156,
      "step": 913300
    },
    {
      "epoch": 14.339089481946624,
      "grad_norm": 3.759890556335449,
      "learning_rate": 4.1038069073783364e-05,
      "loss": 0.5976,
      "step": 913400
    },
    {
      "epoch": 14.340659340659341,
      "grad_norm": 3.1155452728271484,
      "learning_rate": 4.1037087912087915e-05,
      "loss": 0.6017,
      "step": 913500
    },
    {
      "epoch": 14.342229199372056,
      "grad_norm": 2.2233448028564453,
      "learning_rate": 4.1036106750392466e-05,
      "loss": 0.5938,
      "step": 913600
    },
    {
      "epoch": 14.343799058084773,
      "grad_norm": 4.096686363220215,
      "learning_rate": 4.1035125588697024e-05,
      "loss": 0.6372,
      "step": 913700
    },
    {
      "epoch": 14.345368916797488,
      "grad_norm": 4.76038122177124,
      "learning_rate": 4.103414442700157e-05,
      "loss": 0.6227,
      "step": 913800
    },
    {
      "epoch": 14.346938775510203,
      "grad_norm": 3.683870792388916,
      "learning_rate": 4.1033163265306126e-05,
      "loss": 0.6231,
      "step": 913900
    },
    {
      "epoch": 14.34850863422292,
      "grad_norm": 4.090984344482422,
      "learning_rate": 4.103218210361068e-05,
      "loss": 0.5988,
      "step": 914000
    },
    {
      "epoch": 14.350078492935635,
      "grad_norm": 4.087706565856934,
      "learning_rate": 4.1031200941915235e-05,
      "loss": 0.5868,
      "step": 914100
    },
    {
      "epoch": 14.351648351648352,
      "grad_norm": 3.596851348876953,
      "learning_rate": 4.103021978021978e-05,
      "loss": 0.635,
      "step": 914200
    },
    {
      "epoch": 14.353218210361067,
      "grad_norm": 3.898833751678467,
      "learning_rate": 4.1029238618524337e-05,
      "loss": 0.6192,
      "step": 914300
    },
    {
      "epoch": 14.354788069073784,
      "grad_norm": 3.787278413772583,
      "learning_rate": 4.102825745682889e-05,
      "loss": 0.6485,
      "step": 914400
    },
    {
      "epoch": 14.3563579277865,
      "grad_norm": 4.288930892944336,
      "learning_rate": 4.102727629513344e-05,
      "loss": 0.6006,
      "step": 914500
    },
    {
      "epoch": 14.357927786499214,
      "grad_norm": 4.423425674438477,
      "learning_rate": 4.102629513343799e-05,
      "loss": 0.622,
      "step": 914600
    },
    {
      "epoch": 14.359497645211931,
      "grad_norm": 3.8154611587524414,
      "learning_rate": 4.102531397174255e-05,
      "loss": 0.6507,
      "step": 914700
    },
    {
      "epoch": 14.361067503924646,
      "grad_norm": 4.238203525543213,
      "learning_rate": 4.10243328100471e-05,
      "loss": 0.6252,
      "step": 914800
    },
    {
      "epoch": 14.362637362637363,
      "grad_norm": 4.038053035736084,
      "learning_rate": 4.102335164835165e-05,
      "loss": 0.6191,
      "step": 914900
    },
    {
      "epoch": 14.364207221350078,
      "grad_norm": 4.209831237792969,
      "learning_rate": 4.10223704866562e-05,
      "loss": 0.6382,
      "step": 915000
    },
    {
      "epoch": 14.365777080062795,
      "grad_norm": 3.4617676734924316,
      "learning_rate": 4.102138932496076e-05,
      "loss": 0.5859,
      "step": 915100
    },
    {
      "epoch": 14.36734693877551,
      "grad_norm": 2.5734405517578125,
      "learning_rate": 4.102040816326531e-05,
      "loss": 0.5932,
      "step": 915200
    },
    {
      "epoch": 14.368916797488225,
      "grad_norm": 3.6943044662475586,
      "learning_rate": 4.101942700156986e-05,
      "loss": 0.6289,
      "step": 915300
    },
    {
      "epoch": 14.370486656200942,
      "grad_norm": 3.665477991104126,
      "learning_rate": 4.101844583987442e-05,
      "loss": 0.6416,
      "step": 915400
    },
    {
      "epoch": 14.372056514913657,
      "grad_norm": 3.771498680114746,
      "learning_rate": 4.101746467817897e-05,
      "loss": 0.6135,
      "step": 915500
    },
    {
      "epoch": 14.373626373626374,
      "grad_norm": 4.158509254455566,
      "learning_rate": 4.101648351648352e-05,
      "loss": 0.6294,
      "step": 915600
    },
    {
      "epoch": 14.37519623233909,
      "grad_norm": 3.85275936126709,
      "learning_rate": 4.101550235478807e-05,
      "loss": 0.6317,
      "step": 915700
    },
    {
      "epoch": 14.376766091051806,
      "grad_norm": 3.6199047565460205,
      "learning_rate": 4.101452119309263e-05,
      "loss": 0.5951,
      "step": 915800
    },
    {
      "epoch": 14.378335949764521,
      "grad_norm": 3.5158092975616455,
      "learning_rate": 4.101354003139717e-05,
      "loss": 0.6032,
      "step": 915900
    },
    {
      "epoch": 14.379905808477236,
      "grad_norm": 3.100492477416992,
      "learning_rate": 4.101255886970173e-05,
      "loss": 0.6199,
      "step": 916000
    },
    {
      "epoch": 14.381475667189953,
      "grad_norm": 4.152829170227051,
      "learning_rate": 4.101157770800628e-05,
      "loss": 0.6338,
      "step": 916100
    },
    {
      "epoch": 14.383045525902668,
      "grad_norm": 4.076558589935303,
      "learning_rate": 4.101059654631084e-05,
      "loss": 0.5985,
      "step": 916200
    },
    {
      "epoch": 14.384615384615385,
      "grad_norm": 2.544292449951172,
      "learning_rate": 4.100961538461538e-05,
      "loss": 0.6184,
      "step": 916300
    },
    {
      "epoch": 14.3861852433281,
      "grad_norm": 3.8601508140563965,
      "learning_rate": 4.100863422291994e-05,
      "loss": 0.631,
      "step": 916400
    },
    {
      "epoch": 14.387755102040817,
      "grad_norm": 4.6594929695129395,
      "learning_rate": 4.100765306122449e-05,
      "loss": 0.647,
      "step": 916500
    },
    {
      "epoch": 14.389324960753532,
      "grad_norm": 4.01566219329834,
      "learning_rate": 4.100667189952904e-05,
      "loss": 0.6455,
      "step": 916600
    },
    {
      "epoch": 14.390894819466247,
      "grad_norm": 2.9814016819000244,
      "learning_rate": 4.1005690737833594e-05,
      "loss": 0.594,
      "step": 916700
    },
    {
      "epoch": 14.392464678178964,
      "grad_norm": 4.556843280792236,
      "learning_rate": 4.100470957613815e-05,
      "loss": 0.6173,
      "step": 916800
    },
    {
      "epoch": 14.394034536891679,
      "grad_norm": 3.9940900802612305,
      "learning_rate": 4.10037284144427e-05,
      "loss": 0.6187,
      "step": 916900
    },
    {
      "epoch": 14.395604395604396,
      "grad_norm": 4.395738124847412,
      "learning_rate": 4.1002747252747254e-05,
      "loss": 0.6495,
      "step": 917000
    },
    {
      "epoch": 14.397174254317111,
      "grad_norm": 4.207683563232422,
      "learning_rate": 4.1001766091051805e-05,
      "loss": 0.6207,
      "step": 917100
    },
    {
      "epoch": 14.398744113029828,
      "grad_norm": 3.061340093612671,
      "learning_rate": 4.100078492935636e-05,
      "loss": 0.6366,
      "step": 917200
    },
    {
      "epoch": 14.400313971742543,
      "grad_norm": 4.25803804397583,
      "learning_rate": 4.099980376766091e-05,
      "loss": 0.5922,
      "step": 917300
    },
    {
      "epoch": 14.40188383045526,
      "grad_norm": 2.795588731765747,
      "learning_rate": 4.0998822605965464e-05,
      "loss": 0.5898,
      "step": 917400
    },
    {
      "epoch": 14.403453689167975,
      "grad_norm": 4.19483757019043,
      "learning_rate": 4.0997841444270015e-05,
      "loss": 0.6434,
      "step": 917500
    },
    {
      "epoch": 14.40502354788069,
      "grad_norm": 3.5024890899658203,
      "learning_rate": 4.099686028257457e-05,
      "loss": 0.5818,
      "step": 917600
    },
    {
      "epoch": 14.406593406593407,
      "grad_norm": 3.037170886993408,
      "learning_rate": 4.0995879120879124e-05,
      "loss": 0.6017,
      "step": 917700
    },
    {
      "epoch": 14.408163265306122,
      "grad_norm": 4.982255458831787,
      "learning_rate": 4.0994897959183675e-05,
      "loss": 0.647,
      "step": 917800
    },
    {
      "epoch": 14.409733124018839,
      "grad_norm": 3.943603038787842,
      "learning_rate": 4.099391679748823e-05,
      "loss": 0.6416,
      "step": 917900
    },
    {
      "epoch": 14.411302982731554,
      "grad_norm": 2.66961407661438,
      "learning_rate": 4.099293563579278e-05,
      "loss": 0.6435,
      "step": 918000
    },
    {
      "epoch": 14.41287284144427,
      "grad_norm": 3.24721622467041,
      "learning_rate": 4.0991954474097335e-05,
      "loss": 0.6409,
      "step": 918100
    },
    {
      "epoch": 14.414442700156986,
      "grad_norm": 4.89587926864624,
      "learning_rate": 4.0990973312401886e-05,
      "loss": 0.6322,
      "step": 918200
    },
    {
      "epoch": 14.416012558869701,
      "grad_norm": 2.751012086868286,
      "learning_rate": 4.0989992150706444e-05,
      "loss": 0.5979,
      "step": 918300
    },
    {
      "epoch": 14.417582417582418,
      "grad_norm": 2.8773820400238037,
      "learning_rate": 4.098901098901099e-05,
      "loss": 0.6263,
      "step": 918400
    },
    {
      "epoch": 14.419152276295133,
      "grad_norm": 3.2094459533691406,
      "learning_rate": 4.0988029827315545e-05,
      "loss": 0.6388,
      "step": 918500
    },
    {
      "epoch": 14.42072213500785,
      "grad_norm": 4.887709617614746,
      "learning_rate": 4.0987048665620096e-05,
      "loss": 0.602,
      "step": 918600
    },
    {
      "epoch": 14.422291993720565,
      "grad_norm": 3.89631724357605,
      "learning_rate": 4.098606750392465e-05,
      "loss": 0.6622,
      "step": 918700
    },
    {
      "epoch": 14.423861852433282,
      "grad_norm": 3.165295362472534,
      "learning_rate": 4.09850863422292e-05,
      "loss": 0.6073,
      "step": 918800
    },
    {
      "epoch": 14.425431711145997,
      "grad_norm": 4.337272644042969,
      "learning_rate": 4.0984105180533756e-05,
      "loss": 0.6479,
      "step": 918900
    },
    {
      "epoch": 14.427001569858712,
      "grad_norm": 4.640042304992676,
      "learning_rate": 4.098312401883831e-05,
      "loss": 0.65,
      "step": 919000
    },
    {
      "epoch": 14.428571428571429,
      "grad_norm": 3.6336779594421387,
      "learning_rate": 4.098214285714286e-05,
      "loss": 0.5992,
      "step": 919100
    },
    {
      "epoch": 14.430141287284144,
      "grad_norm": 3.0599443912506104,
      "learning_rate": 4.098116169544741e-05,
      "loss": 0.614,
      "step": 919200
    },
    {
      "epoch": 14.43171114599686,
      "grad_norm": 4.723449230194092,
      "learning_rate": 4.098018053375197e-05,
      "loss": 0.5933,
      "step": 919300
    },
    {
      "epoch": 14.433281004709576,
      "grad_norm": 3.446932554244995,
      "learning_rate": 4.097919937205652e-05,
      "loss": 0.6391,
      "step": 919400
    },
    {
      "epoch": 14.434850863422293,
      "grad_norm": 3.3116064071655273,
      "learning_rate": 4.097821821036107e-05,
      "loss": 0.6217,
      "step": 919500
    },
    {
      "epoch": 14.436420722135008,
      "grad_norm": 3.7078301906585693,
      "learning_rate": 4.097723704866562e-05,
      "loss": 0.5812,
      "step": 919600
    },
    {
      "epoch": 14.437990580847723,
      "grad_norm": 4.334987163543701,
      "learning_rate": 4.097625588697018e-05,
      "loss": 0.6083,
      "step": 919700
    },
    {
      "epoch": 14.43956043956044,
      "grad_norm": 4.080104827880859,
      "learning_rate": 4.097527472527473e-05,
      "loss": 0.6292,
      "step": 919800
    },
    {
      "epoch": 14.441130298273155,
      "grad_norm": 3.050719976425171,
      "learning_rate": 4.097429356357928e-05,
      "loss": 0.6215,
      "step": 919900
    },
    {
      "epoch": 14.442700156985872,
      "grad_norm": 3.6539695262908936,
      "learning_rate": 4.097331240188384e-05,
      "loss": 0.6538,
      "step": 920000
    },
    {
      "epoch": 14.444270015698587,
      "grad_norm": 5.02942419052124,
      "learning_rate": 4.097233124018838e-05,
      "loss": 0.606,
      "step": 920100
    },
    {
      "epoch": 14.445839874411304,
      "grad_norm": 3.5380537509918213,
      "learning_rate": 4.097135007849294e-05,
      "loss": 0.5964,
      "step": 920200
    },
    {
      "epoch": 14.447409733124019,
      "grad_norm": 3.9164352416992188,
      "learning_rate": 4.097036891679749e-05,
      "loss": 0.6188,
      "step": 920300
    },
    {
      "epoch": 14.448979591836734,
      "grad_norm": 3.569925546646118,
      "learning_rate": 4.096938775510205e-05,
      "loss": 0.6259,
      "step": 920400
    },
    {
      "epoch": 14.45054945054945,
      "grad_norm": 5.060993671417236,
      "learning_rate": 4.096840659340659e-05,
      "loss": 0.6291,
      "step": 920500
    },
    {
      "epoch": 14.452119309262166,
      "grad_norm": 3.8188111782073975,
      "learning_rate": 4.096742543171115e-05,
      "loss": 0.6249,
      "step": 920600
    },
    {
      "epoch": 14.453689167974883,
      "grad_norm": 3.9431943893432617,
      "learning_rate": 4.09664442700157e-05,
      "loss": 0.6153,
      "step": 920700
    },
    {
      "epoch": 14.455259026687598,
      "grad_norm": 3.592146635055542,
      "learning_rate": 4.096546310832025e-05,
      "loss": 0.6001,
      "step": 920800
    },
    {
      "epoch": 14.456828885400315,
      "grad_norm": 3.7731246948242188,
      "learning_rate": 4.09644819466248e-05,
      "loss": 0.6836,
      "step": 920900
    },
    {
      "epoch": 14.45839874411303,
      "grad_norm": 3.962185859680176,
      "learning_rate": 4.096350078492936e-05,
      "loss": 0.5582,
      "step": 921000
    },
    {
      "epoch": 14.459968602825747,
      "grad_norm": 3.9902968406677246,
      "learning_rate": 4.096251962323391e-05,
      "loss": 0.6433,
      "step": 921100
    },
    {
      "epoch": 14.461538461538462,
      "grad_norm": 3.8362035751342773,
      "learning_rate": 4.096153846153846e-05,
      "loss": 0.6529,
      "step": 921200
    },
    {
      "epoch": 14.463108320251177,
      "grad_norm": 4.931201934814453,
      "learning_rate": 4.0960557299843014e-05,
      "loss": 0.63,
      "step": 921300
    },
    {
      "epoch": 14.464678178963894,
      "grad_norm": 3.487738609313965,
      "learning_rate": 4.095957613814757e-05,
      "loss": 0.6169,
      "step": 921400
    },
    {
      "epoch": 14.466248037676609,
      "grad_norm": 3.42230224609375,
      "learning_rate": 4.095859497645212e-05,
      "loss": 0.6063,
      "step": 921500
    },
    {
      "epoch": 14.467817896389326,
      "grad_norm": 3.616307497024536,
      "learning_rate": 4.095761381475667e-05,
      "loss": 0.6013,
      "step": 921600
    },
    {
      "epoch": 14.46938775510204,
      "grad_norm": 4.100058555603027,
      "learning_rate": 4.0956632653061224e-05,
      "loss": 0.6357,
      "step": 921700
    },
    {
      "epoch": 14.470957613814758,
      "grad_norm": 3.488196611404419,
      "learning_rate": 4.0955651491365775e-05,
      "loss": 0.6247,
      "step": 921800
    },
    {
      "epoch": 14.472527472527473,
      "grad_norm": 3.477829933166504,
      "learning_rate": 4.095467032967033e-05,
      "loss": 0.5827,
      "step": 921900
    },
    {
      "epoch": 14.474097331240188,
      "grad_norm": 3.7836837768554688,
      "learning_rate": 4.0953689167974884e-05,
      "loss": 0.6047,
      "step": 922000
    },
    {
      "epoch": 14.475667189952905,
      "grad_norm": 4.2997965812683105,
      "learning_rate": 4.095270800627944e-05,
      "loss": 0.5915,
      "step": 922100
    },
    {
      "epoch": 14.47723704866562,
      "grad_norm": 3.8451976776123047,
      "learning_rate": 4.0951726844583986e-05,
      "loss": 0.6045,
      "step": 922200
    },
    {
      "epoch": 14.478806907378337,
      "grad_norm": 5.118490695953369,
      "learning_rate": 4.0950745682888544e-05,
      "loss": 0.6704,
      "step": 922300
    },
    {
      "epoch": 14.480376766091052,
      "grad_norm": 3.8735640048980713,
      "learning_rate": 4.0949764521193095e-05,
      "loss": 0.6447,
      "step": 922400
    },
    {
      "epoch": 14.481946624803768,
      "grad_norm": 4.757963180541992,
      "learning_rate": 4.0948783359497646e-05,
      "loss": 0.6193,
      "step": 922500
    },
    {
      "epoch": 14.483516483516484,
      "grad_norm": 4.693172931671143,
      "learning_rate": 4.09478021978022e-05,
      "loss": 0.6251,
      "step": 922600
    },
    {
      "epoch": 14.485086342229199,
      "grad_norm": 3.5122413635253906,
      "learning_rate": 4.0946821036106754e-05,
      "loss": 0.6271,
      "step": 922700
    },
    {
      "epoch": 14.486656200941916,
      "grad_norm": 4.0754241943359375,
      "learning_rate": 4.0945839874411305e-05,
      "loss": 0.6344,
      "step": 922800
    },
    {
      "epoch": 14.48822605965463,
      "grad_norm": 4.079318046569824,
      "learning_rate": 4.0944858712715856e-05,
      "loss": 0.6136,
      "step": 922900
    },
    {
      "epoch": 14.489795918367347,
      "grad_norm": 4.104697227478027,
      "learning_rate": 4.094387755102041e-05,
      "loss": 0.6088,
      "step": 923000
    },
    {
      "epoch": 14.491365777080063,
      "grad_norm": 4.337035655975342,
      "learning_rate": 4.0942896389324965e-05,
      "loss": 0.6374,
      "step": 923100
    },
    {
      "epoch": 14.49293563579278,
      "grad_norm": 3.0413763523101807,
      "learning_rate": 4.094191522762951e-05,
      "loss": 0.6152,
      "step": 923200
    },
    {
      "epoch": 14.494505494505495,
      "grad_norm": 4.0721540451049805,
      "learning_rate": 4.094093406593407e-05,
      "loss": 0.6137,
      "step": 923300
    },
    {
      "epoch": 14.49607535321821,
      "grad_norm": 4.295084476470947,
      "learning_rate": 4.093995290423862e-05,
      "loss": 0.6326,
      "step": 923400
    },
    {
      "epoch": 14.497645211930926,
      "grad_norm": 4.466063976287842,
      "learning_rate": 4.0938971742543176e-05,
      "loss": 0.629,
      "step": 923500
    },
    {
      "epoch": 14.499215070643642,
      "grad_norm": 4.123639106750488,
      "learning_rate": 4.093799058084773e-05,
      "loss": 0.6201,
      "step": 923600
    },
    {
      "epoch": 14.500784929356358,
      "grad_norm": 4.737364292144775,
      "learning_rate": 4.093700941915228e-05,
      "loss": 0.6129,
      "step": 923700
    },
    {
      "epoch": 14.502354788069074,
      "grad_norm": 4.273787498474121,
      "learning_rate": 4.093602825745683e-05,
      "loss": 0.5871,
      "step": 923800
    },
    {
      "epoch": 14.50392464678179,
      "grad_norm": 4.032361030578613,
      "learning_rate": 4.093504709576138e-05,
      "loss": 0.6129,
      "step": 923900
    },
    {
      "epoch": 14.505494505494505,
      "grad_norm": 5.2762770652771,
      "learning_rate": 4.093406593406594e-05,
      "loss": 0.5822,
      "step": 924000
    },
    {
      "epoch": 14.50706436420722,
      "grad_norm": 4.168034076690674,
      "learning_rate": 4.093308477237049e-05,
      "loss": 0.6358,
      "step": 924100
    },
    {
      "epoch": 14.508634222919937,
      "grad_norm": 4.128870964050293,
      "learning_rate": 4.0932103610675046e-05,
      "loss": 0.5649,
      "step": 924200
    },
    {
      "epoch": 14.510204081632653,
      "grad_norm": 4.024103164672852,
      "learning_rate": 4.093112244897959e-05,
      "loss": 0.6301,
      "step": 924300
    },
    {
      "epoch": 14.51177394034537,
      "grad_norm": 4.9610676765441895,
      "learning_rate": 4.093014128728415e-05,
      "loss": 0.6175,
      "step": 924400
    },
    {
      "epoch": 14.513343799058084,
      "grad_norm": 3.640915870666504,
      "learning_rate": 4.09291601255887e-05,
      "loss": 0.6172,
      "step": 924500
    },
    {
      "epoch": 14.514913657770801,
      "grad_norm": 2.906170606613159,
      "learning_rate": 4.092817896389325e-05,
      "loss": 0.6341,
      "step": 924600
    },
    {
      "epoch": 14.516483516483516,
      "grad_norm": 3.6541097164154053,
      "learning_rate": 4.09271978021978e-05,
      "loss": 0.6295,
      "step": 924700
    },
    {
      "epoch": 14.518053375196232,
      "grad_norm": 3.283311128616333,
      "learning_rate": 4.092621664050236e-05,
      "loss": 0.6091,
      "step": 924800
    },
    {
      "epoch": 14.519623233908948,
      "grad_norm": 3.9826643466949463,
      "learning_rate": 4.092523547880691e-05,
      "loss": 0.6058,
      "step": 924900
    },
    {
      "epoch": 14.521193092621663,
      "grad_norm": 3.8052022457122803,
      "learning_rate": 4.092425431711146e-05,
      "loss": 0.6291,
      "step": 925000
    },
    {
      "epoch": 14.52276295133438,
      "grad_norm": 3.7179677486419678,
      "learning_rate": 4.092327315541601e-05,
      "loss": 0.6292,
      "step": 925100
    },
    {
      "epoch": 14.524332810047095,
      "grad_norm": 3.3288774490356445,
      "learning_rate": 4.092229199372057e-05,
      "loss": 0.6044,
      "step": 925200
    },
    {
      "epoch": 14.525902668759812,
      "grad_norm": 4.046182632446289,
      "learning_rate": 4.0921310832025114e-05,
      "loss": 0.5982,
      "step": 925300
    },
    {
      "epoch": 14.527472527472527,
      "grad_norm": 3.445178985595703,
      "learning_rate": 4.092032967032967e-05,
      "loss": 0.6261,
      "step": 925400
    },
    {
      "epoch": 14.529042386185242,
      "grad_norm": 4.437977313995361,
      "learning_rate": 4.091934850863422e-05,
      "loss": 0.6351,
      "step": 925500
    },
    {
      "epoch": 14.53061224489796,
      "grad_norm": 4.735722541809082,
      "learning_rate": 4.091836734693878e-05,
      "loss": 0.6071,
      "step": 925600
    },
    {
      "epoch": 14.532182103610674,
      "grad_norm": 3.0770959854125977,
      "learning_rate": 4.091738618524333e-05,
      "loss": 0.6015,
      "step": 925700
    },
    {
      "epoch": 14.533751962323391,
      "grad_norm": 4.0138654708862305,
      "learning_rate": 4.091640502354788e-05,
      "loss": 0.5835,
      "step": 925800
    },
    {
      "epoch": 14.535321821036106,
      "grad_norm": 4.0557403564453125,
      "learning_rate": 4.091542386185243e-05,
      "loss": 0.6231,
      "step": 925900
    },
    {
      "epoch": 14.536891679748823,
      "grad_norm": 3.7559852600097656,
      "learning_rate": 4.0914442700156984e-05,
      "loss": 0.6045,
      "step": 926000
    },
    {
      "epoch": 14.538461538461538,
      "grad_norm": 4.0762763023376465,
      "learning_rate": 4.091346153846154e-05,
      "loss": 0.6276,
      "step": 926100
    },
    {
      "epoch": 14.540031397174253,
      "grad_norm": 3.317614793777466,
      "learning_rate": 4.091248037676609e-05,
      "loss": 0.6297,
      "step": 926200
    },
    {
      "epoch": 14.54160125588697,
      "grad_norm": 3.6289515495300293,
      "learning_rate": 4.091149921507065e-05,
      "loss": 0.5884,
      "step": 926300
    },
    {
      "epoch": 14.543171114599685,
      "grad_norm": 3.5390896797180176,
      "learning_rate": 4.0910518053375195e-05,
      "loss": 0.6679,
      "step": 926400
    },
    {
      "epoch": 14.544740973312402,
      "grad_norm": 4.611940383911133,
      "learning_rate": 4.090953689167975e-05,
      "loss": 0.5812,
      "step": 926500
    },
    {
      "epoch": 14.546310832025117,
      "grad_norm": 4.114634037017822,
      "learning_rate": 4.0908555729984304e-05,
      "loss": 0.6399,
      "step": 926600
    },
    {
      "epoch": 14.547880690737834,
      "grad_norm": 3.2979986667633057,
      "learning_rate": 4.0907574568288855e-05,
      "loss": 0.6035,
      "step": 926700
    },
    {
      "epoch": 14.54945054945055,
      "grad_norm": 4.384287357330322,
      "learning_rate": 4.0906593406593406e-05,
      "loss": 0.6254,
      "step": 926800
    },
    {
      "epoch": 14.551020408163264,
      "grad_norm": 3.53761625289917,
      "learning_rate": 4.0905612244897963e-05,
      "loss": 0.6291,
      "step": 926900
    },
    {
      "epoch": 14.552590266875981,
      "grad_norm": 4.120248794555664,
      "learning_rate": 4.0904631083202514e-05,
      "loss": 0.6307,
      "step": 927000
    },
    {
      "epoch": 14.554160125588696,
      "grad_norm": 4.043418884277344,
      "learning_rate": 4.0903649921507065e-05,
      "loss": 0.5586,
      "step": 927100
    },
    {
      "epoch": 14.555729984301413,
      "grad_norm": 4.589974403381348,
      "learning_rate": 4.0902668759811616e-05,
      "loss": 0.6222,
      "step": 927200
    },
    {
      "epoch": 14.557299843014128,
      "grad_norm": 3.059467077255249,
      "learning_rate": 4.0901687598116174e-05,
      "loss": 0.5974,
      "step": 927300
    },
    {
      "epoch": 14.558869701726845,
      "grad_norm": 3.3796725273132324,
      "learning_rate": 4.090070643642072e-05,
      "loss": 0.6219,
      "step": 927400
    },
    {
      "epoch": 14.56043956043956,
      "grad_norm": 2.453099012374878,
      "learning_rate": 4.0899725274725276e-05,
      "loss": 0.6376,
      "step": 927500
    },
    {
      "epoch": 14.562009419152277,
      "grad_norm": 3.846057891845703,
      "learning_rate": 4.089874411302983e-05,
      "loss": 0.626,
      "step": 927600
    },
    {
      "epoch": 14.563579277864992,
      "grad_norm": 3.7076966762542725,
      "learning_rate": 4.0897762951334385e-05,
      "loss": 0.6266,
      "step": 927700
    },
    {
      "epoch": 14.565149136577707,
      "grad_norm": 2.515597105026245,
      "learning_rate": 4.0896781789638936e-05,
      "loss": 0.5952,
      "step": 927800
    },
    {
      "epoch": 14.566718995290424,
      "grad_norm": 2.1498188972473145,
      "learning_rate": 4.089580062794349e-05,
      "loss": 0.6425,
      "step": 927900
    },
    {
      "epoch": 14.56828885400314,
      "grad_norm": 3.866081476211548,
      "learning_rate": 4.089481946624804e-05,
      "loss": 0.615,
      "step": 928000
    },
    {
      "epoch": 14.569858712715856,
      "grad_norm": 3.609372615814209,
      "learning_rate": 4.089383830455259e-05,
      "loss": 0.604,
      "step": 928100
    },
    {
      "epoch": 14.571428571428571,
      "grad_norm": 3.8973987102508545,
      "learning_rate": 4.0892857142857147e-05,
      "loss": 0.6427,
      "step": 928200
    },
    {
      "epoch": 14.572998430141288,
      "grad_norm": 3.5435502529144287,
      "learning_rate": 4.08918759811617e-05,
      "loss": 0.6116,
      "step": 928300
    },
    {
      "epoch": 14.574568288854003,
      "grad_norm": 3.60433292388916,
      "learning_rate": 4.0890894819466255e-05,
      "loss": 0.6317,
      "step": 928400
    },
    {
      "epoch": 14.576138147566718,
      "grad_norm": 4.111440181732178,
      "learning_rate": 4.08899136577708e-05,
      "loss": 0.6281,
      "step": 928500
    },
    {
      "epoch": 14.577708006279435,
      "grad_norm": 4.577892303466797,
      "learning_rate": 4.088893249607536e-05,
      "loss": 0.6038,
      "step": 928600
    },
    {
      "epoch": 14.57927786499215,
      "grad_norm": 2.8366215229034424,
      "learning_rate": 4.088795133437991e-05,
      "loss": 0.62,
      "step": 928700
    },
    {
      "epoch": 14.580847723704867,
      "grad_norm": 2.9291129112243652,
      "learning_rate": 4.088697017268446e-05,
      "loss": 0.6592,
      "step": 928800
    },
    {
      "epoch": 14.582417582417582,
      "grad_norm": 2.905212163925171,
      "learning_rate": 4.088598901098901e-05,
      "loss": 0.6067,
      "step": 928900
    },
    {
      "epoch": 14.583987441130299,
      "grad_norm": 3.5517191886901855,
      "learning_rate": 4.088500784929357e-05,
      "loss": 0.6272,
      "step": 929000
    },
    {
      "epoch": 14.585557299843014,
      "grad_norm": 3.8629722595214844,
      "learning_rate": 4.088402668759812e-05,
      "loss": 0.6554,
      "step": 929100
    },
    {
      "epoch": 14.58712715855573,
      "grad_norm": 3.1412644386291504,
      "learning_rate": 4.088304552590267e-05,
      "loss": 0.5883,
      "step": 929200
    },
    {
      "epoch": 14.588697017268446,
      "grad_norm": 4.618147850036621,
      "learning_rate": 4.088206436420722e-05,
      "loss": 0.6416,
      "step": 929300
    },
    {
      "epoch": 14.590266875981161,
      "grad_norm": 4.310697078704834,
      "learning_rate": 4.088108320251178e-05,
      "loss": 0.5868,
      "step": 929400
    },
    {
      "epoch": 14.591836734693878,
      "grad_norm": 3.458505630493164,
      "learning_rate": 4.088010204081632e-05,
      "loss": 0.6138,
      "step": 929500
    },
    {
      "epoch": 14.593406593406593,
      "grad_norm": 3.297051429748535,
      "learning_rate": 4.087912087912088e-05,
      "loss": 0.6054,
      "step": 929600
    },
    {
      "epoch": 14.59497645211931,
      "grad_norm": 3.5538365840911865,
      "learning_rate": 4.087813971742543e-05,
      "loss": 0.6146,
      "step": 929700
    },
    {
      "epoch": 14.596546310832025,
      "grad_norm": 3.4659106731414795,
      "learning_rate": 4.087715855572999e-05,
      "loss": 0.6383,
      "step": 929800
    },
    {
      "epoch": 14.598116169544742,
      "grad_norm": 3.767223834991455,
      "learning_rate": 4.087617739403454e-05,
      "loss": 0.6347,
      "step": 929900
    },
    {
      "epoch": 14.599686028257457,
      "grad_norm": 4.099759101867676,
      "learning_rate": 4.087519623233909e-05,
      "loss": 0.6496,
      "step": 930000
    },
    {
      "epoch": 14.601255886970172,
      "grad_norm": 3.225696325302124,
      "learning_rate": 4.087421507064364e-05,
      "loss": 0.6437,
      "step": 930100
    },
    {
      "epoch": 14.602825745682889,
      "grad_norm": 4.197342395782471,
      "learning_rate": 4.087323390894819e-05,
      "loss": 0.5991,
      "step": 930200
    },
    {
      "epoch": 14.604395604395604,
      "grad_norm": 3.8851208686828613,
      "learning_rate": 4.087225274725275e-05,
      "loss": 0.6028,
      "step": 930300
    },
    {
      "epoch": 14.605965463108321,
      "grad_norm": 3.3729679584503174,
      "learning_rate": 4.08712715855573e-05,
      "loss": 0.6202,
      "step": 930400
    },
    {
      "epoch": 14.607535321821036,
      "grad_norm": 4.003976345062256,
      "learning_rate": 4.087029042386186e-05,
      "loss": 0.6074,
      "step": 930500
    },
    {
      "epoch": 14.609105180533753,
      "grad_norm": 3.3930788040161133,
      "learning_rate": 4.0869309262166404e-05,
      "loss": 0.6245,
      "step": 930600
    },
    {
      "epoch": 14.610675039246468,
      "grad_norm": 3.961097240447998,
      "learning_rate": 4.086832810047096e-05,
      "loss": 0.6239,
      "step": 930700
    },
    {
      "epoch": 14.612244897959183,
      "grad_norm": 4.405267238616943,
      "learning_rate": 4.086734693877551e-05,
      "loss": 0.611,
      "step": 930800
    },
    {
      "epoch": 14.6138147566719,
      "grad_norm": 4.507851600646973,
      "learning_rate": 4.0866365777080064e-05,
      "loss": 0.634,
      "step": 930900
    },
    {
      "epoch": 14.615384615384615,
      "grad_norm": 3.7914016246795654,
      "learning_rate": 4.0865384615384615e-05,
      "loss": 0.6567,
      "step": 931000
    },
    {
      "epoch": 14.616954474097332,
      "grad_norm": 2.768526554107666,
      "learning_rate": 4.086440345368917e-05,
      "loss": 0.5913,
      "step": 931100
    },
    {
      "epoch": 14.618524332810047,
      "grad_norm": 3.1481566429138184,
      "learning_rate": 4.0863422291993723e-05,
      "loss": 0.6055,
      "step": 931200
    },
    {
      "epoch": 14.620094191522764,
      "grad_norm": 4.07328987121582,
      "learning_rate": 4.0862441130298274e-05,
      "loss": 0.5909,
      "step": 931300
    },
    {
      "epoch": 14.621664050235479,
      "grad_norm": 3.848405122756958,
      "learning_rate": 4.0861459968602825e-05,
      "loss": 0.63,
      "step": 931400
    },
    {
      "epoch": 14.623233908948194,
      "grad_norm": 3.4812235832214355,
      "learning_rate": 4.086047880690738e-05,
      "loss": 0.6154,
      "step": 931500
    },
    {
      "epoch": 14.62480376766091,
      "grad_norm": 4.088307857513428,
      "learning_rate": 4.085949764521193e-05,
      "loss": 0.6346,
      "step": 931600
    },
    {
      "epoch": 14.626373626373626,
      "grad_norm": 3.9470725059509277,
      "learning_rate": 4.0858516483516485e-05,
      "loss": 0.6357,
      "step": 931700
    },
    {
      "epoch": 14.627943485086343,
      "grad_norm": 4.241544723510742,
      "learning_rate": 4.0857535321821036e-05,
      "loss": 0.5337,
      "step": 931800
    },
    {
      "epoch": 14.629513343799058,
      "grad_norm": 4.964304447174072,
      "learning_rate": 4.0856554160125594e-05,
      "loss": 0.6626,
      "step": 931900
    },
    {
      "epoch": 14.631083202511775,
      "grad_norm": 3.5160117149353027,
      "learning_rate": 4.0855572998430145e-05,
      "loss": 0.6045,
      "step": 932000
    },
    {
      "epoch": 14.63265306122449,
      "grad_norm": 2.9762630462646484,
      "learning_rate": 4.0854591836734696e-05,
      "loss": 0.6045,
      "step": 932100
    },
    {
      "epoch": 14.634222919937205,
      "grad_norm": 3.4273688793182373,
      "learning_rate": 4.085361067503925e-05,
      "loss": 0.6396,
      "step": 932200
    },
    {
      "epoch": 14.635792778649922,
      "grad_norm": 4.224213123321533,
      "learning_rate": 4.08526295133438e-05,
      "loss": 0.6324,
      "step": 932300
    },
    {
      "epoch": 14.637362637362637,
      "grad_norm": 2.7891101837158203,
      "learning_rate": 4.0851648351648356e-05,
      "loss": 0.6287,
      "step": 932400
    },
    {
      "epoch": 14.638932496075354,
      "grad_norm": 4.707181930541992,
      "learning_rate": 4.0850667189952906e-05,
      "loss": 0.5903,
      "step": 932500
    },
    {
      "epoch": 14.640502354788069,
      "grad_norm": 4.662583827972412,
      "learning_rate": 4.0849686028257464e-05,
      "loss": 0.5986,
      "step": 932600
    },
    {
      "epoch": 14.642072213500786,
      "grad_norm": 3.6944921016693115,
      "learning_rate": 4.084870486656201e-05,
      "loss": 0.6434,
      "step": 932700
    },
    {
      "epoch": 14.6436420722135,
      "grad_norm": 3.2389628887176514,
      "learning_rate": 4.0847723704866566e-05,
      "loss": 0.6272,
      "step": 932800
    },
    {
      "epoch": 14.645211930926216,
      "grad_norm": 3.134216070175171,
      "learning_rate": 4.084674254317112e-05,
      "loss": 0.5822,
      "step": 932900
    },
    {
      "epoch": 14.646781789638933,
      "grad_norm": 3.2171053886413574,
      "learning_rate": 4.084576138147567e-05,
      "loss": 0.6321,
      "step": 933000
    },
    {
      "epoch": 14.648351648351648,
      "grad_norm": 4.0027241706848145,
      "learning_rate": 4.084478021978022e-05,
      "loss": 0.6104,
      "step": 933100
    },
    {
      "epoch": 14.649921507064365,
      "grad_norm": 3.704596996307373,
      "learning_rate": 4.084379905808478e-05,
      "loss": 0.5772,
      "step": 933200
    },
    {
      "epoch": 14.65149136577708,
      "grad_norm": 2.85245680809021,
      "learning_rate": 4.084281789638933e-05,
      "loss": 0.648,
      "step": 933300
    },
    {
      "epoch": 14.653061224489797,
      "grad_norm": 3.413856267929077,
      "learning_rate": 4.084183673469388e-05,
      "loss": 0.6241,
      "step": 933400
    },
    {
      "epoch": 14.654631083202512,
      "grad_norm": 4.271297931671143,
      "learning_rate": 4.084085557299843e-05,
      "loss": 0.6143,
      "step": 933500
    },
    {
      "epoch": 14.656200941915227,
      "grad_norm": 3.7773470878601074,
      "learning_rate": 4.083987441130299e-05,
      "loss": 0.5958,
      "step": 933600
    },
    {
      "epoch": 14.657770800627944,
      "grad_norm": 3.2093300819396973,
      "learning_rate": 4.083889324960753e-05,
      "loss": 0.5978,
      "step": 933700
    },
    {
      "epoch": 14.659340659340659,
      "grad_norm": 3.8144853115081787,
      "learning_rate": 4.083791208791209e-05,
      "loss": 0.5822,
      "step": 933800
    },
    {
      "epoch": 14.660910518053376,
      "grad_norm": 3.106811046600342,
      "learning_rate": 4.083693092621664e-05,
      "loss": 0.5949,
      "step": 933900
    },
    {
      "epoch": 14.66248037676609,
      "grad_norm": 4.240609169006348,
      "learning_rate": 4.08359497645212e-05,
      "loss": 0.6172,
      "step": 934000
    },
    {
      "epoch": 14.664050235478808,
      "grad_norm": 3.8191440105438232,
      "learning_rate": 4.083496860282575e-05,
      "loss": 0.6611,
      "step": 934100
    },
    {
      "epoch": 14.665620094191523,
      "grad_norm": 3.7901034355163574,
      "learning_rate": 4.08339874411303e-05,
      "loss": 0.6239,
      "step": 934200
    },
    {
      "epoch": 14.667189952904238,
      "grad_norm": 3.8252944946289062,
      "learning_rate": 4.083300627943485e-05,
      "loss": 0.6224,
      "step": 934300
    },
    {
      "epoch": 14.668759811616955,
      "grad_norm": 2.788440465927124,
      "learning_rate": 4.08320251177394e-05,
      "loss": 0.6087,
      "step": 934400
    },
    {
      "epoch": 14.67032967032967,
      "grad_norm": 3.4123523235321045,
      "learning_rate": 4.083104395604396e-05,
      "loss": 0.5892,
      "step": 934500
    },
    {
      "epoch": 14.671899529042387,
      "grad_norm": 4.030177116394043,
      "learning_rate": 4.083006279434851e-05,
      "loss": 0.6487,
      "step": 934600
    },
    {
      "epoch": 14.673469387755102,
      "grad_norm": 3.8773460388183594,
      "learning_rate": 4.082908163265307e-05,
      "loss": 0.6102,
      "step": 934700
    },
    {
      "epoch": 14.675039246467819,
      "grad_norm": 4.222064018249512,
      "learning_rate": 4.082810047095761e-05,
      "loss": 0.6414,
      "step": 934800
    },
    {
      "epoch": 14.676609105180534,
      "grad_norm": 3.5902018547058105,
      "learning_rate": 4.082711930926217e-05,
      "loss": 0.6224,
      "step": 934900
    },
    {
      "epoch": 14.678178963893249,
      "grad_norm": 4.270369529724121,
      "learning_rate": 4.082613814756672e-05,
      "loss": 0.6339,
      "step": 935000
    },
    {
      "epoch": 14.679748822605966,
      "grad_norm": 3.7686262130737305,
      "learning_rate": 4.082515698587127e-05,
      "loss": 0.6233,
      "step": 935100
    },
    {
      "epoch": 14.68131868131868,
      "grad_norm": 3.201139211654663,
      "learning_rate": 4.0824175824175824e-05,
      "loss": 0.642,
      "step": 935200
    },
    {
      "epoch": 14.682888540031398,
      "grad_norm": 3.623486042022705,
      "learning_rate": 4.082319466248038e-05,
      "loss": 0.649,
      "step": 935300
    },
    {
      "epoch": 14.684458398744113,
      "grad_norm": 4.5782790184021,
      "learning_rate": 4.082221350078493e-05,
      "loss": 0.6,
      "step": 935400
    },
    {
      "epoch": 14.68602825745683,
      "grad_norm": 3.7900216579437256,
      "learning_rate": 4.082123233908948e-05,
      "loss": 0.6468,
      "step": 935500
    },
    {
      "epoch": 14.687598116169545,
      "grad_norm": 3.930570125579834,
      "learning_rate": 4.0820251177394034e-05,
      "loss": 0.6506,
      "step": 935600
    },
    {
      "epoch": 14.68916797488226,
      "grad_norm": 4.813079357147217,
      "learning_rate": 4.081927001569859e-05,
      "loss": 0.6366,
      "step": 935700
    },
    {
      "epoch": 14.690737833594977,
      "grad_norm": 3.9795339107513428,
      "learning_rate": 4.0818288854003136e-05,
      "loss": 0.6258,
      "step": 935800
    },
    {
      "epoch": 14.692307692307692,
      "grad_norm": 3.8249258995056152,
      "learning_rate": 4.0817307692307694e-05,
      "loss": 0.6458,
      "step": 935900
    },
    {
      "epoch": 14.693877551020408,
      "grad_norm": 3.860130786895752,
      "learning_rate": 4.0816326530612245e-05,
      "loss": 0.6003,
      "step": 936000
    },
    {
      "epoch": 14.695447409733124,
      "grad_norm": 4.514028072357178,
      "learning_rate": 4.08153453689168e-05,
      "loss": 0.5978,
      "step": 936100
    },
    {
      "epoch": 14.69701726844584,
      "grad_norm": 2.478760004043579,
      "learning_rate": 4.0814364207221354e-05,
      "loss": 0.6068,
      "step": 936200
    },
    {
      "epoch": 14.698587127158556,
      "grad_norm": 3.649446487426758,
      "learning_rate": 4.0813383045525905e-05,
      "loss": 0.6311,
      "step": 936300
    },
    {
      "epoch": 14.700156985871272,
      "grad_norm": 3.272183895111084,
      "learning_rate": 4.0812401883830456e-05,
      "loss": 0.6221,
      "step": 936400
    },
    {
      "epoch": 14.701726844583987,
      "grad_norm": 3.6186933517456055,
      "learning_rate": 4.081142072213501e-05,
      "loss": 0.6288,
      "step": 936500
    },
    {
      "epoch": 14.703296703296703,
      "grad_norm": 3.9685041904449463,
      "learning_rate": 4.0810439560439564e-05,
      "loss": 0.5828,
      "step": 936600
    },
    {
      "epoch": 14.70486656200942,
      "grad_norm": 4.376979827880859,
      "learning_rate": 4.0809458398744115e-05,
      "loss": 0.6165,
      "step": 936700
    },
    {
      "epoch": 14.706436420722135,
      "grad_norm": 3.1859548091888428,
      "learning_rate": 4.080847723704867e-05,
      "loss": 0.617,
      "step": 936800
    },
    {
      "epoch": 14.708006279434851,
      "grad_norm": 4.783653736114502,
      "learning_rate": 4.080749607535322e-05,
      "loss": 0.6201,
      "step": 936900
    },
    {
      "epoch": 14.709576138147566,
      "grad_norm": 5.683285713195801,
      "learning_rate": 4.0806514913657775e-05,
      "loss": 0.6219,
      "step": 937000
    },
    {
      "epoch": 14.711145996860283,
      "grad_norm": 3.917140245437622,
      "learning_rate": 4.0805533751962326e-05,
      "loss": 0.6045,
      "step": 937100
    },
    {
      "epoch": 14.712715855572998,
      "grad_norm": 4.416540622711182,
      "learning_rate": 4.080455259026688e-05,
      "loss": 0.6453,
      "step": 937200
    },
    {
      "epoch": 14.714285714285714,
      "grad_norm": 2.752847194671631,
      "learning_rate": 4.080357142857143e-05,
      "loss": 0.6179,
      "step": 937300
    },
    {
      "epoch": 14.71585557299843,
      "grad_norm": 3.973374128341675,
      "learning_rate": 4.0802590266875986e-05,
      "loss": 0.6144,
      "step": 937400
    },
    {
      "epoch": 14.717425431711145,
      "grad_norm": 3.572453498840332,
      "learning_rate": 4.080160910518054e-05,
      "loss": 0.6103,
      "step": 937500
    },
    {
      "epoch": 14.718995290423862,
      "grad_norm": 3.5540802478790283,
      "learning_rate": 4.080062794348509e-05,
      "loss": 0.6299,
      "step": 937600
    },
    {
      "epoch": 14.720565149136577,
      "grad_norm": 4.439208984375,
      "learning_rate": 4.079964678178964e-05,
      "loss": 0.6557,
      "step": 937700
    },
    {
      "epoch": 14.722135007849294,
      "grad_norm": 4.112142562866211,
      "learning_rate": 4.0798665620094197e-05,
      "loss": 0.59,
      "step": 937800
    },
    {
      "epoch": 14.72370486656201,
      "grad_norm": 6.480860710144043,
      "learning_rate": 4.079768445839874e-05,
      "loss": 0.5776,
      "step": 937900
    },
    {
      "epoch": 14.725274725274724,
      "grad_norm": 4.201688289642334,
      "learning_rate": 4.07967032967033e-05,
      "loss": 0.6022,
      "step": 938000
    },
    {
      "epoch": 14.726844583987441,
      "grad_norm": 5.630538463592529,
      "learning_rate": 4.079572213500785e-05,
      "loss": 0.6672,
      "step": 938100
    },
    {
      "epoch": 14.728414442700156,
      "grad_norm": 3.6321661472320557,
      "learning_rate": 4.079474097331241e-05,
      "loss": 0.6164,
      "step": 938200
    },
    {
      "epoch": 14.729984301412873,
      "grad_norm": 4.0001630783081055,
      "learning_rate": 4.079375981161696e-05,
      "loss": 0.6169,
      "step": 938300
    },
    {
      "epoch": 14.731554160125588,
      "grad_norm": 3.3859167098999023,
      "learning_rate": 4.079277864992151e-05,
      "loss": 0.6336,
      "step": 938400
    },
    {
      "epoch": 14.733124018838305,
      "grad_norm": 3.810108184814453,
      "learning_rate": 4.079179748822606e-05,
      "loss": 0.6367,
      "step": 938500
    },
    {
      "epoch": 14.73469387755102,
      "grad_norm": 4.0191802978515625,
      "learning_rate": 4.079081632653061e-05,
      "loss": 0.6627,
      "step": 938600
    },
    {
      "epoch": 14.736263736263737,
      "grad_norm": 3.1542441844940186,
      "learning_rate": 4.078983516483517e-05,
      "loss": 0.5882,
      "step": 938700
    },
    {
      "epoch": 14.737833594976452,
      "grad_norm": 3.069092273712158,
      "learning_rate": 4.078885400313972e-05,
      "loss": 0.5885,
      "step": 938800
    },
    {
      "epoch": 14.739403453689167,
      "grad_norm": 4.127880096435547,
      "learning_rate": 4.078787284144428e-05,
      "loss": 0.6363,
      "step": 938900
    },
    {
      "epoch": 14.740973312401884,
      "grad_norm": 3.971052885055542,
      "learning_rate": 4.078689167974882e-05,
      "loss": 0.6534,
      "step": 939000
    },
    {
      "epoch": 14.7425431711146,
      "grad_norm": 3.9350831508636475,
      "learning_rate": 4.078591051805338e-05,
      "loss": 0.6141,
      "step": 939100
    },
    {
      "epoch": 14.744113029827316,
      "grad_norm": 3.8038103580474854,
      "learning_rate": 4.078492935635793e-05,
      "loss": 0.6437,
      "step": 939200
    },
    {
      "epoch": 14.745682888540031,
      "grad_norm": 4.370100021362305,
      "learning_rate": 4.078394819466248e-05,
      "loss": 0.6283,
      "step": 939300
    },
    {
      "epoch": 14.747252747252748,
      "grad_norm": 4.703456878662109,
      "learning_rate": 4.078296703296703e-05,
      "loss": 0.6306,
      "step": 939400
    },
    {
      "epoch": 14.748822605965463,
      "grad_norm": 3.104508876800537,
      "learning_rate": 4.078198587127159e-05,
      "loss": 0.6333,
      "step": 939500
    },
    {
      "epoch": 14.750392464678178,
      "grad_norm": 3.635781764984131,
      "learning_rate": 4.078100470957614e-05,
      "loss": 0.6179,
      "step": 939600
    },
    {
      "epoch": 14.751962323390895,
      "grad_norm": 4.039908409118652,
      "learning_rate": 4.078002354788069e-05,
      "loss": 0.6112,
      "step": 939700
    },
    {
      "epoch": 14.75353218210361,
      "grad_norm": 3.7779948711395264,
      "learning_rate": 4.077904238618524e-05,
      "loss": 0.6234,
      "step": 939800
    },
    {
      "epoch": 14.755102040816327,
      "grad_norm": 2.5508203506469727,
      "learning_rate": 4.07780612244898e-05,
      "loss": 0.6151,
      "step": 939900
    },
    {
      "epoch": 14.756671899529042,
      "grad_norm": 4.456958293914795,
      "learning_rate": 4.0777080062794345e-05,
      "loss": 0.6133,
      "step": 940000
    },
    {
      "epoch": 14.758241758241759,
      "grad_norm": 4.107554912567139,
      "learning_rate": 4.07760989010989e-05,
      "loss": 0.6228,
      "step": 940100
    },
    {
      "epoch": 14.759811616954474,
      "grad_norm": 3.128948926925659,
      "learning_rate": 4.0775117739403454e-05,
      "loss": 0.6163,
      "step": 940200
    },
    {
      "epoch": 14.76138147566719,
      "grad_norm": 4.75909423828125,
      "learning_rate": 4.077413657770801e-05,
      "loss": 0.6358,
      "step": 940300
    },
    {
      "epoch": 14.762951334379906,
      "grad_norm": 2.4158029556274414,
      "learning_rate": 4.077315541601256e-05,
      "loss": 0.6391,
      "step": 940400
    },
    {
      "epoch": 14.764521193092621,
      "grad_norm": 3.4458396434783936,
      "learning_rate": 4.0772174254317114e-05,
      "loss": 0.6311,
      "step": 940500
    },
    {
      "epoch": 14.766091051805338,
      "grad_norm": 4.247812271118164,
      "learning_rate": 4.0771193092621665e-05,
      "loss": 0.6229,
      "step": 940600
    },
    {
      "epoch": 14.767660910518053,
      "grad_norm": 3.9079973697662354,
      "learning_rate": 4.0770211930926216e-05,
      "loss": 0.629,
      "step": 940700
    },
    {
      "epoch": 14.76923076923077,
      "grad_norm": 3.752312421798706,
      "learning_rate": 4.0769230769230773e-05,
      "loss": 0.6122,
      "step": 940800
    },
    {
      "epoch": 14.770800627943485,
      "grad_norm": 3.925330638885498,
      "learning_rate": 4.0768249607535324e-05,
      "loss": 0.6495,
      "step": 940900
    },
    {
      "epoch": 14.7723704866562,
      "grad_norm": 4.6571173667907715,
      "learning_rate": 4.076726844583988e-05,
      "loss": 0.5876,
      "step": 941000
    },
    {
      "epoch": 14.773940345368917,
      "grad_norm": 4.945971488952637,
      "learning_rate": 4.0766287284144426e-05,
      "loss": 0.64,
      "step": 941100
    },
    {
      "epoch": 14.775510204081632,
      "grad_norm": 4.194026470184326,
      "learning_rate": 4.0765306122448984e-05,
      "loss": 0.5973,
      "step": 941200
    },
    {
      "epoch": 14.777080062794349,
      "grad_norm": 3.8762106895446777,
      "learning_rate": 4.0764324960753535e-05,
      "loss": 0.5839,
      "step": 941300
    },
    {
      "epoch": 14.778649921507064,
      "grad_norm": 3.327214241027832,
      "learning_rate": 4.0763343799058086e-05,
      "loss": 0.6089,
      "step": 941400
    },
    {
      "epoch": 14.780219780219781,
      "grad_norm": 3.172153949737549,
      "learning_rate": 4.076236263736264e-05,
      "loss": 0.624,
      "step": 941500
    },
    {
      "epoch": 14.781789638932496,
      "grad_norm": 3.843756675720215,
      "learning_rate": 4.0761381475667195e-05,
      "loss": 0.6153,
      "step": 941600
    },
    {
      "epoch": 14.783359497645211,
      "grad_norm": 4.575183868408203,
      "learning_rate": 4.0760400313971746e-05,
      "loss": 0.6033,
      "step": 941700
    },
    {
      "epoch": 14.784929356357928,
      "grad_norm": 4.642330169677734,
      "learning_rate": 4.07594191522763e-05,
      "loss": 0.6234,
      "step": 941800
    },
    {
      "epoch": 14.786499215070643,
      "grad_norm": 3.6810319423675537,
      "learning_rate": 4.075843799058085e-05,
      "loss": 0.6175,
      "step": 941900
    },
    {
      "epoch": 14.78806907378336,
      "grad_norm": 4.116706371307373,
      "learning_rate": 4.0757456828885406e-05,
      "loss": 0.6254,
      "step": 942000
    },
    {
      "epoch": 14.789638932496075,
      "grad_norm": 3.623044967651367,
      "learning_rate": 4.075647566718995e-05,
      "loss": 0.6405,
      "step": 942100
    },
    {
      "epoch": 14.791208791208792,
      "grad_norm": 4.659833908081055,
      "learning_rate": 4.075549450549451e-05,
      "loss": 0.6421,
      "step": 942200
    },
    {
      "epoch": 14.792778649921507,
      "grad_norm": 3.5422613620758057,
      "learning_rate": 4.075451334379906e-05,
      "loss": 0.5825,
      "step": 942300
    },
    {
      "epoch": 14.794348508634222,
      "grad_norm": 2.8669986724853516,
      "learning_rate": 4.0753532182103616e-05,
      "loss": 0.6495,
      "step": 942400
    },
    {
      "epoch": 14.795918367346939,
      "grad_norm": 4.134842395782471,
      "learning_rate": 4.075255102040817e-05,
      "loss": 0.5989,
      "step": 942500
    },
    {
      "epoch": 14.797488226059654,
      "grad_norm": 3.956799268722534,
      "learning_rate": 4.075156985871272e-05,
      "loss": 0.5971,
      "step": 942600
    },
    {
      "epoch": 14.799058084772371,
      "grad_norm": 3.7208096981048584,
      "learning_rate": 4.075058869701727e-05,
      "loss": 0.6107,
      "step": 942700
    },
    {
      "epoch": 14.800627943485086,
      "grad_norm": 4.337255001068115,
      "learning_rate": 4.074960753532182e-05,
      "loss": 0.6152,
      "step": 942800
    },
    {
      "epoch": 14.802197802197803,
      "grad_norm": 3.2123634815216064,
      "learning_rate": 4.074862637362638e-05,
      "loss": 0.6451,
      "step": 942900
    },
    {
      "epoch": 14.803767660910518,
      "grad_norm": 3.8219821453094482,
      "learning_rate": 4.074764521193093e-05,
      "loss": 0.6153,
      "step": 943000
    },
    {
      "epoch": 14.805337519623233,
      "grad_norm": 3.371581792831421,
      "learning_rate": 4.074666405023549e-05,
      "loss": 0.6357,
      "step": 943100
    },
    {
      "epoch": 14.80690737833595,
      "grad_norm": 3.008845090866089,
      "learning_rate": 4.074568288854003e-05,
      "loss": 0.6342,
      "step": 943200
    },
    {
      "epoch": 14.808477237048665,
      "grad_norm": 3.8621604442596436,
      "learning_rate": 4.074470172684459e-05,
      "loss": 0.6395,
      "step": 943300
    },
    {
      "epoch": 14.810047095761382,
      "grad_norm": 4.195256233215332,
      "learning_rate": 4.074372056514914e-05,
      "loss": 0.6367,
      "step": 943400
    },
    {
      "epoch": 14.811616954474097,
      "grad_norm": 3.212069272994995,
      "learning_rate": 4.074273940345369e-05,
      "loss": 0.6304,
      "step": 943500
    },
    {
      "epoch": 14.813186813186814,
      "grad_norm": 3.6262362003326416,
      "learning_rate": 4.074175824175824e-05,
      "loss": 0.5927,
      "step": 943600
    },
    {
      "epoch": 14.814756671899529,
      "grad_norm": 3.980574607849121,
      "learning_rate": 4.07407770800628e-05,
      "loss": 0.6114,
      "step": 943700
    },
    {
      "epoch": 14.816326530612244,
      "grad_norm": 3.442060708999634,
      "learning_rate": 4.073979591836735e-05,
      "loss": 0.6187,
      "step": 943800
    },
    {
      "epoch": 14.817896389324961,
      "grad_norm": 4.552962303161621,
      "learning_rate": 4.07388147566719e-05,
      "loss": 0.6108,
      "step": 943900
    },
    {
      "epoch": 14.819466248037676,
      "grad_norm": 4.287910461425781,
      "learning_rate": 4.073783359497645e-05,
      "loss": 0.5713,
      "step": 944000
    },
    {
      "epoch": 14.821036106750393,
      "grad_norm": 3.4112656116485596,
      "learning_rate": 4.073685243328101e-05,
      "loss": 0.6257,
      "step": 944100
    },
    {
      "epoch": 14.822605965463108,
      "grad_norm": 2.5004055500030518,
      "learning_rate": 4.0735871271585554e-05,
      "loss": 0.631,
      "step": 944200
    },
    {
      "epoch": 14.824175824175825,
      "grad_norm": 3.3117849826812744,
      "learning_rate": 4.073489010989011e-05,
      "loss": 0.6147,
      "step": 944300
    },
    {
      "epoch": 14.82574568288854,
      "grad_norm": 2.6370012760162354,
      "learning_rate": 4.073390894819466e-05,
      "loss": 0.6837,
      "step": 944400
    },
    {
      "epoch": 14.827315541601255,
      "grad_norm": 4.220399379730225,
      "learning_rate": 4.0732927786499214e-05,
      "loss": 0.6441,
      "step": 944500
    },
    {
      "epoch": 14.828885400313972,
      "grad_norm": 4.247002124786377,
      "learning_rate": 4.073194662480377e-05,
      "loss": 0.6071,
      "step": 944600
    },
    {
      "epoch": 14.830455259026687,
      "grad_norm": 3.019644021987915,
      "learning_rate": 4.073096546310832e-05,
      "loss": 0.6091,
      "step": 944700
    },
    {
      "epoch": 14.832025117739404,
      "grad_norm": 4.473514080047607,
      "learning_rate": 4.0729984301412874e-05,
      "loss": 0.6165,
      "step": 944800
    },
    {
      "epoch": 14.833594976452119,
      "grad_norm": 3.44492244720459,
      "learning_rate": 4.0729003139717425e-05,
      "loss": 0.6105,
      "step": 944900
    },
    {
      "epoch": 14.835164835164836,
      "grad_norm": 4.344925880432129,
      "learning_rate": 4.072802197802198e-05,
      "loss": 0.6375,
      "step": 945000
    },
    {
      "epoch": 14.83673469387755,
      "grad_norm": 5.657529830932617,
      "learning_rate": 4.0727040816326533e-05,
      "loss": 0.6164,
      "step": 945100
    },
    {
      "epoch": 14.838304552590268,
      "grad_norm": 2.903330087661743,
      "learning_rate": 4.0726059654631084e-05,
      "loss": 0.623,
      "step": 945200
    },
    {
      "epoch": 14.839874411302983,
      "grad_norm": 3.795356035232544,
      "learning_rate": 4.0725078492935635e-05,
      "loss": 0.6009,
      "step": 945300
    },
    {
      "epoch": 14.841444270015698,
      "grad_norm": 4.8277907371521,
      "learning_rate": 4.072409733124019e-05,
      "loss": 0.6226,
      "step": 945400
    },
    {
      "epoch": 14.843014128728415,
      "grad_norm": 2.7475156784057617,
      "learning_rate": 4.0723116169544744e-05,
      "loss": 0.6069,
      "step": 945500
    },
    {
      "epoch": 14.84458398744113,
      "grad_norm": 3.7047994136810303,
      "learning_rate": 4.0722135007849295e-05,
      "loss": 0.5973,
      "step": 945600
    },
    {
      "epoch": 14.846153846153847,
      "grad_norm": 3.6051673889160156,
      "learning_rate": 4.0721153846153846e-05,
      "loss": 0.6193,
      "step": 945700
    },
    {
      "epoch": 14.847723704866562,
      "grad_norm": 4.626852512359619,
      "learning_rate": 4.0720172684458404e-05,
      "loss": 0.5973,
      "step": 945800
    },
    {
      "epoch": 14.849293563579279,
      "grad_norm": 3.356602430343628,
      "learning_rate": 4.071919152276295e-05,
      "loss": 0.6523,
      "step": 945900
    },
    {
      "epoch": 14.850863422291994,
      "grad_norm": 3.5435707569122314,
      "learning_rate": 4.0718210361067506e-05,
      "loss": 0.6237,
      "step": 946000
    },
    {
      "epoch": 14.852433281004709,
      "grad_norm": 3.2556252479553223,
      "learning_rate": 4.071722919937206e-05,
      "loss": 0.6301,
      "step": 946100
    },
    {
      "epoch": 14.854003139717426,
      "grad_norm": 4.2769341468811035,
      "learning_rate": 4.0716248037676615e-05,
      "loss": 0.6161,
      "step": 946200
    },
    {
      "epoch": 14.85557299843014,
      "grad_norm": 3.9491944313049316,
      "learning_rate": 4.071526687598116e-05,
      "loss": 0.6073,
      "step": 946300
    },
    {
      "epoch": 14.857142857142858,
      "grad_norm": 3.590397834777832,
      "learning_rate": 4.0714285714285717e-05,
      "loss": 0.6284,
      "step": 946400
    },
    {
      "epoch": 14.858712715855573,
      "grad_norm": 3.59936785697937,
      "learning_rate": 4.071330455259027e-05,
      "loss": 0.608,
      "step": 946500
    },
    {
      "epoch": 14.86028257456829,
      "grad_norm": 3.5858957767486572,
      "learning_rate": 4.071232339089482e-05,
      "loss": 0.6684,
      "step": 946600
    },
    {
      "epoch": 14.861852433281005,
      "grad_norm": 4.395793914794922,
      "learning_rate": 4.0711342229199376e-05,
      "loss": 0.5621,
      "step": 946700
    },
    {
      "epoch": 14.86342229199372,
      "grad_norm": 3.499803066253662,
      "learning_rate": 4.071036106750393e-05,
      "loss": 0.6119,
      "step": 946800
    },
    {
      "epoch": 14.864992150706437,
      "grad_norm": 2.2925264835357666,
      "learning_rate": 4.070937990580848e-05,
      "loss": 0.5837,
      "step": 946900
    },
    {
      "epoch": 14.866562009419152,
      "grad_norm": 3.837872266769409,
      "learning_rate": 4.070839874411303e-05,
      "loss": 0.6445,
      "step": 947000
    },
    {
      "epoch": 14.868131868131869,
      "grad_norm": 3.3512468338012695,
      "learning_rate": 4.070741758241759e-05,
      "loss": 0.5886,
      "step": 947100
    },
    {
      "epoch": 14.869701726844584,
      "grad_norm": 3.156240463256836,
      "learning_rate": 4.070643642072214e-05,
      "loss": 0.6409,
      "step": 947200
    },
    {
      "epoch": 14.8712715855573,
      "grad_norm": 2.6214938163757324,
      "learning_rate": 4.070545525902669e-05,
      "loss": 0.6098,
      "step": 947300
    },
    {
      "epoch": 14.872841444270016,
      "grad_norm": 3.3123316764831543,
      "learning_rate": 4.070447409733124e-05,
      "loss": 0.6199,
      "step": 947400
    },
    {
      "epoch": 14.87441130298273,
      "grad_norm": 4.969662666320801,
      "learning_rate": 4.07034929356358e-05,
      "loss": 0.6347,
      "step": 947500
    },
    {
      "epoch": 14.875981161695448,
      "grad_norm": 3.256930112838745,
      "learning_rate": 4.070251177394035e-05,
      "loss": 0.6057,
      "step": 947600
    },
    {
      "epoch": 14.877551020408163,
      "grad_norm": 4.463028907775879,
      "learning_rate": 4.07015306122449e-05,
      "loss": 0.6082,
      "step": 947700
    },
    {
      "epoch": 14.87912087912088,
      "grad_norm": 4.254424095153809,
      "learning_rate": 4.070054945054945e-05,
      "loss": 0.6006,
      "step": 947800
    },
    {
      "epoch": 14.880690737833595,
      "grad_norm": 4.118933200836182,
      "learning_rate": 4.069956828885401e-05,
      "loss": 0.6486,
      "step": 947900
    },
    {
      "epoch": 14.882260596546312,
      "grad_norm": 4.053678035736084,
      "learning_rate": 4.069858712715855e-05,
      "loss": 0.5971,
      "step": 948000
    },
    {
      "epoch": 14.883830455259027,
      "grad_norm": 4.210049629211426,
      "learning_rate": 4.069760596546311e-05,
      "loss": 0.6182,
      "step": 948100
    },
    {
      "epoch": 14.885400313971743,
      "grad_norm": 4.445089817047119,
      "learning_rate": 4.069662480376766e-05,
      "loss": 0.626,
      "step": 948200
    },
    {
      "epoch": 14.886970172684459,
      "grad_norm": 3.8354718685150146,
      "learning_rate": 4.069564364207222e-05,
      "loss": 0.6395,
      "step": 948300
    },
    {
      "epoch": 14.888540031397174,
      "grad_norm": 4.601908206939697,
      "learning_rate": 4.069466248037676e-05,
      "loss": 0.6285,
      "step": 948400
    },
    {
      "epoch": 14.89010989010989,
      "grad_norm": 2.8850061893463135,
      "learning_rate": 4.069368131868132e-05,
      "loss": 0.5963,
      "step": 948500
    },
    {
      "epoch": 14.891679748822606,
      "grad_norm": 3.919562816619873,
      "learning_rate": 4.069270015698587e-05,
      "loss": 0.6113,
      "step": 948600
    },
    {
      "epoch": 14.893249607535322,
      "grad_norm": 3.535017251968384,
      "learning_rate": 4.069171899529042e-05,
      "loss": 0.6165,
      "step": 948700
    },
    {
      "epoch": 14.894819466248038,
      "grad_norm": 3.9006965160369873,
      "learning_rate": 4.069073783359498e-05,
      "loss": 0.6301,
      "step": 948800
    },
    {
      "epoch": 14.896389324960754,
      "grad_norm": 3.4771411418914795,
      "learning_rate": 4.068975667189953e-05,
      "loss": 0.6378,
      "step": 948900
    },
    {
      "epoch": 14.89795918367347,
      "grad_norm": 3.1517107486724854,
      "learning_rate": 4.068877551020408e-05,
      "loss": 0.5941,
      "step": 949000
    },
    {
      "epoch": 14.899529042386185,
      "grad_norm": 3.1231958866119385,
      "learning_rate": 4.0687794348508634e-05,
      "loss": 0.5859,
      "step": 949100
    },
    {
      "epoch": 14.901098901098901,
      "grad_norm": 4.304347038269043,
      "learning_rate": 4.068681318681319e-05,
      "loss": 0.619,
      "step": 949200
    },
    {
      "epoch": 14.902668759811617,
      "grad_norm": 3.316225528717041,
      "learning_rate": 4.068583202511774e-05,
      "loss": 0.6458,
      "step": 949300
    },
    {
      "epoch": 14.904238618524333,
      "grad_norm": 4.1699724197387695,
      "learning_rate": 4.0684850863422293e-05,
      "loss": 0.6143,
      "step": 949400
    },
    {
      "epoch": 14.905808477237048,
      "grad_norm": 4.8594069480896,
      "learning_rate": 4.0683869701726844e-05,
      "loss": 0.5654,
      "step": 949500
    },
    {
      "epoch": 14.907378335949765,
      "grad_norm": 4.555063724517822,
      "learning_rate": 4.06828885400314e-05,
      "loss": 0.6321,
      "step": 949600
    },
    {
      "epoch": 14.90894819466248,
      "grad_norm": 4.16908597946167,
      "learning_rate": 4.068190737833595e-05,
      "loss": 0.6325,
      "step": 949700
    },
    {
      "epoch": 14.910518053375196,
      "grad_norm": 3.562187671661377,
      "learning_rate": 4.0680926216640504e-05,
      "loss": 0.6257,
      "step": 949800
    },
    {
      "epoch": 14.912087912087912,
      "grad_norm": 3.6991443634033203,
      "learning_rate": 4.0679945054945055e-05,
      "loss": 0.6676,
      "step": 949900
    },
    {
      "epoch": 14.913657770800627,
      "grad_norm": 5.247241020202637,
      "learning_rate": 4.067896389324961e-05,
      "loss": 0.6155,
      "step": 950000
    },
    {
      "epoch": 14.915227629513344,
      "grad_norm": 3.18326473236084,
      "learning_rate": 4.067798273155416e-05,
      "loss": 0.6677,
      "step": 950100
    },
    {
      "epoch": 14.91679748822606,
      "grad_norm": 3.139638900756836,
      "learning_rate": 4.0677001569858715e-05,
      "loss": 0.616,
      "step": 950200
    },
    {
      "epoch": 14.918367346938776,
      "grad_norm": 4.747535228729248,
      "learning_rate": 4.0676020408163266e-05,
      "loss": 0.6252,
      "step": 950300
    },
    {
      "epoch": 14.919937205651491,
      "grad_norm": 3.175844192504883,
      "learning_rate": 4.0675039246467824e-05,
      "loss": 0.5839,
      "step": 950400
    },
    {
      "epoch": 14.921507064364206,
      "grad_norm": 3.3042142391204834,
      "learning_rate": 4.067405808477237e-05,
      "loss": 0.6063,
      "step": 950500
    },
    {
      "epoch": 14.923076923076923,
      "grad_norm": 3.5372719764709473,
      "learning_rate": 4.0673076923076926e-05,
      "loss": 0.6153,
      "step": 950600
    },
    {
      "epoch": 14.924646781789638,
      "grad_norm": 4.752818584442139,
      "learning_rate": 4.0672095761381476e-05,
      "loss": 0.6342,
      "step": 950700
    },
    {
      "epoch": 14.926216640502355,
      "grad_norm": 3.4605822563171387,
      "learning_rate": 4.067111459968603e-05,
      "loss": 0.5927,
      "step": 950800
    },
    {
      "epoch": 14.92778649921507,
      "grad_norm": 4.3112311363220215,
      "learning_rate": 4.0670133437990585e-05,
      "loss": 0.6174,
      "step": 950900
    },
    {
      "epoch": 14.929356357927787,
      "grad_norm": 4.493751525878906,
      "learning_rate": 4.0669152276295136e-05,
      "loss": 0.6207,
      "step": 951000
    },
    {
      "epoch": 14.930926216640502,
      "grad_norm": 2.6703715324401855,
      "learning_rate": 4.066817111459969e-05,
      "loss": 0.6217,
      "step": 951100
    },
    {
      "epoch": 14.932496075353217,
      "grad_norm": 4.3924431800842285,
      "learning_rate": 4.066718995290424e-05,
      "loss": 0.6011,
      "step": 951200
    },
    {
      "epoch": 14.934065934065934,
      "grad_norm": 4.275234222412109,
      "learning_rate": 4.0666208791208796e-05,
      "loss": 0.645,
      "step": 951300
    },
    {
      "epoch": 14.93563579277865,
      "grad_norm": 3.3836231231689453,
      "learning_rate": 4.066522762951335e-05,
      "loss": 0.6065,
      "step": 951400
    },
    {
      "epoch": 14.937205651491366,
      "grad_norm": 3.736616373062134,
      "learning_rate": 4.06642464678179e-05,
      "loss": 0.5941,
      "step": 951500
    },
    {
      "epoch": 14.938775510204081,
      "grad_norm": 4.027653217315674,
      "learning_rate": 4.066326530612245e-05,
      "loss": 0.6323,
      "step": 951600
    },
    {
      "epoch": 14.940345368916798,
      "grad_norm": 4.3454718589782715,
      "learning_rate": 4.066228414442701e-05,
      "loss": 0.6057,
      "step": 951700
    },
    {
      "epoch": 14.941915227629513,
      "grad_norm": 4.2439446449279785,
      "learning_rate": 4.066130298273156e-05,
      "loss": 0.6115,
      "step": 951800
    },
    {
      "epoch": 14.943485086342228,
      "grad_norm": 3.936133623123169,
      "learning_rate": 4.066032182103611e-05,
      "loss": 0.6024,
      "step": 951900
    },
    {
      "epoch": 14.945054945054945,
      "grad_norm": 4.279112815856934,
      "learning_rate": 4.065934065934066e-05,
      "loss": 0.6407,
      "step": 952000
    },
    {
      "epoch": 14.94662480376766,
      "grad_norm": 3.692075729370117,
      "learning_rate": 4.065835949764522e-05,
      "loss": 0.6277,
      "step": 952100
    },
    {
      "epoch": 14.948194662480377,
      "grad_norm": 3.995126962661743,
      "learning_rate": 4.065737833594976e-05,
      "loss": 0.64,
      "step": 952200
    },
    {
      "epoch": 14.949764521193092,
      "grad_norm": 3.6617138385772705,
      "learning_rate": 4.065639717425432e-05,
      "loss": 0.6232,
      "step": 952300
    },
    {
      "epoch": 14.95133437990581,
      "grad_norm": 4.543691635131836,
      "learning_rate": 4.065541601255887e-05,
      "loss": 0.6521,
      "step": 952400
    },
    {
      "epoch": 14.952904238618524,
      "grad_norm": 4.917044639587402,
      "learning_rate": 4.065443485086343e-05,
      "loss": 0.6457,
      "step": 952500
    },
    {
      "epoch": 14.95447409733124,
      "grad_norm": 3.659238815307617,
      "learning_rate": 4.065345368916797e-05,
      "loss": 0.6415,
      "step": 952600
    },
    {
      "epoch": 14.956043956043956,
      "grad_norm": 3.263278007507324,
      "learning_rate": 4.065247252747253e-05,
      "loss": 0.6078,
      "step": 952700
    },
    {
      "epoch": 14.957613814756671,
      "grad_norm": 3.387815475463867,
      "learning_rate": 4.065149136577708e-05,
      "loss": 0.6078,
      "step": 952800
    },
    {
      "epoch": 14.959183673469388,
      "grad_norm": 4.38808536529541,
      "learning_rate": 4.065051020408163e-05,
      "loss": 0.6022,
      "step": 952900
    },
    {
      "epoch": 14.960753532182103,
      "grad_norm": 4.461512565612793,
      "learning_rate": 4.064952904238618e-05,
      "loss": 0.6278,
      "step": 953000
    },
    {
      "epoch": 14.96232339089482,
      "grad_norm": 2.872098207473755,
      "learning_rate": 4.064854788069074e-05,
      "loss": 0.6569,
      "step": 953100
    },
    {
      "epoch": 14.963893249607535,
      "grad_norm": 3.877516269683838,
      "learning_rate": 4.064756671899529e-05,
      "loss": 0.5909,
      "step": 953200
    },
    {
      "epoch": 14.96546310832025,
      "grad_norm": 3.065145254135132,
      "learning_rate": 4.064658555729984e-05,
      "loss": 0.6354,
      "step": 953300
    },
    {
      "epoch": 14.967032967032967,
      "grad_norm": 2.9012162685394287,
      "learning_rate": 4.06456043956044e-05,
      "loss": 0.5932,
      "step": 953400
    },
    {
      "epoch": 14.968602825745682,
      "grad_norm": 3.7776482105255127,
      "learning_rate": 4.064462323390895e-05,
      "loss": 0.6042,
      "step": 953500
    },
    {
      "epoch": 14.970172684458399,
      "grad_norm": 4.53778076171875,
      "learning_rate": 4.06436420722135e-05,
      "loss": 0.6203,
      "step": 953600
    },
    {
      "epoch": 14.971742543171114,
      "grad_norm": 4.704415321350098,
      "learning_rate": 4.064266091051805e-05,
      "loss": 0.6344,
      "step": 953700
    },
    {
      "epoch": 14.973312401883831,
      "grad_norm": 3.7058942317962646,
      "learning_rate": 4.064167974882261e-05,
      "loss": 0.6527,
      "step": 953800
    },
    {
      "epoch": 14.974882260596546,
      "grad_norm": 5.346062660217285,
      "learning_rate": 4.064069858712716e-05,
      "loss": 0.6046,
      "step": 953900
    },
    {
      "epoch": 14.976452119309261,
      "grad_norm": 2.3895652294158936,
      "learning_rate": 4.063971742543171e-05,
      "loss": 0.6525,
      "step": 954000
    },
    {
      "epoch": 14.978021978021978,
      "grad_norm": 3.0708000659942627,
      "learning_rate": 4.0638736263736264e-05,
      "loss": 0.6115,
      "step": 954100
    },
    {
      "epoch": 14.979591836734693,
      "grad_norm": 3.183156967163086,
      "learning_rate": 4.063775510204082e-05,
      "loss": 0.5901,
      "step": 954200
    },
    {
      "epoch": 14.98116169544741,
      "grad_norm": 3.4041497707366943,
      "learning_rate": 4.0636773940345366e-05,
      "loss": 0.5964,
      "step": 954300
    },
    {
      "epoch": 14.982731554160125,
      "grad_norm": 4.5583176612854,
      "learning_rate": 4.0635792778649924e-05,
      "loss": 0.602,
      "step": 954400
    },
    {
      "epoch": 14.984301412872842,
      "grad_norm": 2.891331195831299,
      "learning_rate": 4.0634811616954475e-05,
      "loss": 0.6186,
      "step": 954500
    },
    {
      "epoch": 14.985871271585557,
      "grad_norm": 3.72452974319458,
      "learning_rate": 4.063383045525903e-05,
      "loss": 0.6162,
      "step": 954600
    },
    {
      "epoch": 14.987441130298274,
      "grad_norm": 4.694773197174072,
      "learning_rate": 4.063284929356358e-05,
      "loss": 0.6306,
      "step": 954700
    },
    {
      "epoch": 14.989010989010989,
      "grad_norm": 3.1525497436523438,
      "learning_rate": 4.0631868131868134e-05,
      "loss": 0.5799,
      "step": 954800
    },
    {
      "epoch": 14.990580847723704,
      "grad_norm": 4.905395030975342,
      "learning_rate": 4.0630886970172685e-05,
      "loss": 0.5766,
      "step": 954900
    },
    {
      "epoch": 14.992150706436421,
      "grad_norm": 4.3859100341796875,
      "learning_rate": 4.0629905808477236e-05,
      "loss": 0.6323,
      "step": 955000
    },
    {
      "epoch": 14.993720565149136,
      "grad_norm": 3.2143120765686035,
      "learning_rate": 4.062892464678179e-05,
      "loss": 0.6086,
      "step": 955100
    },
    {
      "epoch": 14.995290423861853,
      "grad_norm": 4.305099964141846,
      "learning_rate": 4.0627943485086345e-05,
      "loss": 0.6277,
      "step": 955200
    },
    {
      "epoch": 14.996860282574568,
      "grad_norm": 3.0875930786132812,
      "learning_rate": 4.0626962323390896e-05,
      "loss": 0.5833,
      "step": 955300
    },
    {
      "epoch": 14.998430141287285,
      "grad_norm": 2.9272258281707764,
      "learning_rate": 4.062598116169545e-05,
      "loss": 0.6425,
      "step": 955400
    },
    {
      "epoch": 15.0,
      "grad_norm": 4.157111167907715,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 0.6148,
      "step": 955500
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.0357273817062378,
      "eval_runtime": 15.1711,
      "eval_samples_per_second": 221.012,
      "eval_steps_per_second": 221.012,
      "step": 955500
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.47938182950019836,
      "eval_runtime": 281.8438,
      "eval_samples_per_second": 226.012,
      "eval_steps_per_second": 226.012,
      "step": 955500
    },
    {
      "epoch": 15.001569858712715,
      "grad_norm": 3.227778434753418,
      "learning_rate": 4.0624018838304556e-05,
      "loss": 0.6273,
      "step": 955600
    },
    {
      "epoch": 15.003139717425432,
      "grad_norm": 4.133028984069824,
      "learning_rate": 4.062303767660911e-05,
      "loss": 0.6294,
      "step": 955700
    },
    {
      "epoch": 15.004709576138147,
      "grad_norm": 3.45493745803833,
      "learning_rate": 4.062205651491366e-05,
      "loss": 0.5983,
      "step": 955800
    },
    {
      "epoch": 15.006279434850864,
      "grad_norm": 4.42786979675293,
      "learning_rate": 4.0621075353218216e-05,
      "loss": 0.6076,
      "step": 955900
    },
    {
      "epoch": 15.007849293563579,
      "grad_norm": 3.989544153213501,
      "learning_rate": 4.0620094191522767e-05,
      "loss": 0.5931,
      "step": 956000
    },
    {
      "epoch": 15.009419152276296,
      "grad_norm": 4.246575355529785,
      "learning_rate": 4.061911302982732e-05,
      "loss": 0.6443,
      "step": 956100
    },
    {
      "epoch": 15.010989010989011,
      "grad_norm": 3.379533529281616,
      "learning_rate": 4.061813186813187e-05,
      "loss": 0.6048,
      "step": 956200
    },
    {
      "epoch": 15.012558869701726,
      "grad_norm": 4.1124958992004395,
      "learning_rate": 4.0617150706436426e-05,
      "loss": 0.5949,
      "step": 956300
    },
    {
      "epoch": 15.014128728414443,
      "grad_norm": 2.601661443710327,
      "learning_rate": 4.061616954474097e-05,
      "loss": 0.6312,
      "step": 956400
    },
    {
      "epoch": 15.015698587127158,
      "grad_norm": 3.4025747776031494,
      "learning_rate": 4.061518838304553e-05,
      "loss": 0.6198,
      "step": 956500
    },
    {
      "epoch": 15.017268445839875,
      "grad_norm": 4.031339645385742,
      "learning_rate": 4.061420722135008e-05,
      "loss": 0.6042,
      "step": 956600
    },
    {
      "epoch": 15.01883830455259,
      "grad_norm": 3.091945171356201,
      "learning_rate": 4.061322605965464e-05,
      "loss": 0.5772,
      "step": 956700
    },
    {
      "epoch": 15.020408163265307,
      "grad_norm": 4.103001117706299,
      "learning_rate": 4.061224489795918e-05,
      "loss": 0.6116,
      "step": 956800
    },
    {
      "epoch": 15.021978021978022,
      "grad_norm": 3.392543315887451,
      "learning_rate": 4.061126373626374e-05,
      "loss": 0.6064,
      "step": 956900
    },
    {
      "epoch": 15.023547880690737,
      "grad_norm": 4.354056358337402,
      "learning_rate": 4.061028257456829e-05,
      "loss": 0.5993,
      "step": 957000
    },
    {
      "epoch": 15.025117739403454,
      "grad_norm": 4.17401123046875,
      "learning_rate": 4.060930141287284e-05,
      "loss": 0.6635,
      "step": 957100
    },
    {
      "epoch": 15.026687598116169,
      "grad_norm": 4.4341535568237305,
      "learning_rate": 4.060832025117739e-05,
      "loss": 0.6309,
      "step": 957200
    },
    {
      "epoch": 15.028257456828886,
      "grad_norm": 3.529086112976074,
      "learning_rate": 4.060733908948195e-05,
      "loss": 0.6314,
      "step": 957300
    },
    {
      "epoch": 15.029827315541601,
      "grad_norm": 4.147968292236328,
      "learning_rate": 4.06063579277865e-05,
      "loss": 0.5984,
      "step": 957400
    },
    {
      "epoch": 15.031397174254318,
      "grad_norm": 4.350730895996094,
      "learning_rate": 4.060537676609105e-05,
      "loss": 0.633,
      "step": 957500
    },
    {
      "epoch": 15.032967032967033,
      "grad_norm": 3.583587884902954,
      "learning_rate": 4.060439560439561e-05,
      "loss": 0.5768,
      "step": 957600
    },
    {
      "epoch": 15.034536891679748,
      "grad_norm": 3.2164337635040283,
      "learning_rate": 4.060341444270016e-05,
      "loss": 0.6339,
      "step": 957700
    },
    {
      "epoch": 15.036106750392465,
      "grad_norm": 4.4392008781433105,
      "learning_rate": 4.060243328100471e-05,
      "loss": 0.5557,
      "step": 957800
    },
    {
      "epoch": 15.03767660910518,
      "grad_norm": 3.682537317276001,
      "learning_rate": 4.060145211930926e-05,
      "loss": 0.607,
      "step": 957900
    },
    {
      "epoch": 15.039246467817897,
      "grad_norm": 4.16324520111084,
      "learning_rate": 4.060047095761382e-05,
      "loss": 0.601,
      "step": 958000
    },
    {
      "epoch": 15.040816326530612,
      "grad_norm": 3.2371857166290283,
      "learning_rate": 4.059948979591837e-05,
      "loss": 0.6187,
      "step": 958100
    },
    {
      "epoch": 15.042386185243329,
      "grad_norm": 3.3038923740386963,
      "learning_rate": 4.059850863422292e-05,
      "loss": 0.604,
      "step": 958200
    },
    {
      "epoch": 15.043956043956044,
      "grad_norm": 2.8831541538238525,
      "learning_rate": 4.059752747252747e-05,
      "loss": 0.6294,
      "step": 958300
    },
    {
      "epoch": 15.04552590266876,
      "grad_norm": 2.4212839603424072,
      "learning_rate": 4.059654631083203e-05,
      "loss": 0.6249,
      "step": 958400
    },
    {
      "epoch": 15.047095761381476,
      "grad_norm": 3.9267194271087646,
      "learning_rate": 4.0595565149136575e-05,
      "loss": 0.6163,
      "step": 958500
    },
    {
      "epoch": 15.04866562009419,
      "grad_norm": 2.8723504543304443,
      "learning_rate": 4.059458398744113e-05,
      "loss": 0.5896,
      "step": 958600
    },
    {
      "epoch": 15.050235478806908,
      "grad_norm": 3.6320905685424805,
      "learning_rate": 4.0593602825745684e-05,
      "loss": 0.6011,
      "step": 958700
    },
    {
      "epoch": 15.051805337519623,
      "grad_norm": 2.572147846221924,
      "learning_rate": 4.059262166405024e-05,
      "loss": 0.6203,
      "step": 958800
    },
    {
      "epoch": 15.05337519623234,
      "grad_norm": 4.362858295440674,
      "learning_rate": 4.0591640502354786e-05,
      "loss": 0.6313,
      "step": 958900
    },
    {
      "epoch": 15.054945054945055,
      "grad_norm": 4.620977878570557,
      "learning_rate": 4.0590659340659343e-05,
      "loss": 0.6128,
      "step": 959000
    },
    {
      "epoch": 15.056514913657772,
      "grad_norm": 4.577932834625244,
      "learning_rate": 4.0589678178963894e-05,
      "loss": 0.6395,
      "step": 959100
    },
    {
      "epoch": 15.058084772370487,
      "grad_norm": 3.4820501804351807,
      "learning_rate": 4.0588697017268445e-05,
      "loss": 0.6104,
      "step": 959200
    },
    {
      "epoch": 15.059654631083202,
      "grad_norm": 3.6277647018432617,
      "learning_rate": 4.0587715855572996e-05,
      "loss": 0.6185,
      "step": 959300
    },
    {
      "epoch": 15.061224489795919,
      "grad_norm": 5.294102668762207,
      "learning_rate": 4.0586734693877554e-05,
      "loss": 0.5672,
      "step": 959400
    },
    {
      "epoch": 15.062794348508634,
      "grad_norm": 3.8363261222839355,
      "learning_rate": 4.0585753532182105e-05,
      "loss": 0.6171,
      "step": 959500
    },
    {
      "epoch": 15.06436420722135,
      "grad_norm": 3.918919801712036,
      "learning_rate": 4.0584772370486656e-05,
      "loss": 0.6419,
      "step": 959600
    },
    {
      "epoch": 15.065934065934066,
      "grad_norm": 3.110793113708496,
      "learning_rate": 4.0583791208791214e-05,
      "loss": 0.6195,
      "step": 959700
    },
    {
      "epoch": 15.067503924646783,
      "grad_norm": 3.608491897583008,
      "learning_rate": 4.0582810047095765e-05,
      "loss": 0.5889,
      "step": 959800
    },
    {
      "epoch": 15.069073783359498,
      "grad_norm": 2.533220052719116,
      "learning_rate": 4.0581828885400316e-05,
      "loss": 0.5969,
      "step": 959900
    },
    {
      "epoch": 15.070643642072213,
      "grad_norm": 3.3191921710968018,
      "learning_rate": 4.058084772370487e-05,
      "loss": 0.633,
      "step": 960000
    },
    {
      "epoch": 15.07221350078493,
      "grad_norm": 4.400081634521484,
      "learning_rate": 4.0579866562009425e-05,
      "loss": 0.5994,
      "step": 960100
    },
    {
      "epoch": 15.073783359497645,
      "grad_norm": 3.960489511489868,
      "learning_rate": 4.0578885400313976e-05,
      "loss": 0.5847,
      "step": 960200
    },
    {
      "epoch": 15.075353218210362,
      "grad_norm": 3.8613507747650146,
      "learning_rate": 4.0577904238618527e-05,
      "loss": 0.6564,
      "step": 960300
    },
    {
      "epoch": 15.076923076923077,
      "grad_norm": 3.95867657661438,
      "learning_rate": 4.057692307692308e-05,
      "loss": 0.6446,
      "step": 960400
    },
    {
      "epoch": 15.078492935635794,
      "grad_norm": 4.522202014923096,
      "learning_rate": 4.0575941915227635e-05,
      "loss": 0.6113,
      "step": 960500
    },
    {
      "epoch": 15.080062794348509,
      "grad_norm": 3.9841532707214355,
      "learning_rate": 4.057496075353218e-05,
      "loss": 0.6193,
      "step": 960600
    },
    {
      "epoch": 15.081632653061224,
      "grad_norm": 4.4676361083984375,
      "learning_rate": 4.057397959183674e-05,
      "loss": 0.6124,
      "step": 960700
    },
    {
      "epoch": 15.08320251177394,
      "grad_norm": 3.673931360244751,
      "learning_rate": 4.057299843014129e-05,
      "loss": 0.6469,
      "step": 960800
    },
    {
      "epoch": 15.084772370486656,
      "grad_norm": 3.154404640197754,
      "learning_rate": 4.0572017268445846e-05,
      "loss": 0.6075,
      "step": 960900
    },
    {
      "epoch": 15.086342229199373,
      "grad_norm": 3.097708225250244,
      "learning_rate": 4.057103610675039e-05,
      "loss": 0.5717,
      "step": 961000
    },
    {
      "epoch": 15.087912087912088,
      "grad_norm": 3.8781304359436035,
      "learning_rate": 4.057005494505495e-05,
      "loss": 0.6123,
      "step": 961100
    },
    {
      "epoch": 15.089481946624804,
      "grad_norm": 2.9144935607910156,
      "learning_rate": 4.05690737833595e-05,
      "loss": 0.6284,
      "step": 961200
    },
    {
      "epoch": 15.09105180533752,
      "grad_norm": 3.1128830909729004,
      "learning_rate": 4.056809262166405e-05,
      "loss": 0.5976,
      "step": 961300
    },
    {
      "epoch": 15.092621664050235,
      "grad_norm": 3.9431283473968506,
      "learning_rate": 4.05671114599686e-05,
      "loss": 0.598,
      "step": 961400
    },
    {
      "epoch": 15.094191522762952,
      "grad_norm": 3.4102156162261963,
      "learning_rate": 4.056613029827316e-05,
      "loss": 0.5786,
      "step": 961500
    },
    {
      "epoch": 15.095761381475667,
      "grad_norm": 3.5932886600494385,
      "learning_rate": 4.056514913657771e-05,
      "loss": 0.6015,
      "step": 961600
    },
    {
      "epoch": 15.097331240188383,
      "grad_norm": 3.527339458465576,
      "learning_rate": 4.056416797488226e-05,
      "loss": 0.6382,
      "step": 961700
    },
    {
      "epoch": 15.098901098901099,
      "grad_norm": 3.3868629932403564,
      "learning_rate": 4.056318681318682e-05,
      "loss": 0.6261,
      "step": 961800
    },
    {
      "epoch": 15.100470957613815,
      "grad_norm": 3.841729164123535,
      "learning_rate": 4.056220565149137e-05,
      "loss": 0.5576,
      "step": 961900
    },
    {
      "epoch": 15.10204081632653,
      "grad_norm": 2.720735788345337,
      "learning_rate": 4.056122448979592e-05,
      "loss": 0.6161,
      "step": 962000
    },
    {
      "epoch": 15.103610675039246,
      "grad_norm": 3.027149200439453,
      "learning_rate": 4.056024332810047e-05,
      "loss": 0.6274,
      "step": 962100
    },
    {
      "epoch": 15.105180533751962,
      "grad_norm": 4.1373701095581055,
      "learning_rate": 4.055926216640503e-05,
      "loss": 0.6499,
      "step": 962200
    },
    {
      "epoch": 15.106750392464678,
      "grad_norm": 3.026979684829712,
      "learning_rate": 4.055828100470958e-05,
      "loss": 0.6051,
      "step": 962300
    },
    {
      "epoch": 15.108320251177394,
      "grad_norm": 3.7598659992218018,
      "learning_rate": 4.055729984301413e-05,
      "loss": 0.5879,
      "step": 962400
    },
    {
      "epoch": 15.10989010989011,
      "grad_norm": 4.157015323638916,
      "learning_rate": 4.055631868131868e-05,
      "loss": 0.6573,
      "step": 962500
    },
    {
      "epoch": 15.111459968602826,
      "grad_norm": 3.2941930294036865,
      "learning_rate": 4.055533751962324e-05,
      "loss": 0.6089,
      "step": 962600
    },
    {
      "epoch": 15.113029827315541,
      "grad_norm": 2.856947660446167,
      "learning_rate": 4.0554356357927784e-05,
      "loss": 0.6141,
      "step": 962700
    },
    {
      "epoch": 15.114599686028258,
      "grad_norm": 3.9438769817352295,
      "learning_rate": 4.055337519623234e-05,
      "loss": 0.6368,
      "step": 962800
    },
    {
      "epoch": 15.116169544740973,
      "grad_norm": 2.7597525119781494,
      "learning_rate": 4.055239403453689e-05,
      "loss": 0.6272,
      "step": 962900
    },
    {
      "epoch": 15.117739403453688,
      "grad_norm": 2.215453863143921,
      "learning_rate": 4.055141287284145e-05,
      "loss": 0.5968,
      "step": 963000
    },
    {
      "epoch": 15.119309262166405,
      "grad_norm": 4.033047199249268,
      "learning_rate": 4.0550431711145995e-05,
      "loss": 0.5868,
      "step": 963100
    },
    {
      "epoch": 15.12087912087912,
      "grad_norm": 3.184680938720703,
      "learning_rate": 4.054945054945055e-05,
      "loss": 0.611,
      "step": 963200
    },
    {
      "epoch": 15.122448979591837,
      "grad_norm": 3.28337025642395,
      "learning_rate": 4.0548469387755103e-05,
      "loss": 0.5954,
      "step": 963300
    },
    {
      "epoch": 15.124018838304552,
      "grad_norm": 5.066764831542969,
      "learning_rate": 4.0547488226059654e-05,
      "loss": 0.6481,
      "step": 963400
    },
    {
      "epoch": 15.12558869701727,
      "grad_norm": 5.4132513999938965,
      "learning_rate": 4.0546507064364205e-05,
      "loss": 0.5896,
      "step": 963500
    },
    {
      "epoch": 15.127158555729984,
      "grad_norm": 3.5415308475494385,
      "learning_rate": 4.054552590266876e-05,
      "loss": 0.6211,
      "step": 963600
    },
    {
      "epoch": 15.1287284144427,
      "grad_norm": 2.939028739929199,
      "learning_rate": 4.0544544740973314e-05,
      "loss": 0.5861,
      "step": 963700
    },
    {
      "epoch": 15.130298273155416,
      "grad_norm": 4.117015361785889,
      "learning_rate": 4.0543563579277865e-05,
      "loss": 0.5867,
      "step": 963800
    },
    {
      "epoch": 15.131868131868131,
      "grad_norm": 3.820540189743042,
      "learning_rate": 4.054258241758242e-05,
      "loss": 0.6275,
      "step": 963900
    },
    {
      "epoch": 15.133437990580848,
      "grad_norm": 4.339441299438477,
      "learning_rate": 4.0541601255886974e-05,
      "loss": 0.6249,
      "step": 964000
    },
    {
      "epoch": 15.135007849293563,
      "grad_norm": 4.626208782196045,
      "learning_rate": 4.0540620094191525e-05,
      "loss": 0.6203,
      "step": 964100
    },
    {
      "epoch": 15.13657770800628,
      "grad_norm": 3.5348618030548096,
      "learning_rate": 4.0539638932496076e-05,
      "loss": 0.6532,
      "step": 964200
    },
    {
      "epoch": 15.138147566718995,
      "grad_norm": 4.357620716094971,
      "learning_rate": 4.0538657770800634e-05,
      "loss": 0.6287,
      "step": 964300
    },
    {
      "epoch": 15.13971742543171,
      "grad_norm": 2.4356801509857178,
      "learning_rate": 4.0537676609105185e-05,
      "loss": 0.605,
      "step": 964400
    },
    {
      "epoch": 15.141287284144427,
      "grad_norm": 5.025331497192383,
      "learning_rate": 4.0536695447409736e-05,
      "loss": 0.6021,
      "step": 964500
    },
    {
      "epoch": 15.142857142857142,
      "grad_norm": 4.63831901550293,
      "learning_rate": 4.0535714285714287e-05,
      "loss": 0.5818,
      "step": 964600
    },
    {
      "epoch": 15.14442700156986,
      "grad_norm": 4.494921684265137,
      "learning_rate": 4.0534733124018844e-05,
      "loss": 0.6215,
      "step": 964700
    },
    {
      "epoch": 15.145996860282574,
      "grad_norm": 4.554730415344238,
      "learning_rate": 4.053375196232339e-05,
      "loss": 0.6052,
      "step": 964800
    },
    {
      "epoch": 15.147566718995291,
      "grad_norm": 3.9587645530700684,
      "learning_rate": 4.0532770800627946e-05,
      "loss": 0.5615,
      "step": 964900
    },
    {
      "epoch": 15.149136577708006,
      "grad_norm": 4.561827182769775,
      "learning_rate": 4.05317896389325e-05,
      "loss": 0.6114,
      "step": 965000
    },
    {
      "epoch": 15.150706436420721,
      "grad_norm": 4.397789001464844,
      "learning_rate": 4.0530808477237055e-05,
      "loss": 0.5822,
      "step": 965100
    },
    {
      "epoch": 15.152276295133438,
      "grad_norm": 4.204269886016846,
      "learning_rate": 4.05298273155416e-05,
      "loss": 0.6283,
      "step": 965200
    },
    {
      "epoch": 15.153846153846153,
      "grad_norm": 3.831064462661743,
      "learning_rate": 4.052884615384616e-05,
      "loss": 0.6534,
      "step": 965300
    },
    {
      "epoch": 15.15541601255887,
      "grad_norm": 4.8431782722473145,
      "learning_rate": 4.052786499215071e-05,
      "loss": 0.6471,
      "step": 965400
    },
    {
      "epoch": 15.156985871271585,
      "grad_norm": 3.8148229122161865,
      "learning_rate": 4.052688383045526e-05,
      "loss": 0.6164,
      "step": 965500
    },
    {
      "epoch": 15.158555729984302,
      "grad_norm": 4.324310779571533,
      "learning_rate": 4.052590266875981e-05,
      "loss": 0.647,
      "step": 965600
    },
    {
      "epoch": 15.160125588697017,
      "grad_norm": 3.3623039722442627,
      "learning_rate": 4.052492150706437e-05,
      "loss": 0.635,
      "step": 965700
    },
    {
      "epoch": 15.161695447409732,
      "grad_norm": 3.7205300331115723,
      "learning_rate": 4.052394034536892e-05,
      "loss": 0.5951,
      "step": 965800
    },
    {
      "epoch": 15.16326530612245,
      "grad_norm": 3.9832003116607666,
      "learning_rate": 4.052295918367347e-05,
      "loss": 0.6528,
      "step": 965900
    },
    {
      "epoch": 15.164835164835164,
      "grad_norm": 4.141756057739258,
      "learning_rate": 4.052197802197803e-05,
      "loss": 0.6094,
      "step": 966000
    },
    {
      "epoch": 15.166405023547881,
      "grad_norm": 4.17954683303833,
      "learning_rate": 4.052099686028258e-05,
      "loss": 0.6014,
      "step": 966100
    },
    {
      "epoch": 15.167974882260596,
      "grad_norm": 4.746174335479736,
      "learning_rate": 4.052001569858713e-05,
      "loss": 0.5931,
      "step": 966200
    },
    {
      "epoch": 15.169544740973313,
      "grad_norm": 4.2625017166137695,
      "learning_rate": 4.051903453689168e-05,
      "loss": 0.6212,
      "step": 966300
    },
    {
      "epoch": 15.171114599686028,
      "grad_norm": 3.843352794647217,
      "learning_rate": 4.051805337519624e-05,
      "loss": 0.6259,
      "step": 966400
    },
    {
      "epoch": 15.172684458398743,
      "grad_norm": 3.8896846771240234,
      "learning_rate": 4.051707221350079e-05,
      "loss": 0.6368,
      "step": 966500
    },
    {
      "epoch": 15.17425431711146,
      "grad_norm": 3.364628314971924,
      "learning_rate": 4.051609105180534e-05,
      "loss": 0.6495,
      "step": 966600
    },
    {
      "epoch": 15.175824175824175,
      "grad_norm": 3.4849298000335693,
      "learning_rate": 4.051510989010989e-05,
      "loss": 0.6354,
      "step": 966700
    },
    {
      "epoch": 15.177394034536892,
      "grad_norm": 4.036842346191406,
      "learning_rate": 4.051412872841445e-05,
      "loss": 0.6494,
      "step": 966800
    },
    {
      "epoch": 15.178963893249607,
      "grad_norm": 3.801252603530884,
      "learning_rate": 4.051314756671899e-05,
      "loss": 0.6042,
      "step": 966900
    },
    {
      "epoch": 15.180533751962324,
      "grad_norm": 4.763089656829834,
      "learning_rate": 4.051216640502355e-05,
      "loss": 0.5782,
      "step": 967000
    },
    {
      "epoch": 15.182103610675039,
      "grad_norm": 3.254875898361206,
      "learning_rate": 4.05111852433281e-05,
      "loss": 0.6376,
      "step": 967100
    },
    {
      "epoch": 15.183673469387756,
      "grad_norm": 3.775984048843384,
      "learning_rate": 4.051020408163265e-05,
      "loss": 0.6007,
      "step": 967200
    },
    {
      "epoch": 15.185243328100471,
      "grad_norm": 4.212721347808838,
      "learning_rate": 4.0509222919937204e-05,
      "loss": 0.6405,
      "step": 967300
    },
    {
      "epoch": 15.186813186813186,
      "grad_norm": 3.611699104309082,
      "learning_rate": 4.050824175824176e-05,
      "loss": 0.5844,
      "step": 967400
    },
    {
      "epoch": 15.188383045525903,
      "grad_norm": 3.3476767539978027,
      "learning_rate": 4.050726059654631e-05,
      "loss": 0.6039,
      "step": 967500
    },
    {
      "epoch": 15.189952904238618,
      "grad_norm": 4.344593524932861,
      "learning_rate": 4.0506279434850863e-05,
      "loss": 0.6141,
      "step": 967600
    },
    {
      "epoch": 15.191522762951335,
      "grad_norm": 4.246260643005371,
      "learning_rate": 4.0505298273155414e-05,
      "loss": 0.5999,
      "step": 967700
    },
    {
      "epoch": 15.19309262166405,
      "grad_norm": 3.625302314758301,
      "learning_rate": 4.050431711145997e-05,
      "loss": 0.6227,
      "step": 967800
    },
    {
      "epoch": 15.194662480376767,
      "grad_norm": 2.650503396987915,
      "learning_rate": 4.050333594976452e-05,
      "loss": 0.603,
      "step": 967900
    },
    {
      "epoch": 15.196232339089482,
      "grad_norm": 4.738675594329834,
      "learning_rate": 4.0502354788069074e-05,
      "loss": 0.5867,
      "step": 968000
    },
    {
      "epoch": 15.197802197802197,
      "grad_norm": 3.9636518955230713,
      "learning_rate": 4.050137362637363e-05,
      "loss": 0.6246,
      "step": 968100
    },
    {
      "epoch": 15.199372056514914,
      "grad_norm": 3.9837396144866943,
      "learning_rate": 4.050039246467818e-05,
      "loss": 0.6637,
      "step": 968200
    },
    {
      "epoch": 15.200941915227629,
      "grad_norm": 3.7326321601867676,
      "learning_rate": 4.0499411302982734e-05,
      "loss": 0.6176,
      "step": 968300
    },
    {
      "epoch": 15.202511773940346,
      "grad_norm": 3.4857285022735596,
      "learning_rate": 4.0498430141287285e-05,
      "loss": 0.6012,
      "step": 968400
    },
    {
      "epoch": 15.204081632653061,
      "grad_norm": 3.388421058654785,
      "learning_rate": 4.049744897959184e-05,
      "loss": 0.6031,
      "step": 968500
    },
    {
      "epoch": 15.205651491365778,
      "grad_norm": 4.123369216918945,
      "learning_rate": 4.049646781789639e-05,
      "loss": 0.6506,
      "step": 968600
    },
    {
      "epoch": 15.207221350078493,
      "grad_norm": 4.328873634338379,
      "learning_rate": 4.0495486656200945e-05,
      "loss": 0.6343,
      "step": 968700
    },
    {
      "epoch": 15.208791208791208,
      "grad_norm": 4.07933235168457,
      "learning_rate": 4.0494505494505496e-05,
      "loss": 0.6236,
      "step": 968800
    },
    {
      "epoch": 15.210361067503925,
      "grad_norm": 2.643679618835449,
      "learning_rate": 4.049352433281005e-05,
      "loss": 0.6054,
      "step": 968900
    },
    {
      "epoch": 15.21193092621664,
      "grad_norm": 2.568671703338623,
      "learning_rate": 4.04925431711146e-05,
      "loss": 0.5864,
      "step": 969000
    },
    {
      "epoch": 15.213500784929357,
      "grad_norm": 3.171335220336914,
      "learning_rate": 4.0491562009419155e-05,
      "loss": 0.6334,
      "step": 969100
    },
    {
      "epoch": 15.215070643642072,
      "grad_norm": 2.508969306945801,
      "learning_rate": 4.0490580847723706e-05,
      "loss": 0.5857,
      "step": 969200
    },
    {
      "epoch": 15.216640502354789,
      "grad_norm": 3.6888206005096436,
      "learning_rate": 4.048959968602826e-05,
      "loss": 0.6284,
      "step": 969300
    },
    {
      "epoch": 15.218210361067504,
      "grad_norm": 4.51943826675415,
      "learning_rate": 4.048861852433281e-05,
      "loss": 0.6076,
      "step": 969400
    },
    {
      "epoch": 15.219780219780219,
      "grad_norm": 4.530274868011475,
      "learning_rate": 4.0487637362637366e-05,
      "loss": 0.6566,
      "step": 969500
    },
    {
      "epoch": 15.221350078492936,
      "grad_norm": 3.9331471920013428,
      "learning_rate": 4.048665620094192e-05,
      "loss": 0.6158,
      "step": 969600
    },
    {
      "epoch": 15.222919937205651,
      "grad_norm": 3.7312488555908203,
      "learning_rate": 4.048567503924647e-05,
      "loss": 0.6081,
      "step": 969700
    },
    {
      "epoch": 15.224489795918368,
      "grad_norm": 4.370490074157715,
      "learning_rate": 4.048469387755102e-05,
      "loss": 0.6447,
      "step": 969800
    },
    {
      "epoch": 15.226059654631083,
      "grad_norm": 4.226686000823975,
      "learning_rate": 4.048371271585558e-05,
      "loss": 0.6068,
      "step": 969900
    },
    {
      "epoch": 15.2276295133438,
      "grad_norm": 3.75527286529541,
      "learning_rate": 4.048273155416013e-05,
      "loss": 0.5709,
      "step": 970000
    },
    {
      "epoch": 15.229199372056515,
      "grad_norm": 3.333306312561035,
      "learning_rate": 4.048175039246468e-05,
      "loss": 0.603,
      "step": 970100
    },
    {
      "epoch": 15.23076923076923,
      "grad_norm": 3.742980480194092,
      "learning_rate": 4.0480769230769236e-05,
      "loss": 0.6212,
      "step": 970200
    },
    {
      "epoch": 15.232339089481947,
      "grad_norm": 4.238915920257568,
      "learning_rate": 4.047978806907379e-05,
      "loss": 0.6103,
      "step": 970300
    },
    {
      "epoch": 15.233908948194662,
      "grad_norm": 4.070437908172607,
      "learning_rate": 4.047880690737834e-05,
      "loss": 0.6216,
      "step": 970400
    },
    {
      "epoch": 15.235478806907379,
      "grad_norm": 4.351040840148926,
      "learning_rate": 4.047782574568289e-05,
      "loss": 0.6094,
      "step": 970500
    },
    {
      "epoch": 15.237048665620094,
      "grad_norm": 3.253178119659424,
      "learning_rate": 4.047684458398745e-05,
      "loss": 0.6152,
      "step": 970600
    },
    {
      "epoch": 15.23861852433281,
      "grad_norm": 2.918356418609619,
      "learning_rate": 4.047586342229199e-05,
      "loss": 0.5874,
      "step": 970700
    },
    {
      "epoch": 15.240188383045526,
      "grad_norm": 3.9015233516693115,
      "learning_rate": 4.047488226059655e-05,
      "loss": 0.6143,
      "step": 970800
    },
    {
      "epoch": 15.241758241758241,
      "grad_norm": 3.8106234073638916,
      "learning_rate": 4.04739010989011e-05,
      "loss": 0.6406,
      "step": 970900
    },
    {
      "epoch": 15.243328100470958,
      "grad_norm": 4.523531436920166,
      "learning_rate": 4.047291993720566e-05,
      "loss": 0.6204,
      "step": 971000
    },
    {
      "epoch": 15.244897959183673,
      "grad_norm": 4.285119533538818,
      "learning_rate": 4.04719387755102e-05,
      "loss": 0.6411,
      "step": 971100
    },
    {
      "epoch": 15.24646781789639,
      "grad_norm": 4.358101844787598,
      "learning_rate": 4.047095761381476e-05,
      "loss": 0.6458,
      "step": 971200
    },
    {
      "epoch": 15.248037676609105,
      "grad_norm": 4.061094284057617,
      "learning_rate": 4.046997645211931e-05,
      "loss": 0.6035,
      "step": 971300
    },
    {
      "epoch": 15.249607535321822,
      "grad_norm": 3.471071243286133,
      "learning_rate": 4.046899529042386e-05,
      "loss": 0.6839,
      "step": 971400
    },
    {
      "epoch": 15.251177394034537,
      "grad_norm": 4.498246669769287,
      "learning_rate": 4.046801412872841e-05,
      "loss": 0.6575,
      "step": 971500
    },
    {
      "epoch": 15.252747252747252,
      "grad_norm": 3.4718868732452393,
      "learning_rate": 4.046703296703297e-05,
      "loss": 0.5816,
      "step": 971600
    },
    {
      "epoch": 15.254317111459969,
      "grad_norm": 4.289302825927734,
      "learning_rate": 4.046605180533752e-05,
      "loss": 0.6231,
      "step": 971700
    },
    {
      "epoch": 15.255886970172684,
      "grad_norm": 4.064246654510498,
      "learning_rate": 4.046507064364207e-05,
      "loss": 0.6348,
      "step": 971800
    },
    {
      "epoch": 15.2574568288854,
      "grad_norm": 2.8356642723083496,
      "learning_rate": 4.046408948194662e-05,
      "loss": 0.6149,
      "step": 971900
    },
    {
      "epoch": 15.259026687598116,
      "grad_norm": 2.623812198638916,
      "learning_rate": 4.046310832025118e-05,
      "loss": 0.5963,
      "step": 972000
    },
    {
      "epoch": 15.260596546310833,
      "grad_norm": 2.7120232582092285,
      "learning_rate": 4.046212715855573e-05,
      "loss": 0.6121,
      "step": 972100
    },
    {
      "epoch": 15.262166405023548,
      "grad_norm": 4.4200921058654785,
      "learning_rate": 4.046114599686028e-05,
      "loss": 0.619,
      "step": 972200
    },
    {
      "epoch": 15.263736263736265,
      "grad_norm": 3.660111427307129,
      "learning_rate": 4.046016483516484e-05,
      "loss": 0.6205,
      "step": 972300
    },
    {
      "epoch": 15.26530612244898,
      "grad_norm": 4.024929523468018,
      "learning_rate": 4.045918367346939e-05,
      "loss": 0.6323,
      "step": 972400
    },
    {
      "epoch": 15.266875981161695,
      "grad_norm": 2.9894773960113525,
      "learning_rate": 4.045820251177394e-05,
      "loss": 0.5855,
      "step": 972500
    },
    {
      "epoch": 15.268445839874412,
      "grad_norm": 2.889162063598633,
      "learning_rate": 4.0457221350078494e-05,
      "loss": 0.6286,
      "step": 972600
    },
    {
      "epoch": 15.270015698587127,
      "grad_norm": 3.773237466812134,
      "learning_rate": 4.045624018838305e-05,
      "loss": 0.6132,
      "step": 972700
    },
    {
      "epoch": 15.271585557299844,
      "grad_norm": 4.078195095062256,
      "learning_rate": 4.0455259026687596e-05,
      "loss": 0.5999,
      "step": 972800
    },
    {
      "epoch": 15.273155416012559,
      "grad_norm": 4.425858020782471,
      "learning_rate": 4.0454277864992153e-05,
      "loss": 0.6356,
      "step": 972900
    },
    {
      "epoch": 15.274725274725276,
      "grad_norm": 4.588228225708008,
      "learning_rate": 4.0453296703296704e-05,
      "loss": 0.622,
      "step": 973000
    },
    {
      "epoch": 15.27629513343799,
      "grad_norm": 3.9020655155181885,
      "learning_rate": 4.045231554160126e-05,
      "loss": 0.645,
      "step": 973100
    },
    {
      "epoch": 15.277864992150706,
      "grad_norm": 3.9545321464538574,
      "learning_rate": 4.0451334379905806e-05,
      "loss": 0.631,
      "step": 973200
    },
    {
      "epoch": 15.279434850863423,
      "grad_norm": 4.682989597320557,
      "learning_rate": 4.0450353218210364e-05,
      "loss": 0.6314,
      "step": 973300
    },
    {
      "epoch": 15.281004709576138,
      "grad_norm": 3.509141445159912,
      "learning_rate": 4.0449372056514915e-05,
      "loss": 0.6332,
      "step": 973400
    },
    {
      "epoch": 15.282574568288855,
      "grad_norm": 4.2707839012146,
      "learning_rate": 4.0448390894819466e-05,
      "loss": 0.5932,
      "step": 973500
    },
    {
      "epoch": 15.28414442700157,
      "grad_norm": 3.247253656387329,
      "learning_rate": 4.044740973312402e-05,
      "loss": 0.6139,
      "step": 973600
    },
    {
      "epoch": 15.285714285714286,
      "grad_norm": 4.047101020812988,
      "learning_rate": 4.0446428571428575e-05,
      "loss": 0.614,
      "step": 973700
    },
    {
      "epoch": 15.287284144427002,
      "grad_norm": 4.197593688964844,
      "learning_rate": 4.0445447409733126e-05,
      "loss": 0.6695,
      "step": 973800
    },
    {
      "epoch": 15.288854003139717,
      "grad_norm": 3.296875238418579,
      "learning_rate": 4.044446624803768e-05,
      "loss": 0.6444,
      "step": 973900
    },
    {
      "epoch": 15.290423861852434,
      "grad_norm": 3.4557857513427734,
      "learning_rate": 4.044348508634223e-05,
      "loss": 0.5811,
      "step": 974000
    },
    {
      "epoch": 15.291993720565149,
      "grad_norm": 3.453889846801758,
      "learning_rate": 4.0442503924646786e-05,
      "loss": 0.6416,
      "step": 974100
    },
    {
      "epoch": 15.293563579277865,
      "grad_norm": 4.199393272399902,
      "learning_rate": 4.0441522762951337e-05,
      "loss": 0.6159,
      "step": 974200
    },
    {
      "epoch": 15.29513343799058,
      "grad_norm": 4.246718883514404,
      "learning_rate": 4.044054160125589e-05,
      "loss": 0.6188,
      "step": 974300
    },
    {
      "epoch": 15.296703296703297,
      "grad_norm": 2.536282539367676,
      "learning_rate": 4.0439560439560445e-05,
      "loss": 0.6222,
      "step": 974400
    },
    {
      "epoch": 15.298273155416013,
      "grad_norm": 3.6576993465423584,
      "learning_rate": 4.0438579277864996e-05,
      "loss": 0.6093,
      "step": 974500
    },
    {
      "epoch": 15.299843014128728,
      "grad_norm": 3.0934295654296875,
      "learning_rate": 4.043759811616955e-05,
      "loss": 0.592,
      "step": 974600
    },
    {
      "epoch": 15.301412872841444,
      "grad_norm": 4.212525844573975,
      "learning_rate": 4.04366169544741e-05,
      "loss": 0.6226,
      "step": 974700
    },
    {
      "epoch": 15.30298273155416,
      "grad_norm": 4.007069110870361,
      "learning_rate": 4.0435635792778656e-05,
      "loss": 0.5795,
      "step": 974800
    },
    {
      "epoch": 15.304552590266876,
      "grad_norm": 3.749323606491089,
      "learning_rate": 4.04346546310832e-05,
      "loss": 0.6225,
      "step": 974900
    },
    {
      "epoch": 15.306122448979592,
      "grad_norm": 3.781043291091919,
      "learning_rate": 4.043367346938776e-05,
      "loss": 0.6177,
      "step": 975000
    },
    {
      "epoch": 15.307692307692308,
      "grad_norm": 4.457696914672852,
      "learning_rate": 4.043269230769231e-05,
      "loss": 0.6235,
      "step": 975100
    },
    {
      "epoch": 15.309262166405023,
      "grad_norm": 4.313539505004883,
      "learning_rate": 4.043171114599687e-05,
      "loss": 0.6446,
      "step": 975200
    },
    {
      "epoch": 15.310832025117739,
      "grad_norm": 4.094893455505371,
      "learning_rate": 4.043072998430141e-05,
      "loss": 0.5557,
      "step": 975300
    },
    {
      "epoch": 15.312401883830455,
      "grad_norm": 4.525437831878662,
      "learning_rate": 4.042974882260597e-05,
      "loss": 0.6373,
      "step": 975400
    },
    {
      "epoch": 15.31397174254317,
      "grad_norm": 3.1466846466064453,
      "learning_rate": 4.042876766091052e-05,
      "loss": 0.6237,
      "step": 975500
    },
    {
      "epoch": 15.315541601255887,
      "grad_norm": 3.6213810443878174,
      "learning_rate": 4.042778649921507e-05,
      "loss": 0.6126,
      "step": 975600
    },
    {
      "epoch": 15.317111459968602,
      "grad_norm": 3.8902547359466553,
      "learning_rate": 4.042680533751962e-05,
      "loss": 0.6351,
      "step": 975700
    },
    {
      "epoch": 15.31868131868132,
      "grad_norm": 5.593356609344482,
      "learning_rate": 4.042582417582418e-05,
      "loss": 0.6212,
      "step": 975800
    },
    {
      "epoch": 15.320251177394034,
      "grad_norm": 4.113149642944336,
      "learning_rate": 4.042484301412873e-05,
      "loss": 0.6104,
      "step": 975900
    },
    {
      "epoch": 15.321821036106751,
      "grad_norm": 3.6648058891296387,
      "learning_rate": 4.042386185243328e-05,
      "loss": 0.6314,
      "step": 976000
    },
    {
      "epoch": 15.323390894819466,
      "grad_norm": 3.9391257762908936,
      "learning_rate": 4.042288069073783e-05,
      "loss": 0.6535,
      "step": 976100
    },
    {
      "epoch": 15.324960753532181,
      "grad_norm": 3.941990852355957,
      "learning_rate": 4.042189952904239e-05,
      "loss": 0.625,
      "step": 976200
    },
    {
      "epoch": 15.326530612244898,
      "grad_norm": 3.732907772064209,
      "learning_rate": 4.042091836734694e-05,
      "loss": 0.5888,
      "step": 976300
    },
    {
      "epoch": 15.328100470957613,
      "grad_norm": 3.627875566482544,
      "learning_rate": 4.041993720565149e-05,
      "loss": 0.6068,
      "step": 976400
    },
    {
      "epoch": 15.32967032967033,
      "grad_norm": 4.641923904418945,
      "learning_rate": 4.041895604395605e-05,
      "loss": 0.6185,
      "step": 976500
    },
    {
      "epoch": 15.331240188383045,
      "grad_norm": 3.4763147830963135,
      "learning_rate": 4.04179748822606e-05,
      "loss": 0.6433,
      "step": 976600
    },
    {
      "epoch": 15.332810047095762,
      "grad_norm": 1.9859774112701416,
      "learning_rate": 4.041699372056515e-05,
      "loss": 0.6136,
      "step": 976700
    },
    {
      "epoch": 15.334379905808477,
      "grad_norm": 4.475180625915527,
      "learning_rate": 4.04160125588697e-05,
      "loss": 0.6034,
      "step": 976800
    },
    {
      "epoch": 15.335949764521192,
      "grad_norm": 4.604820251464844,
      "learning_rate": 4.041503139717426e-05,
      "loss": 0.5792,
      "step": 976900
    },
    {
      "epoch": 15.33751962323391,
      "grad_norm": 3.2569637298583984,
      "learning_rate": 4.0414050235478805e-05,
      "loss": 0.6431,
      "step": 977000
    },
    {
      "epoch": 15.339089481946624,
      "grad_norm": 3.923426389694214,
      "learning_rate": 4.041306907378336e-05,
      "loss": 0.5826,
      "step": 977100
    },
    {
      "epoch": 15.340659340659341,
      "grad_norm": 4.4554290771484375,
      "learning_rate": 4.0412087912087913e-05,
      "loss": 0.606,
      "step": 977200
    },
    {
      "epoch": 15.342229199372056,
      "grad_norm": 4.115380764007568,
      "learning_rate": 4.041110675039247e-05,
      "loss": 0.6011,
      "step": 977300
    },
    {
      "epoch": 15.343799058084773,
      "grad_norm": 3.5381124019622803,
      "learning_rate": 4.0410125588697015e-05,
      "loss": 0.6359,
      "step": 977400
    },
    {
      "epoch": 15.345368916797488,
      "grad_norm": 3.197657585144043,
      "learning_rate": 4.040914442700157e-05,
      "loss": 0.6171,
      "step": 977500
    },
    {
      "epoch": 15.346938775510203,
      "grad_norm": 3.5360987186431885,
      "learning_rate": 4.0408163265306124e-05,
      "loss": 0.6188,
      "step": 977600
    },
    {
      "epoch": 15.34850863422292,
      "grad_norm": 3.907137870788574,
      "learning_rate": 4.0407182103610675e-05,
      "loss": 0.5943,
      "step": 977700
    },
    {
      "epoch": 15.350078492935635,
      "grad_norm": 3.009775400161743,
      "learning_rate": 4.0406200941915226e-05,
      "loss": 0.6025,
      "step": 977800
    },
    {
      "epoch": 15.351648351648352,
      "grad_norm": 3.9943079948425293,
      "learning_rate": 4.0405219780219784e-05,
      "loss": 0.6089,
      "step": 977900
    },
    {
      "epoch": 15.353218210361067,
      "grad_norm": 3.210695743560791,
      "learning_rate": 4.0404238618524335e-05,
      "loss": 0.6034,
      "step": 978000
    },
    {
      "epoch": 15.354788069073784,
      "grad_norm": 3.549262762069702,
      "learning_rate": 4.0403257456828886e-05,
      "loss": 0.6021,
      "step": 978100
    },
    {
      "epoch": 15.3563579277865,
      "grad_norm": 3.3008744716644287,
      "learning_rate": 4.040227629513344e-05,
      "loss": 0.6436,
      "step": 978200
    },
    {
      "epoch": 15.357927786499214,
      "grad_norm": 4.146064281463623,
      "learning_rate": 4.0401295133437995e-05,
      "loss": 0.5943,
      "step": 978300
    },
    {
      "epoch": 15.359497645211931,
      "grad_norm": 3.8187248706817627,
      "learning_rate": 4.0400313971742546e-05,
      "loss": 0.6201,
      "step": 978400
    },
    {
      "epoch": 15.361067503924646,
      "grad_norm": 3.854290723800659,
      "learning_rate": 4.0399332810047097e-05,
      "loss": 0.6049,
      "step": 978500
    },
    {
      "epoch": 15.362637362637363,
      "grad_norm": 3.9191722869873047,
      "learning_rate": 4.0398351648351654e-05,
      "loss": 0.6504,
      "step": 978600
    },
    {
      "epoch": 15.364207221350078,
      "grad_norm": 4.0889973640441895,
      "learning_rate": 4.0397370486656205e-05,
      "loss": 0.6046,
      "step": 978700
    },
    {
      "epoch": 15.365777080062795,
      "grad_norm": 4.269308090209961,
      "learning_rate": 4.0396389324960756e-05,
      "loss": 0.6122,
      "step": 978800
    },
    {
      "epoch": 15.36734693877551,
      "grad_norm": 3.7634410858154297,
      "learning_rate": 4.039540816326531e-05,
      "loss": 0.6628,
      "step": 978900
    },
    {
      "epoch": 15.368916797488225,
      "grad_norm": 3.3059275150299072,
      "learning_rate": 4.0394427001569865e-05,
      "loss": 0.6013,
      "step": 979000
    },
    {
      "epoch": 15.370486656200942,
      "grad_norm": 4.1912689208984375,
      "learning_rate": 4.039344583987441e-05,
      "loss": 0.5893,
      "step": 979100
    },
    {
      "epoch": 15.372056514913657,
      "grad_norm": 3.4274275302886963,
      "learning_rate": 4.039246467817897e-05,
      "loss": 0.6311,
      "step": 979200
    },
    {
      "epoch": 15.373626373626374,
      "grad_norm": 4.0394206047058105,
      "learning_rate": 4.039148351648352e-05,
      "loss": 0.6362,
      "step": 979300
    },
    {
      "epoch": 15.37519623233909,
      "grad_norm": 5.240106105804443,
      "learning_rate": 4.0390502354788076e-05,
      "loss": 0.6084,
      "step": 979400
    },
    {
      "epoch": 15.376766091051806,
      "grad_norm": 3.5766725540161133,
      "learning_rate": 4.038952119309262e-05,
      "loss": 0.6373,
      "step": 979500
    },
    {
      "epoch": 15.378335949764521,
      "grad_norm": 3.892918348312378,
      "learning_rate": 4.038854003139718e-05,
      "loss": 0.635,
      "step": 979600
    },
    {
      "epoch": 15.379905808477236,
      "grad_norm": 4.081254005432129,
      "learning_rate": 4.038755886970173e-05,
      "loss": 0.6121,
      "step": 979700
    },
    {
      "epoch": 15.381475667189953,
      "grad_norm": 3.82934308052063,
      "learning_rate": 4.038657770800628e-05,
      "loss": 0.6082,
      "step": 979800
    },
    {
      "epoch": 15.383045525902668,
      "grad_norm": 3.570322036743164,
      "learning_rate": 4.038559654631083e-05,
      "loss": 0.6202,
      "step": 979900
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 3.9853267669677734,
      "learning_rate": 4.038461538461539e-05,
      "loss": 0.6062,
      "step": 980000
    },
    {
      "epoch": 15.3861852433281,
      "grad_norm": 4.307486534118652,
      "learning_rate": 4.038363422291994e-05,
      "loss": 0.6077,
      "step": 980100
    },
    {
      "epoch": 15.387755102040817,
      "grad_norm": 4.508021831512451,
      "learning_rate": 4.038265306122449e-05,
      "loss": 0.6466,
      "step": 980200
    },
    {
      "epoch": 15.389324960753532,
      "grad_norm": 5.074851989746094,
      "learning_rate": 4.038167189952904e-05,
      "loss": 0.6216,
      "step": 980300
    },
    {
      "epoch": 15.390894819466247,
      "grad_norm": 3.8975489139556885,
      "learning_rate": 4.03806907378336e-05,
      "loss": 0.6194,
      "step": 980400
    },
    {
      "epoch": 15.392464678178964,
      "grad_norm": 4.258746147155762,
      "learning_rate": 4.037970957613815e-05,
      "loss": 0.6166,
      "step": 980500
    },
    {
      "epoch": 15.394034536891679,
      "grad_norm": 4.565067768096924,
      "learning_rate": 4.03787284144427e-05,
      "loss": 0.6237,
      "step": 980600
    },
    {
      "epoch": 15.395604395604396,
      "grad_norm": 3.833312749862671,
      "learning_rate": 4.037774725274726e-05,
      "loss": 0.5753,
      "step": 980700
    },
    {
      "epoch": 15.397174254317111,
      "grad_norm": 3.8467071056365967,
      "learning_rate": 4.037676609105181e-05,
      "loss": 0.6163,
      "step": 980800
    },
    {
      "epoch": 15.398744113029828,
      "grad_norm": 5.340819835662842,
      "learning_rate": 4.037578492935636e-05,
      "loss": 0.6425,
      "step": 980900
    },
    {
      "epoch": 15.400313971742543,
      "grad_norm": 3.4557881355285645,
      "learning_rate": 4.037480376766091e-05,
      "loss": 0.6167,
      "step": 981000
    },
    {
      "epoch": 15.40188383045526,
      "grad_norm": 3.9691946506500244,
      "learning_rate": 4.037382260596547e-05,
      "loss": 0.5867,
      "step": 981100
    },
    {
      "epoch": 15.403453689167975,
      "grad_norm": 4.698385715484619,
      "learning_rate": 4.0372841444270014e-05,
      "loss": 0.6286,
      "step": 981200
    },
    {
      "epoch": 15.40502354788069,
      "grad_norm": 4.2797369956970215,
      "learning_rate": 4.037186028257457e-05,
      "loss": 0.5999,
      "step": 981300
    },
    {
      "epoch": 15.406593406593407,
      "grad_norm": 3.8583455085754395,
      "learning_rate": 4.037087912087912e-05,
      "loss": 0.5825,
      "step": 981400
    },
    {
      "epoch": 15.408163265306122,
      "grad_norm": 4.0728607177734375,
      "learning_rate": 4.036989795918368e-05,
      "loss": 0.6088,
      "step": 981500
    },
    {
      "epoch": 15.409733124018839,
      "grad_norm": 3.931307077407837,
      "learning_rate": 4.0368916797488224e-05,
      "loss": 0.6305,
      "step": 981600
    },
    {
      "epoch": 15.411302982731554,
      "grad_norm": 3.182983160018921,
      "learning_rate": 4.036793563579278e-05,
      "loss": 0.6164,
      "step": 981700
    },
    {
      "epoch": 15.41287284144427,
      "grad_norm": 3.7509500980377197,
      "learning_rate": 4.036695447409733e-05,
      "loss": 0.616,
      "step": 981800
    },
    {
      "epoch": 15.414442700156986,
      "grad_norm": 3.495511293411255,
      "learning_rate": 4.0365973312401884e-05,
      "loss": 0.6355,
      "step": 981900
    },
    {
      "epoch": 15.416012558869701,
      "grad_norm": 4.331511497497559,
      "learning_rate": 4.0364992150706435e-05,
      "loss": 0.6347,
      "step": 982000
    },
    {
      "epoch": 15.417582417582418,
      "grad_norm": 3.500650644302368,
      "learning_rate": 4.036401098901099e-05,
      "loss": 0.624,
      "step": 982100
    },
    {
      "epoch": 15.419152276295133,
      "grad_norm": 4.165780067443848,
      "learning_rate": 4.0363029827315544e-05,
      "loss": 0.6394,
      "step": 982200
    },
    {
      "epoch": 15.42072213500785,
      "grad_norm": 3.770169734954834,
      "learning_rate": 4.0362048665620095e-05,
      "loss": 0.6517,
      "step": 982300
    },
    {
      "epoch": 15.422291993720565,
      "grad_norm": 4.860445976257324,
      "learning_rate": 4.0361067503924646e-05,
      "loss": 0.6359,
      "step": 982400
    },
    {
      "epoch": 15.423861852433282,
      "grad_norm": 4.5744709968566895,
      "learning_rate": 4.0360086342229204e-05,
      "loss": 0.6435,
      "step": 982500
    },
    {
      "epoch": 15.425431711145997,
      "grad_norm": 4.434398651123047,
      "learning_rate": 4.0359105180533755e-05,
      "loss": 0.6254,
      "step": 982600
    },
    {
      "epoch": 15.427001569858712,
      "grad_norm": 4.877608299255371,
      "learning_rate": 4.0358124018838306e-05,
      "loss": 0.6038,
      "step": 982700
    },
    {
      "epoch": 15.428571428571429,
      "grad_norm": 3.7875430583953857,
      "learning_rate": 4.035714285714286e-05,
      "loss": 0.5926,
      "step": 982800
    },
    {
      "epoch": 15.430141287284144,
      "grad_norm": 3.420874834060669,
      "learning_rate": 4.0356161695447414e-05,
      "loss": 0.6042,
      "step": 982900
    },
    {
      "epoch": 15.43171114599686,
      "grad_norm": 3.6279518604278564,
      "learning_rate": 4.0355180533751965e-05,
      "loss": 0.6051,
      "step": 983000
    },
    {
      "epoch": 15.433281004709576,
      "grad_norm": 3.9472341537475586,
      "learning_rate": 4.0354199372056516e-05,
      "loss": 0.631,
      "step": 983100
    },
    {
      "epoch": 15.434850863422293,
      "grad_norm": 4.0430588722229,
      "learning_rate": 4.0353218210361074e-05,
      "loss": 0.6541,
      "step": 983200
    },
    {
      "epoch": 15.436420722135008,
      "grad_norm": 4.247318744659424,
      "learning_rate": 4.035223704866562e-05,
      "loss": 0.6335,
      "step": 983300
    },
    {
      "epoch": 15.437990580847723,
      "grad_norm": 3.390928268432617,
      "learning_rate": 4.0351255886970176e-05,
      "loss": 0.6534,
      "step": 983400
    },
    {
      "epoch": 15.43956043956044,
      "grad_norm": 4.38439416885376,
      "learning_rate": 4.035027472527473e-05,
      "loss": 0.664,
      "step": 983500
    },
    {
      "epoch": 15.441130298273155,
      "grad_norm": 3.325014114379883,
      "learning_rate": 4.0349293563579285e-05,
      "loss": 0.6251,
      "step": 983600
    },
    {
      "epoch": 15.442700156985872,
      "grad_norm": 3.2148375511169434,
      "learning_rate": 4.034831240188383e-05,
      "loss": 0.6397,
      "step": 983700
    },
    {
      "epoch": 15.444270015698587,
      "grad_norm": 4.314784049987793,
      "learning_rate": 4.034733124018839e-05,
      "loss": 0.613,
      "step": 983800
    },
    {
      "epoch": 15.445839874411304,
      "grad_norm": 4.704837322235107,
      "learning_rate": 4.034635007849294e-05,
      "loss": 0.6423,
      "step": 983900
    },
    {
      "epoch": 15.447409733124019,
      "grad_norm": 3.74257755279541,
      "learning_rate": 4.034536891679749e-05,
      "loss": 0.6025,
      "step": 984000
    },
    {
      "epoch": 15.448979591836734,
      "grad_norm": 2.6991710662841797,
      "learning_rate": 4.034438775510204e-05,
      "loss": 0.6758,
      "step": 984100
    },
    {
      "epoch": 15.45054945054945,
      "grad_norm": 4.188107967376709,
      "learning_rate": 4.03434065934066e-05,
      "loss": 0.626,
      "step": 984200
    },
    {
      "epoch": 15.452119309262166,
      "grad_norm": 4.459606647491455,
      "learning_rate": 4.034242543171115e-05,
      "loss": 0.6032,
      "step": 984300
    },
    {
      "epoch": 15.453689167974883,
      "grad_norm": 3.877767562866211,
      "learning_rate": 4.03414442700157e-05,
      "loss": 0.6259,
      "step": 984400
    },
    {
      "epoch": 15.455259026687598,
      "grad_norm": 4.119400501251221,
      "learning_rate": 4.034046310832025e-05,
      "loss": 0.5954,
      "step": 984500
    },
    {
      "epoch": 15.456828885400315,
      "grad_norm": 4.228007793426514,
      "learning_rate": 4.033948194662481e-05,
      "loss": 0.6376,
      "step": 984600
    },
    {
      "epoch": 15.45839874411303,
      "grad_norm": 5.163758277893066,
      "learning_rate": 4.033850078492936e-05,
      "loss": 0.6248,
      "step": 984700
    },
    {
      "epoch": 15.459968602825747,
      "grad_norm": 3.964296579360962,
      "learning_rate": 4.033751962323391e-05,
      "loss": 0.6237,
      "step": 984800
    },
    {
      "epoch": 15.461538461538462,
      "grad_norm": 4.454452991485596,
      "learning_rate": 4.033653846153847e-05,
      "loss": 0.6021,
      "step": 984900
    },
    {
      "epoch": 15.463108320251177,
      "grad_norm": 3.6771538257598877,
      "learning_rate": 4.033555729984302e-05,
      "loss": 0.6365,
      "step": 985000
    },
    {
      "epoch": 15.464678178963894,
      "grad_norm": 3.804749011993408,
      "learning_rate": 4.033457613814757e-05,
      "loss": 0.599,
      "step": 985100
    },
    {
      "epoch": 15.466248037676609,
      "grad_norm": 2.491678237915039,
      "learning_rate": 4.033359497645212e-05,
      "loss": 0.6215,
      "step": 985200
    },
    {
      "epoch": 15.467817896389326,
      "grad_norm": 4.439589023590088,
      "learning_rate": 4.033261381475668e-05,
      "loss": 0.6169,
      "step": 985300
    },
    {
      "epoch": 15.46938775510204,
      "grad_norm": 3.7163989543914795,
      "learning_rate": 4.033163265306122e-05,
      "loss": 0.65,
      "step": 985400
    },
    {
      "epoch": 15.470957613814758,
      "grad_norm": 4.168482780456543,
      "learning_rate": 4.033065149136578e-05,
      "loss": 0.6344,
      "step": 985500
    },
    {
      "epoch": 15.472527472527473,
      "grad_norm": 5.181370735168457,
      "learning_rate": 4.032967032967033e-05,
      "loss": 0.6054,
      "step": 985600
    },
    {
      "epoch": 15.474097331240188,
      "grad_norm": 3.2664120197296143,
      "learning_rate": 4.032868916797489e-05,
      "loss": 0.5761,
      "step": 985700
    },
    {
      "epoch": 15.475667189952905,
      "grad_norm": 2.5767641067504883,
      "learning_rate": 4.032770800627943e-05,
      "loss": 0.6256,
      "step": 985800
    },
    {
      "epoch": 15.47723704866562,
      "grad_norm": 4.069309711456299,
      "learning_rate": 4.032672684458399e-05,
      "loss": 0.5969,
      "step": 985900
    },
    {
      "epoch": 15.478806907378337,
      "grad_norm": 3.5981953144073486,
      "learning_rate": 4.032574568288854e-05,
      "loss": 0.6249,
      "step": 986000
    },
    {
      "epoch": 15.480376766091052,
      "grad_norm": 4.109607219696045,
      "learning_rate": 4.032476452119309e-05,
      "loss": 0.5987,
      "step": 986100
    },
    {
      "epoch": 15.481946624803768,
      "grad_norm": 4.938810348510742,
      "learning_rate": 4.0323783359497644e-05,
      "loss": 0.6148,
      "step": 986200
    },
    {
      "epoch": 15.483516483516484,
      "grad_norm": 3.9872772693634033,
      "learning_rate": 4.03228021978022e-05,
      "loss": 0.614,
      "step": 986300
    },
    {
      "epoch": 15.485086342229199,
      "grad_norm": 5.012022495269775,
      "learning_rate": 4.032182103610675e-05,
      "loss": 0.641,
      "step": 986400
    },
    {
      "epoch": 15.486656200941916,
      "grad_norm": 4.582091808319092,
      "learning_rate": 4.0320839874411304e-05,
      "loss": 0.6307,
      "step": 986500
    },
    {
      "epoch": 15.48822605965463,
      "grad_norm": 3.7426562309265137,
      "learning_rate": 4.0319858712715855e-05,
      "loss": 0.6146,
      "step": 986600
    },
    {
      "epoch": 15.489795918367347,
      "grad_norm": 4.324507713317871,
      "learning_rate": 4.031887755102041e-05,
      "loss": 0.5944,
      "step": 986700
    },
    {
      "epoch": 15.491365777080063,
      "grad_norm": 3.6902272701263428,
      "learning_rate": 4.0317896389324964e-05,
      "loss": 0.6276,
      "step": 986800
    },
    {
      "epoch": 15.49293563579278,
      "grad_norm": 2.8530471324920654,
      "learning_rate": 4.0316915227629515e-05,
      "loss": 0.6349,
      "step": 986900
    },
    {
      "epoch": 15.494505494505495,
      "grad_norm": 4.384097099304199,
      "learning_rate": 4.031593406593407e-05,
      "loss": 0.626,
      "step": 987000
    },
    {
      "epoch": 15.49607535321821,
      "grad_norm": 3.2081291675567627,
      "learning_rate": 4.031495290423862e-05,
      "loss": 0.6131,
      "step": 987100
    },
    {
      "epoch": 15.497645211930926,
      "grad_norm": 3.7441203594207764,
      "learning_rate": 4.0313971742543174e-05,
      "loss": 0.6255,
      "step": 987200
    },
    {
      "epoch": 15.499215070643642,
      "grad_norm": 2.8153507709503174,
      "learning_rate": 4.0312990580847725e-05,
      "loss": 0.5877,
      "step": 987300
    },
    {
      "epoch": 15.500784929356358,
      "grad_norm": 3.7916321754455566,
      "learning_rate": 4.031200941915228e-05,
      "loss": 0.6138,
      "step": 987400
    },
    {
      "epoch": 15.502354788069074,
      "grad_norm": 3.4655439853668213,
      "learning_rate": 4.031102825745683e-05,
      "loss": 0.5848,
      "step": 987500
    },
    {
      "epoch": 15.50392464678179,
      "grad_norm": 2.537623405456543,
      "learning_rate": 4.0310047095761385e-05,
      "loss": 0.5912,
      "step": 987600
    },
    {
      "epoch": 15.505494505494505,
      "grad_norm": 4.137875556945801,
      "learning_rate": 4.0309065934065936e-05,
      "loss": 0.6377,
      "step": 987700
    },
    {
      "epoch": 15.50706436420722,
      "grad_norm": 4.281276226043701,
      "learning_rate": 4.0308084772370494e-05,
      "loss": 0.6208,
      "step": 987800
    },
    {
      "epoch": 15.508634222919937,
      "grad_norm": 3.7354156970977783,
      "learning_rate": 4.030710361067504e-05,
      "loss": 0.6023,
      "step": 987900
    },
    {
      "epoch": 15.510204081632653,
      "grad_norm": 5.0530242919921875,
      "learning_rate": 4.0306122448979596e-05,
      "loss": 0.5777,
      "step": 988000
    },
    {
      "epoch": 15.51177394034537,
      "grad_norm": 2.8705475330352783,
      "learning_rate": 4.030514128728415e-05,
      "loss": 0.6357,
      "step": 988100
    },
    {
      "epoch": 15.513343799058084,
      "grad_norm": 5.282970428466797,
      "learning_rate": 4.03041601255887e-05,
      "loss": 0.6218,
      "step": 988200
    },
    {
      "epoch": 15.514913657770801,
      "grad_norm": 3.2860758304595947,
      "learning_rate": 4.030317896389325e-05,
      "loss": 0.6059,
      "step": 988300
    },
    {
      "epoch": 15.516483516483516,
      "grad_norm": 2.7426528930664062,
      "learning_rate": 4.0302197802197806e-05,
      "loss": 0.661,
      "step": 988400
    },
    {
      "epoch": 15.518053375196232,
      "grad_norm": 4.211884498596191,
      "learning_rate": 4.030121664050236e-05,
      "loss": 0.6136,
      "step": 988500
    },
    {
      "epoch": 15.519623233908948,
      "grad_norm": 3.195328712463379,
      "learning_rate": 4.030023547880691e-05,
      "loss": 0.6353,
      "step": 988600
    },
    {
      "epoch": 15.521193092621663,
      "grad_norm": 4.170492172241211,
      "learning_rate": 4.029925431711146e-05,
      "loss": 0.6116,
      "step": 988700
    },
    {
      "epoch": 15.52276295133438,
      "grad_norm": 2.9264919757843018,
      "learning_rate": 4.029827315541602e-05,
      "loss": 0.6287,
      "step": 988800
    },
    {
      "epoch": 15.524332810047095,
      "grad_norm": 5.271449565887451,
      "learning_rate": 4.029729199372057e-05,
      "loss": 0.6244,
      "step": 988900
    },
    {
      "epoch": 15.525902668759812,
      "grad_norm": 3.969649076461792,
      "learning_rate": 4.029631083202512e-05,
      "loss": 0.6044,
      "step": 989000
    },
    {
      "epoch": 15.527472527472527,
      "grad_norm": 4.098609924316406,
      "learning_rate": 4.029532967032968e-05,
      "loss": 0.6132,
      "step": 989100
    },
    {
      "epoch": 15.529042386185242,
      "grad_norm": 5.37047004699707,
      "learning_rate": 4.029434850863423e-05,
      "loss": 0.6158,
      "step": 989200
    },
    {
      "epoch": 15.53061224489796,
      "grad_norm": 3.9862070083618164,
      "learning_rate": 4.029336734693878e-05,
      "loss": 0.5827,
      "step": 989300
    },
    {
      "epoch": 15.532182103610674,
      "grad_norm": 3.875807285308838,
      "learning_rate": 4.029238618524333e-05,
      "loss": 0.6439,
      "step": 989400
    },
    {
      "epoch": 15.533751962323391,
      "grad_norm": 3.625866413116455,
      "learning_rate": 4.029140502354789e-05,
      "loss": 0.606,
      "step": 989500
    },
    {
      "epoch": 15.535321821036106,
      "grad_norm": 2.456183433532715,
      "learning_rate": 4.029042386185243e-05,
      "loss": 0.574,
      "step": 989600
    },
    {
      "epoch": 15.536891679748823,
      "grad_norm": 3.761237621307373,
      "learning_rate": 4.028944270015699e-05,
      "loss": 0.5889,
      "step": 989700
    },
    {
      "epoch": 15.538461538461538,
      "grad_norm": 1.7731066942214966,
      "learning_rate": 4.028846153846154e-05,
      "loss": 0.5734,
      "step": 989800
    },
    {
      "epoch": 15.540031397174253,
      "grad_norm": 4.342081069946289,
      "learning_rate": 4.028748037676609e-05,
      "loss": 0.6438,
      "step": 989900
    },
    {
      "epoch": 15.54160125588697,
      "grad_norm": 4.522640705108643,
      "learning_rate": 4.028649921507064e-05,
      "loss": 0.634,
      "step": 990000
    },
    {
      "epoch": 15.543171114599685,
      "grad_norm": 4.421795845031738,
      "learning_rate": 4.02855180533752e-05,
      "loss": 0.6271,
      "step": 990100
    },
    {
      "epoch": 15.544740973312402,
      "grad_norm": 3.7004520893096924,
      "learning_rate": 4.028453689167975e-05,
      "loss": 0.6462,
      "step": 990200
    },
    {
      "epoch": 15.546310832025117,
      "grad_norm": 2.9348833560943604,
      "learning_rate": 4.02835557299843e-05,
      "loss": 0.5962,
      "step": 990300
    },
    {
      "epoch": 15.547880690737834,
      "grad_norm": 2.7460103034973145,
      "learning_rate": 4.028257456828885e-05,
      "loss": 0.6097,
      "step": 990400
    },
    {
      "epoch": 15.54945054945055,
      "grad_norm": 3.7317631244659424,
      "learning_rate": 4.028159340659341e-05,
      "loss": 0.5763,
      "step": 990500
    },
    {
      "epoch": 15.551020408163264,
      "grad_norm": 4.2224836349487305,
      "learning_rate": 4.0280612244897955e-05,
      "loss": 0.5574,
      "step": 990600
    },
    {
      "epoch": 15.552590266875981,
      "grad_norm": 4.0942301750183105,
      "learning_rate": 4.027963108320251e-05,
      "loss": 0.6203,
      "step": 990700
    },
    {
      "epoch": 15.554160125588696,
      "grad_norm": 4.965145111083984,
      "learning_rate": 4.0278649921507064e-05,
      "loss": 0.5901,
      "step": 990800
    },
    {
      "epoch": 15.555729984301413,
      "grad_norm": 4.045239448547363,
      "learning_rate": 4.027766875981162e-05,
      "loss": 0.5934,
      "step": 990900
    },
    {
      "epoch": 15.557299843014128,
      "grad_norm": 4.484174728393555,
      "learning_rate": 4.027668759811617e-05,
      "loss": 0.623,
      "step": 991000
    },
    {
      "epoch": 15.558869701726845,
      "grad_norm": 2.9171743392944336,
      "learning_rate": 4.0275706436420723e-05,
      "loss": 0.6005,
      "step": 991100
    },
    {
      "epoch": 15.56043956043956,
      "grad_norm": 4.240190029144287,
      "learning_rate": 4.027472527472528e-05,
      "loss": 0.6146,
      "step": 991200
    },
    {
      "epoch": 15.562009419152277,
      "grad_norm": 4.764307975769043,
      "learning_rate": 4.0273744113029825e-05,
      "loss": 0.648,
      "step": 991300
    },
    {
      "epoch": 15.563579277864992,
      "grad_norm": 4.80652379989624,
      "learning_rate": 4.027276295133438e-05,
      "loss": 0.6224,
      "step": 991400
    },
    {
      "epoch": 15.565149136577707,
      "grad_norm": 4.533143997192383,
      "learning_rate": 4.0271781789638934e-05,
      "loss": 0.6443,
      "step": 991500
    },
    {
      "epoch": 15.566718995290424,
      "grad_norm": 3.903496742248535,
      "learning_rate": 4.027080062794349e-05,
      "loss": 0.6167,
      "step": 991600
    },
    {
      "epoch": 15.56828885400314,
      "grad_norm": 3.908911943435669,
      "learning_rate": 4.0269819466248036e-05,
      "loss": 0.6191,
      "step": 991700
    },
    {
      "epoch": 15.569858712715856,
      "grad_norm": 4.5009355545043945,
      "learning_rate": 4.0268838304552594e-05,
      "loss": 0.5881,
      "step": 991800
    },
    {
      "epoch": 15.571428571428571,
      "grad_norm": 4.283634662628174,
      "learning_rate": 4.0267857142857145e-05,
      "loss": 0.6043,
      "step": 991900
    },
    {
      "epoch": 15.572998430141288,
      "grad_norm": 3.386080741882324,
      "learning_rate": 4.0266875981161696e-05,
      "loss": 0.6346,
      "step": 992000
    },
    {
      "epoch": 15.574568288854003,
      "grad_norm": 3.9732813835144043,
      "learning_rate": 4.026589481946625e-05,
      "loss": 0.6177,
      "step": 992100
    },
    {
      "epoch": 15.576138147566718,
      "grad_norm": 3.71533203125,
      "learning_rate": 4.0264913657770805e-05,
      "loss": 0.642,
      "step": 992200
    },
    {
      "epoch": 15.577708006279435,
      "grad_norm": 3.786890745162964,
      "learning_rate": 4.0263932496075356e-05,
      "loss": 0.6336,
      "step": 992300
    },
    {
      "epoch": 15.57927786499215,
      "grad_norm": 4.153512954711914,
      "learning_rate": 4.0262951334379907e-05,
      "loss": 0.6118,
      "step": 992400
    },
    {
      "epoch": 15.580847723704867,
      "grad_norm": 4.69281005859375,
      "learning_rate": 4.026197017268446e-05,
      "loss": 0.622,
      "step": 992500
    },
    {
      "epoch": 15.582417582417582,
      "grad_norm": 4.021861553192139,
      "learning_rate": 4.0260989010989015e-05,
      "loss": 0.5784,
      "step": 992600
    },
    {
      "epoch": 15.583987441130299,
      "grad_norm": 3.084254741668701,
      "learning_rate": 4.026000784929356e-05,
      "loss": 0.6368,
      "step": 992700
    },
    {
      "epoch": 15.585557299843014,
      "grad_norm": 3.804844379425049,
      "learning_rate": 4.025902668759812e-05,
      "loss": 0.639,
      "step": 992800
    },
    {
      "epoch": 15.58712715855573,
      "grad_norm": 3.370546817779541,
      "learning_rate": 4.025804552590267e-05,
      "loss": 0.6407,
      "step": 992900
    },
    {
      "epoch": 15.588697017268446,
      "grad_norm": 3.087125301361084,
      "learning_rate": 4.0257064364207226e-05,
      "loss": 0.6083,
      "step": 993000
    },
    {
      "epoch": 15.590266875981161,
      "grad_norm": 3.929898262023926,
      "learning_rate": 4.025608320251178e-05,
      "loss": 0.5956,
      "step": 993100
    },
    {
      "epoch": 15.591836734693878,
      "grad_norm": 4.693792819976807,
      "learning_rate": 4.025510204081633e-05,
      "loss": 0.6128,
      "step": 993200
    },
    {
      "epoch": 15.593406593406593,
      "grad_norm": 2.527869701385498,
      "learning_rate": 4.0254120879120886e-05,
      "loss": 0.6736,
      "step": 993300
    },
    {
      "epoch": 15.59497645211931,
      "grad_norm": 3.601815700531006,
      "learning_rate": 4.025313971742543e-05,
      "loss": 0.5952,
      "step": 993400
    },
    {
      "epoch": 15.596546310832025,
      "grad_norm": 3.441333055496216,
      "learning_rate": 4.025215855572999e-05,
      "loss": 0.5883,
      "step": 993500
    },
    {
      "epoch": 15.598116169544742,
      "grad_norm": 4.016682147979736,
      "learning_rate": 4.025117739403454e-05,
      "loss": 0.6385,
      "step": 993600
    },
    {
      "epoch": 15.599686028257457,
      "grad_norm": 4.303449630737305,
      "learning_rate": 4.0250196232339096e-05,
      "loss": 0.6318,
      "step": 993700
    },
    {
      "epoch": 15.601255886970172,
      "grad_norm": 2.8713197708129883,
      "learning_rate": 4.024921507064364e-05,
      "loss": 0.6026,
      "step": 993800
    },
    {
      "epoch": 15.602825745682889,
      "grad_norm": 2.6720359325408936,
      "learning_rate": 4.02482339089482e-05,
      "loss": 0.61,
      "step": 993900
    },
    {
      "epoch": 15.604395604395604,
      "grad_norm": 4.456275463104248,
      "learning_rate": 4.024725274725275e-05,
      "loss": 0.6022,
      "step": 994000
    },
    {
      "epoch": 15.605965463108321,
      "grad_norm": 3.7683353424072266,
      "learning_rate": 4.02462715855573e-05,
      "loss": 0.6457,
      "step": 994100
    },
    {
      "epoch": 15.607535321821036,
      "grad_norm": 3.1929240226745605,
      "learning_rate": 4.024529042386185e-05,
      "loss": 0.6121,
      "step": 994200
    },
    {
      "epoch": 15.609105180533753,
      "grad_norm": 4.449462890625,
      "learning_rate": 4.024430926216641e-05,
      "loss": 0.6332,
      "step": 994300
    },
    {
      "epoch": 15.610675039246468,
      "grad_norm": 3.2993011474609375,
      "learning_rate": 4.024332810047096e-05,
      "loss": 0.6177,
      "step": 994400
    },
    {
      "epoch": 15.612244897959183,
      "grad_norm": 3.718923330307007,
      "learning_rate": 4.024234693877551e-05,
      "loss": 0.6092,
      "step": 994500
    },
    {
      "epoch": 15.6138147566719,
      "grad_norm": 3.5001370906829834,
      "learning_rate": 4.024136577708006e-05,
      "loss": 0.6052,
      "step": 994600
    },
    {
      "epoch": 15.615384615384615,
      "grad_norm": 4.554654121398926,
      "learning_rate": 4.024038461538462e-05,
      "loss": 0.5968,
      "step": 994700
    },
    {
      "epoch": 15.616954474097332,
      "grad_norm": 3.854567766189575,
      "learning_rate": 4.0239403453689164e-05,
      "loss": 0.6011,
      "step": 994800
    },
    {
      "epoch": 15.618524332810047,
      "grad_norm": 5.069676876068115,
      "learning_rate": 4.023842229199372e-05,
      "loss": 0.6429,
      "step": 994900
    },
    {
      "epoch": 15.620094191522764,
      "grad_norm": 3.855372190475464,
      "learning_rate": 4.023744113029827e-05,
      "loss": 0.5898,
      "step": 995000
    },
    {
      "epoch": 15.621664050235479,
      "grad_norm": 3.463672399520874,
      "learning_rate": 4.023645996860283e-05,
      "loss": 0.6443,
      "step": 995100
    },
    {
      "epoch": 15.623233908948194,
      "grad_norm": 4.706298351287842,
      "learning_rate": 4.023547880690738e-05,
      "loss": 0.6282,
      "step": 995200
    },
    {
      "epoch": 15.62480376766091,
      "grad_norm": 3.921542167663574,
      "learning_rate": 4.023449764521193e-05,
      "loss": 0.6391,
      "step": 995300
    },
    {
      "epoch": 15.626373626373626,
      "grad_norm": 3.4894888401031494,
      "learning_rate": 4.0233516483516483e-05,
      "loss": 0.6142,
      "step": 995400
    },
    {
      "epoch": 15.627943485086343,
      "grad_norm": 3.4100005626678467,
      "learning_rate": 4.0232535321821034e-05,
      "loss": 0.6037,
      "step": 995500
    },
    {
      "epoch": 15.629513343799058,
      "grad_norm": 3.6078455448150635,
      "learning_rate": 4.023155416012559e-05,
      "loss": 0.6121,
      "step": 995600
    },
    {
      "epoch": 15.631083202511775,
      "grad_norm": 4.171703338623047,
      "learning_rate": 4.023057299843014e-05,
      "loss": 0.575,
      "step": 995700
    },
    {
      "epoch": 15.63265306122449,
      "grad_norm": 3.1738362312316895,
      "learning_rate": 4.02295918367347e-05,
      "loss": 0.6385,
      "step": 995800
    },
    {
      "epoch": 15.634222919937205,
      "grad_norm": 3.673492193222046,
      "learning_rate": 4.0228610675039245e-05,
      "loss": 0.6185,
      "step": 995900
    },
    {
      "epoch": 15.635792778649922,
      "grad_norm": 3.096587657928467,
      "learning_rate": 4.02276295133438e-05,
      "loss": 0.5936,
      "step": 996000
    },
    {
      "epoch": 15.637362637362637,
      "grad_norm": 4.314740180969238,
      "learning_rate": 4.0226648351648354e-05,
      "loss": 0.6197,
      "step": 996100
    },
    {
      "epoch": 15.638932496075354,
      "grad_norm": 4.397936820983887,
      "learning_rate": 4.0225667189952905e-05,
      "loss": 0.6051,
      "step": 996200
    },
    {
      "epoch": 15.640502354788069,
      "grad_norm": 4.024405479431152,
      "learning_rate": 4.0224686028257456e-05,
      "loss": 0.5979,
      "step": 996300
    },
    {
      "epoch": 15.642072213500786,
      "grad_norm": 3.2648544311523438,
      "learning_rate": 4.0223704866562014e-05,
      "loss": 0.6241,
      "step": 996400
    },
    {
      "epoch": 15.6436420722135,
      "grad_norm": 2.628864288330078,
      "learning_rate": 4.0222723704866565e-05,
      "loss": 0.6137,
      "step": 996500
    },
    {
      "epoch": 15.645211930926216,
      "grad_norm": 3.505415439605713,
      "learning_rate": 4.0221742543171116e-05,
      "loss": 0.6068,
      "step": 996600
    },
    {
      "epoch": 15.646781789638933,
      "grad_norm": 4.164831161499023,
      "learning_rate": 4.0220761381475667e-05,
      "loss": 0.6312,
      "step": 996700
    },
    {
      "epoch": 15.648351648351648,
      "grad_norm": 3.48030424118042,
      "learning_rate": 4.0219780219780224e-05,
      "loss": 0.5838,
      "step": 996800
    },
    {
      "epoch": 15.649921507064365,
      "grad_norm": 4.056135654449463,
      "learning_rate": 4.021879905808477e-05,
      "loss": 0.621,
      "step": 996900
    },
    {
      "epoch": 15.65149136577708,
      "grad_norm": 3.614995002746582,
      "learning_rate": 4.0217817896389326e-05,
      "loss": 0.6401,
      "step": 997000
    },
    {
      "epoch": 15.653061224489797,
      "grad_norm": 4.490878105163574,
      "learning_rate": 4.021683673469388e-05,
      "loss": 0.5944,
      "step": 997100
    },
    {
      "epoch": 15.654631083202512,
      "grad_norm": 3.2357850074768066,
      "learning_rate": 4.0215855572998435e-05,
      "loss": 0.5964,
      "step": 997200
    },
    {
      "epoch": 15.656200941915227,
      "grad_norm": 3.8318469524383545,
      "learning_rate": 4.0214874411302986e-05,
      "loss": 0.5988,
      "step": 997300
    },
    {
      "epoch": 15.657770800627944,
      "grad_norm": 4.518754005432129,
      "learning_rate": 4.021389324960754e-05,
      "loss": 0.647,
      "step": 997400
    },
    {
      "epoch": 15.659340659340659,
      "grad_norm": 3.9191884994506836,
      "learning_rate": 4.021291208791209e-05,
      "loss": 0.5789,
      "step": 997500
    },
    {
      "epoch": 15.660910518053376,
      "grad_norm": 3.953660726547241,
      "learning_rate": 4.021193092621664e-05,
      "loss": 0.6136,
      "step": 997600
    },
    {
      "epoch": 15.66248037676609,
      "grad_norm": 3.6479928493499756,
      "learning_rate": 4.02109497645212e-05,
      "loss": 0.6418,
      "step": 997700
    },
    {
      "epoch": 15.664050235478808,
      "grad_norm": 4.199211597442627,
      "learning_rate": 4.020996860282575e-05,
      "loss": 0.6406,
      "step": 997800
    },
    {
      "epoch": 15.665620094191523,
      "grad_norm": 4.088290691375732,
      "learning_rate": 4.0208987441130305e-05,
      "loss": 0.5856,
      "step": 997900
    },
    {
      "epoch": 15.667189952904238,
      "grad_norm": 2.744749069213867,
      "learning_rate": 4.020800627943485e-05,
      "loss": 0.6217,
      "step": 998000
    },
    {
      "epoch": 15.668759811616955,
      "grad_norm": 4.239880561828613,
      "learning_rate": 4.020702511773941e-05,
      "loss": 0.624,
      "step": 998100
    },
    {
      "epoch": 15.67032967032967,
      "grad_norm": 4.048953056335449,
      "learning_rate": 4.020604395604396e-05,
      "loss": 0.5685,
      "step": 998200
    },
    {
      "epoch": 15.671899529042387,
      "grad_norm": 4.239737510681152,
      "learning_rate": 4.020506279434851e-05,
      "loss": 0.5961,
      "step": 998300
    },
    {
      "epoch": 15.673469387755102,
      "grad_norm": 3.5981736183166504,
      "learning_rate": 4.020408163265306e-05,
      "loss": 0.6096,
      "step": 998400
    },
    {
      "epoch": 15.675039246467819,
      "grad_norm": 4.3645782470703125,
      "learning_rate": 4.020310047095762e-05,
      "loss": 0.6269,
      "step": 998500
    },
    {
      "epoch": 15.676609105180534,
      "grad_norm": 3.737623691558838,
      "learning_rate": 4.020211930926217e-05,
      "loss": 0.6901,
      "step": 998600
    },
    {
      "epoch": 15.678178963893249,
      "grad_norm": 3.226102352142334,
      "learning_rate": 4.020113814756672e-05,
      "loss": 0.5954,
      "step": 998700
    },
    {
      "epoch": 15.679748822605966,
      "grad_norm": 4.671793460845947,
      "learning_rate": 4.020015698587127e-05,
      "loss": 0.6087,
      "step": 998800
    },
    {
      "epoch": 15.68131868131868,
      "grad_norm": 3.6409401893615723,
      "learning_rate": 4.019917582417583e-05,
      "loss": 0.6112,
      "step": 998900
    },
    {
      "epoch": 15.682888540031398,
      "grad_norm": 3.894735813140869,
      "learning_rate": 4.019819466248037e-05,
      "loss": 0.6389,
      "step": 999000
    },
    {
      "epoch": 15.684458398744113,
      "grad_norm": 3.2166459560394287,
      "learning_rate": 4.019721350078493e-05,
      "loss": 0.6325,
      "step": 999100
    },
    {
      "epoch": 15.68602825745683,
      "grad_norm": 3.5476760864257812,
      "learning_rate": 4.019623233908948e-05,
      "loss": 0.6323,
      "step": 999200
    },
    {
      "epoch": 15.687598116169545,
      "grad_norm": 3.306513547897339,
      "learning_rate": 4.019525117739404e-05,
      "loss": 0.6296,
      "step": 999300
    },
    {
      "epoch": 15.68916797488226,
      "grad_norm": 3.73544979095459,
      "learning_rate": 4.019427001569859e-05,
      "loss": 0.5953,
      "step": 999400
    },
    {
      "epoch": 15.690737833594977,
      "grad_norm": 3.699349880218506,
      "learning_rate": 4.019328885400314e-05,
      "loss": 0.6024,
      "step": 999500
    },
    {
      "epoch": 15.692307692307692,
      "grad_norm": 3.176968812942505,
      "learning_rate": 4.019230769230769e-05,
      "loss": 0.6208,
      "step": 999600
    },
    {
      "epoch": 15.693877551020408,
      "grad_norm": 3.1310901641845703,
      "learning_rate": 4.0191326530612243e-05,
      "loss": 0.6328,
      "step": 999700
    },
    {
      "epoch": 15.695447409733124,
      "grad_norm": 4.120497703552246,
      "learning_rate": 4.01903453689168e-05,
      "loss": 0.5844,
      "step": 999800
    },
    {
      "epoch": 15.69701726844584,
      "grad_norm": 3.66291880607605,
      "learning_rate": 4.018936420722135e-05,
      "loss": 0.6439,
      "step": 999900
    },
    {
      "epoch": 15.698587127158556,
      "grad_norm": 3.459182024002075,
      "learning_rate": 4.018838304552591e-05,
      "loss": 0.5984,
      "step": 1000000
    },
    {
      "epoch": 15.700156985871272,
      "grad_norm": 3.4881060123443604,
      "learning_rate": 4.0187401883830454e-05,
      "loss": 0.611,
      "step": 1000100
    },
    {
      "epoch": 15.701726844583987,
      "grad_norm": 3.634197235107422,
      "learning_rate": 4.018642072213501e-05,
      "loss": 0.6254,
      "step": 1000200
    },
    {
      "epoch": 15.703296703296703,
      "grad_norm": 4.560070991516113,
      "learning_rate": 4.018543956043956e-05,
      "loss": 0.6094,
      "step": 1000300
    },
    {
      "epoch": 15.70486656200942,
      "grad_norm": 3.905686378479004,
      "learning_rate": 4.0184458398744114e-05,
      "loss": 0.592,
      "step": 1000400
    },
    {
      "epoch": 15.706436420722135,
      "grad_norm": 4.475665092468262,
      "learning_rate": 4.0183477237048665e-05,
      "loss": 0.6399,
      "step": 1000500
    },
    {
      "epoch": 15.708006279434851,
      "grad_norm": 4.374888896942139,
      "learning_rate": 4.018249607535322e-05,
      "loss": 0.6244,
      "step": 1000600
    },
    {
      "epoch": 15.709576138147566,
      "grad_norm": 4.372244358062744,
      "learning_rate": 4.0181514913657774e-05,
      "loss": 0.6084,
      "step": 1000700
    },
    {
      "epoch": 15.711145996860283,
      "grad_norm": 3.777266025543213,
      "learning_rate": 4.0180533751962325e-05,
      "loss": 0.5917,
      "step": 1000800
    },
    {
      "epoch": 15.712715855572998,
      "grad_norm": 3.7527987957000732,
      "learning_rate": 4.0179552590266876e-05,
      "loss": 0.6281,
      "step": 1000900
    },
    {
      "epoch": 15.714285714285714,
      "grad_norm": 4.384467124938965,
      "learning_rate": 4.017857142857143e-05,
      "loss": 0.6189,
      "step": 1001000
    },
    {
      "epoch": 15.71585557299843,
      "grad_norm": 4.418508052825928,
      "learning_rate": 4.017759026687598e-05,
      "loss": 0.6523,
      "step": 1001100
    },
    {
      "epoch": 15.717425431711145,
      "grad_norm": 3.665576457977295,
      "learning_rate": 4.0176609105180535e-05,
      "loss": 0.5972,
      "step": 1001200
    },
    {
      "epoch": 15.718995290423862,
      "grad_norm": 4.558030605316162,
      "learning_rate": 4.0175627943485086e-05,
      "loss": 0.6428,
      "step": 1001300
    },
    {
      "epoch": 15.720565149136577,
      "grad_norm": 3.8069989681243896,
      "learning_rate": 4.0174646781789644e-05,
      "loss": 0.6352,
      "step": 1001400
    },
    {
      "epoch": 15.722135007849294,
      "grad_norm": 2.9535510540008545,
      "learning_rate": 4.0173665620094195e-05,
      "loss": 0.638,
      "step": 1001500
    },
    {
      "epoch": 15.72370486656201,
      "grad_norm": 2.906613349914551,
      "learning_rate": 4.0172684458398746e-05,
      "loss": 0.6375,
      "step": 1001600
    },
    {
      "epoch": 15.725274725274724,
      "grad_norm": 4.409367561340332,
      "learning_rate": 4.01717032967033e-05,
      "loss": 0.6313,
      "step": 1001700
    },
    {
      "epoch": 15.726844583987441,
      "grad_norm": 3.3258161544799805,
      "learning_rate": 4.017072213500785e-05,
      "loss": 0.5962,
      "step": 1001800
    },
    {
      "epoch": 15.728414442700156,
      "grad_norm": 3.834296941757202,
      "learning_rate": 4.0169740973312406e-05,
      "loss": 0.586,
      "step": 1001900
    },
    {
      "epoch": 15.729984301412873,
      "grad_norm": 3.1507058143615723,
      "learning_rate": 4.016875981161696e-05,
      "loss": 0.6104,
      "step": 1002000
    },
    {
      "epoch": 15.731554160125588,
      "grad_norm": 3.0957913398742676,
      "learning_rate": 4.0167778649921514e-05,
      "loss": 0.6067,
      "step": 1002100
    },
    {
      "epoch": 15.733124018838305,
      "grad_norm": 3.164198875427246,
      "learning_rate": 4.016679748822606e-05,
      "loss": 0.6168,
      "step": 1002200
    },
    {
      "epoch": 15.73469387755102,
      "grad_norm": 3.6448464393615723,
      "learning_rate": 4.0165816326530616e-05,
      "loss": 0.5988,
      "step": 1002300
    },
    {
      "epoch": 15.736263736263737,
      "grad_norm": 4.370697021484375,
      "learning_rate": 4.016483516483517e-05,
      "loss": 0.6491,
      "step": 1002400
    },
    {
      "epoch": 15.737833594976452,
      "grad_norm": 3.5706565380096436,
      "learning_rate": 4.016385400313972e-05,
      "loss": 0.6323,
      "step": 1002500
    },
    {
      "epoch": 15.739403453689167,
      "grad_norm": 3.76240873336792,
      "learning_rate": 4.016287284144427e-05,
      "loss": 0.6207,
      "step": 1002600
    },
    {
      "epoch": 15.740973312401884,
      "grad_norm": 3.292597770690918,
      "learning_rate": 4.016189167974883e-05,
      "loss": 0.6348,
      "step": 1002700
    },
    {
      "epoch": 15.7425431711146,
      "grad_norm": 4.232109069824219,
      "learning_rate": 4.016091051805338e-05,
      "loss": 0.6094,
      "step": 1002800
    },
    {
      "epoch": 15.744113029827316,
      "grad_norm": 3.908905029296875,
      "learning_rate": 4.015992935635793e-05,
      "loss": 0.6432,
      "step": 1002900
    },
    {
      "epoch": 15.745682888540031,
      "grad_norm": 3.964010238647461,
      "learning_rate": 4.015894819466248e-05,
      "loss": 0.5986,
      "step": 1003000
    },
    {
      "epoch": 15.747252747252748,
      "grad_norm": 2.1539463996887207,
      "learning_rate": 4.015796703296704e-05,
      "loss": 0.626,
      "step": 1003100
    },
    {
      "epoch": 15.748822605965463,
      "grad_norm": 4.018498420715332,
      "learning_rate": 4.015698587127158e-05,
      "loss": 0.6286,
      "step": 1003200
    },
    {
      "epoch": 15.750392464678178,
      "grad_norm": 4.032742500305176,
      "learning_rate": 4.015600470957614e-05,
      "loss": 0.5887,
      "step": 1003300
    },
    {
      "epoch": 15.751962323390895,
      "grad_norm": 4.286139011383057,
      "learning_rate": 4.015502354788069e-05,
      "loss": 0.6151,
      "step": 1003400
    },
    {
      "epoch": 15.75353218210361,
      "grad_norm": 3.7554571628570557,
      "learning_rate": 4.015404238618525e-05,
      "loss": 0.5839,
      "step": 1003500
    },
    {
      "epoch": 15.755102040816327,
      "grad_norm": 4.16919469833374,
      "learning_rate": 4.01530612244898e-05,
      "loss": 0.6236,
      "step": 1003600
    },
    {
      "epoch": 15.756671899529042,
      "grad_norm": 4.046139240264893,
      "learning_rate": 4.015208006279435e-05,
      "loss": 0.6244,
      "step": 1003700
    },
    {
      "epoch": 15.758241758241759,
      "grad_norm": 2.728436231613159,
      "learning_rate": 4.01510989010989e-05,
      "loss": 0.5482,
      "step": 1003800
    },
    {
      "epoch": 15.759811616954474,
      "grad_norm": 4.203488826751709,
      "learning_rate": 4.015011773940345e-05,
      "loss": 0.6243,
      "step": 1003900
    },
    {
      "epoch": 15.76138147566719,
      "grad_norm": 3.2855889797210693,
      "learning_rate": 4.014913657770801e-05,
      "loss": 0.6144,
      "step": 1004000
    },
    {
      "epoch": 15.762951334379906,
      "grad_norm": 4.174563407897949,
      "learning_rate": 4.014815541601256e-05,
      "loss": 0.6641,
      "step": 1004100
    },
    {
      "epoch": 15.764521193092621,
      "grad_norm": 3.3394901752471924,
      "learning_rate": 4.014717425431712e-05,
      "loss": 0.6146,
      "step": 1004200
    },
    {
      "epoch": 15.766091051805338,
      "grad_norm": 4.172513484954834,
      "learning_rate": 4.014619309262166e-05,
      "loss": 0.6315,
      "step": 1004300
    },
    {
      "epoch": 15.767660910518053,
      "grad_norm": 3.1140551567077637,
      "learning_rate": 4.014521193092622e-05,
      "loss": 0.6063,
      "step": 1004400
    },
    {
      "epoch": 15.76923076923077,
      "grad_norm": 3.8705899715423584,
      "learning_rate": 4.014423076923077e-05,
      "loss": 0.6401,
      "step": 1004500
    },
    {
      "epoch": 15.770800627943485,
      "grad_norm": 4.349766731262207,
      "learning_rate": 4.014324960753532e-05,
      "loss": 0.6139,
      "step": 1004600
    },
    {
      "epoch": 15.7723704866562,
      "grad_norm": 4.249251842498779,
      "learning_rate": 4.0142268445839874e-05,
      "loss": 0.6357,
      "step": 1004700
    },
    {
      "epoch": 15.773940345368917,
      "grad_norm": 4.093973159790039,
      "learning_rate": 4.014128728414443e-05,
      "loss": 0.6235,
      "step": 1004800
    },
    {
      "epoch": 15.775510204081632,
      "grad_norm": 2.903074026107788,
      "learning_rate": 4.014030612244898e-05,
      "loss": 0.6128,
      "step": 1004900
    },
    {
      "epoch": 15.777080062794349,
      "grad_norm": 3.6000397205352783,
      "learning_rate": 4.0139324960753534e-05,
      "loss": 0.6231,
      "step": 1005000
    },
    {
      "epoch": 15.778649921507064,
      "grad_norm": 3.7743818759918213,
      "learning_rate": 4.0138343799058085e-05,
      "loss": 0.6155,
      "step": 1005100
    },
    {
      "epoch": 15.780219780219781,
      "grad_norm": 4.352747440338135,
      "learning_rate": 4.013736263736264e-05,
      "loss": 0.623,
      "step": 1005200
    },
    {
      "epoch": 15.781789638932496,
      "grad_norm": 5.9135541915893555,
      "learning_rate": 4.0136381475667186e-05,
      "loss": 0.589,
      "step": 1005300
    },
    {
      "epoch": 15.783359497645211,
      "grad_norm": 4.507457733154297,
      "learning_rate": 4.0135400313971744e-05,
      "loss": 0.6363,
      "step": 1005400
    },
    {
      "epoch": 15.784929356357928,
      "grad_norm": 4.426671028137207,
      "learning_rate": 4.0134419152276295e-05,
      "loss": 0.6393,
      "step": 1005500
    },
    {
      "epoch": 15.786499215070643,
      "grad_norm": 4.139009475708008,
      "learning_rate": 4.013343799058085e-05,
      "loss": 0.6336,
      "step": 1005600
    },
    {
      "epoch": 15.78806907378336,
      "grad_norm": 4.238621711730957,
      "learning_rate": 4.0132456828885404e-05,
      "loss": 0.6124,
      "step": 1005700
    },
    {
      "epoch": 15.789638932496075,
      "grad_norm": 3.538593053817749,
      "learning_rate": 4.0131475667189955e-05,
      "loss": 0.6543,
      "step": 1005800
    },
    {
      "epoch": 15.791208791208792,
      "grad_norm": 4.1613593101501465,
      "learning_rate": 4.0130494505494506e-05,
      "loss": 0.5966,
      "step": 1005900
    },
    {
      "epoch": 15.792778649921507,
      "grad_norm": 4.099936485290527,
      "learning_rate": 4.012951334379906e-05,
      "loss": 0.6165,
      "step": 1006000
    },
    {
      "epoch": 15.794348508634222,
      "grad_norm": 4.645261287689209,
      "learning_rate": 4.0128532182103615e-05,
      "loss": 0.5982,
      "step": 1006100
    },
    {
      "epoch": 15.795918367346939,
      "grad_norm": 4.290156364440918,
      "learning_rate": 4.0127551020408166e-05,
      "loss": 0.6586,
      "step": 1006200
    },
    {
      "epoch": 15.797488226059654,
      "grad_norm": 4.543240070343018,
      "learning_rate": 4.012656985871272e-05,
      "loss": 0.6194,
      "step": 1006300
    },
    {
      "epoch": 15.799058084772371,
      "grad_norm": 4.432972431182861,
      "learning_rate": 4.012558869701727e-05,
      "loss": 0.6245,
      "step": 1006400
    },
    {
      "epoch": 15.800627943485086,
      "grad_norm": 3.981832981109619,
      "learning_rate": 4.0124607535321825e-05,
      "loss": 0.6004,
      "step": 1006500
    },
    {
      "epoch": 15.802197802197803,
      "grad_norm": 2.3569107055664062,
      "learning_rate": 4.0123626373626376e-05,
      "loss": 0.6444,
      "step": 1006600
    },
    {
      "epoch": 15.803767660910518,
      "grad_norm": 4.164034843444824,
      "learning_rate": 4.012264521193093e-05,
      "loss": 0.619,
      "step": 1006700
    },
    {
      "epoch": 15.805337519623233,
      "grad_norm": 4.87741231918335,
      "learning_rate": 4.012166405023548e-05,
      "loss": 0.6739,
      "step": 1006800
    },
    {
      "epoch": 15.80690737833595,
      "grad_norm": 4.337113380432129,
      "learning_rate": 4.0120682888540036e-05,
      "loss": 0.6477,
      "step": 1006900
    },
    {
      "epoch": 15.808477237048665,
      "grad_norm": 3.5742785930633545,
      "learning_rate": 4.011970172684459e-05,
      "loss": 0.626,
      "step": 1007000
    },
    {
      "epoch": 15.810047095761382,
      "grad_norm": 2.6408920288085938,
      "learning_rate": 4.011872056514914e-05,
      "loss": 0.6161,
      "step": 1007100
    },
    {
      "epoch": 15.811616954474097,
      "grad_norm": 3.9510533809661865,
      "learning_rate": 4.011773940345369e-05,
      "loss": 0.6086,
      "step": 1007200
    },
    {
      "epoch": 15.813186813186814,
      "grad_norm": 4.676992416381836,
      "learning_rate": 4.011675824175825e-05,
      "loss": 0.6356,
      "step": 1007300
    },
    {
      "epoch": 15.814756671899529,
      "grad_norm": 4.533234596252441,
      "learning_rate": 4.011577708006279e-05,
      "loss": 0.633,
      "step": 1007400
    },
    {
      "epoch": 15.816326530612244,
      "grad_norm": 4.586882591247559,
      "learning_rate": 4.011479591836735e-05,
      "loss": 0.5695,
      "step": 1007500
    },
    {
      "epoch": 15.817896389324961,
      "grad_norm": 4.03257942199707,
      "learning_rate": 4.01138147566719e-05,
      "loss": 0.6468,
      "step": 1007600
    },
    {
      "epoch": 15.819466248037676,
      "grad_norm": 4.338665962219238,
      "learning_rate": 4.011283359497646e-05,
      "loss": 0.6211,
      "step": 1007700
    },
    {
      "epoch": 15.821036106750393,
      "grad_norm": 3.969607353210449,
      "learning_rate": 4.011185243328101e-05,
      "loss": 0.5835,
      "step": 1007800
    },
    {
      "epoch": 15.822605965463108,
      "grad_norm": 3.3528847694396973,
      "learning_rate": 4.011087127158556e-05,
      "loss": 0.6229,
      "step": 1007900
    },
    {
      "epoch": 15.824175824175825,
      "grad_norm": 2.039944887161255,
      "learning_rate": 4.010989010989011e-05,
      "loss": 0.5951,
      "step": 1008000
    },
    {
      "epoch": 15.82574568288854,
      "grad_norm": 4.645031929016113,
      "learning_rate": 4.010890894819466e-05,
      "loss": 0.6311,
      "step": 1008100
    },
    {
      "epoch": 15.827315541601255,
      "grad_norm": 2.734562397003174,
      "learning_rate": 4.010792778649922e-05,
      "loss": 0.6309,
      "step": 1008200
    },
    {
      "epoch": 15.828885400313972,
      "grad_norm": 3.951777458190918,
      "learning_rate": 4.010694662480377e-05,
      "loss": 0.6319,
      "step": 1008300
    },
    {
      "epoch": 15.830455259026687,
      "grad_norm": 3.69153094291687,
      "learning_rate": 4.010596546310833e-05,
      "loss": 0.626,
      "step": 1008400
    },
    {
      "epoch": 15.832025117739404,
      "grad_norm": 2.6200098991394043,
      "learning_rate": 4.010498430141287e-05,
      "loss": 0.5866,
      "step": 1008500
    },
    {
      "epoch": 15.833594976452119,
      "grad_norm": 4.3444294929504395,
      "learning_rate": 4.010400313971743e-05,
      "loss": 0.6066,
      "step": 1008600
    },
    {
      "epoch": 15.835164835164836,
      "grad_norm": 3.402970552444458,
      "learning_rate": 4.010302197802198e-05,
      "loss": 0.6008,
      "step": 1008700
    },
    {
      "epoch": 15.83673469387755,
      "grad_norm": 4.219482898712158,
      "learning_rate": 4.010204081632653e-05,
      "loss": 0.6248,
      "step": 1008800
    },
    {
      "epoch": 15.838304552590268,
      "grad_norm": 4.737692356109619,
      "learning_rate": 4.010105965463108e-05,
      "loss": 0.6002,
      "step": 1008900
    },
    {
      "epoch": 15.839874411302983,
      "grad_norm": 5.188748359680176,
      "learning_rate": 4.010007849293564e-05,
      "loss": 0.6389,
      "step": 1009000
    },
    {
      "epoch": 15.841444270015698,
      "grad_norm": 4.163222312927246,
      "learning_rate": 4.009909733124019e-05,
      "loss": 0.6257,
      "step": 1009100
    },
    {
      "epoch": 15.843014128728415,
      "grad_norm": 3.0793464183807373,
      "learning_rate": 4.009811616954474e-05,
      "loss": 0.5971,
      "step": 1009200
    },
    {
      "epoch": 15.84458398744113,
      "grad_norm": 3.303102970123291,
      "learning_rate": 4.0097135007849293e-05,
      "loss": 0.5939,
      "step": 1009300
    },
    {
      "epoch": 15.846153846153847,
      "grad_norm": 3.139500856399536,
      "learning_rate": 4.009615384615385e-05,
      "loss": 0.6292,
      "step": 1009400
    },
    {
      "epoch": 15.847723704866562,
      "grad_norm": 3.1445815563201904,
      "learning_rate": 4.0095172684458395e-05,
      "loss": 0.595,
      "step": 1009500
    },
    {
      "epoch": 15.849293563579279,
      "grad_norm": 3.628995895385742,
      "learning_rate": 4.009419152276295e-05,
      "loss": 0.6285,
      "step": 1009600
    },
    {
      "epoch": 15.850863422291994,
      "grad_norm": 4.327652931213379,
      "learning_rate": 4.0093210361067504e-05,
      "loss": 0.6076,
      "step": 1009700
    },
    {
      "epoch": 15.852433281004709,
      "grad_norm": 4.0342631340026855,
      "learning_rate": 4.009222919937206e-05,
      "loss": 0.5902,
      "step": 1009800
    },
    {
      "epoch": 15.854003139717426,
      "grad_norm": 3.451444149017334,
      "learning_rate": 4.009124803767661e-05,
      "loss": 0.6182,
      "step": 1009900
    },
    {
      "epoch": 15.85557299843014,
      "grad_norm": 3.7060978412628174,
      "learning_rate": 4.0090266875981164e-05,
      "loss": 0.585,
      "step": 1010000
    },
    {
      "epoch": 15.857142857142858,
      "grad_norm": 3.2420847415924072,
      "learning_rate": 4.0089285714285715e-05,
      "loss": 0.6423,
      "step": 1010100
    },
    {
      "epoch": 15.858712715855573,
      "grad_norm": 3.7242655754089355,
      "learning_rate": 4.0088304552590266e-05,
      "loss": 0.6481,
      "step": 1010200
    },
    {
      "epoch": 15.86028257456829,
      "grad_norm": 4.0823493003845215,
      "learning_rate": 4.0087323390894824e-05,
      "loss": 0.6284,
      "step": 1010300
    },
    {
      "epoch": 15.861852433281005,
      "grad_norm": 4.483154773712158,
      "learning_rate": 4.0086342229199375e-05,
      "loss": 0.6257,
      "step": 1010400
    },
    {
      "epoch": 15.86342229199372,
      "grad_norm": 4.792947292327881,
      "learning_rate": 4.008536106750393e-05,
      "loss": 0.6191,
      "step": 1010500
    },
    {
      "epoch": 15.864992150706437,
      "grad_norm": 3.219780206680298,
      "learning_rate": 4.0084379905808477e-05,
      "loss": 0.619,
      "step": 1010600
    },
    {
      "epoch": 15.866562009419152,
      "grad_norm": 3.766662359237671,
      "learning_rate": 4.0083398744113034e-05,
      "loss": 0.6459,
      "step": 1010700
    },
    {
      "epoch": 15.868131868131869,
      "grad_norm": 2.8093597888946533,
      "learning_rate": 4.0082417582417585e-05,
      "loss": 0.6192,
      "step": 1010800
    },
    {
      "epoch": 15.869701726844584,
      "grad_norm": 4.360258102416992,
      "learning_rate": 4.0081436420722136e-05,
      "loss": 0.6172,
      "step": 1010900
    },
    {
      "epoch": 15.8712715855573,
      "grad_norm": 2.7245280742645264,
      "learning_rate": 4.008045525902669e-05,
      "loss": 0.6567,
      "step": 1011000
    },
    {
      "epoch": 15.872841444270016,
      "grad_norm": 4.1247878074646,
      "learning_rate": 4.0079474097331245e-05,
      "loss": 0.6025,
      "step": 1011100
    },
    {
      "epoch": 15.87441130298273,
      "grad_norm": 3.347965717315674,
      "learning_rate": 4.0078492935635796e-05,
      "loss": 0.5974,
      "step": 1011200
    },
    {
      "epoch": 15.875981161695448,
      "grad_norm": 4.138843536376953,
      "learning_rate": 4.007751177394035e-05,
      "loss": 0.5846,
      "step": 1011300
    },
    {
      "epoch": 15.877551020408163,
      "grad_norm": 3.666947364807129,
      "learning_rate": 4.00765306122449e-05,
      "loss": 0.6283,
      "step": 1011400
    },
    {
      "epoch": 15.87912087912088,
      "grad_norm": 3.805579662322998,
      "learning_rate": 4.0075549450549456e-05,
      "loss": 0.6491,
      "step": 1011500
    },
    {
      "epoch": 15.880690737833595,
      "grad_norm": 3.6721723079681396,
      "learning_rate": 4.0074568288854e-05,
      "loss": 0.6263,
      "step": 1011600
    },
    {
      "epoch": 15.882260596546312,
      "grad_norm": 3.527837038040161,
      "learning_rate": 4.007358712715856e-05,
      "loss": 0.6266,
      "step": 1011700
    },
    {
      "epoch": 15.883830455259027,
      "grad_norm": 4.222832202911377,
      "learning_rate": 4.007260596546311e-05,
      "loss": 0.6464,
      "step": 1011800
    },
    {
      "epoch": 15.885400313971743,
      "grad_norm": 4.284339427947998,
      "learning_rate": 4.0071624803767666e-05,
      "loss": 0.6192,
      "step": 1011900
    },
    {
      "epoch": 15.886970172684459,
      "grad_norm": 3.71378755569458,
      "learning_rate": 4.007064364207222e-05,
      "loss": 0.6274,
      "step": 1012000
    },
    {
      "epoch": 15.888540031397174,
      "grad_norm": 3.3811302185058594,
      "learning_rate": 4.006966248037677e-05,
      "loss": 0.6455,
      "step": 1012100
    },
    {
      "epoch": 15.89010989010989,
      "grad_norm": 4.46189022064209,
      "learning_rate": 4.006868131868132e-05,
      "loss": 0.5677,
      "step": 1012200
    },
    {
      "epoch": 15.891679748822606,
      "grad_norm": 3.722154378890991,
      "learning_rate": 4.006770015698587e-05,
      "loss": 0.5872,
      "step": 1012300
    },
    {
      "epoch": 15.893249607535322,
      "grad_norm": 4.449441432952881,
      "learning_rate": 4.006671899529043e-05,
      "loss": 0.6072,
      "step": 1012400
    },
    {
      "epoch": 15.894819466248038,
      "grad_norm": 2.866950750350952,
      "learning_rate": 4.006573783359498e-05,
      "loss": 0.6218,
      "step": 1012500
    },
    {
      "epoch": 15.896389324960754,
      "grad_norm": 3.198359251022339,
      "learning_rate": 4.006475667189953e-05,
      "loss": 0.5714,
      "step": 1012600
    },
    {
      "epoch": 15.89795918367347,
      "grad_norm": 3.5067007541656494,
      "learning_rate": 4.006377551020408e-05,
      "loss": 0.6573,
      "step": 1012700
    },
    {
      "epoch": 15.899529042386185,
      "grad_norm": 3.626884937286377,
      "learning_rate": 4.006279434850864e-05,
      "loss": 0.6398,
      "step": 1012800
    },
    {
      "epoch": 15.901098901098901,
      "grad_norm": 3.5310304164886475,
      "learning_rate": 4.006181318681319e-05,
      "loss": 0.6163,
      "step": 1012900
    },
    {
      "epoch": 15.902668759811617,
      "grad_norm": 4.067119598388672,
      "learning_rate": 4.006083202511774e-05,
      "loss": 0.6115,
      "step": 1013000
    },
    {
      "epoch": 15.904238618524333,
      "grad_norm": 3.662792682647705,
      "learning_rate": 4.005985086342229e-05,
      "loss": 0.6373,
      "step": 1013100
    },
    {
      "epoch": 15.905808477237048,
      "grad_norm": 3.909104347229004,
      "learning_rate": 4.005886970172685e-05,
      "loss": 0.6133,
      "step": 1013200
    },
    {
      "epoch": 15.907378335949765,
      "grad_norm": 3.506396770477295,
      "learning_rate": 4.0057888540031394e-05,
      "loss": 0.6212,
      "step": 1013300
    },
    {
      "epoch": 15.90894819466248,
      "grad_norm": 4.052821159362793,
      "learning_rate": 4.005690737833595e-05,
      "loss": 0.6076,
      "step": 1013400
    },
    {
      "epoch": 15.910518053375196,
      "grad_norm": 4.709903717041016,
      "learning_rate": 4.00559262166405e-05,
      "loss": 0.6581,
      "step": 1013500
    },
    {
      "epoch": 15.912087912087912,
      "grad_norm": 3.8276538848876953,
      "learning_rate": 4.005494505494506e-05,
      "loss": 0.5875,
      "step": 1013600
    },
    {
      "epoch": 15.913657770800627,
      "grad_norm": 2.2040514945983887,
      "learning_rate": 4.0053963893249604e-05,
      "loss": 0.6072,
      "step": 1013700
    },
    {
      "epoch": 15.915227629513344,
      "grad_norm": 3.711188793182373,
      "learning_rate": 4.005298273155416e-05,
      "loss": 0.6434,
      "step": 1013800
    },
    {
      "epoch": 15.91679748822606,
      "grad_norm": 3.84169864654541,
      "learning_rate": 4.005200156985871e-05,
      "loss": 0.5933,
      "step": 1013900
    },
    {
      "epoch": 15.918367346938776,
      "grad_norm": 3.512361764907837,
      "learning_rate": 4.0051020408163264e-05,
      "loss": 0.5482,
      "step": 1014000
    },
    {
      "epoch": 15.919937205651491,
      "grad_norm": 3.968069076538086,
      "learning_rate": 4.005003924646782e-05,
      "loss": 0.6615,
      "step": 1014100
    },
    {
      "epoch": 15.921507064364206,
      "grad_norm": 3.5993292331695557,
      "learning_rate": 4.004905808477237e-05,
      "loss": 0.6071,
      "step": 1014200
    },
    {
      "epoch": 15.923076923076923,
      "grad_norm": 3.278759717941284,
      "learning_rate": 4.0048076923076924e-05,
      "loss": 0.6271,
      "step": 1014300
    },
    {
      "epoch": 15.924646781789638,
      "grad_norm": 4.053682804107666,
      "learning_rate": 4.0047095761381475e-05,
      "loss": 0.637,
      "step": 1014400
    },
    {
      "epoch": 15.926216640502355,
      "grad_norm": 2.7715859413146973,
      "learning_rate": 4.004611459968603e-05,
      "loss": 0.5935,
      "step": 1014500
    },
    {
      "epoch": 15.92778649921507,
      "grad_norm": 3.577066659927368,
      "learning_rate": 4.0045133437990584e-05,
      "loss": 0.6021,
      "step": 1014600
    },
    {
      "epoch": 15.929356357927787,
      "grad_norm": 2.9982473850250244,
      "learning_rate": 4.0044152276295135e-05,
      "loss": 0.6295,
      "step": 1014700
    },
    {
      "epoch": 15.930926216640502,
      "grad_norm": 3.2237625122070312,
      "learning_rate": 4.0043171114599686e-05,
      "loss": 0.6356,
      "step": 1014800
    },
    {
      "epoch": 15.932496075353217,
      "grad_norm": 4.890004634857178,
      "learning_rate": 4.004218995290424e-05,
      "loss": 0.6585,
      "step": 1014900
    },
    {
      "epoch": 15.934065934065934,
      "grad_norm": 4.294485092163086,
      "learning_rate": 4.0041208791208794e-05,
      "loss": 0.626,
      "step": 1015000
    },
    {
      "epoch": 15.93563579277865,
      "grad_norm": 3.5434694290161133,
      "learning_rate": 4.0040227629513345e-05,
      "loss": 0.6438,
      "step": 1015100
    },
    {
      "epoch": 15.937205651491366,
      "grad_norm": 3.7361252307891846,
      "learning_rate": 4.0039246467817896e-05,
      "loss": 0.6287,
      "step": 1015200
    },
    {
      "epoch": 15.938775510204081,
      "grad_norm": 4.161046981811523,
      "learning_rate": 4.0038265306122454e-05,
      "loss": 0.5581,
      "step": 1015300
    },
    {
      "epoch": 15.940345368916798,
      "grad_norm": 3.393202781677246,
      "learning_rate": 4.0037284144427e-05,
      "loss": 0.6326,
      "step": 1015400
    },
    {
      "epoch": 15.941915227629513,
      "grad_norm": 3.800342321395874,
      "learning_rate": 4.0036302982731556e-05,
      "loss": 0.6283,
      "step": 1015500
    },
    {
      "epoch": 15.943485086342228,
      "grad_norm": 3.5861480236053467,
      "learning_rate": 4.003532182103611e-05,
      "loss": 0.5926,
      "step": 1015600
    },
    {
      "epoch": 15.945054945054945,
      "grad_norm": 3.974351406097412,
      "learning_rate": 4.0034340659340665e-05,
      "loss": 0.5671,
      "step": 1015700
    },
    {
      "epoch": 15.94662480376766,
      "grad_norm": 3.5147933959960938,
      "learning_rate": 4.003335949764521e-05,
      "loss": 0.5994,
      "step": 1015800
    },
    {
      "epoch": 15.948194662480377,
      "grad_norm": 3.206454277038574,
      "learning_rate": 4.003237833594977e-05,
      "loss": 0.6075,
      "step": 1015900
    },
    {
      "epoch": 15.949764521193092,
      "grad_norm": 4.5616455078125,
      "learning_rate": 4.003139717425432e-05,
      "loss": 0.6093,
      "step": 1016000
    },
    {
      "epoch": 15.95133437990581,
      "grad_norm": 3.3873682022094727,
      "learning_rate": 4.003041601255887e-05,
      "loss": 0.6146,
      "step": 1016100
    },
    {
      "epoch": 15.952904238618524,
      "grad_norm": 3.51841139793396,
      "learning_rate": 4.0029434850863426e-05,
      "loss": 0.5878,
      "step": 1016200
    },
    {
      "epoch": 15.95447409733124,
      "grad_norm": 3.787992477416992,
      "learning_rate": 4.002845368916798e-05,
      "loss": 0.5828,
      "step": 1016300
    },
    {
      "epoch": 15.956043956043956,
      "grad_norm": 4.096798419952393,
      "learning_rate": 4.002747252747253e-05,
      "loss": 0.576,
      "step": 1016400
    },
    {
      "epoch": 15.957613814756671,
      "grad_norm": 5.051456928253174,
      "learning_rate": 4.002649136577708e-05,
      "loss": 0.6178,
      "step": 1016500
    },
    {
      "epoch": 15.959183673469388,
      "grad_norm": 3.2868125438690186,
      "learning_rate": 4.002551020408164e-05,
      "loss": 0.5972,
      "step": 1016600
    },
    {
      "epoch": 15.960753532182103,
      "grad_norm": 3.951012134552002,
      "learning_rate": 4.002452904238619e-05,
      "loss": 0.647,
      "step": 1016700
    },
    {
      "epoch": 15.96232339089482,
      "grad_norm": 3.9255387783050537,
      "learning_rate": 4.002354788069074e-05,
      "loss": 0.6236,
      "step": 1016800
    },
    {
      "epoch": 15.963893249607535,
      "grad_norm": 3.8479607105255127,
      "learning_rate": 4.002256671899529e-05,
      "loss": 0.6127,
      "step": 1016900
    },
    {
      "epoch": 15.96546310832025,
      "grad_norm": 3.2888286113739014,
      "learning_rate": 4.002158555729985e-05,
      "loss": 0.6463,
      "step": 1017000
    },
    {
      "epoch": 15.967032967032967,
      "grad_norm": 4.017586708068848,
      "learning_rate": 4.00206043956044e-05,
      "loss": 0.5786,
      "step": 1017100
    },
    {
      "epoch": 15.968602825745682,
      "grad_norm": 4.474569320678711,
      "learning_rate": 4.001962323390895e-05,
      "loss": 0.6263,
      "step": 1017200
    },
    {
      "epoch": 15.970172684458399,
      "grad_norm": 3.9212870597839355,
      "learning_rate": 4.00186420722135e-05,
      "loss": 0.641,
      "step": 1017300
    },
    {
      "epoch": 15.971742543171114,
      "grad_norm": 3.948239803314209,
      "learning_rate": 4.001766091051806e-05,
      "loss": 0.6397,
      "step": 1017400
    },
    {
      "epoch": 15.973312401883831,
      "grad_norm": 3.023343801498413,
      "learning_rate": 4.00166797488226e-05,
      "loss": 0.6474,
      "step": 1017500
    },
    {
      "epoch": 15.974882260596546,
      "grad_norm": 3.7941677570343018,
      "learning_rate": 4.001569858712716e-05,
      "loss": 0.601,
      "step": 1017600
    },
    {
      "epoch": 15.976452119309261,
      "grad_norm": 3.6737029552459717,
      "learning_rate": 4.001471742543171e-05,
      "loss": 0.5991,
      "step": 1017700
    },
    {
      "epoch": 15.978021978021978,
      "grad_norm": 4.039212703704834,
      "learning_rate": 4.001373626373627e-05,
      "loss": 0.6424,
      "step": 1017800
    },
    {
      "epoch": 15.979591836734693,
      "grad_norm": 3.857400417327881,
      "learning_rate": 4.0012755102040813e-05,
      "loss": 0.6376,
      "step": 1017900
    },
    {
      "epoch": 15.98116169544741,
      "grad_norm": 2.975388765335083,
      "learning_rate": 4.001177394034537e-05,
      "loss": 0.5982,
      "step": 1018000
    },
    {
      "epoch": 15.982731554160125,
      "grad_norm": 3.202338933944702,
      "learning_rate": 4.001079277864992e-05,
      "loss": 0.5931,
      "step": 1018100
    },
    {
      "epoch": 15.984301412872842,
      "grad_norm": 4.235343933105469,
      "learning_rate": 4.000981161695447e-05,
      "loss": 0.6094,
      "step": 1018200
    },
    {
      "epoch": 15.985871271585557,
      "grad_norm": 3.1989128589630127,
      "learning_rate": 4.000883045525903e-05,
      "loss": 0.5835,
      "step": 1018300
    },
    {
      "epoch": 15.987441130298274,
      "grad_norm": 3.797959327697754,
      "learning_rate": 4.000784929356358e-05,
      "loss": 0.5987,
      "step": 1018400
    },
    {
      "epoch": 15.989010989010989,
      "grad_norm": 2.2155606746673584,
      "learning_rate": 4.000686813186813e-05,
      "loss": 0.6234,
      "step": 1018500
    },
    {
      "epoch": 15.990580847723704,
      "grad_norm": 3.0550425052642822,
      "learning_rate": 4.0005886970172684e-05,
      "loss": 0.663,
      "step": 1018600
    },
    {
      "epoch": 15.992150706436421,
      "grad_norm": 3.0463662147521973,
      "learning_rate": 4.000490580847724e-05,
      "loss": 0.6505,
      "step": 1018700
    },
    {
      "epoch": 15.993720565149136,
      "grad_norm": 4.336211204528809,
      "learning_rate": 4.000392464678179e-05,
      "loss": 0.6312,
      "step": 1018800
    },
    {
      "epoch": 15.995290423861853,
      "grad_norm": 3.3818957805633545,
      "learning_rate": 4.0002943485086344e-05,
      "loss": 0.5676,
      "step": 1018900
    },
    {
      "epoch": 15.996860282574568,
      "grad_norm": 3.666349172592163,
      "learning_rate": 4.0001962323390895e-05,
      "loss": 0.5664,
      "step": 1019000
    },
    {
      "epoch": 15.998430141287285,
      "grad_norm": 2.6956284046173096,
      "learning_rate": 4.000098116169545e-05,
      "loss": 0.616,
      "step": 1019100
    },
    {
      "epoch": 16.0,
      "grad_norm": 4.141114234924316,
      "learning_rate": 4e-05,
      "loss": 0.5917,
      "step": 1019200
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.0421854257583618,
      "eval_runtime": 14.7173,
      "eval_samples_per_second": 227.827,
      "eval_steps_per_second": 227.827,
      "step": 1019200
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.4788580536842346,
      "eval_runtime": 280.8857,
      "eval_samples_per_second": 226.783,
      "eval_steps_per_second": 226.783,
      "step": 1019200
    },
    {
      "epoch": 16.001569858712717,
      "grad_norm": 3.6214795112609863,
      "learning_rate": 3.9999018838304554e-05,
      "loss": 0.6217,
      "step": 1019300
    },
    {
      "epoch": 16.00313971742543,
      "grad_norm": 3.962265729904175,
      "learning_rate": 3.9998037676609105e-05,
      "loss": 0.6372,
      "step": 1019400
    },
    {
      "epoch": 16.004709576138147,
      "grad_norm": 4.226359844207764,
      "learning_rate": 3.999705651491366e-05,
      "loss": 0.5893,
      "step": 1019500
    },
    {
      "epoch": 16.006279434850864,
      "grad_norm": 3.716937780380249,
      "learning_rate": 3.999607535321821e-05,
      "loss": 0.5737,
      "step": 1019600
    },
    {
      "epoch": 16.00784929356358,
      "grad_norm": 4.4578046798706055,
      "learning_rate": 3.9995094191522765e-05,
      "loss": 0.6444,
      "step": 1019700
    },
    {
      "epoch": 16.009419152276294,
      "grad_norm": 4.0095906257629395,
      "learning_rate": 3.9994113029827316e-05,
      "loss": 0.5651,
      "step": 1019800
    },
    {
      "epoch": 16.01098901098901,
      "grad_norm": 3.745739221572876,
      "learning_rate": 3.9993131868131874e-05,
      "loss": 0.6228,
      "step": 1019900
    },
    {
      "epoch": 16.012558869701728,
      "grad_norm": 4.272826671600342,
      "learning_rate": 3.999215070643642e-05,
      "loss": 0.6373,
      "step": 1020000
    },
    {
      "epoch": 16.01412872841444,
      "grad_norm": 3.8102738857269287,
      "learning_rate": 3.9991169544740976e-05,
      "loss": 0.6084,
      "step": 1020100
    },
    {
      "epoch": 16.015698587127158,
      "grad_norm": 5.064266204833984,
      "learning_rate": 3.999018838304553e-05,
      "loss": 0.6054,
      "step": 1020200
    },
    {
      "epoch": 16.017268445839875,
      "grad_norm": 4.945486068725586,
      "learning_rate": 3.998920722135008e-05,
      "loss": 0.6462,
      "step": 1020300
    },
    {
      "epoch": 16.01883830455259,
      "grad_norm": 3.3369667530059814,
      "learning_rate": 3.9988226059654635e-05,
      "loss": 0.6204,
      "step": 1020400
    },
    {
      "epoch": 16.020408163265305,
      "grad_norm": 3.5999066829681396,
      "learning_rate": 3.9987244897959186e-05,
      "loss": 0.615,
      "step": 1020500
    },
    {
      "epoch": 16.021978021978022,
      "grad_norm": 3.76662015914917,
      "learning_rate": 3.998626373626374e-05,
      "loss": 0.598,
      "step": 1020600
    },
    {
      "epoch": 16.02354788069074,
      "grad_norm": 3.517601251602173,
      "learning_rate": 3.998528257456829e-05,
      "loss": 0.6076,
      "step": 1020700
    },
    {
      "epoch": 16.025117739403452,
      "grad_norm": 2.32441782951355,
      "learning_rate": 3.9984301412872846e-05,
      "loss": 0.5697,
      "step": 1020800
    },
    {
      "epoch": 16.02668759811617,
      "grad_norm": 2.9554147720336914,
      "learning_rate": 3.99833202511774e-05,
      "loss": 0.6639,
      "step": 1020900
    },
    {
      "epoch": 16.028257456828886,
      "grad_norm": 3.4371185302734375,
      "learning_rate": 3.998233908948195e-05,
      "loss": 0.5996,
      "step": 1021000
    },
    {
      "epoch": 16.029827315541603,
      "grad_norm": 4.022027492523193,
      "learning_rate": 3.99813579277865e-05,
      "loss": 0.5964,
      "step": 1021100
    },
    {
      "epoch": 16.031397174254316,
      "grad_norm": 3.8526108264923096,
      "learning_rate": 3.998037676609106e-05,
      "loss": 0.6066,
      "step": 1021200
    },
    {
      "epoch": 16.032967032967033,
      "grad_norm": 2.7215158939361572,
      "learning_rate": 3.997939560439561e-05,
      "loss": 0.6365,
      "step": 1021300
    },
    {
      "epoch": 16.03453689167975,
      "grad_norm": 3.7873239517211914,
      "learning_rate": 3.997841444270016e-05,
      "loss": 0.5958,
      "step": 1021400
    },
    {
      "epoch": 16.036106750392463,
      "grad_norm": 3.7137534618377686,
      "learning_rate": 3.997743328100471e-05,
      "loss": 0.5921,
      "step": 1021500
    },
    {
      "epoch": 16.03767660910518,
      "grad_norm": 4.643965244293213,
      "learning_rate": 3.997645211930927e-05,
      "loss": 0.5888,
      "step": 1021600
    },
    {
      "epoch": 16.039246467817897,
      "grad_norm": 2.938854217529297,
      "learning_rate": 3.997547095761381e-05,
      "loss": 0.611,
      "step": 1021700
    },
    {
      "epoch": 16.040816326530614,
      "grad_norm": 3.1752593517303467,
      "learning_rate": 3.997448979591837e-05,
      "loss": 0.5821,
      "step": 1021800
    },
    {
      "epoch": 16.042386185243327,
      "grad_norm": 4.863728046417236,
      "learning_rate": 3.997350863422292e-05,
      "loss": 0.6129,
      "step": 1021900
    },
    {
      "epoch": 16.043956043956044,
      "grad_norm": 3.8077518939971924,
      "learning_rate": 3.997252747252748e-05,
      "loss": 0.644,
      "step": 1022000
    },
    {
      "epoch": 16.04552590266876,
      "grad_norm": 4.418673038482666,
      "learning_rate": 3.997154631083202e-05,
      "loss": 0.6416,
      "step": 1022100
    },
    {
      "epoch": 16.047095761381474,
      "grad_norm": 3.5381691455841064,
      "learning_rate": 3.997056514913658e-05,
      "loss": 0.5952,
      "step": 1022200
    },
    {
      "epoch": 16.04866562009419,
      "grad_norm": 2.3902475833892822,
      "learning_rate": 3.996958398744113e-05,
      "loss": 0.6297,
      "step": 1022300
    },
    {
      "epoch": 16.050235478806908,
      "grad_norm": 3.4884281158447266,
      "learning_rate": 3.996860282574568e-05,
      "loss": 0.6036,
      "step": 1022400
    },
    {
      "epoch": 16.051805337519625,
      "grad_norm": 4.561399936676025,
      "learning_rate": 3.996762166405024e-05,
      "loss": 0.6134,
      "step": 1022500
    },
    {
      "epoch": 16.053375196232338,
      "grad_norm": 3.8419442176818848,
      "learning_rate": 3.996664050235479e-05,
      "loss": 0.5673,
      "step": 1022600
    },
    {
      "epoch": 16.054945054945055,
      "grad_norm": 3.4130148887634277,
      "learning_rate": 3.996565934065934e-05,
      "loss": 0.579,
      "step": 1022700
    },
    {
      "epoch": 16.05651491365777,
      "grad_norm": 4.653896331787109,
      "learning_rate": 3.996467817896389e-05,
      "loss": 0.6033,
      "step": 1022800
    },
    {
      "epoch": 16.058084772370485,
      "grad_norm": 4.28139591217041,
      "learning_rate": 3.996369701726845e-05,
      "loss": 0.5475,
      "step": 1022900
    },
    {
      "epoch": 16.059654631083202,
      "grad_norm": 3.941763401031494,
      "learning_rate": 3.9962715855573e-05,
      "loss": 0.6326,
      "step": 1023000
    },
    {
      "epoch": 16.06122448979592,
      "grad_norm": 3.4689748287200928,
      "learning_rate": 3.996173469387755e-05,
      "loss": 0.5717,
      "step": 1023100
    },
    {
      "epoch": 16.062794348508636,
      "grad_norm": 2.77607798576355,
      "learning_rate": 3.9960753532182104e-05,
      "loss": 0.5611,
      "step": 1023200
    },
    {
      "epoch": 16.06436420722135,
      "grad_norm": 3.7138540744781494,
      "learning_rate": 3.995977237048666e-05,
      "loss": 0.6261,
      "step": 1023300
    },
    {
      "epoch": 16.065934065934066,
      "grad_norm": 4.066751480102539,
      "learning_rate": 3.995879120879121e-05,
      "loss": 0.6199,
      "step": 1023400
    },
    {
      "epoch": 16.067503924646783,
      "grad_norm": 3.7760887145996094,
      "learning_rate": 3.995781004709576e-05,
      "loss": 0.6436,
      "step": 1023500
    },
    {
      "epoch": 16.069073783359496,
      "grad_norm": 3.789252996444702,
      "learning_rate": 3.9956828885400314e-05,
      "loss": 0.6282,
      "step": 1023600
    },
    {
      "epoch": 16.070643642072213,
      "grad_norm": 5.052570343017578,
      "learning_rate": 3.995584772370487e-05,
      "loss": 0.6026,
      "step": 1023700
    },
    {
      "epoch": 16.07221350078493,
      "grad_norm": 2.82959246635437,
      "learning_rate": 3.9954866562009416e-05,
      "loss": 0.6053,
      "step": 1023800
    },
    {
      "epoch": 16.073783359497646,
      "grad_norm": 4.25690221786499,
      "learning_rate": 3.9953885400313974e-05,
      "loss": 0.5956,
      "step": 1023900
    },
    {
      "epoch": 16.07535321821036,
      "grad_norm": 2.8624227046966553,
      "learning_rate": 3.9952904238618525e-05,
      "loss": 0.6071,
      "step": 1024000
    },
    {
      "epoch": 16.076923076923077,
      "grad_norm": 6.5133256912231445,
      "learning_rate": 3.995192307692308e-05,
      "loss": 0.6386,
      "step": 1024100
    },
    {
      "epoch": 16.078492935635794,
      "grad_norm": 4.540363311767578,
      "learning_rate": 3.995094191522763e-05,
      "loss": 0.6071,
      "step": 1024200
    },
    {
      "epoch": 16.08006279434851,
      "grad_norm": 3.4647774696350098,
      "learning_rate": 3.9949960753532185e-05,
      "loss": 0.6245,
      "step": 1024300
    },
    {
      "epoch": 16.081632653061224,
      "grad_norm": 4.4670090675354,
      "learning_rate": 3.9948979591836736e-05,
      "loss": 0.6446,
      "step": 1024400
    },
    {
      "epoch": 16.08320251177394,
      "grad_norm": 3.7211880683898926,
      "learning_rate": 3.9947998430141287e-05,
      "loss": 0.5677,
      "step": 1024500
    },
    {
      "epoch": 16.084772370486657,
      "grad_norm": 3.3988397121429443,
      "learning_rate": 3.9947017268445844e-05,
      "loss": 0.6729,
      "step": 1024600
    },
    {
      "epoch": 16.08634222919937,
      "grad_norm": 3.628037929534912,
      "learning_rate": 3.9946036106750395e-05,
      "loss": 0.6215,
      "step": 1024700
    },
    {
      "epoch": 16.087912087912088,
      "grad_norm": 3.6064677238464355,
      "learning_rate": 3.9945054945054946e-05,
      "loss": 0.6017,
      "step": 1024800
    },
    {
      "epoch": 16.089481946624804,
      "grad_norm": 2.8578405380249023,
      "learning_rate": 3.99440737833595e-05,
      "loss": 0.6147,
      "step": 1024900
    },
    {
      "epoch": 16.09105180533752,
      "grad_norm": 4.198859214782715,
      "learning_rate": 3.9943092621664055e-05,
      "loss": 0.622,
      "step": 1025000
    },
    {
      "epoch": 16.092621664050235,
      "grad_norm": 4.056251049041748,
      "learning_rate": 3.9942111459968606e-05,
      "loss": 0.6338,
      "step": 1025100
    },
    {
      "epoch": 16.09419152276295,
      "grad_norm": 3.1242549419403076,
      "learning_rate": 3.994113029827316e-05,
      "loss": 0.6174,
      "step": 1025200
    },
    {
      "epoch": 16.09576138147567,
      "grad_norm": 3.662485361099243,
      "learning_rate": 3.994014913657771e-05,
      "loss": 0.6691,
      "step": 1025300
    },
    {
      "epoch": 16.09733124018838,
      "grad_norm": 4.522532939910889,
      "learning_rate": 3.9939167974882266e-05,
      "loss": 0.6261,
      "step": 1025400
    },
    {
      "epoch": 16.0989010989011,
      "grad_norm": 3.471827983856201,
      "learning_rate": 3.993818681318682e-05,
      "loss": 0.6253,
      "step": 1025500
    },
    {
      "epoch": 16.100470957613815,
      "grad_norm": 3.923189640045166,
      "learning_rate": 3.993720565149137e-05,
      "loss": 0.6093,
      "step": 1025600
    },
    {
      "epoch": 16.102040816326532,
      "grad_norm": 2.782465696334839,
      "learning_rate": 3.993622448979592e-05,
      "loss": 0.5973,
      "step": 1025700
    },
    {
      "epoch": 16.103610675039246,
      "grad_norm": 3.4537432193756104,
      "learning_rate": 3.9935243328100476e-05,
      "loss": 0.6036,
      "step": 1025800
    },
    {
      "epoch": 16.105180533751962,
      "grad_norm": 4.18417501449585,
      "learning_rate": 3.993426216640502e-05,
      "loss": 0.6608,
      "step": 1025900
    },
    {
      "epoch": 16.10675039246468,
      "grad_norm": 3.2479851245880127,
      "learning_rate": 3.993328100470958e-05,
      "loss": 0.5991,
      "step": 1026000
    },
    {
      "epoch": 16.108320251177393,
      "grad_norm": 4.018260478973389,
      "learning_rate": 3.993229984301413e-05,
      "loss": 0.6222,
      "step": 1026100
    },
    {
      "epoch": 16.10989010989011,
      "grad_norm": 4.544004440307617,
      "learning_rate": 3.993131868131869e-05,
      "loss": 0.6073,
      "step": 1026200
    },
    {
      "epoch": 16.111459968602826,
      "grad_norm": 3.816331624984741,
      "learning_rate": 3.993033751962323e-05,
      "loss": 0.5876,
      "step": 1026300
    },
    {
      "epoch": 16.113029827315543,
      "grad_norm": 4.238347053527832,
      "learning_rate": 3.992935635792779e-05,
      "loss": 0.6265,
      "step": 1026400
    },
    {
      "epoch": 16.114599686028257,
      "grad_norm": 3.4362308979034424,
      "learning_rate": 3.992837519623234e-05,
      "loss": 0.588,
      "step": 1026500
    },
    {
      "epoch": 16.116169544740973,
      "grad_norm": 4.124907493591309,
      "learning_rate": 3.992739403453689e-05,
      "loss": 0.5988,
      "step": 1026600
    },
    {
      "epoch": 16.11773940345369,
      "grad_norm": 3.7726402282714844,
      "learning_rate": 3.992641287284145e-05,
      "loss": 0.6379,
      "step": 1026700
    },
    {
      "epoch": 16.119309262166404,
      "grad_norm": 4.505302906036377,
      "learning_rate": 3.9925431711146e-05,
      "loss": 0.6243,
      "step": 1026800
    },
    {
      "epoch": 16.12087912087912,
      "grad_norm": 3.58444881439209,
      "learning_rate": 3.992445054945055e-05,
      "loss": 0.6322,
      "step": 1026900
    },
    {
      "epoch": 16.122448979591837,
      "grad_norm": 2.8223154544830322,
      "learning_rate": 3.99234693877551e-05,
      "loss": 0.6391,
      "step": 1027000
    },
    {
      "epoch": 16.124018838304554,
      "grad_norm": 4.08555793762207,
      "learning_rate": 3.992248822605966e-05,
      "loss": 0.641,
      "step": 1027100
    },
    {
      "epoch": 16.125588697017267,
      "grad_norm": 4.414169788360596,
      "learning_rate": 3.992150706436421e-05,
      "loss": 0.5995,
      "step": 1027200
    },
    {
      "epoch": 16.127158555729984,
      "grad_norm": 3.6839046478271484,
      "learning_rate": 3.992052590266876e-05,
      "loss": 0.6251,
      "step": 1027300
    },
    {
      "epoch": 16.1287284144427,
      "grad_norm": 3.1684763431549072,
      "learning_rate": 3.991954474097331e-05,
      "loss": 0.6333,
      "step": 1027400
    },
    {
      "epoch": 16.130298273155415,
      "grad_norm": 3.5594849586486816,
      "learning_rate": 3.991856357927787e-05,
      "loss": 0.6132,
      "step": 1027500
    },
    {
      "epoch": 16.13186813186813,
      "grad_norm": 3.4030377864837646,
      "learning_rate": 3.991758241758242e-05,
      "loss": 0.5965,
      "step": 1027600
    },
    {
      "epoch": 16.13343799058085,
      "grad_norm": 4.245976448059082,
      "learning_rate": 3.991660125588697e-05,
      "loss": 0.6127,
      "step": 1027700
    },
    {
      "epoch": 16.135007849293565,
      "grad_norm": 3.2375094890594482,
      "learning_rate": 3.991562009419152e-05,
      "loss": 0.6242,
      "step": 1027800
    },
    {
      "epoch": 16.13657770800628,
      "grad_norm": 4.349336624145508,
      "learning_rate": 3.991463893249608e-05,
      "loss": 0.6217,
      "step": 1027900
    },
    {
      "epoch": 16.138147566718995,
      "grad_norm": 4.108527183532715,
      "learning_rate": 3.9913657770800625e-05,
      "loss": 0.6436,
      "step": 1028000
    },
    {
      "epoch": 16.139717425431712,
      "grad_norm": 4.234941005706787,
      "learning_rate": 3.991267660910518e-05,
      "loss": 0.6222,
      "step": 1028100
    },
    {
      "epoch": 16.141287284144425,
      "grad_norm": 2.4436190128326416,
      "learning_rate": 3.9911695447409734e-05,
      "loss": 0.6011,
      "step": 1028200
    },
    {
      "epoch": 16.142857142857142,
      "grad_norm": 4.699390411376953,
      "learning_rate": 3.991071428571429e-05,
      "loss": 0.5902,
      "step": 1028300
    },
    {
      "epoch": 16.14442700156986,
      "grad_norm": 4.003451824188232,
      "learning_rate": 3.9909733124018836e-05,
      "loss": 0.5972,
      "step": 1028400
    },
    {
      "epoch": 16.145996860282576,
      "grad_norm": 3.853952407836914,
      "learning_rate": 3.9908751962323394e-05,
      "loss": 0.5504,
      "step": 1028500
    },
    {
      "epoch": 16.14756671899529,
      "grad_norm": 4.333915710449219,
      "learning_rate": 3.9907770800627945e-05,
      "loss": 0.6314,
      "step": 1028600
    },
    {
      "epoch": 16.149136577708006,
      "grad_norm": 4.639565467834473,
      "learning_rate": 3.9906789638932496e-05,
      "loss": 0.6187,
      "step": 1028700
    },
    {
      "epoch": 16.150706436420723,
      "grad_norm": 4.0923895835876465,
      "learning_rate": 3.990580847723705e-05,
      "loss": 0.6659,
      "step": 1028800
    },
    {
      "epoch": 16.152276295133436,
      "grad_norm": 3.8264410495758057,
      "learning_rate": 3.9904827315541604e-05,
      "loss": 0.5897,
      "step": 1028900
    },
    {
      "epoch": 16.153846153846153,
      "grad_norm": 4.519890785217285,
      "learning_rate": 3.9903846153846155e-05,
      "loss": 0.6122,
      "step": 1029000
    },
    {
      "epoch": 16.15541601255887,
      "grad_norm": 3.7155447006225586,
      "learning_rate": 3.9902864992150706e-05,
      "loss": 0.6251,
      "step": 1029100
    },
    {
      "epoch": 16.156985871271587,
      "grad_norm": 3.834454298019409,
      "learning_rate": 3.9901883830455264e-05,
      "loss": 0.6548,
      "step": 1029200
    },
    {
      "epoch": 16.1585557299843,
      "grad_norm": 2.4730474948883057,
      "learning_rate": 3.9900902668759815e-05,
      "loss": 0.6258,
      "step": 1029300
    },
    {
      "epoch": 16.160125588697017,
      "grad_norm": 4.14800500869751,
      "learning_rate": 3.9899921507064366e-05,
      "loss": 0.5735,
      "step": 1029400
    },
    {
      "epoch": 16.161695447409734,
      "grad_norm": 2.628446340560913,
      "learning_rate": 3.989894034536892e-05,
      "loss": 0.6172,
      "step": 1029500
    },
    {
      "epoch": 16.163265306122447,
      "grad_norm": 4.265909671783447,
      "learning_rate": 3.9897959183673475e-05,
      "loss": 0.6166,
      "step": 1029600
    },
    {
      "epoch": 16.164835164835164,
      "grad_norm": 3.9129295349121094,
      "learning_rate": 3.9896978021978026e-05,
      "loss": 0.5999,
      "step": 1029700
    },
    {
      "epoch": 16.16640502354788,
      "grad_norm": 3.4559388160705566,
      "learning_rate": 3.989599686028258e-05,
      "loss": 0.6111,
      "step": 1029800
    },
    {
      "epoch": 16.167974882260598,
      "grad_norm": 4.380212783813477,
      "learning_rate": 3.989501569858713e-05,
      "loss": 0.5561,
      "step": 1029900
    },
    {
      "epoch": 16.16954474097331,
      "grad_norm": 3.654278516769409,
      "learning_rate": 3.9894034536891685e-05,
      "loss": 0.6056,
      "step": 1030000
    },
    {
      "epoch": 16.171114599686028,
      "grad_norm": 3.949650287628174,
      "learning_rate": 3.989305337519623e-05,
      "loss": 0.6032,
      "step": 1030100
    },
    {
      "epoch": 16.172684458398745,
      "grad_norm": 4.611827850341797,
      "learning_rate": 3.989207221350079e-05,
      "loss": 0.6428,
      "step": 1030200
    },
    {
      "epoch": 16.17425431711146,
      "grad_norm": 2.184204339981079,
      "learning_rate": 3.989109105180534e-05,
      "loss": 0.6148,
      "step": 1030300
    },
    {
      "epoch": 16.175824175824175,
      "grad_norm": 3.9706506729125977,
      "learning_rate": 3.9890109890109896e-05,
      "loss": 0.6112,
      "step": 1030400
    },
    {
      "epoch": 16.177394034536892,
      "grad_norm": 4.369668960571289,
      "learning_rate": 3.988912872841444e-05,
      "loss": 0.6116,
      "step": 1030500
    },
    {
      "epoch": 16.17896389324961,
      "grad_norm": 3.7992262840270996,
      "learning_rate": 3.9888147566719e-05,
      "loss": 0.5898,
      "step": 1030600
    },
    {
      "epoch": 16.180533751962322,
      "grad_norm": 3.3706202507019043,
      "learning_rate": 3.988716640502355e-05,
      "loss": 0.6317,
      "step": 1030700
    },
    {
      "epoch": 16.18210361067504,
      "grad_norm": 4.39630651473999,
      "learning_rate": 3.98861852433281e-05,
      "loss": 0.5629,
      "step": 1030800
    },
    {
      "epoch": 16.183673469387756,
      "grad_norm": 2.9747564792633057,
      "learning_rate": 3.988520408163265e-05,
      "loss": 0.6398,
      "step": 1030900
    },
    {
      "epoch": 16.18524332810047,
      "grad_norm": 4.140312194824219,
      "learning_rate": 3.988422291993721e-05,
      "loss": 0.6225,
      "step": 1031000
    },
    {
      "epoch": 16.186813186813186,
      "grad_norm": 4.71516752243042,
      "learning_rate": 3.988324175824176e-05,
      "loss": 0.6099,
      "step": 1031100
    },
    {
      "epoch": 16.188383045525903,
      "grad_norm": 3.7121732234954834,
      "learning_rate": 3.988226059654631e-05,
      "loss": 0.6451,
      "step": 1031200
    },
    {
      "epoch": 16.18995290423862,
      "grad_norm": 4.1039605140686035,
      "learning_rate": 3.988127943485087e-05,
      "loss": 0.5968,
      "step": 1031300
    },
    {
      "epoch": 16.191522762951333,
      "grad_norm": 3.801215410232544,
      "learning_rate": 3.988029827315542e-05,
      "loss": 0.6001,
      "step": 1031400
    },
    {
      "epoch": 16.19309262166405,
      "grad_norm": 4.210669040679932,
      "learning_rate": 3.987931711145997e-05,
      "loss": 0.6297,
      "step": 1031500
    },
    {
      "epoch": 16.194662480376767,
      "grad_norm": 3.7731528282165527,
      "learning_rate": 3.987833594976452e-05,
      "loss": 0.6077,
      "step": 1031600
    },
    {
      "epoch": 16.19623233908948,
      "grad_norm": 3.692399263381958,
      "learning_rate": 3.987735478806908e-05,
      "loss": 0.6077,
      "step": 1031700
    },
    {
      "epoch": 16.197802197802197,
      "grad_norm": 3.460190773010254,
      "learning_rate": 3.987637362637363e-05,
      "loss": 0.6097,
      "step": 1031800
    },
    {
      "epoch": 16.199372056514914,
      "grad_norm": 3.5865163803100586,
      "learning_rate": 3.987539246467818e-05,
      "loss": 0.6093,
      "step": 1031900
    },
    {
      "epoch": 16.20094191522763,
      "grad_norm": 4.914509296417236,
      "learning_rate": 3.987441130298273e-05,
      "loss": 0.6379,
      "step": 1032000
    },
    {
      "epoch": 16.202511773940344,
      "grad_norm": 3.1593406200408936,
      "learning_rate": 3.987343014128729e-05,
      "loss": 0.5769,
      "step": 1032100
    },
    {
      "epoch": 16.20408163265306,
      "grad_norm": 3.5441901683807373,
      "learning_rate": 3.9872448979591834e-05,
      "loss": 0.6123,
      "step": 1032200
    },
    {
      "epoch": 16.205651491365778,
      "grad_norm": 4.351707935333252,
      "learning_rate": 3.987146781789639e-05,
      "loss": 0.6155,
      "step": 1032300
    },
    {
      "epoch": 16.20722135007849,
      "grad_norm": 2.5860893726348877,
      "learning_rate": 3.987048665620094e-05,
      "loss": 0.5956,
      "step": 1032400
    },
    {
      "epoch": 16.208791208791208,
      "grad_norm": 5.1898627281188965,
      "learning_rate": 3.98695054945055e-05,
      "loss": 0.5871,
      "step": 1032500
    },
    {
      "epoch": 16.210361067503925,
      "grad_norm": 4.1583781242370605,
      "learning_rate": 3.9868524332810045e-05,
      "loss": 0.6102,
      "step": 1032600
    },
    {
      "epoch": 16.211930926216642,
      "grad_norm": 3.9165425300598145,
      "learning_rate": 3.98675431711146e-05,
      "loss": 0.5796,
      "step": 1032700
    },
    {
      "epoch": 16.213500784929355,
      "grad_norm": 3.567183017730713,
      "learning_rate": 3.9866562009419154e-05,
      "loss": 0.6206,
      "step": 1032800
    },
    {
      "epoch": 16.215070643642072,
      "grad_norm": 4.075011730194092,
      "learning_rate": 3.9865580847723705e-05,
      "loss": 0.6165,
      "step": 1032900
    },
    {
      "epoch": 16.21664050235479,
      "grad_norm": 2.4748311042785645,
      "learning_rate": 3.9864599686028256e-05,
      "loss": 0.6163,
      "step": 1033000
    },
    {
      "epoch": 16.218210361067506,
      "grad_norm": 3.294569730758667,
      "learning_rate": 3.986361852433281e-05,
      "loss": 0.6096,
      "step": 1033100
    },
    {
      "epoch": 16.21978021978022,
      "grad_norm": 3.7757346630096436,
      "learning_rate": 3.9862637362637364e-05,
      "loss": 0.6009,
      "step": 1033200
    },
    {
      "epoch": 16.221350078492936,
      "grad_norm": 3.434622049331665,
      "learning_rate": 3.9861656200941915e-05,
      "loss": 0.6158,
      "step": 1033300
    },
    {
      "epoch": 16.222919937205653,
      "grad_norm": 4.340639114379883,
      "learning_rate": 3.986067503924647e-05,
      "loss": 0.6347,
      "step": 1033400
    },
    {
      "epoch": 16.224489795918366,
      "grad_norm": 3.83516001701355,
      "learning_rate": 3.9859693877551024e-05,
      "loss": 0.5828,
      "step": 1033500
    },
    {
      "epoch": 16.226059654631083,
      "grad_norm": 4.3487548828125,
      "learning_rate": 3.9858712715855575e-05,
      "loss": 0.6691,
      "step": 1033600
    },
    {
      "epoch": 16.2276295133438,
      "grad_norm": 3.084792137145996,
      "learning_rate": 3.9857731554160126e-05,
      "loss": 0.6427,
      "step": 1033700
    },
    {
      "epoch": 16.229199372056517,
      "grad_norm": 4.894927978515625,
      "learning_rate": 3.9856750392464684e-05,
      "loss": 0.5903,
      "step": 1033800
    },
    {
      "epoch": 16.23076923076923,
      "grad_norm": 4.2414727210998535,
      "learning_rate": 3.9855769230769235e-05,
      "loss": 0.628,
      "step": 1033900
    },
    {
      "epoch": 16.232339089481947,
      "grad_norm": 4.597458362579346,
      "learning_rate": 3.9854788069073786e-05,
      "loss": 0.5817,
      "step": 1034000
    },
    {
      "epoch": 16.233908948194664,
      "grad_norm": 3.3782734870910645,
      "learning_rate": 3.985380690737834e-05,
      "loss": 0.5983,
      "step": 1034100
    },
    {
      "epoch": 16.235478806907377,
      "grad_norm": 4.087244987487793,
      "learning_rate": 3.9852825745682894e-05,
      "loss": 0.6134,
      "step": 1034200
    },
    {
      "epoch": 16.237048665620094,
      "grad_norm": 2.6826605796813965,
      "learning_rate": 3.985184458398744e-05,
      "loss": 0.5979,
      "step": 1034300
    },
    {
      "epoch": 16.23861852433281,
      "grad_norm": 3.195009708404541,
      "learning_rate": 3.9850863422291996e-05,
      "loss": 0.6136,
      "step": 1034400
    },
    {
      "epoch": 16.240188383045528,
      "grad_norm": 4.037432670593262,
      "learning_rate": 3.984988226059655e-05,
      "loss": 0.6101,
      "step": 1034500
    },
    {
      "epoch": 16.24175824175824,
      "grad_norm": 3.6858248710632324,
      "learning_rate": 3.9848901098901105e-05,
      "loss": 0.5963,
      "step": 1034600
    },
    {
      "epoch": 16.243328100470958,
      "grad_norm": 3.9682018756866455,
      "learning_rate": 3.984791993720565e-05,
      "loss": 0.6078,
      "step": 1034700
    },
    {
      "epoch": 16.244897959183675,
      "grad_norm": 3.4756946563720703,
      "learning_rate": 3.984693877551021e-05,
      "loss": 0.6255,
      "step": 1034800
    },
    {
      "epoch": 16.246467817896388,
      "grad_norm": 3.16508412361145,
      "learning_rate": 3.984595761381476e-05,
      "loss": 0.6247,
      "step": 1034900
    },
    {
      "epoch": 16.248037676609105,
      "grad_norm": 3.5985054969787598,
      "learning_rate": 3.984497645211931e-05,
      "loss": 0.5975,
      "step": 1035000
    },
    {
      "epoch": 16.24960753532182,
      "grad_norm": 3.7754480838775635,
      "learning_rate": 3.984399529042386e-05,
      "loss": 0.6101,
      "step": 1035100
    },
    {
      "epoch": 16.25117739403454,
      "grad_norm": 4.061981201171875,
      "learning_rate": 3.984301412872842e-05,
      "loss": 0.6262,
      "step": 1035200
    },
    {
      "epoch": 16.252747252747252,
      "grad_norm": 4.004501819610596,
      "learning_rate": 3.984203296703297e-05,
      "loss": 0.6106,
      "step": 1035300
    },
    {
      "epoch": 16.25431711145997,
      "grad_norm": 4.419517993927002,
      "learning_rate": 3.984105180533752e-05,
      "loss": 0.6315,
      "step": 1035400
    },
    {
      "epoch": 16.255886970172686,
      "grad_norm": 3.380153179168701,
      "learning_rate": 3.984007064364208e-05,
      "loss": 0.6181,
      "step": 1035500
    },
    {
      "epoch": 16.2574568288854,
      "grad_norm": 3.1451425552368164,
      "learning_rate": 3.983908948194663e-05,
      "loss": 0.5962,
      "step": 1035600
    },
    {
      "epoch": 16.259026687598116,
      "grad_norm": 4.402097225189209,
      "learning_rate": 3.983810832025118e-05,
      "loss": 0.6253,
      "step": 1035700
    },
    {
      "epoch": 16.260596546310833,
      "grad_norm": 4.44016170501709,
      "learning_rate": 3.983712715855573e-05,
      "loss": 0.567,
      "step": 1035800
    },
    {
      "epoch": 16.26216640502355,
      "grad_norm": 3.7043609619140625,
      "learning_rate": 3.983614599686029e-05,
      "loss": 0.6219,
      "step": 1035900
    },
    {
      "epoch": 16.263736263736263,
      "grad_norm": 3.779385805130005,
      "learning_rate": 3.983516483516483e-05,
      "loss": 0.6227,
      "step": 1036000
    },
    {
      "epoch": 16.26530612244898,
      "grad_norm": 3.8582956790924072,
      "learning_rate": 3.983418367346939e-05,
      "loss": 0.5973,
      "step": 1036100
    },
    {
      "epoch": 16.266875981161697,
      "grad_norm": 4.511317729949951,
      "learning_rate": 3.983320251177394e-05,
      "loss": 0.6022,
      "step": 1036200
    },
    {
      "epoch": 16.26844583987441,
      "grad_norm": 3.8029749393463135,
      "learning_rate": 3.98322213500785e-05,
      "loss": 0.666,
      "step": 1036300
    },
    {
      "epoch": 16.270015698587127,
      "grad_norm": 3.842397689819336,
      "learning_rate": 3.983124018838304e-05,
      "loss": 0.635,
      "step": 1036400
    },
    {
      "epoch": 16.271585557299844,
      "grad_norm": 3.604290723800659,
      "learning_rate": 3.98302590266876e-05,
      "loss": 0.5914,
      "step": 1036500
    },
    {
      "epoch": 16.27315541601256,
      "grad_norm": 2.5367624759674072,
      "learning_rate": 3.982927786499215e-05,
      "loss": 0.6299,
      "step": 1036600
    },
    {
      "epoch": 16.274725274725274,
      "grad_norm": 2.75264310836792,
      "learning_rate": 3.98282967032967e-05,
      "loss": 0.6194,
      "step": 1036700
    },
    {
      "epoch": 16.27629513343799,
      "grad_norm": 3.49517560005188,
      "learning_rate": 3.9827315541601254e-05,
      "loss": 0.609,
      "step": 1036800
    },
    {
      "epoch": 16.277864992150707,
      "grad_norm": 3.646294593811035,
      "learning_rate": 3.982633437990581e-05,
      "loss": 0.6359,
      "step": 1036900
    },
    {
      "epoch": 16.27943485086342,
      "grad_norm": 3.199272632598877,
      "learning_rate": 3.982535321821036e-05,
      "loss": 0.6183,
      "step": 1037000
    },
    {
      "epoch": 16.281004709576138,
      "grad_norm": 2.4592108726501465,
      "learning_rate": 3.9824372056514914e-05,
      "loss": 0.5936,
      "step": 1037100
    },
    {
      "epoch": 16.282574568288855,
      "grad_norm": 4.275839805603027,
      "learning_rate": 3.9823390894819465e-05,
      "loss": 0.6112,
      "step": 1037200
    },
    {
      "epoch": 16.28414442700157,
      "grad_norm": 2.6543161869049072,
      "learning_rate": 3.982240973312402e-05,
      "loss": 0.6053,
      "step": 1037300
    },
    {
      "epoch": 16.285714285714285,
      "grad_norm": 3.357271432876587,
      "learning_rate": 3.982142857142857e-05,
      "loss": 0.5971,
      "step": 1037400
    },
    {
      "epoch": 16.287284144427,
      "grad_norm": 3.884500741958618,
      "learning_rate": 3.9820447409733124e-05,
      "loss": 0.592,
      "step": 1037500
    },
    {
      "epoch": 16.28885400313972,
      "grad_norm": 3.2000207901000977,
      "learning_rate": 3.981946624803768e-05,
      "loss": 0.6143,
      "step": 1037600
    },
    {
      "epoch": 16.29042386185243,
      "grad_norm": 4.200596332550049,
      "learning_rate": 3.981848508634223e-05,
      "loss": 0.6118,
      "step": 1037700
    },
    {
      "epoch": 16.29199372056515,
      "grad_norm": 3.745955467224121,
      "learning_rate": 3.9817503924646784e-05,
      "loss": 0.6379,
      "step": 1037800
    },
    {
      "epoch": 16.293563579277865,
      "grad_norm": 3.541213035583496,
      "learning_rate": 3.9816522762951335e-05,
      "loss": 0.6392,
      "step": 1037900
    },
    {
      "epoch": 16.295133437990582,
      "grad_norm": 3.408998489379883,
      "learning_rate": 3.981554160125589e-05,
      "loss": 0.6322,
      "step": 1038000
    },
    {
      "epoch": 16.296703296703296,
      "grad_norm": 3.3183014392852783,
      "learning_rate": 3.981456043956044e-05,
      "loss": 0.5756,
      "step": 1038100
    },
    {
      "epoch": 16.298273155416013,
      "grad_norm": 2.914180278778076,
      "learning_rate": 3.9813579277864995e-05,
      "loss": 0.5799,
      "step": 1038200
    },
    {
      "epoch": 16.29984301412873,
      "grad_norm": 3.6619718074798584,
      "learning_rate": 3.9812598116169546e-05,
      "loss": 0.611,
      "step": 1038300
    },
    {
      "epoch": 16.301412872841443,
      "grad_norm": 3.6717023849487305,
      "learning_rate": 3.9811616954474103e-05,
      "loss": 0.6036,
      "step": 1038400
    },
    {
      "epoch": 16.30298273155416,
      "grad_norm": 3.971522808074951,
      "learning_rate": 3.981063579277865e-05,
      "loss": 0.6372,
      "step": 1038500
    },
    {
      "epoch": 16.304552590266876,
      "grad_norm": 2.6752970218658447,
      "learning_rate": 3.9809654631083205e-05,
      "loss": 0.6446,
      "step": 1038600
    },
    {
      "epoch": 16.306122448979593,
      "grad_norm": 3.8935627937316895,
      "learning_rate": 3.9808673469387756e-05,
      "loss": 0.5924,
      "step": 1038700
    },
    {
      "epoch": 16.307692307692307,
      "grad_norm": 3.188345193862915,
      "learning_rate": 3.980769230769231e-05,
      "loss": 0.6194,
      "step": 1038800
    },
    {
      "epoch": 16.309262166405023,
      "grad_norm": 3.553136110305786,
      "learning_rate": 3.980671114599686e-05,
      "loss": 0.6208,
      "step": 1038900
    },
    {
      "epoch": 16.31083202511774,
      "grad_norm": 4.014810085296631,
      "learning_rate": 3.9805729984301416e-05,
      "loss": 0.5939,
      "step": 1039000
    },
    {
      "epoch": 16.312401883830454,
      "grad_norm": 3.5916221141815186,
      "learning_rate": 3.980474882260597e-05,
      "loss": 0.6182,
      "step": 1039100
    },
    {
      "epoch": 16.31397174254317,
      "grad_norm": 4.000926494598389,
      "learning_rate": 3.980376766091052e-05,
      "loss": 0.6402,
      "step": 1039200
    },
    {
      "epoch": 16.315541601255887,
      "grad_norm": 3.20493221282959,
      "learning_rate": 3.980278649921507e-05,
      "loss": 0.5847,
      "step": 1039300
    },
    {
      "epoch": 16.317111459968604,
      "grad_norm": 2.768937587738037,
      "learning_rate": 3.980180533751963e-05,
      "loss": 0.5937,
      "step": 1039400
    },
    {
      "epoch": 16.318681318681318,
      "grad_norm": 4.039799213409424,
      "learning_rate": 3.980082417582418e-05,
      "loss": 0.63,
      "step": 1039500
    },
    {
      "epoch": 16.320251177394034,
      "grad_norm": 3.9687745571136475,
      "learning_rate": 3.979984301412873e-05,
      "loss": 0.6125,
      "step": 1039600
    },
    {
      "epoch": 16.32182103610675,
      "grad_norm": 4.219301700592041,
      "learning_rate": 3.9798861852433286e-05,
      "loss": 0.5891,
      "step": 1039700
    },
    {
      "epoch": 16.323390894819465,
      "grad_norm": 3.4633610248565674,
      "learning_rate": 3.979788069073784e-05,
      "loss": 0.6411,
      "step": 1039800
    },
    {
      "epoch": 16.32496075353218,
      "grad_norm": 4.588022232055664,
      "learning_rate": 3.979689952904239e-05,
      "loss": 0.5901,
      "step": 1039900
    },
    {
      "epoch": 16.3265306122449,
      "grad_norm": 4.192451000213623,
      "learning_rate": 3.979591836734694e-05,
      "loss": 0.6007,
      "step": 1040000
    },
    {
      "epoch": 16.328100470957615,
      "grad_norm": 4.249614238739014,
      "learning_rate": 3.97949372056515e-05,
      "loss": 0.6184,
      "step": 1040100
    },
    {
      "epoch": 16.32967032967033,
      "grad_norm": 3.0494959354400635,
      "learning_rate": 3.979395604395604e-05,
      "loss": 0.6021,
      "step": 1040200
    },
    {
      "epoch": 16.331240188383045,
      "grad_norm": 4.193072319030762,
      "learning_rate": 3.97929748822606e-05,
      "loss": 0.6571,
      "step": 1040300
    },
    {
      "epoch": 16.332810047095762,
      "grad_norm": 3.16255784034729,
      "learning_rate": 3.979199372056515e-05,
      "loss": 0.6083,
      "step": 1040400
    },
    {
      "epoch": 16.334379905808476,
      "grad_norm": 3.97729229927063,
      "learning_rate": 3.979101255886971e-05,
      "loss": 0.643,
      "step": 1040500
    },
    {
      "epoch": 16.335949764521192,
      "grad_norm": 3.9259819984436035,
      "learning_rate": 3.979003139717425e-05,
      "loss": 0.5888,
      "step": 1040600
    },
    {
      "epoch": 16.33751962323391,
      "grad_norm": 3.718287706375122,
      "learning_rate": 3.978905023547881e-05,
      "loss": 0.5706,
      "step": 1040700
    },
    {
      "epoch": 16.339089481946626,
      "grad_norm": 4.63533878326416,
      "learning_rate": 3.978806907378336e-05,
      "loss": 0.6264,
      "step": 1040800
    },
    {
      "epoch": 16.34065934065934,
      "grad_norm": 3.817166566848755,
      "learning_rate": 3.978708791208791e-05,
      "loss": 0.6334,
      "step": 1040900
    },
    {
      "epoch": 16.342229199372056,
      "grad_norm": 3.4991018772125244,
      "learning_rate": 3.978610675039246e-05,
      "loss": 0.6059,
      "step": 1041000
    },
    {
      "epoch": 16.343799058084773,
      "grad_norm": 4.1651692390441895,
      "learning_rate": 3.978512558869702e-05,
      "loss": 0.5867,
      "step": 1041100
    },
    {
      "epoch": 16.345368916797486,
      "grad_norm": 3.4244165420532227,
      "learning_rate": 3.978414442700157e-05,
      "loss": 0.647,
      "step": 1041200
    },
    {
      "epoch": 16.346938775510203,
      "grad_norm": 2.7873425483703613,
      "learning_rate": 3.978316326530612e-05,
      "loss": 0.6259,
      "step": 1041300
    },
    {
      "epoch": 16.34850863422292,
      "grad_norm": 3.44502592086792,
      "learning_rate": 3.9782182103610674e-05,
      "loss": 0.6146,
      "step": 1041400
    },
    {
      "epoch": 16.350078492935637,
      "grad_norm": 3.735203981399536,
      "learning_rate": 3.978120094191523e-05,
      "loss": 0.6124,
      "step": 1041500
    },
    {
      "epoch": 16.35164835164835,
      "grad_norm": 4.887070178985596,
      "learning_rate": 3.978021978021978e-05,
      "loss": 0.6322,
      "step": 1041600
    },
    {
      "epoch": 16.353218210361067,
      "grad_norm": 3.6936607360839844,
      "learning_rate": 3.977923861852433e-05,
      "loss": 0.5897,
      "step": 1041700
    },
    {
      "epoch": 16.354788069073784,
      "grad_norm": 3.1230852603912354,
      "learning_rate": 3.977825745682889e-05,
      "loss": 0.6188,
      "step": 1041800
    },
    {
      "epoch": 16.356357927786497,
      "grad_norm": 2.3640952110290527,
      "learning_rate": 3.977727629513344e-05,
      "loss": 0.5877,
      "step": 1041900
    },
    {
      "epoch": 16.357927786499214,
      "grad_norm": 3.7628376483917236,
      "learning_rate": 3.977629513343799e-05,
      "loss": 0.6648,
      "step": 1042000
    },
    {
      "epoch": 16.35949764521193,
      "grad_norm": 4.411596775054932,
      "learning_rate": 3.9775313971742544e-05,
      "loss": 0.6538,
      "step": 1042100
    },
    {
      "epoch": 16.361067503924648,
      "grad_norm": 4.241724014282227,
      "learning_rate": 3.97743328100471e-05,
      "loss": 0.5792,
      "step": 1042200
    },
    {
      "epoch": 16.36263736263736,
      "grad_norm": 4.948775768280029,
      "learning_rate": 3.9773351648351646e-05,
      "loss": 0.6462,
      "step": 1042300
    },
    {
      "epoch": 16.364207221350078,
      "grad_norm": 4.025317192077637,
      "learning_rate": 3.9772370486656204e-05,
      "loss": 0.5695,
      "step": 1042400
    },
    {
      "epoch": 16.365777080062795,
      "grad_norm": 4.383937358856201,
      "learning_rate": 3.9771389324960755e-05,
      "loss": 0.6739,
      "step": 1042500
    },
    {
      "epoch": 16.367346938775512,
      "grad_norm": 3.21659255027771,
      "learning_rate": 3.977040816326531e-05,
      "loss": 0.6181,
      "step": 1042600
    },
    {
      "epoch": 16.368916797488225,
      "grad_norm": 4.657289981842041,
      "learning_rate": 3.9769427001569857e-05,
      "loss": 0.5966,
      "step": 1042700
    },
    {
      "epoch": 16.370486656200942,
      "grad_norm": 4.799230098724365,
      "learning_rate": 3.9768445839874414e-05,
      "loss": 0.6202,
      "step": 1042800
    },
    {
      "epoch": 16.37205651491366,
      "grad_norm": 3.8533525466918945,
      "learning_rate": 3.9767464678178965e-05,
      "loss": 0.5967,
      "step": 1042900
    },
    {
      "epoch": 16.373626373626372,
      "grad_norm": 3.865534782409668,
      "learning_rate": 3.9766483516483516e-05,
      "loss": 0.6378,
      "step": 1043000
    },
    {
      "epoch": 16.37519623233909,
      "grad_norm": 4.49001407623291,
      "learning_rate": 3.976550235478807e-05,
      "loss": 0.6133,
      "step": 1043100
    },
    {
      "epoch": 16.376766091051806,
      "grad_norm": 4.082493305206299,
      "learning_rate": 3.9764521193092625e-05,
      "loss": 0.6113,
      "step": 1043200
    },
    {
      "epoch": 16.378335949764523,
      "grad_norm": 3.1826303005218506,
      "learning_rate": 3.9763540031397176e-05,
      "loss": 0.6351,
      "step": 1043300
    },
    {
      "epoch": 16.379905808477236,
      "grad_norm": 3.38498854637146,
      "learning_rate": 3.976255886970173e-05,
      "loss": 0.6029,
      "step": 1043400
    },
    {
      "epoch": 16.381475667189953,
      "grad_norm": 3.4269587993621826,
      "learning_rate": 3.976157770800628e-05,
      "loss": 0.5895,
      "step": 1043500
    },
    {
      "epoch": 16.38304552590267,
      "grad_norm": 3.708494186401367,
      "learning_rate": 3.9760596546310836e-05,
      "loss": 0.6245,
      "step": 1043600
    },
    {
      "epoch": 16.384615384615383,
      "grad_norm": 4.102942943572998,
      "learning_rate": 3.975961538461539e-05,
      "loss": 0.6329,
      "step": 1043700
    },
    {
      "epoch": 16.3861852433281,
      "grad_norm": 2.803960084915161,
      "learning_rate": 3.975863422291994e-05,
      "loss": 0.6269,
      "step": 1043800
    },
    {
      "epoch": 16.387755102040817,
      "grad_norm": 2.656020402908325,
      "learning_rate": 3.9757653061224495e-05,
      "loss": 0.5902,
      "step": 1043900
    },
    {
      "epoch": 16.389324960753534,
      "grad_norm": 2.793168783187866,
      "learning_rate": 3.9756671899529046e-05,
      "loss": 0.6005,
      "step": 1044000
    },
    {
      "epoch": 16.390894819466247,
      "grad_norm": 2.2103583812713623,
      "learning_rate": 3.97556907378336e-05,
      "loss": 0.6109,
      "step": 1044100
    },
    {
      "epoch": 16.392464678178964,
      "grad_norm": 3.1662757396698,
      "learning_rate": 3.975470957613815e-05,
      "loss": 0.6326,
      "step": 1044200
    },
    {
      "epoch": 16.39403453689168,
      "grad_norm": 3.6452627182006836,
      "learning_rate": 3.9753728414442706e-05,
      "loss": 0.6593,
      "step": 1044300
    },
    {
      "epoch": 16.395604395604394,
      "grad_norm": 3.2884581089019775,
      "learning_rate": 3.975274725274725e-05,
      "loss": 0.5961,
      "step": 1044400
    },
    {
      "epoch": 16.39717425431711,
      "grad_norm": 3.816696882247925,
      "learning_rate": 3.975176609105181e-05,
      "loss": 0.6299,
      "step": 1044500
    },
    {
      "epoch": 16.398744113029828,
      "grad_norm": 3.793264865875244,
      "learning_rate": 3.975078492935636e-05,
      "loss": 0.6205,
      "step": 1044600
    },
    {
      "epoch": 16.400313971742545,
      "grad_norm": 4.005448818206787,
      "learning_rate": 3.974980376766092e-05,
      "loss": 0.6412,
      "step": 1044700
    },
    {
      "epoch": 16.401883830455258,
      "grad_norm": 3.2647368907928467,
      "learning_rate": 3.974882260596546e-05,
      "loss": 0.601,
      "step": 1044800
    },
    {
      "epoch": 16.403453689167975,
      "grad_norm": 3.285529136657715,
      "learning_rate": 3.974784144427002e-05,
      "loss": 0.6078,
      "step": 1044900
    },
    {
      "epoch": 16.405023547880692,
      "grad_norm": 3.0415854454040527,
      "learning_rate": 3.974686028257457e-05,
      "loss": 0.5861,
      "step": 1045000
    },
    {
      "epoch": 16.406593406593405,
      "grad_norm": 4.363478183746338,
      "learning_rate": 3.974587912087912e-05,
      "loss": 0.6063,
      "step": 1045100
    },
    {
      "epoch": 16.408163265306122,
      "grad_norm": 2.68123197555542,
      "learning_rate": 3.974489795918367e-05,
      "loss": 0.6452,
      "step": 1045200
    },
    {
      "epoch": 16.40973312401884,
      "grad_norm": 3.559396743774414,
      "learning_rate": 3.974391679748823e-05,
      "loss": 0.6372,
      "step": 1045300
    },
    {
      "epoch": 16.411302982731556,
      "grad_norm": 4.701694011688232,
      "learning_rate": 3.974293563579278e-05,
      "loss": 0.5997,
      "step": 1045400
    },
    {
      "epoch": 16.41287284144427,
      "grad_norm": 4.42908239364624,
      "learning_rate": 3.974195447409733e-05,
      "loss": 0.5976,
      "step": 1045500
    },
    {
      "epoch": 16.414442700156986,
      "grad_norm": 3.877066135406494,
      "learning_rate": 3.974097331240188e-05,
      "loss": 0.5884,
      "step": 1045600
    },
    {
      "epoch": 16.416012558869703,
      "grad_norm": 4.171919345855713,
      "learning_rate": 3.973999215070644e-05,
      "loss": 0.5881,
      "step": 1045700
    },
    {
      "epoch": 16.417582417582416,
      "grad_norm": 5.057480335235596,
      "learning_rate": 3.973901098901099e-05,
      "loss": 0.625,
      "step": 1045800
    },
    {
      "epoch": 16.419152276295133,
      "grad_norm": 3.334340810775757,
      "learning_rate": 3.973802982731554e-05,
      "loss": 0.5974,
      "step": 1045900
    },
    {
      "epoch": 16.42072213500785,
      "grad_norm": 4.013969898223877,
      "learning_rate": 3.97370486656201e-05,
      "loss": 0.6106,
      "step": 1046000
    },
    {
      "epoch": 16.422291993720567,
      "grad_norm": 2.8926355838775635,
      "learning_rate": 3.973606750392465e-05,
      "loss": 0.6125,
      "step": 1046100
    },
    {
      "epoch": 16.42386185243328,
      "grad_norm": 3.928772211074829,
      "learning_rate": 3.97350863422292e-05,
      "loss": 0.584,
      "step": 1046200
    },
    {
      "epoch": 16.425431711145997,
      "grad_norm": 3.4397380352020264,
      "learning_rate": 3.973410518053375e-05,
      "loss": 0.5746,
      "step": 1046300
    },
    {
      "epoch": 16.427001569858714,
      "grad_norm": 3.597264289855957,
      "learning_rate": 3.973312401883831e-05,
      "loss": 0.6246,
      "step": 1046400
    },
    {
      "epoch": 16.428571428571427,
      "grad_norm": 3.6149351596832275,
      "learning_rate": 3.9732142857142855e-05,
      "loss": 0.6262,
      "step": 1046500
    },
    {
      "epoch": 16.430141287284144,
      "grad_norm": 3.7206337451934814,
      "learning_rate": 3.973116169544741e-05,
      "loss": 0.6513,
      "step": 1046600
    },
    {
      "epoch": 16.43171114599686,
      "grad_norm": 3.606504201889038,
      "learning_rate": 3.9730180533751964e-05,
      "loss": 0.5882,
      "step": 1046700
    },
    {
      "epoch": 16.433281004709578,
      "grad_norm": 4.224310874938965,
      "learning_rate": 3.972919937205652e-05,
      "loss": 0.6272,
      "step": 1046800
    },
    {
      "epoch": 16.43485086342229,
      "grad_norm": 2.5983469486236572,
      "learning_rate": 3.9728218210361066e-05,
      "loss": 0.6209,
      "step": 1046900
    },
    {
      "epoch": 16.436420722135008,
      "grad_norm": 4.426998615264893,
      "learning_rate": 3.972723704866562e-05,
      "loss": 0.6503,
      "step": 1047000
    },
    {
      "epoch": 16.437990580847725,
      "grad_norm": 5.176449775695801,
      "learning_rate": 3.9726255886970174e-05,
      "loss": 0.6099,
      "step": 1047100
    },
    {
      "epoch": 16.439560439560438,
      "grad_norm": 3.8076257705688477,
      "learning_rate": 3.9725274725274725e-05,
      "loss": 0.586,
      "step": 1047200
    },
    {
      "epoch": 16.441130298273155,
      "grad_norm": 4.463309288024902,
      "learning_rate": 3.9724293563579276e-05,
      "loss": 0.6109,
      "step": 1047300
    },
    {
      "epoch": 16.44270015698587,
      "grad_norm": 4.885228157043457,
      "learning_rate": 3.9723312401883834e-05,
      "loss": 0.6338,
      "step": 1047400
    },
    {
      "epoch": 16.44427001569859,
      "grad_norm": 3.100739002227783,
      "learning_rate": 3.9722331240188385e-05,
      "loss": 0.6005,
      "step": 1047500
    },
    {
      "epoch": 16.445839874411302,
      "grad_norm": 4.157425880432129,
      "learning_rate": 3.9721350078492936e-05,
      "loss": 0.6445,
      "step": 1047600
    },
    {
      "epoch": 16.44740973312402,
      "grad_norm": 3.7591753005981445,
      "learning_rate": 3.972036891679749e-05,
      "loss": 0.6281,
      "step": 1047700
    },
    {
      "epoch": 16.448979591836736,
      "grad_norm": 4.674069881439209,
      "learning_rate": 3.9719387755102045e-05,
      "loss": 0.6356,
      "step": 1047800
    },
    {
      "epoch": 16.45054945054945,
      "grad_norm": 4.9081034660339355,
      "learning_rate": 3.9718406593406596e-05,
      "loss": 0.6319,
      "step": 1047900
    },
    {
      "epoch": 16.452119309262166,
      "grad_norm": 4.182753562927246,
      "learning_rate": 3.971742543171115e-05,
      "loss": 0.6142,
      "step": 1048000
    },
    {
      "epoch": 16.453689167974883,
      "grad_norm": 4.028323173522949,
      "learning_rate": 3.9716444270015704e-05,
      "loss": 0.6264,
      "step": 1048100
    },
    {
      "epoch": 16.4552590266876,
      "grad_norm": 3.7846243381500244,
      "learning_rate": 3.9715463108320255e-05,
      "loss": 0.6119,
      "step": 1048200
    },
    {
      "epoch": 16.456828885400313,
      "grad_norm": 4.650815486907959,
      "learning_rate": 3.9714481946624806e-05,
      "loss": 0.5841,
      "step": 1048300
    },
    {
      "epoch": 16.45839874411303,
      "grad_norm": 4.410383224487305,
      "learning_rate": 3.971350078492936e-05,
      "loss": 0.5973,
      "step": 1048400
    },
    {
      "epoch": 16.459968602825747,
      "grad_norm": 4.957955837249756,
      "learning_rate": 3.9712519623233915e-05,
      "loss": 0.6491,
      "step": 1048500
    },
    {
      "epoch": 16.46153846153846,
      "grad_norm": 4.736730098724365,
      "learning_rate": 3.971153846153846e-05,
      "loss": 0.5946,
      "step": 1048600
    },
    {
      "epoch": 16.463108320251177,
      "grad_norm": 3.8691701889038086,
      "learning_rate": 3.971055729984302e-05,
      "loss": 0.6258,
      "step": 1048700
    },
    {
      "epoch": 16.464678178963894,
      "grad_norm": 2.9419729709625244,
      "learning_rate": 3.970957613814757e-05,
      "loss": 0.6395,
      "step": 1048800
    },
    {
      "epoch": 16.46624803767661,
      "grad_norm": 4.5196733474731445,
      "learning_rate": 3.9708594976452126e-05,
      "loss": 0.619,
      "step": 1048900
    },
    {
      "epoch": 16.467817896389324,
      "grad_norm": 4.708014011383057,
      "learning_rate": 3.970761381475667e-05,
      "loss": 0.6195,
      "step": 1049000
    },
    {
      "epoch": 16.46938775510204,
      "grad_norm": 3.490921974182129,
      "learning_rate": 3.970663265306123e-05,
      "loss": 0.6267,
      "step": 1049100
    },
    {
      "epoch": 16.470957613814758,
      "grad_norm": 3.6808996200561523,
      "learning_rate": 3.970565149136578e-05,
      "loss": 0.6107,
      "step": 1049200
    },
    {
      "epoch": 16.47252747252747,
      "grad_norm": 4.054728031158447,
      "learning_rate": 3.970467032967033e-05,
      "loss": 0.6024,
      "step": 1049300
    },
    {
      "epoch": 16.474097331240188,
      "grad_norm": 3.753040075302124,
      "learning_rate": 3.970368916797488e-05,
      "loss": 0.5919,
      "step": 1049400
    },
    {
      "epoch": 16.475667189952905,
      "grad_norm": 3.6931910514831543,
      "learning_rate": 3.970270800627944e-05,
      "loss": 0.5828,
      "step": 1049500
    },
    {
      "epoch": 16.47723704866562,
      "grad_norm": 2.838101625442505,
      "learning_rate": 3.970172684458399e-05,
      "loss": 0.5827,
      "step": 1049600
    },
    {
      "epoch": 16.478806907378335,
      "grad_norm": 3.945312023162842,
      "learning_rate": 3.970074568288854e-05,
      "loss": 0.6066,
      "step": 1049700
    },
    {
      "epoch": 16.48037676609105,
      "grad_norm": 3.645231246948242,
      "learning_rate": 3.969976452119309e-05,
      "loss": 0.664,
      "step": 1049800
    },
    {
      "epoch": 16.48194662480377,
      "grad_norm": 3.9054014682769775,
      "learning_rate": 3.969878335949765e-05,
      "loss": 0.6016,
      "step": 1049900
    },
    {
      "epoch": 16.483516483516482,
      "grad_norm": 4.889203071594238,
      "learning_rate": 3.96978021978022e-05,
      "loss": 0.6126,
      "step": 1050000
    },
    {
      "epoch": 16.4850863422292,
      "grad_norm": 3.6925525665283203,
      "learning_rate": 3.969682103610675e-05,
      "loss": 0.6436,
      "step": 1050100
    },
    {
      "epoch": 16.486656200941916,
      "grad_norm": 3.636842966079712,
      "learning_rate": 3.969583987441131e-05,
      "loss": 0.5983,
      "step": 1050200
    },
    {
      "epoch": 16.488226059654632,
      "grad_norm": 3.0968286991119385,
      "learning_rate": 3.969485871271586e-05,
      "loss": 0.605,
      "step": 1050300
    },
    {
      "epoch": 16.489795918367346,
      "grad_norm": 3.536267042160034,
      "learning_rate": 3.969387755102041e-05,
      "loss": 0.6249,
      "step": 1050400
    },
    {
      "epoch": 16.491365777080063,
      "grad_norm": 3.9593288898468018,
      "learning_rate": 3.969289638932496e-05,
      "loss": 0.6167,
      "step": 1050500
    },
    {
      "epoch": 16.49293563579278,
      "grad_norm": 3.1006741523742676,
      "learning_rate": 3.969191522762952e-05,
      "loss": 0.6559,
      "step": 1050600
    },
    {
      "epoch": 16.494505494505496,
      "grad_norm": 3.413547992706299,
      "learning_rate": 3.9690934065934064e-05,
      "loss": 0.6001,
      "step": 1050700
    },
    {
      "epoch": 16.49607535321821,
      "grad_norm": 2.578684091567993,
      "learning_rate": 3.968995290423862e-05,
      "loss": 0.5985,
      "step": 1050800
    },
    {
      "epoch": 16.497645211930926,
      "grad_norm": 2.892078161239624,
      "learning_rate": 3.968897174254317e-05,
      "loss": 0.6025,
      "step": 1050900
    },
    {
      "epoch": 16.499215070643643,
      "grad_norm": 3.4854774475097656,
      "learning_rate": 3.968799058084773e-05,
      "loss": 0.5733,
      "step": 1051000
    },
    {
      "epoch": 16.500784929356357,
      "grad_norm": 4.144067764282227,
      "learning_rate": 3.9687009419152275e-05,
      "loss": 0.5768,
      "step": 1051100
    },
    {
      "epoch": 16.502354788069074,
      "grad_norm": 3.9433035850524902,
      "learning_rate": 3.968602825745683e-05,
      "loss": 0.626,
      "step": 1051200
    },
    {
      "epoch": 16.50392464678179,
      "grad_norm": 4.130827903747559,
      "learning_rate": 3.968504709576138e-05,
      "loss": 0.6465,
      "step": 1051300
    },
    {
      "epoch": 16.505494505494504,
      "grad_norm": 4.314450740814209,
      "learning_rate": 3.9684065934065934e-05,
      "loss": 0.6243,
      "step": 1051400
    },
    {
      "epoch": 16.50706436420722,
      "grad_norm": 4.106142044067383,
      "learning_rate": 3.9683084772370485e-05,
      "loss": 0.6289,
      "step": 1051500
    },
    {
      "epoch": 16.508634222919937,
      "grad_norm": 4.382819652557373,
      "learning_rate": 3.968210361067504e-05,
      "loss": 0.5942,
      "step": 1051600
    },
    {
      "epoch": 16.510204081632654,
      "grad_norm": 4.07304573059082,
      "learning_rate": 3.9681122448979594e-05,
      "loss": 0.5972,
      "step": 1051700
    },
    {
      "epoch": 16.511773940345368,
      "grad_norm": 3.5168919563293457,
      "learning_rate": 3.9680141287284145e-05,
      "loss": 0.6253,
      "step": 1051800
    },
    {
      "epoch": 16.513343799058084,
      "grad_norm": 3.549206256866455,
      "learning_rate": 3.9679160125588696e-05,
      "loss": 0.6453,
      "step": 1051900
    },
    {
      "epoch": 16.5149136577708,
      "grad_norm": 3.946056842803955,
      "learning_rate": 3.9678178963893254e-05,
      "loss": 0.6076,
      "step": 1052000
    },
    {
      "epoch": 16.516483516483518,
      "grad_norm": 4.187831401824951,
      "learning_rate": 3.9677197802197805e-05,
      "loss": 0.6154,
      "step": 1052100
    },
    {
      "epoch": 16.51805337519623,
      "grad_norm": 3.5033152103424072,
      "learning_rate": 3.9676216640502356e-05,
      "loss": 0.6164,
      "step": 1052200
    },
    {
      "epoch": 16.51962323390895,
      "grad_norm": 3.793313980102539,
      "learning_rate": 3.9675235478806913e-05,
      "loss": 0.6356,
      "step": 1052300
    },
    {
      "epoch": 16.521193092621665,
      "grad_norm": 4.2113142013549805,
      "learning_rate": 3.9674254317111464e-05,
      "loss": 0.5989,
      "step": 1052400
    },
    {
      "epoch": 16.52276295133438,
      "grad_norm": 4.345027446746826,
      "learning_rate": 3.9673273155416015e-05,
      "loss": 0.5903,
      "step": 1052500
    },
    {
      "epoch": 16.524332810047095,
      "grad_norm": 4.002065658569336,
      "learning_rate": 3.9672291993720566e-05,
      "loss": 0.6465,
      "step": 1052600
    },
    {
      "epoch": 16.525902668759812,
      "grad_norm": 3.9015097618103027,
      "learning_rate": 3.9671310832025124e-05,
      "loss": 0.6118,
      "step": 1052700
    },
    {
      "epoch": 16.52747252747253,
      "grad_norm": 4.997506141662598,
      "learning_rate": 3.967032967032967e-05,
      "loss": 0.6133,
      "step": 1052800
    },
    {
      "epoch": 16.529042386185242,
      "grad_norm": 3.929295301437378,
      "learning_rate": 3.9669348508634226e-05,
      "loss": 0.6131,
      "step": 1052900
    },
    {
      "epoch": 16.53061224489796,
      "grad_norm": 3.148404359817505,
      "learning_rate": 3.966836734693878e-05,
      "loss": 0.6283,
      "step": 1053000
    },
    {
      "epoch": 16.532182103610676,
      "grad_norm": 4.2873969078063965,
      "learning_rate": 3.9667386185243335e-05,
      "loss": 0.5792,
      "step": 1053100
    },
    {
      "epoch": 16.53375196232339,
      "grad_norm": 3.9745795726776123,
      "learning_rate": 3.966640502354788e-05,
      "loss": 0.6381,
      "step": 1053200
    },
    {
      "epoch": 16.535321821036106,
      "grad_norm": 4.3646464347839355,
      "learning_rate": 3.966542386185244e-05,
      "loss": 0.6705,
      "step": 1053300
    },
    {
      "epoch": 16.536891679748823,
      "grad_norm": 4.30827522277832,
      "learning_rate": 3.966444270015699e-05,
      "loss": 0.6248,
      "step": 1053400
    },
    {
      "epoch": 16.53846153846154,
      "grad_norm": 4.192785263061523,
      "learning_rate": 3.966346153846154e-05,
      "loss": 0.6235,
      "step": 1053500
    },
    {
      "epoch": 16.540031397174253,
      "grad_norm": 4.325706481933594,
      "learning_rate": 3.966248037676609e-05,
      "loss": 0.6299,
      "step": 1053600
    },
    {
      "epoch": 16.54160125588697,
      "grad_norm": 4.6658806800842285,
      "learning_rate": 3.966149921507065e-05,
      "loss": 0.627,
      "step": 1053700
    },
    {
      "epoch": 16.543171114599687,
      "grad_norm": 2.925835371017456,
      "learning_rate": 3.96605180533752e-05,
      "loss": 0.6121,
      "step": 1053800
    },
    {
      "epoch": 16.5447409733124,
      "grad_norm": 4.334888935089111,
      "learning_rate": 3.965953689167975e-05,
      "loss": 0.5727,
      "step": 1053900
    },
    {
      "epoch": 16.546310832025117,
      "grad_norm": 4.235415458679199,
      "learning_rate": 3.96585557299843e-05,
      "loss": 0.6677,
      "step": 1054000
    },
    {
      "epoch": 16.547880690737834,
      "grad_norm": 3.5849320888519287,
      "learning_rate": 3.965757456828886e-05,
      "loss": 0.6008,
      "step": 1054100
    },
    {
      "epoch": 16.54945054945055,
      "grad_norm": 4.095568656921387,
      "learning_rate": 3.965659340659341e-05,
      "loss": 0.6081,
      "step": 1054200
    },
    {
      "epoch": 16.551020408163264,
      "grad_norm": 4.696496486663818,
      "learning_rate": 3.965561224489796e-05,
      "loss": 0.6343,
      "step": 1054300
    },
    {
      "epoch": 16.55259026687598,
      "grad_norm": 3.2580106258392334,
      "learning_rate": 3.965463108320252e-05,
      "loss": 0.619,
      "step": 1054400
    },
    {
      "epoch": 16.554160125588698,
      "grad_norm": 3.4185166358947754,
      "learning_rate": 3.965364992150707e-05,
      "loss": 0.6174,
      "step": 1054500
    },
    {
      "epoch": 16.55572998430141,
      "grad_norm": 4.295569896697998,
      "learning_rate": 3.965266875981162e-05,
      "loss": 0.6051,
      "step": 1054600
    },
    {
      "epoch": 16.55729984301413,
      "grad_norm": 3.5350513458251953,
      "learning_rate": 3.965168759811617e-05,
      "loss": 0.6435,
      "step": 1054700
    },
    {
      "epoch": 16.558869701726845,
      "grad_norm": 3.982403039932251,
      "learning_rate": 3.965070643642073e-05,
      "loss": 0.6653,
      "step": 1054800
    },
    {
      "epoch": 16.560439560439562,
      "grad_norm": 3.644054651260376,
      "learning_rate": 3.964972527472527e-05,
      "loss": 0.6102,
      "step": 1054900
    },
    {
      "epoch": 16.562009419152275,
      "grad_norm": 3.77243971824646,
      "learning_rate": 3.964874411302983e-05,
      "loss": 0.6055,
      "step": 1055000
    },
    {
      "epoch": 16.563579277864992,
      "grad_norm": 3.4895243644714355,
      "learning_rate": 3.964776295133438e-05,
      "loss": 0.6283,
      "step": 1055100
    },
    {
      "epoch": 16.56514913657771,
      "grad_norm": 3.8486521244049072,
      "learning_rate": 3.964678178963894e-05,
      "loss": 0.581,
      "step": 1055200
    },
    {
      "epoch": 16.566718995290422,
      "grad_norm": 3.7337687015533447,
      "learning_rate": 3.9645800627943484e-05,
      "loss": 0.6154,
      "step": 1055300
    },
    {
      "epoch": 16.56828885400314,
      "grad_norm": 3.969472646713257,
      "learning_rate": 3.964481946624804e-05,
      "loss": 0.6001,
      "step": 1055400
    },
    {
      "epoch": 16.569858712715856,
      "grad_norm": 4.013433456420898,
      "learning_rate": 3.964383830455259e-05,
      "loss": 0.6018,
      "step": 1055500
    },
    {
      "epoch": 16.571428571428573,
      "grad_norm": 3.449199676513672,
      "learning_rate": 3.964285714285714e-05,
      "loss": 0.5934,
      "step": 1055600
    },
    {
      "epoch": 16.572998430141286,
      "grad_norm": 3.7910501956939697,
      "learning_rate": 3.9641875981161694e-05,
      "loss": 0.6098,
      "step": 1055700
    },
    {
      "epoch": 16.574568288854003,
      "grad_norm": 4.131692886352539,
      "learning_rate": 3.964089481946625e-05,
      "loss": 0.5921,
      "step": 1055800
    },
    {
      "epoch": 16.57613814756672,
      "grad_norm": 3.840010166168213,
      "learning_rate": 3.96399136577708e-05,
      "loss": 0.6064,
      "step": 1055900
    },
    {
      "epoch": 16.577708006279433,
      "grad_norm": 3.8836700916290283,
      "learning_rate": 3.9638932496075354e-05,
      "loss": 0.6475,
      "step": 1056000
    },
    {
      "epoch": 16.57927786499215,
      "grad_norm": 3.415639877319336,
      "learning_rate": 3.9637951334379905e-05,
      "loss": 0.6093,
      "step": 1056100
    },
    {
      "epoch": 16.580847723704867,
      "grad_norm": 2.705181121826172,
      "learning_rate": 3.963697017268446e-05,
      "loss": 0.661,
      "step": 1056200
    },
    {
      "epoch": 16.582417582417584,
      "grad_norm": 3.8004515171051025,
      "learning_rate": 3.9635989010989014e-05,
      "loss": 0.5925,
      "step": 1056300
    },
    {
      "epoch": 16.583987441130297,
      "grad_norm": 3.6543843746185303,
      "learning_rate": 3.9635007849293565e-05,
      "loss": 0.5919,
      "step": 1056400
    },
    {
      "epoch": 16.585557299843014,
      "grad_norm": 4.338865756988525,
      "learning_rate": 3.963402668759812e-05,
      "loss": 0.5761,
      "step": 1056500
    },
    {
      "epoch": 16.58712715855573,
      "grad_norm": 3.469882011413574,
      "learning_rate": 3.9633045525902673e-05,
      "loss": 0.5966,
      "step": 1056600
    },
    {
      "epoch": 16.588697017268444,
      "grad_norm": 2.534191370010376,
      "learning_rate": 3.9632064364207224e-05,
      "loss": 0.6279,
      "step": 1056700
    },
    {
      "epoch": 16.59026687598116,
      "grad_norm": 3.108835458755493,
      "learning_rate": 3.9631083202511775e-05,
      "loss": 0.5752,
      "step": 1056800
    },
    {
      "epoch": 16.591836734693878,
      "grad_norm": 3.395148515701294,
      "learning_rate": 3.963010204081633e-05,
      "loss": 0.6451,
      "step": 1056900
    },
    {
      "epoch": 16.593406593406595,
      "grad_norm": 3.399898052215576,
      "learning_rate": 3.962912087912088e-05,
      "loss": 0.5825,
      "step": 1057000
    },
    {
      "epoch": 16.594976452119308,
      "grad_norm": 3.1614248752593994,
      "learning_rate": 3.9628139717425435e-05,
      "loss": 0.6336,
      "step": 1057100
    },
    {
      "epoch": 16.596546310832025,
      "grad_norm": 4.813140392303467,
      "learning_rate": 3.9627158555729986e-05,
      "loss": 0.6475,
      "step": 1057200
    },
    {
      "epoch": 16.598116169544742,
      "grad_norm": 4.425111293792725,
      "learning_rate": 3.9626177394034544e-05,
      "loss": 0.6145,
      "step": 1057300
    },
    {
      "epoch": 16.599686028257455,
      "grad_norm": 3.3359463214874268,
      "learning_rate": 3.962519623233909e-05,
      "loss": 0.5987,
      "step": 1057400
    },
    {
      "epoch": 16.601255886970172,
      "grad_norm": 3.979078531265259,
      "learning_rate": 3.9624215070643646e-05,
      "loss": 0.6267,
      "step": 1057500
    },
    {
      "epoch": 16.60282574568289,
      "grad_norm": 3.9223005771636963,
      "learning_rate": 3.96232339089482e-05,
      "loss": 0.5877,
      "step": 1057600
    },
    {
      "epoch": 16.604395604395606,
      "grad_norm": 4.130446910858154,
      "learning_rate": 3.962225274725275e-05,
      "loss": 0.5941,
      "step": 1057700
    },
    {
      "epoch": 16.60596546310832,
      "grad_norm": 4.087376594543457,
      "learning_rate": 3.96212715855573e-05,
      "loss": 0.6164,
      "step": 1057800
    },
    {
      "epoch": 16.607535321821036,
      "grad_norm": 3.3203845024108887,
      "learning_rate": 3.9620290423861856e-05,
      "loss": 0.6596,
      "step": 1057900
    },
    {
      "epoch": 16.609105180533753,
      "grad_norm": 3.8220763206481934,
      "learning_rate": 3.961930926216641e-05,
      "loss": 0.652,
      "step": 1058000
    },
    {
      "epoch": 16.610675039246466,
      "grad_norm": 3.689998149871826,
      "learning_rate": 3.961832810047096e-05,
      "loss": 0.5864,
      "step": 1058100
    },
    {
      "epoch": 16.612244897959183,
      "grad_norm": 2.4847164154052734,
      "learning_rate": 3.961734693877551e-05,
      "loss": 0.5888,
      "step": 1058200
    },
    {
      "epoch": 16.6138147566719,
      "grad_norm": 4.624892711639404,
      "learning_rate": 3.961636577708007e-05,
      "loss": 0.5719,
      "step": 1058300
    },
    {
      "epoch": 16.615384615384617,
      "grad_norm": 4.195837020874023,
      "learning_rate": 3.961538461538462e-05,
      "loss": 0.5982,
      "step": 1058400
    },
    {
      "epoch": 16.61695447409733,
      "grad_norm": 4.173031806945801,
      "learning_rate": 3.961440345368917e-05,
      "loss": 0.6469,
      "step": 1058500
    },
    {
      "epoch": 16.618524332810047,
      "grad_norm": 5.00888729095459,
      "learning_rate": 3.961342229199373e-05,
      "loss": 0.6071,
      "step": 1058600
    },
    {
      "epoch": 16.620094191522764,
      "grad_norm": 5.175078392028809,
      "learning_rate": 3.961244113029827e-05,
      "loss": 0.6042,
      "step": 1058700
    },
    {
      "epoch": 16.621664050235477,
      "grad_norm": 4.42214298248291,
      "learning_rate": 3.961145996860283e-05,
      "loss": 0.5596,
      "step": 1058800
    },
    {
      "epoch": 16.623233908948194,
      "grad_norm": 4.0931878089904785,
      "learning_rate": 3.961047880690738e-05,
      "loss": 0.6067,
      "step": 1058900
    },
    {
      "epoch": 16.62480376766091,
      "grad_norm": 5.0626678466796875,
      "learning_rate": 3.960949764521194e-05,
      "loss": 0.5845,
      "step": 1059000
    },
    {
      "epoch": 16.626373626373628,
      "grad_norm": 3.2736332416534424,
      "learning_rate": 3.960851648351648e-05,
      "loss": 0.6129,
      "step": 1059100
    },
    {
      "epoch": 16.62794348508634,
      "grad_norm": 4.439913749694824,
      "learning_rate": 3.960753532182104e-05,
      "loss": 0.5978,
      "step": 1059200
    },
    {
      "epoch": 16.629513343799058,
      "grad_norm": 3.2808279991149902,
      "learning_rate": 3.960655416012559e-05,
      "loss": 0.5869,
      "step": 1059300
    },
    {
      "epoch": 16.631083202511775,
      "grad_norm": 3.893688440322876,
      "learning_rate": 3.960557299843014e-05,
      "loss": 0.6181,
      "step": 1059400
    },
    {
      "epoch": 16.632653061224488,
      "grad_norm": 4.158638000488281,
      "learning_rate": 3.960459183673469e-05,
      "loss": 0.598,
      "step": 1059500
    },
    {
      "epoch": 16.634222919937205,
      "grad_norm": 4.556519985198975,
      "learning_rate": 3.960361067503925e-05,
      "loss": 0.5861,
      "step": 1059600
    },
    {
      "epoch": 16.635792778649922,
      "grad_norm": 3.201617479324341,
      "learning_rate": 3.96026295133438e-05,
      "loss": 0.6031,
      "step": 1059700
    },
    {
      "epoch": 16.63736263736264,
      "grad_norm": 4.494123458862305,
      "learning_rate": 3.960164835164835e-05,
      "loss": 0.5949,
      "step": 1059800
    },
    {
      "epoch": 16.638932496075352,
      "grad_norm": 4.243503093719482,
      "learning_rate": 3.96006671899529e-05,
      "loss": 0.617,
      "step": 1059900
    },
    {
      "epoch": 16.64050235478807,
      "grad_norm": 3.7292144298553467,
      "learning_rate": 3.959968602825746e-05,
      "loss": 0.6142,
      "step": 1060000
    },
    {
      "epoch": 16.642072213500786,
      "grad_norm": 3.6565849781036377,
      "learning_rate": 3.959870486656201e-05,
      "loss": 0.6598,
      "step": 1060100
    },
    {
      "epoch": 16.643642072213503,
      "grad_norm": 4.19276237487793,
      "learning_rate": 3.959772370486656e-05,
      "loss": 0.6234,
      "step": 1060200
    },
    {
      "epoch": 16.645211930926216,
      "grad_norm": 3.761702537536621,
      "learning_rate": 3.9596742543171114e-05,
      "loss": 0.58,
      "step": 1060300
    },
    {
      "epoch": 16.646781789638933,
      "grad_norm": 3.6086623668670654,
      "learning_rate": 3.959576138147567e-05,
      "loss": 0.5757,
      "step": 1060400
    },
    {
      "epoch": 16.64835164835165,
      "grad_norm": 4.1865339279174805,
      "learning_rate": 3.959478021978022e-05,
      "loss": 0.6322,
      "step": 1060500
    },
    {
      "epoch": 16.649921507064363,
      "grad_norm": 4.912058353424072,
      "learning_rate": 3.9593799058084774e-05,
      "loss": 0.6076,
      "step": 1060600
    },
    {
      "epoch": 16.65149136577708,
      "grad_norm": 3.3348450660705566,
      "learning_rate": 3.959281789638933e-05,
      "loss": 0.6225,
      "step": 1060700
    },
    {
      "epoch": 16.653061224489797,
      "grad_norm": 4.564725399017334,
      "learning_rate": 3.9591836734693876e-05,
      "loss": 0.6204,
      "step": 1060800
    },
    {
      "epoch": 16.65463108320251,
      "grad_norm": 3.9113481044769287,
      "learning_rate": 3.959085557299843e-05,
      "loss": 0.6235,
      "step": 1060900
    },
    {
      "epoch": 16.656200941915227,
      "grad_norm": 5.2354583740234375,
      "learning_rate": 3.9589874411302984e-05,
      "loss": 0.6202,
      "step": 1061000
    },
    {
      "epoch": 16.657770800627944,
      "grad_norm": 3.3485641479492188,
      "learning_rate": 3.958889324960754e-05,
      "loss": 0.6339,
      "step": 1061100
    },
    {
      "epoch": 16.65934065934066,
      "grad_norm": 4.416288375854492,
      "learning_rate": 3.9587912087912086e-05,
      "loss": 0.5962,
      "step": 1061200
    },
    {
      "epoch": 16.660910518053374,
      "grad_norm": 3.207326889038086,
      "learning_rate": 3.9586930926216644e-05,
      "loss": 0.646,
      "step": 1061300
    },
    {
      "epoch": 16.66248037676609,
      "grad_norm": 3.227393627166748,
      "learning_rate": 3.9585949764521195e-05,
      "loss": 0.5746,
      "step": 1061400
    },
    {
      "epoch": 16.664050235478808,
      "grad_norm": 4.437182426452637,
      "learning_rate": 3.9584968602825746e-05,
      "loss": 0.6119,
      "step": 1061500
    },
    {
      "epoch": 16.665620094191524,
      "grad_norm": 4.043601036071777,
      "learning_rate": 3.95839874411303e-05,
      "loss": 0.6226,
      "step": 1061600
    },
    {
      "epoch": 16.667189952904238,
      "grad_norm": 2.826770305633545,
      "learning_rate": 3.9583006279434855e-05,
      "loss": 0.6625,
      "step": 1061700
    },
    {
      "epoch": 16.668759811616955,
      "grad_norm": 3.8076486587524414,
      "learning_rate": 3.9582025117739406e-05,
      "loss": 0.5877,
      "step": 1061800
    },
    {
      "epoch": 16.67032967032967,
      "grad_norm": 3.8071987628936768,
      "learning_rate": 3.958104395604396e-05,
      "loss": 0.614,
      "step": 1061900
    },
    {
      "epoch": 16.671899529042385,
      "grad_norm": 3.696200132369995,
      "learning_rate": 3.958006279434851e-05,
      "loss": 0.6809,
      "step": 1062000
    },
    {
      "epoch": 16.6734693877551,
      "grad_norm": 3.144794464111328,
      "learning_rate": 3.9579081632653065e-05,
      "loss": 0.59,
      "step": 1062100
    },
    {
      "epoch": 16.67503924646782,
      "grad_norm": 4.71080207824707,
      "learning_rate": 3.9578100470957616e-05,
      "loss": 0.6364,
      "step": 1062200
    },
    {
      "epoch": 16.676609105180535,
      "grad_norm": 5.152230262756348,
      "learning_rate": 3.957711930926217e-05,
      "loss": 0.6212,
      "step": 1062300
    },
    {
      "epoch": 16.67817896389325,
      "grad_norm": 3.481583833694458,
      "learning_rate": 3.957613814756672e-05,
      "loss": 0.6601,
      "step": 1062400
    },
    {
      "epoch": 16.679748822605966,
      "grad_norm": 3.488767623901367,
      "learning_rate": 3.9575156985871276e-05,
      "loss": 0.6253,
      "step": 1062500
    },
    {
      "epoch": 16.681318681318682,
      "grad_norm": 3.107381820678711,
      "learning_rate": 3.957417582417583e-05,
      "loss": 0.5905,
      "step": 1062600
    },
    {
      "epoch": 16.682888540031396,
      "grad_norm": 4.438399791717529,
      "learning_rate": 3.957319466248038e-05,
      "loss": 0.6179,
      "step": 1062700
    },
    {
      "epoch": 16.684458398744113,
      "grad_norm": 3.9684455394744873,
      "learning_rate": 3.9572213500784936e-05,
      "loss": 0.6486,
      "step": 1062800
    },
    {
      "epoch": 16.68602825745683,
      "grad_norm": 2.998194694519043,
      "learning_rate": 3.957123233908948e-05,
      "loss": 0.6041,
      "step": 1062900
    },
    {
      "epoch": 16.687598116169546,
      "grad_norm": 2.5811216831207275,
      "learning_rate": 3.957025117739404e-05,
      "loss": 0.5671,
      "step": 1063000
    },
    {
      "epoch": 16.68916797488226,
      "grad_norm": 3.9111876487731934,
      "learning_rate": 3.956927001569859e-05,
      "loss": 0.6095,
      "step": 1063100
    },
    {
      "epoch": 16.690737833594977,
      "grad_norm": 3.6950173377990723,
      "learning_rate": 3.9568288854003147e-05,
      "loss": 0.6234,
      "step": 1063200
    },
    {
      "epoch": 16.692307692307693,
      "grad_norm": 3.1039113998413086,
      "learning_rate": 3.956730769230769e-05,
      "loss": 0.6103,
      "step": 1063300
    },
    {
      "epoch": 16.693877551020407,
      "grad_norm": 3.5252997875213623,
      "learning_rate": 3.956632653061225e-05,
      "loss": 0.5913,
      "step": 1063400
    },
    {
      "epoch": 16.695447409733124,
      "grad_norm": 4.813661098480225,
      "learning_rate": 3.95653453689168e-05,
      "loss": 0.5931,
      "step": 1063500
    },
    {
      "epoch": 16.69701726844584,
      "grad_norm": 4.51411771774292,
      "learning_rate": 3.956436420722135e-05,
      "loss": 0.6004,
      "step": 1063600
    },
    {
      "epoch": 16.698587127158557,
      "grad_norm": 3.6009302139282227,
      "learning_rate": 3.95633830455259e-05,
      "loss": 0.5961,
      "step": 1063700
    },
    {
      "epoch": 16.70015698587127,
      "grad_norm": 3.8453567028045654,
      "learning_rate": 3.956240188383046e-05,
      "loss": 0.6162,
      "step": 1063800
    },
    {
      "epoch": 16.701726844583987,
      "grad_norm": 2.988750457763672,
      "learning_rate": 3.956142072213501e-05,
      "loss": 0.5844,
      "step": 1063900
    },
    {
      "epoch": 16.703296703296704,
      "grad_norm": 4.394978046417236,
      "learning_rate": 3.956043956043956e-05,
      "loss": 0.5652,
      "step": 1064000
    },
    {
      "epoch": 16.704866562009418,
      "grad_norm": 3.5887980461120605,
      "learning_rate": 3.955945839874411e-05,
      "loss": 0.6368,
      "step": 1064100
    },
    {
      "epoch": 16.706436420722135,
      "grad_norm": 3.6562836170196533,
      "learning_rate": 3.955847723704867e-05,
      "loss": 0.6055,
      "step": 1064200
    },
    {
      "epoch": 16.70800627943485,
      "grad_norm": 4.664178848266602,
      "learning_rate": 3.955749607535322e-05,
      "loss": 0.5875,
      "step": 1064300
    },
    {
      "epoch": 16.70957613814757,
      "grad_norm": 3.1978673934936523,
      "learning_rate": 3.955651491365777e-05,
      "loss": 0.6074,
      "step": 1064400
    },
    {
      "epoch": 16.71114599686028,
      "grad_norm": 3.832332134246826,
      "learning_rate": 3.955553375196232e-05,
      "loss": 0.617,
      "step": 1064500
    },
    {
      "epoch": 16.712715855573,
      "grad_norm": 3.176987886428833,
      "learning_rate": 3.955455259026688e-05,
      "loss": 0.6162,
      "step": 1064600
    },
    {
      "epoch": 16.714285714285715,
      "grad_norm": 2.3540894985198975,
      "learning_rate": 3.955357142857143e-05,
      "loss": 0.6132,
      "step": 1064700
    },
    {
      "epoch": 16.71585557299843,
      "grad_norm": 4.380238056182861,
      "learning_rate": 3.955259026687598e-05,
      "loss": 0.6001,
      "step": 1064800
    },
    {
      "epoch": 16.717425431711145,
      "grad_norm": 2.9053220748901367,
      "learning_rate": 3.955160910518054e-05,
      "loss": 0.6535,
      "step": 1064900
    },
    {
      "epoch": 16.718995290423862,
      "grad_norm": 3.5694797039031982,
      "learning_rate": 3.9550627943485085e-05,
      "loss": 0.624,
      "step": 1065000
    },
    {
      "epoch": 16.72056514913658,
      "grad_norm": 2.954615354537964,
      "learning_rate": 3.954964678178964e-05,
      "loss": 0.6123,
      "step": 1065100
    },
    {
      "epoch": 16.722135007849293,
      "grad_norm": 4.793539047241211,
      "learning_rate": 3.954866562009419e-05,
      "loss": 0.6311,
      "step": 1065200
    },
    {
      "epoch": 16.72370486656201,
      "grad_norm": 3.3444106578826904,
      "learning_rate": 3.954768445839875e-05,
      "loss": 0.6309,
      "step": 1065300
    },
    {
      "epoch": 16.725274725274726,
      "grad_norm": 3.6625282764434814,
      "learning_rate": 3.9546703296703295e-05,
      "loss": 0.5869,
      "step": 1065400
    },
    {
      "epoch": 16.72684458398744,
      "grad_norm": 4.1600022315979,
      "learning_rate": 3.954572213500785e-05,
      "loss": 0.6159,
      "step": 1065500
    },
    {
      "epoch": 16.728414442700156,
      "grad_norm": 3.7263569831848145,
      "learning_rate": 3.9544740973312404e-05,
      "loss": 0.6467,
      "step": 1065600
    },
    {
      "epoch": 16.729984301412873,
      "grad_norm": 3.58193039894104,
      "learning_rate": 3.9543759811616955e-05,
      "loss": 0.6642,
      "step": 1065700
    },
    {
      "epoch": 16.73155416012559,
      "grad_norm": 3.8416788578033447,
      "learning_rate": 3.9542778649921506e-05,
      "loss": 0.5941,
      "step": 1065800
    },
    {
      "epoch": 16.733124018838303,
      "grad_norm": 3.7456648349761963,
      "learning_rate": 3.9541797488226064e-05,
      "loss": 0.6024,
      "step": 1065900
    },
    {
      "epoch": 16.73469387755102,
      "grad_norm": 3.2061235904693604,
      "learning_rate": 3.9540816326530615e-05,
      "loss": 0.597,
      "step": 1066000
    },
    {
      "epoch": 16.736263736263737,
      "grad_norm": 3.6361682415008545,
      "learning_rate": 3.9539835164835166e-05,
      "loss": 0.6033,
      "step": 1066100
    },
    {
      "epoch": 16.73783359497645,
      "grad_norm": 2.9559402465820312,
      "learning_rate": 3.953885400313972e-05,
      "loss": 0.6289,
      "step": 1066200
    },
    {
      "epoch": 16.739403453689167,
      "grad_norm": 3.917508363723755,
      "learning_rate": 3.9537872841444274e-05,
      "loss": 0.6436,
      "step": 1066300
    },
    {
      "epoch": 16.740973312401884,
      "grad_norm": 4.13547945022583,
      "learning_rate": 3.953689167974882e-05,
      "loss": 0.6093,
      "step": 1066400
    },
    {
      "epoch": 16.7425431711146,
      "grad_norm": 3.731226682662964,
      "learning_rate": 3.9535910518053376e-05,
      "loss": 0.616,
      "step": 1066500
    },
    {
      "epoch": 16.744113029827314,
      "grad_norm": 3.010880470275879,
      "learning_rate": 3.953492935635793e-05,
      "loss": 0.623,
      "step": 1066600
    },
    {
      "epoch": 16.74568288854003,
      "grad_norm": 4.833454608917236,
      "learning_rate": 3.9533948194662485e-05,
      "loss": 0.6071,
      "step": 1066700
    },
    {
      "epoch": 16.747252747252748,
      "grad_norm": 3.74509334564209,
      "learning_rate": 3.9532967032967036e-05,
      "loss": 0.641,
      "step": 1066800
    },
    {
      "epoch": 16.74882260596546,
      "grad_norm": 4.351890563964844,
      "learning_rate": 3.953198587127159e-05,
      "loss": 0.6019,
      "step": 1066900
    },
    {
      "epoch": 16.75039246467818,
      "grad_norm": 3.8936378955841064,
      "learning_rate": 3.9531004709576145e-05,
      "loss": 0.6218,
      "step": 1067000
    },
    {
      "epoch": 16.751962323390895,
      "grad_norm": 4.703162670135498,
      "learning_rate": 3.953002354788069e-05,
      "loss": 0.6009,
      "step": 1067100
    },
    {
      "epoch": 16.753532182103612,
      "grad_norm": 3.6910836696624756,
      "learning_rate": 3.952904238618525e-05,
      "loss": 0.627,
      "step": 1067200
    },
    {
      "epoch": 16.755102040816325,
      "grad_norm": 3.682481527328491,
      "learning_rate": 3.95280612244898e-05,
      "loss": 0.6101,
      "step": 1067300
    },
    {
      "epoch": 16.756671899529042,
      "grad_norm": 3.413952112197876,
      "learning_rate": 3.9527080062794356e-05,
      "loss": 0.6096,
      "step": 1067400
    },
    {
      "epoch": 16.75824175824176,
      "grad_norm": 4.330448150634766,
      "learning_rate": 3.95260989010989e-05,
      "loss": 0.6239,
      "step": 1067500
    },
    {
      "epoch": 16.759811616954472,
      "grad_norm": 3.5704965591430664,
      "learning_rate": 3.952511773940346e-05,
      "loss": 0.6254,
      "step": 1067600
    },
    {
      "epoch": 16.76138147566719,
      "grad_norm": 3.5852973461151123,
      "learning_rate": 3.952413657770801e-05,
      "loss": 0.5897,
      "step": 1067700
    },
    {
      "epoch": 16.762951334379906,
      "grad_norm": 3.795138120651245,
      "learning_rate": 3.952315541601256e-05,
      "loss": 0.5936,
      "step": 1067800
    },
    {
      "epoch": 16.764521193092623,
      "grad_norm": 3.58488392829895,
      "learning_rate": 3.952217425431711e-05,
      "loss": 0.6112,
      "step": 1067900
    },
    {
      "epoch": 16.766091051805336,
      "grad_norm": 4.107273101806641,
      "learning_rate": 3.952119309262167e-05,
      "loss": 0.6254,
      "step": 1068000
    },
    {
      "epoch": 16.767660910518053,
      "grad_norm": 4.68392276763916,
      "learning_rate": 3.952021193092622e-05,
      "loss": 0.6463,
      "step": 1068100
    },
    {
      "epoch": 16.76923076923077,
      "grad_norm": 2.6219918727874756,
      "learning_rate": 3.951923076923077e-05,
      "loss": 0.5883,
      "step": 1068200
    },
    {
      "epoch": 16.770800627943487,
      "grad_norm": 4.597445011138916,
      "learning_rate": 3.951824960753532e-05,
      "loss": 0.6134,
      "step": 1068300
    },
    {
      "epoch": 16.7723704866562,
      "grad_norm": 3.625262975692749,
      "learning_rate": 3.951726844583988e-05,
      "loss": 0.6109,
      "step": 1068400
    },
    {
      "epoch": 16.773940345368917,
      "grad_norm": 4.437422752380371,
      "learning_rate": 3.951628728414442e-05,
      "loss": 0.6733,
      "step": 1068500
    },
    {
      "epoch": 16.775510204081634,
      "grad_norm": 4.543973445892334,
      "learning_rate": 3.951530612244898e-05,
      "loss": 0.6262,
      "step": 1068600
    },
    {
      "epoch": 16.777080062794347,
      "grad_norm": 3.9856748580932617,
      "learning_rate": 3.951432496075353e-05,
      "loss": 0.6215,
      "step": 1068700
    },
    {
      "epoch": 16.778649921507064,
      "grad_norm": 4.29216194152832,
      "learning_rate": 3.951334379905809e-05,
      "loss": 0.5893,
      "step": 1068800
    },
    {
      "epoch": 16.78021978021978,
      "grad_norm": 4.057108402252197,
      "learning_rate": 3.951236263736264e-05,
      "loss": 0.6197,
      "step": 1068900
    },
    {
      "epoch": 16.781789638932494,
      "grad_norm": 4.45285177230835,
      "learning_rate": 3.951138147566719e-05,
      "loss": 0.623,
      "step": 1069000
    },
    {
      "epoch": 16.78335949764521,
      "grad_norm": 3.892880439758301,
      "learning_rate": 3.951040031397175e-05,
      "loss": 0.6129,
      "step": 1069100
    },
    {
      "epoch": 16.784929356357928,
      "grad_norm": 3.9238851070404053,
      "learning_rate": 3.9509419152276294e-05,
      "loss": 0.6509,
      "step": 1069200
    },
    {
      "epoch": 16.786499215070645,
      "grad_norm": 3.540980339050293,
      "learning_rate": 3.950843799058085e-05,
      "loss": 0.592,
      "step": 1069300
    },
    {
      "epoch": 16.788069073783358,
      "grad_norm": 4.306789398193359,
      "learning_rate": 3.95074568288854e-05,
      "loss": 0.6171,
      "step": 1069400
    },
    {
      "epoch": 16.789638932496075,
      "grad_norm": 3.9338061809539795,
      "learning_rate": 3.950647566718996e-05,
      "loss": 0.576,
      "step": 1069500
    },
    {
      "epoch": 16.791208791208792,
      "grad_norm": 3.771056890487671,
      "learning_rate": 3.9505494505494504e-05,
      "loss": 0.5939,
      "step": 1069600
    },
    {
      "epoch": 16.79277864992151,
      "grad_norm": 4.277011394500732,
      "learning_rate": 3.950451334379906e-05,
      "loss": 0.6201,
      "step": 1069700
    },
    {
      "epoch": 16.794348508634222,
      "grad_norm": 3.912297248840332,
      "learning_rate": 3.950353218210361e-05,
      "loss": 0.5832,
      "step": 1069800
    },
    {
      "epoch": 16.79591836734694,
      "grad_norm": 4.089778900146484,
      "learning_rate": 3.9502551020408164e-05,
      "loss": 0.6412,
      "step": 1069900
    },
    {
      "epoch": 16.797488226059656,
      "grad_norm": 4.330603122711182,
      "learning_rate": 3.9501569858712715e-05,
      "loss": 0.6315,
      "step": 1070000
    },
    {
      "epoch": 16.79905808477237,
      "grad_norm": 4.485729694366455,
      "learning_rate": 3.950058869701727e-05,
      "loss": 0.6508,
      "step": 1070100
    },
    {
      "epoch": 16.800627943485086,
      "grad_norm": 1.7191592454910278,
      "learning_rate": 3.9499607535321824e-05,
      "loss": 0.5938,
      "step": 1070200
    },
    {
      "epoch": 16.802197802197803,
      "grad_norm": 3.3341994285583496,
      "learning_rate": 3.9498626373626375e-05,
      "loss": 0.6367,
      "step": 1070300
    },
    {
      "epoch": 16.80376766091052,
      "grad_norm": 3.848182201385498,
      "learning_rate": 3.9497645211930926e-05,
      "loss": 0.6009,
      "step": 1070400
    },
    {
      "epoch": 16.805337519623233,
      "grad_norm": 3.203425645828247,
      "learning_rate": 3.9496664050235483e-05,
      "loss": 0.6008,
      "step": 1070500
    },
    {
      "epoch": 16.80690737833595,
      "grad_norm": 4.224574565887451,
      "learning_rate": 3.949568288854003e-05,
      "loss": 0.6062,
      "step": 1070600
    },
    {
      "epoch": 16.808477237048667,
      "grad_norm": 3.8344240188598633,
      "learning_rate": 3.9494701726844585e-05,
      "loss": 0.5896,
      "step": 1070700
    },
    {
      "epoch": 16.81004709576138,
      "grad_norm": 4.224987506866455,
      "learning_rate": 3.9493720565149136e-05,
      "loss": 0.5762,
      "step": 1070800
    },
    {
      "epoch": 16.811616954474097,
      "grad_norm": 3.3467800617218018,
      "learning_rate": 3.9492739403453694e-05,
      "loss": 0.6367,
      "step": 1070900
    },
    {
      "epoch": 16.813186813186814,
      "grad_norm": 3.8834075927734375,
      "learning_rate": 3.9491758241758245e-05,
      "loss": 0.5993,
      "step": 1071000
    },
    {
      "epoch": 16.81475667189953,
      "grad_norm": 4.120640277862549,
      "learning_rate": 3.9490777080062796e-05,
      "loss": 0.5897,
      "step": 1071100
    },
    {
      "epoch": 16.816326530612244,
      "grad_norm": 4.104921340942383,
      "learning_rate": 3.9489795918367354e-05,
      "loss": 0.5995,
      "step": 1071200
    },
    {
      "epoch": 16.81789638932496,
      "grad_norm": 3.7481231689453125,
      "learning_rate": 3.94888147566719e-05,
      "loss": 0.6136,
      "step": 1071300
    },
    {
      "epoch": 16.819466248037678,
      "grad_norm": 4.566549301147461,
      "learning_rate": 3.9487833594976456e-05,
      "loss": 0.6217,
      "step": 1071400
    },
    {
      "epoch": 16.82103610675039,
      "grad_norm": 3.75728702545166,
      "learning_rate": 3.948685243328101e-05,
      "loss": 0.6162,
      "step": 1071500
    },
    {
      "epoch": 16.822605965463108,
      "grad_norm": 3.812299966812134,
      "learning_rate": 3.9485871271585565e-05,
      "loss": 0.6312,
      "step": 1071600
    },
    {
      "epoch": 16.824175824175825,
      "grad_norm": 5.05180549621582,
      "learning_rate": 3.948489010989011e-05,
      "loss": 0.6156,
      "step": 1071700
    },
    {
      "epoch": 16.82574568288854,
      "grad_norm": 4.221175193786621,
      "learning_rate": 3.9483908948194667e-05,
      "loss": 0.6243,
      "step": 1071800
    },
    {
      "epoch": 16.827315541601255,
      "grad_norm": 4.808264255523682,
      "learning_rate": 3.948292778649922e-05,
      "loss": 0.5967,
      "step": 1071900
    },
    {
      "epoch": 16.828885400313972,
      "grad_norm": 3.6903164386749268,
      "learning_rate": 3.948194662480377e-05,
      "loss": 0.6464,
      "step": 1072000
    },
    {
      "epoch": 16.83045525902669,
      "grad_norm": 3.991389274597168,
      "learning_rate": 3.948096546310832e-05,
      "loss": 0.5524,
      "step": 1072100
    },
    {
      "epoch": 16.832025117739402,
      "grad_norm": 3.8331222534179688,
      "learning_rate": 3.947998430141288e-05,
      "loss": 0.65,
      "step": 1072200
    },
    {
      "epoch": 16.83359497645212,
      "grad_norm": 3.904270648956299,
      "learning_rate": 3.947900313971743e-05,
      "loss": 0.6608,
      "step": 1072300
    },
    {
      "epoch": 16.835164835164836,
      "grad_norm": 3.113532543182373,
      "learning_rate": 3.947802197802198e-05,
      "loss": 0.6221,
      "step": 1072400
    },
    {
      "epoch": 16.836734693877553,
      "grad_norm": 2.914370059967041,
      "learning_rate": 3.947704081632653e-05,
      "loss": 0.605,
      "step": 1072500
    },
    {
      "epoch": 16.838304552590266,
      "grad_norm": 4.311029434204102,
      "learning_rate": 3.947605965463109e-05,
      "loss": 0.634,
      "step": 1072600
    },
    {
      "epoch": 16.839874411302983,
      "grad_norm": 2.760777235031128,
      "learning_rate": 3.947507849293563e-05,
      "loss": 0.6257,
      "step": 1072700
    },
    {
      "epoch": 16.8414442700157,
      "grad_norm": 4.131372451782227,
      "learning_rate": 3.947409733124019e-05,
      "loss": 0.6144,
      "step": 1072800
    },
    {
      "epoch": 16.843014128728413,
      "grad_norm": 3.391852855682373,
      "learning_rate": 3.947311616954474e-05,
      "loss": 0.6329,
      "step": 1072900
    },
    {
      "epoch": 16.84458398744113,
      "grad_norm": 3.871330976486206,
      "learning_rate": 3.94721350078493e-05,
      "loss": 0.5847,
      "step": 1073000
    },
    {
      "epoch": 16.846153846153847,
      "grad_norm": 3.8272910118103027,
      "learning_rate": 3.947115384615385e-05,
      "loss": 0.6512,
      "step": 1073100
    },
    {
      "epoch": 16.847723704866564,
      "grad_norm": 3.7494349479675293,
      "learning_rate": 3.94701726844584e-05,
      "loss": 0.6065,
      "step": 1073200
    },
    {
      "epoch": 16.849293563579277,
      "grad_norm": 2.8244903087615967,
      "learning_rate": 3.946919152276295e-05,
      "loss": 0.634,
      "step": 1073300
    },
    {
      "epoch": 16.850863422291994,
      "grad_norm": 4.130417346954346,
      "learning_rate": 3.94682103610675e-05,
      "loss": 0.5979,
      "step": 1073400
    },
    {
      "epoch": 16.85243328100471,
      "grad_norm": 3.3904805183410645,
      "learning_rate": 3.946722919937206e-05,
      "loss": 0.6214,
      "step": 1073500
    },
    {
      "epoch": 16.854003139717424,
      "grad_norm": 5.357414245605469,
      "learning_rate": 3.946624803767661e-05,
      "loss": 0.5986,
      "step": 1073600
    },
    {
      "epoch": 16.85557299843014,
      "grad_norm": 4.671593189239502,
      "learning_rate": 3.946526687598117e-05,
      "loss": 0.643,
      "step": 1073700
    },
    {
      "epoch": 16.857142857142858,
      "grad_norm": 3.058945417404175,
      "learning_rate": 3.946428571428571e-05,
      "loss": 0.5825,
      "step": 1073800
    },
    {
      "epoch": 16.858712715855575,
      "grad_norm": 2.4467873573303223,
      "learning_rate": 3.946330455259027e-05,
      "loss": 0.6017,
      "step": 1073900
    },
    {
      "epoch": 16.860282574568288,
      "grad_norm": 3.2072432041168213,
      "learning_rate": 3.946232339089482e-05,
      "loss": 0.6263,
      "step": 1074000
    },
    {
      "epoch": 16.861852433281005,
      "grad_norm": 2.947251319885254,
      "learning_rate": 3.946134222919937e-05,
      "loss": 0.6017,
      "step": 1074100
    },
    {
      "epoch": 16.86342229199372,
      "grad_norm": 4.07515287399292,
      "learning_rate": 3.9460361067503924e-05,
      "loss": 0.5538,
      "step": 1074200
    },
    {
      "epoch": 16.864992150706435,
      "grad_norm": 4.182477951049805,
      "learning_rate": 3.945937990580848e-05,
      "loss": 0.6503,
      "step": 1074300
    },
    {
      "epoch": 16.86656200941915,
      "grad_norm": 3.129812717437744,
      "learning_rate": 3.945839874411303e-05,
      "loss": 0.6125,
      "step": 1074400
    },
    {
      "epoch": 16.86813186813187,
      "grad_norm": 4.549986839294434,
      "learning_rate": 3.9457417582417584e-05,
      "loss": 0.6469,
      "step": 1074500
    },
    {
      "epoch": 16.869701726844585,
      "grad_norm": 2.6329843997955322,
      "learning_rate": 3.9456436420722135e-05,
      "loss": 0.6138,
      "step": 1074600
    },
    {
      "epoch": 16.8712715855573,
      "grad_norm": 3.352308750152588,
      "learning_rate": 3.945545525902669e-05,
      "loss": 0.6161,
      "step": 1074700
    },
    {
      "epoch": 16.872841444270016,
      "grad_norm": 4.1424078941345215,
      "learning_rate": 3.945447409733124e-05,
      "loss": 0.6031,
      "step": 1074800
    },
    {
      "epoch": 16.874411302982733,
      "grad_norm": 3.606402635574341,
      "learning_rate": 3.9453492935635794e-05,
      "loss": 0.6311,
      "step": 1074900
    },
    {
      "epoch": 16.875981161695446,
      "grad_norm": 5.309230327606201,
      "learning_rate": 3.9452511773940345e-05,
      "loss": 0.6487,
      "step": 1075000
    },
    {
      "epoch": 16.877551020408163,
      "grad_norm": 3.375577688217163,
      "learning_rate": 3.94515306122449e-05,
      "loss": 0.6516,
      "step": 1075100
    },
    {
      "epoch": 16.87912087912088,
      "grad_norm": 4.290028095245361,
      "learning_rate": 3.9450549450549454e-05,
      "loss": 0.6227,
      "step": 1075200
    },
    {
      "epoch": 16.880690737833596,
      "grad_norm": 3.2704217433929443,
      "learning_rate": 3.9449568288854005e-05,
      "loss": 0.5802,
      "step": 1075300
    },
    {
      "epoch": 16.88226059654631,
      "grad_norm": 3.886003255844116,
      "learning_rate": 3.9448587127158556e-05,
      "loss": 0.5966,
      "step": 1075400
    },
    {
      "epoch": 16.883830455259027,
      "grad_norm": 3.9085867404937744,
      "learning_rate": 3.944760596546311e-05,
      "loss": 0.5998,
      "step": 1075500
    },
    {
      "epoch": 16.885400313971743,
      "grad_norm": 3.0394692420959473,
      "learning_rate": 3.9446624803767665e-05,
      "loss": 0.5908,
      "step": 1075600
    },
    {
      "epoch": 16.886970172684457,
      "grad_norm": 4.674275875091553,
      "learning_rate": 3.9445643642072216e-05,
      "loss": 0.6222,
      "step": 1075700
    },
    {
      "epoch": 16.888540031397174,
      "grad_norm": 4.115952491760254,
      "learning_rate": 3.9444662480376774e-05,
      "loss": 0.6078,
      "step": 1075800
    },
    {
      "epoch": 16.89010989010989,
      "grad_norm": 4.843014240264893,
      "learning_rate": 3.944368131868132e-05,
      "loss": 0.6324,
      "step": 1075900
    },
    {
      "epoch": 16.891679748822607,
      "grad_norm": 4.038269519805908,
      "learning_rate": 3.9442700156985875e-05,
      "loss": 0.6015,
      "step": 1076000
    },
    {
      "epoch": 16.89324960753532,
      "grad_norm": 3.858333110809326,
      "learning_rate": 3.9441718995290426e-05,
      "loss": 0.6428,
      "step": 1076100
    },
    {
      "epoch": 16.894819466248038,
      "grad_norm": 4.210413455963135,
      "learning_rate": 3.944073783359498e-05,
      "loss": 0.6485,
      "step": 1076200
    },
    {
      "epoch": 16.896389324960754,
      "grad_norm": 2.200605869293213,
      "learning_rate": 3.943975667189953e-05,
      "loss": 0.6247,
      "step": 1076300
    },
    {
      "epoch": 16.897959183673468,
      "grad_norm": 3.9321789741516113,
      "learning_rate": 3.9438775510204086e-05,
      "loss": 0.6157,
      "step": 1076400
    },
    {
      "epoch": 16.899529042386185,
      "grad_norm": 3.865403413772583,
      "learning_rate": 3.943779434850864e-05,
      "loss": 0.6038,
      "step": 1076500
    },
    {
      "epoch": 16.9010989010989,
      "grad_norm": 3.6788113117218018,
      "learning_rate": 3.943681318681319e-05,
      "loss": 0.6337,
      "step": 1076600
    },
    {
      "epoch": 16.90266875981162,
      "grad_norm": 3.6087300777435303,
      "learning_rate": 3.943583202511774e-05,
      "loss": 0.6394,
      "step": 1076700
    },
    {
      "epoch": 16.90423861852433,
      "grad_norm": 3.271862506866455,
      "learning_rate": 3.94348508634223e-05,
      "loss": 0.6423,
      "step": 1076800
    },
    {
      "epoch": 16.90580847723705,
      "grad_norm": 3.3588011264801025,
      "learning_rate": 3.943386970172684e-05,
      "loss": 0.6123,
      "step": 1076900
    },
    {
      "epoch": 16.907378335949765,
      "grad_norm": 4.056124210357666,
      "learning_rate": 3.94328885400314e-05,
      "loss": 0.6075,
      "step": 1077000
    },
    {
      "epoch": 16.90894819466248,
      "grad_norm": 2.9989075660705566,
      "learning_rate": 3.943190737833595e-05,
      "loss": 0.6223,
      "step": 1077100
    },
    {
      "epoch": 16.910518053375196,
      "grad_norm": 3.0196683406829834,
      "learning_rate": 3.943092621664051e-05,
      "loss": 0.5919,
      "step": 1077200
    },
    {
      "epoch": 16.912087912087912,
      "grad_norm": 4.6074018478393555,
      "learning_rate": 3.942994505494506e-05,
      "loss": 0.63,
      "step": 1077300
    },
    {
      "epoch": 16.91365777080063,
      "grad_norm": 3.493037462234497,
      "learning_rate": 3.942896389324961e-05,
      "loss": 0.6177,
      "step": 1077400
    },
    {
      "epoch": 16.915227629513343,
      "grad_norm": 2.593501329421997,
      "learning_rate": 3.942798273155416e-05,
      "loss": 0.5942,
      "step": 1077500
    },
    {
      "epoch": 16.91679748822606,
      "grad_norm": 4.512657642364502,
      "learning_rate": 3.942700156985871e-05,
      "loss": 0.6148,
      "step": 1077600
    },
    {
      "epoch": 16.918367346938776,
      "grad_norm": 1.72034752368927,
      "learning_rate": 3.942602040816327e-05,
      "loss": 0.6057,
      "step": 1077700
    },
    {
      "epoch": 16.919937205651493,
      "grad_norm": 4.201684474945068,
      "learning_rate": 3.942503924646782e-05,
      "loss": 0.6296,
      "step": 1077800
    },
    {
      "epoch": 16.921507064364206,
      "grad_norm": 4.293059825897217,
      "learning_rate": 3.942405808477238e-05,
      "loss": 0.6149,
      "step": 1077900
    },
    {
      "epoch": 16.923076923076923,
      "grad_norm": 4.200222969055176,
      "learning_rate": 3.942307692307692e-05,
      "loss": 0.5982,
      "step": 1078000
    },
    {
      "epoch": 16.92464678178964,
      "grad_norm": 4.392626762390137,
      "learning_rate": 3.942209576138148e-05,
      "loss": 0.6354,
      "step": 1078100
    },
    {
      "epoch": 16.926216640502354,
      "grad_norm": 4.052535533905029,
      "learning_rate": 3.942111459968603e-05,
      "loss": 0.5857,
      "step": 1078200
    },
    {
      "epoch": 16.92778649921507,
      "grad_norm": 4.333165168762207,
      "learning_rate": 3.942013343799058e-05,
      "loss": 0.6107,
      "step": 1078300
    },
    {
      "epoch": 16.929356357927787,
      "grad_norm": 4.855522632598877,
      "learning_rate": 3.941915227629513e-05,
      "loss": 0.6129,
      "step": 1078400
    },
    {
      "epoch": 16.9309262166405,
      "grad_norm": 3.6921045780181885,
      "learning_rate": 3.941817111459969e-05,
      "loss": 0.6272,
      "step": 1078500
    },
    {
      "epoch": 16.932496075353217,
      "grad_norm": 4.06590461730957,
      "learning_rate": 3.941718995290424e-05,
      "loss": 0.5951,
      "step": 1078600
    },
    {
      "epoch": 16.934065934065934,
      "grad_norm": 4.647412300109863,
      "learning_rate": 3.941620879120879e-05,
      "loss": 0.6477,
      "step": 1078700
    },
    {
      "epoch": 16.93563579277865,
      "grad_norm": 3.5031707286834717,
      "learning_rate": 3.9415227629513344e-05,
      "loss": 0.6059,
      "step": 1078800
    },
    {
      "epoch": 16.937205651491364,
      "grad_norm": 4.231245994567871,
      "learning_rate": 3.94142464678179e-05,
      "loss": 0.6567,
      "step": 1078900
    },
    {
      "epoch": 16.93877551020408,
      "grad_norm": 3.907130241394043,
      "learning_rate": 3.9413265306122446e-05,
      "loss": 0.5805,
      "step": 1079000
    },
    {
      "epoch": 16.940345368916798,
      "grad_norm": 2.741391897201538,
      "learning_rate": 3.9412284144427e-05,
      "loss": 0.5889,
      "step": 1079100
    },
    {
      "epoch": 16.941915227629515,
      "grad_norm": 5.722649097442627,
      "learning_rate": 3.9411302982731554e-05,
      "loss": 0.6633,
      "step": 1079200
    },
    {
      "epoch": 16.94348508634223,
      "grad_norm": 3.9026265144348145,
      "learning_rate": 3.941032182103611e-05,
      "loss": 0.6285,
      "step": 1079300
    },
    {
      "epoch": 16.945054945054945,
      "grad_norm": 3.7186779975891113,
      "learning_rate": 3.940934065934066e-05,
      "loss": 0.5886,
      "step": 1079400
    },
    {
      "epoch": 16.946624803767662,
      "grad_norm": 3.457801580429077,
      "learning_rate": 3.9408359497645214e-05,
      "loss": 0.6424,
      "step": 1079500
    },
    {
      "epoch": 16.948194662480375,
      "grad_norm": 4.083186149597168,
      "learning_rate": 3.9407378335949765e-05,
      "loss": 0.6357,
      "step": 1079600
    },
    {
      "epoch": 16.949764521193092,
      "grad_norm": 3.622589588165283,
      "learning_rate": 3.9406397174254316e-05,
      "loss": 0.6024,
      "step": 1079700
    },
    {
      "epoch": 16.95133437990581,
      "grad_norm": 4.3531389236450195,
      "learning_rate": 3.9405416012558874e-05,
      "loss": 0.6009,
      "step": 1079800
    },
    {
      "epoch": 16.952904238618526,
      "grad_norm": 4.019946575164795,
      "learning_rate": 3.9404434850863425e-05,
      "loss": 0.6218,
      "step": 1079900
    },
    {
      "epoch": 16.95447409733124,
      "grad_norm": 3.781083583831787,
      "learning_rate": 3.940345368916798e-05,
      "loss": 0.6511,
      "step": 1080000
    },
    {
      "epoch": 16.956043956043956,
      "grad_norm": 3.9336090087890625,
      "learning_rate": 3.940247252747253e-05,
      "loss": 0.6346,
      "step": 1080100
    },
    {
      "epoch": 16.957613814756673,
      "grad_norm": 3.2740867137908936,
      "learning_rate": 3.9401491365777084e-05,
      "loss": 0.601,
      "step": 1080200
    },
    {
      "epoch": 16.959183673469386,
      "grad_norm": 3.215649127960205,
      "learning_rate": 3.9400510204081635e-05,
      "loss": 0.5767,
      "step": 1080300
    },
    {
      "epoch": 16.960753532182103,
      "grad_norm": 4.422214984893799,
      "learning_rate": 3.9399529042386186e-05,
      "loss": 0.605,
      "step": 1080400
    },
    {
      "epoch": 16.96232339089482,
      "grad_norm": 3.976848602294922,
      "learning_rate": 3.939854788069074e-05,
      "loss": 0.6156,
      "step": 1080500
    },
    {
      "epoch": 16.963893249607537,
      "grad_norm": 3.838608741760254,
      "learning_rate": 3.9397566718995295e-05,
      "loss": 0.6215,
      "step": 1080600
    },
    {
      "epoch": 16.96546310832025,
      "grad_norm": 3.9446253776550293,
      "learning_rate": 3.939658555729984e-05,
      "loss": 0.6165,
      "step": 1080700
    },
    {
      "epoch": 16.967032967032967,
      "grad_norm": 3.9913294315338135,
      "learning_rate": 3.93956043956044e-05,
      "loss": 0.6157,
      "step": 1080800
    },
    {
      "epoch": 16.968602825745684,
      "grad_norm": 3.520761251449585,
      "learning_rate": 3.939462323390895e-05,
      "loss": 0.5726,
      "step": 1080900
    },
    {
      "epoch": 16.970172684458397,
      "grad_norm": 3.451333522796631,
      "learning_rate": 3.9393642072213506e-05,
      "loss": 0.5641,
      "step": 1081000
    },
    {
      "epoch": 16.971742543171114,
      "grad_norm": 3.6241586208343506,
      "learning_rate": 3.939266091051805e-05,
      "loss": 0.5916,
      "step": 1081100
    },
    {
      "epoch": 16.97331240188383,
      "grad_norm": 3.1801912784576416,
      "learning_rate": 3.939167974882261e-05,
      "loss": 0.6133,
      "step": 1081200
    },
    {
      "epoch": 16.974882260596548,
      "grad_norm": 3.273082971572876,
      "learning_rate": 3.939069858712716e-05,
      "loss": 0.5928,
      "step": 1081300
    },
    {
      "epoch": 16.97645211930926,
      "grad_norm": 5.357057571411133,
      "learning_rate": 3.938971742543171e-05,
      "loss": 0.6165,
      "step": 1081400
    },
    {
      "epoch": 16.978021978021978,
      "grad_norm": 4.281798839569092,
      "learning_rate": 3.938873626373627e-05,
      "loss": 0.6517,
      "step": 1081500
    },
    {
      "epoch": 16.979591836734695,
      "grad_norm": 4.231369972229004,
      "learning_rate": 3.938775510204082e-05,
      "loss": 0.6473,
      "step": 1081600
    },
    {
      "epoch": 16.98116169544741,
      "grad_norm": 3.5972535610198975,
      "learning_rate": 3.938677394034537e-05,
      "loss": 0.6012,
      "step": 1081700
    },
    {
      "epoch": 16.982731554160125,
      "grad_norm": 3.830296754837036,
      "learning_rate": 3.938579277864992e-05,
      "loss": 0.5987,
      "step": 1081800
    },
    {
      "epoch": 16.984301412872842,
      "grad_norm": 3.506795644760132,
      "learning_rate": 3.938481161695448e-05,
      "loss": 0.5585,
      "step": 1081900
    },
    {
      "epoch": 16.98587127158556,
      "grad_norm": 4.916816711425781,
      "learning_rate": 3.938383045525903e-05,
      "loss": 0.6095,
      "step": 1082000
    },
    {
      "epoch": 16.987441130298272,
      "grad_norm": 4.047723770141602,
      "learning_rate": 3.938284929356358e-05,
      "loss": 0.6287,
      "step": 1082100
    },
    {
      "epoch": 16.98901098901099,
      "grad_norm": 4.061415195465088,
      "learning_rate": 3.938186813186813e-05,
      "loss": 0.6246,
      "step": 1082200
    },
    {
      "epoch": 16.990580847723706,
      "grad_norm": 3.8513622283935547,
      "learning_rate": 3.938088697017269e-05,
      "loss": 0.6229,
      "step": 1082300
    },
    {
      "epoch": 16.99215070643642,
      "grad_norm": 3.1848742961883545,
      "learning_rate": 3.937990580847724e-05,
      "loss": 0.6257,
      "step": 1082400
    },
    {
      "epoch": 16.993720565149136,
      "grad_norm": 4.15418815612793,
      "learning_rate": 3.937892464678179e-05,
      "loss": 0.6247,
      "step": 1082500
    },
    {
      "epoch": 16.995290423861853,
      "grad_norm": 4.740945339202881,
      "learning_rate": 3.937794348508634e-05,
      "loss": 0.6438,
      "step": 1082600
    },
    {
      "epoch": 16.99686028257457,
      "grad_norm": 3.428192138671875,
      "learning_rate": 3.93769623233909e-05,
      "loss": 0.6209,
      "step": 1082700
    },
    {
      "epoch": 16.998430141287283,
      "grad_norm": 3.6342430114746094,
      "learning_rate": 3.9375981161695444e-05,
      "loss": 0.5993,
      "step": 1082800
    },
    {
      "epoch": 17.0,
      "grad_norm": 3.3664236068725586,
      "learning_rate": 3.9375e-05,
      "loss": 0.6053,
      "step": 1082900
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.0600413084030151,
      "eval_runtime": 14.7097,
      "eval_samples_per_second": 227.945,
      "eval_steps_per_second": 227.945,
      "step": 1082900
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.47938045859336853,
      "eval_runtime": 281.9366,
      "eval_samples_per_second": 225.937,
      "eval_steps_per_second": 225.937,
      "step": 1082900
    },
    {
      "epoch": 17.001569858712717,
      "grad_norm": 5.020745754241943,
      "learning_rate": 3.937401883830455e-05,
      "loss": 0.6221,
      "step": 1083000
    },
    {
      "epoch": 17.00313971742543,
      "grad_norm": 3.881584405899048,
      "learning_rate": 3.937303767660911e-05,
      "loss": 0.6054,
      "step": 1083100
    },
    {
      "epoch": 17.004709576138147,
      "grad_norm": 3.9756972789764404,
      "learning_rate": 3.9372056514913655e-05,
      "loss": 0.6034,
      "step": 1083200
    },
    {
      "epoch": 17.006279434850864,
      "grad_norm": 2.926516532897949,
      "learning_rate": 3.937107535321821e-05,
      "loss": 0.6151,
      "step": 1083300
    },
    {
      "epoch": 17.00784929356358,
      "grad_norm": 3.626845598220825,
      "learning_rate": 3.937009419152276e-05,
      "loss": 0.5928,
      "step": 1083400
    },
    {
      "epoch": 17.009419152276294,
      "grad_norm": 4.208758354187012,
      "learning_rate": 3.9369113029827314e-05,
      "loss": 0.6012,
      "step": 1083500
    },
    {
      "epoch": 17.01098901098901,
      "grad_norm": 3.8931846618652344,
      "learning_rate": 3.936813186813187e-05,
      "loss": 0.6437,
      "step": 1083600
    },
    {
      "epoch": 17.012558869701728,
      "grad_norm": 4.396378040313721,
      "learning_rate": 3.936715070643642e-05,
      "loss": 0.5846,
      "step": 1083700
    },
    {
      "epoch": 17.01412872841444,
      "grad_norm": 3.0223820209503174,
      "learning_rate": 3.9366169544740974e-05,
      "loss": 0.6112,
      "step": 1083800
    },
    {
      "epoch": 17.015698587127158,
      "grad_norm": 4.194214344024658,
      "learning_rate": 3.9365188383045525e-05,
      "loss": 0.6116,
      "step": 1083900
    },
    {
      "epoch": 17.017268445839875,
      "grad_norm": 4.21725606918335,
      "learning_rate": 3.936420722135008e-05,
      "loss": 0.6519,
      "step": 1084000
    },
    {
      "epoch": 17.01883830455259,
      "grad_norm": 3.591306209564209,
      "learning_rate": 3.9363226059654634e-05,
      "loss": 0.6584,
      "step": 1084100
    },
    {
      "epoch": 17.020408163265305,
      "grad_norm": 3.467489719390869,
      "learning_rate": 3.9362244897959185e-05,
      "loss": 0.6221,
      "step": 1084200
    },
    {
      "epoch": 17.021978021978022,
      "grad_norm": 3.5921378135681152,
      "learning_rate": 3.9361263736263736e-05,
      "loss": 0.5799,
      "step": 1084300
    },
    {
      "epoch": 17.02354788069074,
      "grad_norm": 3.297750473022461,
      "learning_rate": 3.9360282574568293e-05,
      "loss": 0.5985,
      "step": 1084400
    },
    {
      "epoch": 17.025117739403452,
      "grad_norm": 3.8569421768188477,
      "learning_rate": 3.9359301412872844e-05,
      "loss": 0.6013,
      "step": 1084500
    },
    {
      "epoch": 17.02668759811617,
      "grad_norm": 3.226807117462158,
      "learning_rate": 3.9358320251177395e-05,
      "loss": 0.6213,
      "step": 1084600
    },
    {
      "epoch": 17.028257456828886,
      "grad_norm": 4.4621686935424805,
      "learning_rate": 3.9357339089481946e-05,
      "loss": 0.6399,
      "step": 1084700
    },
    {
      "epoch": 17.029827315541603,
      "grad_norm": 4.850215911865234,
      "learning_rate": 3.9356357927786504e-05,
      "loss": 0.6269,
      "step": 1084800
    },
    {
      "epoch": 17.031397174254316,
      "grad_norm": 2.5321803092956543,
      "learning_rate": 3.935537676609105e-05,
      "loss": 0.5963,
      "step": 1084900
    },
    {
      "epoch": 17.032967032967033,
      "grad_norm": 3.186319351196289,
      "learning_rate": 3.9354395604395606e-05,
      "loss": 0.6382,
      "step": 1085000
    },
    {
      "epoch": 17.03453689167975,
      "grad_norm": 3.0346522331237793,
      "learning_rate": 3.935341444270016e-05,
      "loss": 0.5986,
      "step": 1085100
    },
    {
      "epoch": 17.036106750392463,
      "grad_norm": 4.189295768737793,
      "learning_rate": 3.9352433281004715e-05,
      "loss": 0.6252,
      "step": 1085200
    },
    {
      "epoch": 17.03767660910518,
      "grad_norm": 4.39819860458374,
      "learning_rate": 3.935145211930926e-05,
      "loss": 0.6021,
      "step": 1085300
    },
    {
      "epoch": 17.039246467817897,
      "grad_norm": 4.06125545501709,
      "learning_rate": 3.935047095761382e-05,
      "loss": 0.5827,
      "step": 1085400
    },
    {
      "epoch": 17.040816326530614,
      "grad_norm": 3.9922170639038086,
      "learning_rate": 3.934948979591837e-05,
      "loss": 0.5936,
      "step": 1085500
    },
    {
      "epoch": 17.042386185243327,
      "grad_norm": 4.029703140258789,
      "learning_rate": 3.934850863422292e-05,
      "loss": 0.6272,
      "step": 1085600
    },
    {
      "epoch": 17.043956043956044,
      "grad_norm": 2.9250357151031494,
      "learning_rate": 3.9347527472527477e-05,
      "loss": 0.6229,
      "step": 1085700
    },
    {
      "epoch": 17.04552590266876,
      "grad_norm": 3.087589740753174,
      "learning_rate": 3.934654631083203e-05,
      "loss": 0.5922,
      "step": 1085800
    },
    {
      "epoch": 17.047095761381474,
      "grad_norm": 3.8986427783966064,
      "learning_rate": 3.934556514913658e-05,
      "loss": 0.5764,
      "step": 1085900
    },
    {
      "epoch": 17.04866562009419,
      "grad_norm": 2.9630115032196045,
      "learning_rate": 3.934458398744113e-05,
      "loss": 0.6376,
      "step": 1086000
    },
    {
      "epoch": 17.050235478806908,
      "grad_norm": 3.726288318634033,
      "learning_rate": 3.934360282574569e-05,
      "loss": 0.5625,
      "step": 1086100
    },
    {
      "epoch": 17.051805337519625,
      "grad_norm": 4.3211350440979,
      "learning_rate": 3.934262166405024e-05,
      "loss": 0.5729,
      "step": 1086200
    },
    {
      "epoch": 17.053375196232338,
      "grad_norm": 2.91497802734375,
      "learning_rate": 3.934164050235479e-05,
      "loss": 0.6137,
      "step": 1086300
    },
    {
      "epoch": 17.054945054945055,
      "grad_norm": 3.8484086990356445,
      "learning_rate": 3.934065934065934e-05,
      "loss": 0.5961,
      "step": 1086400
    },
    {
      "epoch": 17.05651491365777,
      "grad_norm": 4.268094539642334,
      "learning_rate": 3.93396781789639e-05,
      "loss": 0.5692,
      "step": 1086500
    },
    {
      "epoch": 17.058084772370485,
      "grad_norm": 3.772050142288208,
      "learning_rate": 3.933869701726845e-05,
      "loss": 0.6223,
      "step": 1086600
    },
    {
      "epoch": 17.059654631083202,
      "grad_norm": 3.727588653564453,
      "learning_rate": 3.9337715855573e-05,
      "loss": 0.5624,
      "step": 1086700
    },
    {
      "epoch": 17.06122448979592,
      "grad_norm": 2.769282341003418,
      "learning_rate": 3.933673469387755e-05,
      "loss": 0.6171,
      "step": 1086800
    },
    {
      "epoch": 17.062794348508636,
      "grad_norm": 4.338707447052002,
      "learning_rate": 3.933575353218211e-05,
      "loss": 0.6399,
      "step": 1086900
    },
    {
      "epoch": 17.06436420722135,
      "grad_norm": 3.5593225955963135,
      "learning_rate": 3.933477237048665e-05,
      "loss": 0.6192,
      "step": 1087000
    },
    {
      "epoch": 17.065934065934066,
      "grad_norm": 3.9003639221191406,
      "learning_rate": 3.933379120879121e-05,
      "loss": 0.5772,
      "step": 1087100
    },
    {
      "epoch": 17.067503924646783,
      "grad_norm": 3.9753787517547607,
      "learning_rate": 3.933281004709576e-05,
      "loss": 0.6068,
      "step": 1087200
    },
    {
      "epoch": 17.069073783359496,
      "grad_norm": 4.57284688949585,
      "learning_rate": 3.933182888540032e-05,
      "loss": 0.6504,
      "step": 1087300
    },
    {
      "epoch": 17.070643642072213,
      "grad_norm": 4.226075172424316,
      "learning_rate": 3.9330847723704864e-05,
      "loss": 0.5959,
      "step": 1087400
    },
    {
      "epoch": 17.07221350078493,
      "grad_norm": 4.924014091491699,
      "learning_rate": 3.932986656200942e-05,
      "loss": 0.5896,
      "step": 1087500
    },
    {
      "epoch": 17.073783359497646,
      "grad_norm": 3.6707160472869873,
      "learning_rate": 3.932888540031397e-05,
      "loss": 0.6318,
      "step": 1087600
    },
    {
      "epoch": 17.07535321821036,
      "grad_norm": 3.6907317638397217,
      "learning_rate": 3.932790423861852e-05,
      "loss": 0.6338,
      "step": 1087700
    },
    {
      "epoch": 17.076923076923077,
      "grad_norm": 4.154927730560303,
      "learning_rate": 3.932692307692308e-05,
      "loss": 0.6398,
      "step": 1087800
    },
    {
      "epoch": 17.078492935635794,
      "grad_norm": 2.611489772796631,
      "learning_rate": 3.932594191522763e-05,
      "loss": 0.6207,
      "step": 1087900
    },
    {
      "epoch": 17.08006279434851,
      "grad_norm": 3.3530147075653076,
      "learning_rate": 3.932496075353218e-05,
      "loss": 0.6285,
      "step": 1088000
    },
    {
      "epoch": 17.081632653061224,
      "grad_norm": 4.646693706512451,
      "learning_rate": 3.9323979591836734e-05,
      "loss": 0.5969,
      "step": 1088100
    },
    {
      "epoch": 17.08320251177394,
      "grad_norm": 4.1116943359375,
      "learning_rate": 3.932299843014129e-05,
      "loss": 0.5983,
      "step": 1088200
    },
    {
      "epoch": 17.084772370486657,
      "grad_norm": 3.3056538105010986,
      "learning_rate": 3.932201726844584e-05,
      "loss": 0.6327,
      "step": 1088300
    },
    {
      "epoch": 17.08634222919937,
      "grad_norm": 3.9897053241729736,
      "learning_rate": 3.9321036106750394e-05,
      "loss": 0.6119,
      "step": 1088400
    },
    {
      "epoch": 17.087912087912088,
      "grad_norm": 3.0179102420806885,
      "learning_rate": 3.9320054945054945e-05,
      "loss": 0.63,
      "step": 1088500
    },
    {
      "epoch": 17.089481946624804,
      "grad_norm": 2.9776535034179688,
      "learning_rate": 3.93190737833595e-05,
      "loss": 0.5961,
      "step": 1088600
    },
    {
      "epoch": 17.09105180533752,
      "grad_norm": 3.7519028186798096,
      "learning_rate": 3.9318092621664053e-05,
      "loss": 0.6029,
      "step": 1088700
    },
    {
      "epoch": 17.092621664050235,
      "grad_norm": 4.044906139373779,
      "learning_rate": 3.9317111459968604e-05,
      "loss": 0.6358,
      "step": 1088800
    },
    {
      "epoch": 17.09419152276295,
      "grad_norm": 3.5198159217834473,
      "learning_rate": 3.9316130298273155e-05,
      "loss": 0.6233,
      "step": 1088900
    },
    {
      "epoch": 17.09576138147567,
      "grad_norm": 3.48832368850708,
      "learning_rate": 3.931514913657771e-05,
      "loss": 0.6072,
      "step": 1089000
    },
    {
      "epoch": 17.09733124018838,
      "grad_norm": 3.9333856105804443,
      "learning_rate": 3.931416797488226e-05,
      "loss": 0.6272,
      "step": 1089100
    },
    {
      "epoch": 17.0989010989011,
      "grad_norm": 5.190308570861816,
      "learning_rate": 3.9313186813186815e-05,
      "loss": 0.605,
      "step": 1089200
    },
    {
      "epoch": 17.100470957613815,
      "grad_norm": 3.9886209964752197,
      "learning_rate": 3.9312205651491366e-05,
      "loss": 0.6171,
      "step": 1089300
    },
    {
      "epoch": 17.102040816326532,
      "grad_norm": 4.515726089477539,
      "learning_rate": 3.9311224489795924e-05,
      "loss": 0.5782,
      "step": 1089400
    },
    {
      "epoch": 17.103610675039246,
      "grad_norm": 2.631000518798828,
      "learning_rate": 3.931024332810047e-05,
      "loss": 0.5904,
      "step": 1089500
    },
    {
      "epoch": 17.105180533751962,
      "grad_norm": 4.917405605316162,
      "learning_rate": 3.9309262166405026e-05,
      "loss": 0.6595,
      "step": 1089600
    },
    {
      "epoch": 17.10675039246468,
      "grad_norm": 4.113672256469727,
      "learning_rate": 3.930828100470958e-05,
      "loss": 0.6273,
      "step": 1089700
    },
    {
      "epoch": 17.108320251177393,
      "grad_norm": 3.8280763626098633,
      "learning_rate": 3.930729984301413e-05,
      "loss": 0.6201,
      "step": 1089800
    },
    {
      "epoch": 17.10989010989011,
      "grad_norm": 3.349271535873413,
      "learning_rate": 3.9306318681318686e-05,
      "loss": 0.5886,
      "step": 1089900
    },
    {
      "epoch": 17.111459968602826,
      "grad_norm": 4.467348098754883,
      "learning_rate": 3.9305337519623237e-05,
      "loss": 0.6292,
      "step": 1090000
    },
    {
      "epoch": 17.113029827315543,
      "grad_norm": 3.776945114135742,
      "learning_rate": 3.930435635792779e-05,
      "loss": 0.6013,
      "step": 1090100
    },
    {
      "epoch": 17.114599686028257,
      "grad_norm": 3.9764223098754883,
      "learning_rate": 3.930337519623234e-05,
      "loss": 0.6216,
      "step": 1090200
    },
    {
      "epoch": 17.116169544740973,
      "grad_norm": 3.79923939704895,
      "learning_rate": 3.9302394034536896e-05,
      "loss": 0.6301,
      "step": 1090300
    },
    {
      "epoch": 17.11773940345369,
      "grad_norm": 4.382959842681885,
      "learning_rate": 3.930141287284145e-05,
      "loss": 0.6633,
      "step": 1090400
    },
    {
      "epoch": 17.119309262166404,
      "grad_norm": 4.553414821624756,
      "learning_rate": 3.9300431711146e-05,
      "loss": 0.6162,
      "step": 1090500
    },
    {
      "epoch": 17.12087912087912,
      "grad_norm": 3.7382614612579346,
      "learning_rate": 3.929945054945055e-05,
      "loss": 0.5964,
      "step": 1090600
    },
    {
      "epoch": 17.122448979591837,
      "grad_norm": 4.639995574951172,
      "learning_rate": 3.929846938775511e-05,
      "loss": 0.6169,
      "step": 1090700
    },
    {
      "epoch": 17.124018838304554,
      "grad_norm": 3.5235073566436768,
      "learning_rate": 3.929748822605966e-05,
      "loss": 0.5655,
      "step": 1090800
    },
    {
      "epoch": 17.125588697017267,
      "grad_norm": 3.5222275257110596,
      "learning_rate": 3.929650706436421e-05,
      "loss": 0.5899,
      "step": 1090900
    },
    {
      "epoch": 17.127158555729984,
      "grad_norm": 3.3333616256713867,
      "learning_rate": 3.929552590266876e-05,
      "loss": 0.5779,
      "step": 1091000
    },
    {
      "epoch": 17.1287284144427,
      "grad_norm": 4.267271041870117,
      "learning_rate": 3.929454474097332e-05,
      "loss": 0.6037,
      "step": 1091100
    },
    {
      "epoch": 17.130298273155415,
      "grad_norm": 3.1766300201416016,
      "learning_rate": 3.929356357927786e-05,
      "loss": 0.5721,
      "step": 1091200
    },
    {
      "epoch": 17.13186813186813,
      "grad_norm": 3.5585761070251465,
      "learning_rate": 3.929258241758242e-05,
      "loss": 0.5798,
      "step": 1091300
    },
    {
      "epoch": 17.13343799058085,
      "grad_norm": 3.880962610244751,
      "learning_rate": 3.929160125588697e-05,
      "loss": 0.6204,
      "step": 1091400
    },
    {
      "epoch": 17.135007849293565,
      "grad_norm": 3.3451571464538574,
      "learning_rate": 3.929062009419153e-05,
      "loss": 0.6012,
      "step": 1091500
    },
    {
      "epoch": 17.13657770800628,
      "grad_norm": 4.273266792297363,
      "learning_rate": 3.928963893249607e-05,
      "loss": 0.594,
      "step": 1091600
    },
    {
      "epoch": 17.138147566718995,
      "grad_norm": 3.711681604385376,
      "learning_rate": 3.928865777080063e-05,
      "loss": 0.6352,
      "step": 1091700
    },
    {
      "epoch": 17.139717425431712,
      "grad_norm": 3.4622058868408203,
      "learning_rate": 3.928767660910518e-05,
      "loss": 0.5854,
      "step": 1091800
    },
    {
      "epoch": 17.141287284144425,
      "grad_norm": 3.0733654499053955,
      "learning_rate": 3.928669544740973e-05,
      "loss": 0.624,
      "step": 1091900
    },
    {
      "epoch": 17.142857142857142,
      "grad_norm": 2.830469846725464,
      "learning_rate": 3.928571428571429e-05,
      "loss": 0.6232,
      "step": 1092000
    },
    {
      "epoch": 17.14442700156986,
      "grad_norm": 3.3657002449035645,
      "learning_rate": 3.928473312401884e-05,
      "loss": 0.5737,
      "step": 1092100
    },
    {
      "epoch": 17.145996860282576,
      "grad_norm": 4.293219089508057,
      "learning_rate": 3.928375196232339e-05,
      "loss": 0.621,
      "step": 1092200
    },
    {
      "epoch": 17.14756671899529,
      "grad_norm": 3.663043737411499,
      "learning_rate": 3.928277080062794e-05,
      "loss": 0.5529,
      "step": 1092300
    },
    {
      "epoch": 17.149136577708006,
      "grad_norm": 3.756596326828003,
      "learning_rate": 3.92817896389325e-05,
      "loss": 0.6428,
      "step": 1092400
    },
    {
      "epoch": 17.150706436420723,
      "grad_norm": 3.9546022415161133,
      "learning_rate": 3.928080847723705e-05,
      "loss": 0.5812,
      "step": 1092500
    },
    {
      "epoch": 17.152276295133436,
      "grad_norm": 3.0639472007751465,
      "learning_rate": 3.92798273155416e-05,
      "loss": 0.5972,
      "step": 1092600
    },
    {
      "epoch": 17.153846153846153,
      "grad_norm": 2.9841291904449463,
      "learning_rate": 3.9278846153846154e-05,
      "loss": 0.6262,
      "step": 1092700
    },
    {
      "epoch": 17.15541601255887,
      "grad_norm": 3.235800266265869,
      "learning_rate": 3.927786499215071e-05,
      "loss": 0.6108,
      "step": 1092800
    },
    {
      "epoch": 17.156985871271587,
      "grad_norm": 3.428492546081543,
      "learning_rate": 3.927688383045526e-05,
      "loss": 0.6066,
      "step": 1092900
    },
    {
      "epoch": 17.1585557299843,
      "grad_norm": 3.589107036590576,
      "learning_rate": 3.927590266875981e-05,
      "loss": 0.6365,
      "step": 1093000
    },
    {
      "epoch": 17.160125588697017,
      "grad_norm": 4.812054634094238,
      "learning_rate": 3.9274921507064364e-05,
      "loss": 0.5971,
      "step": 1093100
    },
    {
      "epoch": 17.161695447409734,
      "grad_norm": 4.140240669250488,
      "learning_rate": 3.927394034536892e-05,
      "loss": 0.6296,
      "step": 1093200
    },
    {
      "epoch": 17.163265306122447,
      "grad_norm": 4.824772357940674,
      "learning_rate": 3.9272959183673466e-05,
      "loss": 0.6201,
      "step": 1093300
    },
    {
      "epoch": 17.164835164835164,
      "grad_norm": 4.395975589752197,
      "learning_rate": 3.9271978021978024e-05,
      "loss": 0.6313,
      "step": 1093400
    },
    {
      "epoch": 17.16640502354788,
      "grad_norm": 4.09782600402832,
      "learning_rate": 3.9270996860282575e-05,
      "loss": 0.6405,
      "step": 1093500
    },
    {
      "epoch": 17.167974882260598,
      "grad_norm": 3.0114200115203857,
      "learning_rate": 3.927001569858713e-05,
      "loss": 0.5795,
      "step": 1093600
    },
    {
      "epoch": 17.16954474097331,
      "grad_norm": 3.6878836154937744,
      "learning_rate": 3.926903453689168e-05,
      "loss": 0.63,
      "step": 1093700
    },
    {
      "epoch": 17.171114599686028,
      "grad_norm": 3.822024345397949,
      "learning_rate": 3.9268053375196235e-05,
      "loss": 0.607,
      "step": 1093800
    },
    {
      "epoch": 17.172684458398745,
      "grad_norm": 3.1090166568756104,
      "learning_rate": 3.9267072213500786e-05,
      "loss": 0.6241,
      "step": 1093900
    },
    {
      "epoch": 17.17425431711146,
      "grad_norm": 3.763693332672119,
      "learning_rate": 3.926609105180534e-05,
      "loss": 0.6148,
      "step": 1094000
    },
    {
      "epoch": 17.175824175824175,
      "grad_norm": 3.16291880607605,
      "learning_rate": 3.9265109890109895e-05,
      "loss": 0.5779,
      "step": 1094100
    },
    {
      "epoch": 17.177394034536892,
      "grad_norm": 2.815758228302002,
      "learning_rate": 3.9264128728414445e-05,
      "loss": 0.6148,
      "step": 1094200
    },
    {
      "epoch": 17.17896389324961,
      "grad_norm": 2.9696223735809326,
      "learning_rate": 3.9263147566718996e-05,
      "loss": 0.5694,
      "step": 1094300
    },
    {
      "epoch": 17.180533751962322,
      "grad_norm": 4.8542914390563965,
      "learning_rate": 3.926216640502355e-05,
      "loss": 0.6351,
      "step": 1094400
    },
    {
      "epoch": 17.18210361067504,
      "grad_norm": 4.142323970794678,
      "learning_rate": 3.9261185243328105e-05,
      "loss": 0.6298,
      "step": 1094500
    },
    {
      "epoch": 17.183673469387756,
      "grad_norm": 2.8495540618896484,
      "learning_rate": 3.9260204081632656e-05,
      "loss": 0.612,
      "step": 1094600
    },
    {
      "epoch": 17.18524332810047,
      "grad_norm": 3.972155809402466,
      "learning_rate": 3.925922291993721e-05,
      "loss": 0.5708,
      "step": 1094700
    },
    {
      "epoch": 17.186813186813186,
      "grad_norm": 4.643947124481201,
      "learning_rate": 3.925824175824176e-05,
      "loss": 0.5879,
      "step": 1094800
    },
    {
      "epoch": 17.188383045525903,
      "grad_norm": 4.684816837310791,
      "learning_rate": 3.9257260596546316e-05,
      "loss": 0.5957,
      "step": 1094900
    },
    {
      "epoch": 17.18995290423862,
      "grad_norm": 2.8922061920166016,
      "learning_rate": 3.925627943485087e-05,
      "loss": 0.5667,
      "step": 1095000
    },
    {
      "epoch": 17.191522762951333,
      "grad_norm": 3.146282196044922,
      "learning_rate": 3.925529827315542e-05,
      "loss": 0.5877,
      "step": 1095100
    },
    {
      "epoch": 17.19309262166405,
      "grad_norm": 1.907191276550293,
      "learning_rate": 3.925431711145997e-05,
      "loss": 0.612,
      "step": 1095200
    },
    {
      "epoch": 17.194662480376767,
      "grad_norm": 3.852071762084961,
      "learning_rate": 3.925333594976453e-05,
      "loss": 0.5999,
      "step": 1095300
    },
    {
      "epoch": 17.19623233908948,
      "grad_norm": 4.012694835662842,
      "learning_rate": 3.925235478806907e-05,
      "loss": 0.6114,
      "step": 1095400
    },
    {
      "epoch": 17.197802197802197,
      "grad_norm": 3.5598654747009277,
      "learning_rate": 3.925137362637363e-05,
      "loss": 0.6206,
      "step": 1095500
    },
    {
      "epoch": 17.199372056514914,
      "grad_norm": 3.7688443660736084,
      "learning_rate": 3.925039246467818e-05,
      "loss": 0.6472,
      "step": 1095600
    },
    {
      "epoch": 17.20094191522763,
      "grad_norm": 3.7484211921691895,
      "learning_rate": 3.924941130298274e-05,
      "loss": 0.5701,
      "step": 1095700
    },
    {
      "epoch": 17.202511773940344,
      "grad_norm": 4.101646900177002,
      "learning_rate": 3.924843014128728e-05,
      "loss": 0.5833,
      "step": 1095800
    },
    {
      "epoch": 17.20408163265306,
      "grad_norm": 3.7810280323028564,
      "learning_rate": 3.924744897959184e-05,
      "loss": 0.6239,
      "step": 1095900
    },
    {
      "epoch": 17.205651491365778,
      "grad_norm": 4.3417158126831055,
      "learning_rate": 3.924646781789639e-05,
      "loss": 0.6452,
      "step": 1096000
    },
    {
      "epoch": 17.20722135007849,
      "grad_norm": 4.491786479949951,
      "learning_rate": 3.924548665620094e-05,
      "loss": 0.6099,
      "step": 1096100
    },
    {
      "epoch": 17.208791208791208,
      "grad_norm": 2.907484769821167,
      "learning_rate": 3.92445054945055e-05,
      "loss": 0.5694,
      "step": 1096200
    },
    {
      "epoch": 17.210361067503925,
      "grad_norm": 3.581082582473755,
      "learning_rate": 3.924352433281005e-05,
      "loss": 0.5985,
      "step": 1096300
    },
    {
      "epoch": 17.211930926216642,
      "grad_norm": 3.757235050201416,
      "learning_rate": 3.92425431711146e-05,
      "loss": 0.5863,
      "step": 1096400
    },
    {
      "epoch": 17.213500784929355,
      "grad_norm": 3.8556880950927734,
      "learning_rate": 3.924156200941915e-05,
      "loss": 0.6381,
      "step": 1096500
    },
    {
      "epoch": 17.215070643642072,
      "grad_norm": 3.757193088531494,
      "learning_rate": 3.924058084772371e-05,
      "loss": 0.5795,
      "step": 1096600
    },
    {
      "epoch": 17.21664050235479,
      "grad_norm": 3.877833127975464,
      "learning_rate": 3.923959968602826e-05,
      "loss": 0.6088,
      "step": 1096700
    },
    {
      "epoch": 17.218210361067506,
      "grad_norm": 4.610469818115234,
      "learning_rate": 3.923861852433281e-05,
      "loss": 0.6475,
      "step": 1096800
    },
    {
      "epoch": 17.21978021978022,
      "grad_norm": 4.079679489135742,
      "learning_rate": 3.923763736263736e-05,
      "loss": 0.6032,
      "step": 1096900
    },
    {
      "epoch": 17.221350078492936,
      "grad_norm": 4.310054779052734,
      "learning_rate": 3.923665620094192e-05,
      "loss": 0.6062,
      "step": 1097000
    },
    {
      "epoch": 17.222919937205653,
      "grad_norm": 2.5903050899505615,
      "learning_rate": 3.923567503924647e-05,
      "loss": 0.6085,
      "step": 1097100
    },
    {
      "epoch": 17.224489795918366,
      "grad_norm": 3.233001947402954,
      "learning_rate": 3.923469387755102e-05,
      "loss": 0.6253,
      "step": 1097200
    },
    {
      "epoch": 17.226059654631083,
      "grad_norm": 4.094986915588379,
      "learning_rate": 3.923371271585557e-05,
      "loss": 0.5694,
      "step": 1097300
    },
    {
      "epoch": 17.2276295133438,
      "grad_norm": 4.180394649505615,
      "learning_rate": 3.923273155416013e-05,
      "loss": 0.6641,
      "step": 1097400
    },
    {
      "epoch": 17.229199372056517,
      "grad_norm": 3.2754876613616943,
      "learning_rate": 3.9231750392464675e-05,
      "loss": 0.5964,
      "step": 1097500
    },
    {
      "epoch": 17.23076923076923,
      "grad_norm": 4.468984127044678,
      "learning_rate": 3.923076923076923e-05,
      "loss": 0.5998,
      "step": 1097600
    },
    {
      "epoch": 17.232339089481947,
      "grad_norm": 4.76255464553833,
      "learning_rate": 3.9229788069073784e-05,
      "loss": 0.6045,
      "step": 1097700
    },
    {
      "epoch": 17.233908948194664,
      "grad_norm": 5.0771260261535645,
      "learning_rate": 3.922880690737834e-05,
      "loss": 0.6251,
      "step": 1097800
    },
    {
      "epoch": 17.235478806907377,
      "grad_norm": 4.359558582305908,
      "learning_rate": 3.9227825745682886e-05,
      "loss": 0.6357,
      "step": 1097900
    },
    {
      "epoch": 17.237048665620094,
      "grad_norm": 3.9167094230651855,
      "learning_rate": 3.9226844583987444e-05,
      "loss": 0.6106,
      "step": 1098000
    },
    {
      "epoch": 17.23861852433281,
      "grad_norm": 3.0386619567871094,
      "learning_rate": 3.9225863422291995e-05,
      "loss": 0.6029,
      "step": 1098100
    },
    {
      "epoch": 17.240188383045528,
      "grad_norm": 4.2695794105529785,
      "learning_rate": 3.9224882260596546e-05,
      "loss": 0.562,
      "step": 1098200
    },
    {
      "epoch": 17.24175824175824,
      "grad_norm": 3.3182413578033447,
      "learning_rate": 3.9223901098901103e-05,
      "loss": 0.6245,
      "step": 1098300
    },
    {
      "epoch": 17.243328100470958,
      "grad_norm": 3.19059681892395,
      "learning_rate": 3.9222919937205654e-05,
      "loss": 0.6257,
      "step": 1098400
    },
    {
      "epoch": 17.244897959183675,
      "grad_norm": 3.8112056255340576,
      "learning_rate": 3.9221938775510205e-05,
      "loss": 0.5997,
      "step": 1098500
    },
    {
      "epoch": 17.246467817896388,
      "grad_norm": 29.520906448364258,
      "learning_rate": 3.9220957613814756e-05,
      "loss": 0.6241,
      "step": 1098600
    },
    {
      "epoch": 17.248037676609105,
      "grad_norm": 4.172863960266113,
      "learning_rate": 3.9219976452119314e-05,
      "loss": 0.597,
      "step": 1098700
    },
    {
      "epoch": 17.24960753532182,
      "grad_norm": 3.439277410507202,
      "learning_rate": 3.9218995290423865e-05,
      "loss": 0.591,
      "step": 1098800
    },
    {
      "epoch": 17.25117739403454,
      "grad_norm": 4.2127227783203125,
      "learning_rate": 3.9218014128728416e-05,
      "loss": 0.6107,
      "step": 1098900
    },
    {
      "epoch": 17.252747252747252,
      "grad_norm": 4.178818702697754,
      "learning_rate": 3.921703296703297e-05,
      "loss": 0.6229,
      "step": 1099000
    },
    {
      "epoch": 17.25431711145997,
      "grad_norm": 2.839247465133667,
      "learning_rate": 3.9216051805337525e-05,
      "loss": 0.5954,
      "step": 1099100
    },
    {
      "epoch": 17.255886970172686,
      "grad_norm": 3.0724921226501465,
      "learning_rate": 3.9215070643642076e-05,
      "loss": 0.625,
      "step": 1099200
    },
    {
      "epoch": 17.2574568288854,
      "grad_norm": 4.252447128295898,
      "learning_rate": 3.921408948194663e-05,
      "loss": 0.6429,
      "step": 1099300
    },
    {
      "epoch": 17.259026687598116,
      "grad_norm": 4.359589576721191,
      "learning_rate": 3.921310832025118e-05,
      "loss": 0.6049,
      "step": 1099400
    },
    {
      "epoch": 17.260596546310833,
      "grad_norm": 3.9751574993133545,
      "learning_rate": 3.9212127158555736e-05,
      "loss": 0.6153,
      "step": 1099500
    },
    {
      "epoch": 17.26216640502355,
      "grad_norm": 3.033520460128784,
      "learning_rate": 3.921114599686028e-05,
      "loss": 0.5992,
      "step": 1099600
    },
    {
      "epoch": 17.263736263736263,
      "grad_norm": 3.242705821990967,
      "learning_rate": 3.921016483516484e-05,
      "loss": 0.5902,
      "step": 1099700
    },
    {
      "epoch": 17.26530612244898,
      "grad_norm": 5.1123151779174805,
      "learning_rate": 3.920918367346939e-05,
      "loss": 0.6303,
      "step": 1099800
    },
    {
      "epoch": 17.266875981161697,
      "grad_norm": 4.728257656097412,
      "learning_rate": 3.9208202511773946e-05,
      "loss": 0.6185,
      "step": 1099900
    },
    {
      "epoch": 17.26844583987441,
      "grad_norm": 3.726388454437256,
      "learning_rate": 3.920722135007849e-05,
      "loss": 0.6267,
      "step": 1100000
    },
    {
      "epoch": 17.270015698587127,
      "grad_norm": 3.7073476314544678,
      "learning_rate": 3.920624018838305e-05,
      "loss": 0.5839,
      "step": 1100100
    },
    {
      "epoch": 17.271585557299844,
      "grad_norm": 4.169319152832031,
      "learning_rate": 3.92052590266876e-05,
      "loss": 0.5933,
      "step": 1100200
    },
    {
      "epoch": 17.27315541601256,
      "grad_norm": 5.474578857421875,
      "learning_rate": 3.920427786499215e-05,
      "loss": 0.6267,
      "step": 1100300
    },
    {
      "epoch": 17.274725274725274,
      "grad_norm": 4.131192207336426,
      "learning_rate": 3.920329670329671e-05,
      "loss": 0.5665,
      "step": 1100400
    },
    {
      "epoch": 17.27629513343799,
      "grad_norm": 5.32420015335083,
      "learning_rate": 3.920231554160126e-05,
      "loss": 0.572,
      "step": 1100500
    },
    {
      "epoch": 17.277864992150707,
      "grad_norm": 3.4754726886749268,
      "learning_rate": 3.920133437990581e-05,
      "loss": 0.5916,
      "step": 1100600
    },
    {
      "epoch": 17.27943485086342,
      "grad_norm": 3.4362998008728027,
      "learning_rate": 3.920035321821036e-05,
      "loss": 0.617,
      "step": 1100700
    },
    {
      "epoch": 17.281004709576138,
      "grad_norm": 3.9239065647125244,
      "learning_rate": 3.919937205651492e-05,
      "loss": 0.6093,
      "step": 1100800
    },
    {
      "epoch": 17.282574568288855,
      "grad_norm": 3.362178087234497,
      "learning_rate": 3.919839089481947e-05,
      "loss": 0.6186,
      "step": 1100900
    },
    {
      "epoch": 17.28414442700157,
      "grad_norm": 2.9185128211975098,
      "learning_rate": 3.919740973312402e-05,
      "loss": 0.5979,
      "step": 1101000
    },
    {
      "epoch": 17.285714285714285,
      "grad_norm": 4.136373519897461,
      "learning_rate": 3.919642857142857e-05,
      "loss": 0.6376,
      "step": 1101100
    },
    {
      "epoch": 17.287284144427,
      "grad_norm": 2.916771650314331,
      "learning_rate": 3.919544740973313e-05,
      "loss": 0.6107,
      "step": 1101200
    },
    {
      "epoch": 17.28885400313972,
      "grad_norm": 3.4872148036956787,
      "learning_rate": 3.919446624803768e-05,
      "loss": 0.604,
      "step": 1101300
    },
    {
      "epoch": 17.29042386185243,
      "grad_norm": 4.113574028015137,
      "learning_rate": 3.919348508634223e-05,
      "loss": 0.6105,
      "step": 1101400
    },
    {
      "epoch": 17.29199372056515,
      "grad_norm": 4.3203840255737305,
      "learning_rate": 3.919250392464678e-05,
      "loss": 0.6154,
      "step": 1101500
    },
    {
      "epoch": 17.293563579277865,
      "grad_norm": 3.5676522254943848,
      "learning_rate": 3.919152276295134e-05,
      "loss": 0.5946,
      "step": 1101600
    },
    {
      "epoch": 17.295133437990582,
      "grad_norm": 4.529278755187988,
      "learning_rate": 3.9190541601255884e-05,
      "loss": 0.6008,
      "step": 1101700
    },
    {
      "epoch": 17.296703296703296,
      "grad_norm": 4.8075785636901855,
      "learning_rate": 3.918956043956044e-05,
      "loss": 0.6346,
      "step": 1101800
    },
    {
      "epoch": 17.298273155416013,
      "grad_norm": 4.046856880187988,
      "learning_rate": 3.918857927786499e-05,
      "loss": 0.6175,
      "step": 1101900
    },
    {
      "epoch": 17.29984301412873,
      "grad_norm": 3.928542375564575,
      "learning_rate": 3.918759811616955e-05,
      "loss": 0.6753,
      "step": 1102000
    },
    {
      "epoch": 17.301412872841443,
      "grad_norm": 3.4263534545898438,
      "learning_rate": 3.9186616954474095e-05,
      "loss": 0.5956,
      "step": 1102100
    },
    {
      "epoch": 17.30298273155416,
      "grad_norm": 3.320267915725708,
      "learning_rate": 3.918563579277865e-05,
      "loss": 0.6061,
      "step": 1102200
    },
    {
      "epoch": 17.304552590266876,
      "grad_norm": 3.749216318130493,
      "learning_rate": 3.9184654631083204e-05,
      "loss": 0.6063,
      "step": 1102300
    },
    {
      "epoch": 17.306122448979593,
      "grad_norm": 4.135012626647949,
      "learning_rate": 3.9183673469387755e-05,
      "loss": 0.6206,
      "step": 1102400
    },
    {
      "epoch": 17.307692307692307,
      "grad_norm": 2.580462694168091,
      "learning_rate": 3.918269230769231e-05,
      "loss": 0.6024,
      "step": 1102500
    },
    {
      "epoch": 17.309262166405023,
      "grad_norm": 4.432753086090088,
      "learning_rate": 3.9181711145996863e-05,
      "loss": 0.6351,
      "step": 1102600
    },
    {
      "epoch": 17.31083202511774,
      "grad_norm": 4.268945217132568,
      "learning_rate": 3.9180729984301414e-05,
      "loss": 0.6156,
      "step": 1102700
    },
    {
      "epoch": 17.312401883830454,
      "grad_norm": 3.35129976272583,
      "learning_rate": 3.9179748822605965e-05,
      "loss": 0.6193,
      "step": 1102800
    },
    {
      "epoch": 17.31397174254317,
      "grad_norm": 3.5957300662994385,
      "learning_rate": 3.917876766091052e-05,
      "loss": 0.6156,
      "step": 1102900
    },
    {
      "epoch": 17.315541601255887,
      "grad_norm": 4.045999526977539,
      "learning_rate": 3.9177786499215074e-05,
      "loss": 0.6721,
      "step": 1103000
    },
    {
      "epoch": 17.317111459968604,
      "grad_norm": 4.018594264984131,
      "learning_rate": 3.9176805337519625e-05,
      "loss": 0.6596,
      "step": 1103100
    },
    {
      "epoch": 17.318681318681318,
      "grad_norm": 4.109140396118164,
      "learning_rate": 3.9175824175824176e-05,
      "loss": 0.5686,
      "step": 1103200
    },
    {
      "epoch": 17.320251177394034,
      "grad_norm": 3.687136650085449,
      "learning_rate": 3.9174843014128734e-05,
      "loss": 0.6283,
      "step": 1103300
    },
    {
      "epoch": 17.32182103610675,
      "grad_norm": 2.2990996837615967,
      "learning_rate": 3.917386185243328e-05,
      "loss": 0.6273,
      "step": 1103400
    },
    {
      "epoch": 17.323390894819465,
      "grad_norm": 4.024464130401611,
      "learning_rate": 3.9172880690737836e-05,
      "loss": 0.5828,
      "step": 1103500
    },
    {
      "epoch": 17.32496075353218,
      "grad_norm": 2.7527968883514404,
      "learning_rate": 3.917189952904239e-05,
      "loss": 0.6312,
      "step": 1103600
    },
    {
      "epoch": 17.3265306122449,
      "grad_norm": 3.357480764389038,
      "learning_rate": 3.9170918367346945e-05,
      "loss": 0.5731,
      "step": 1103700
    },
    {
      "epoch": 17.328100470957615,
      "grad_norm": 3.830691337585449,
      "learning_rate": 3.916993720565149e-05,
      "loss": 0.5812,
      "step": 1103800
    },
    {
      "epoch": 17.32967032967033,
      "grad_norm": 3.4340453147888184,
      "learning_rate": 3.9168956043956047e-05,
      "loss": 0.6325,
      "step": 1103900
    },
    {
      "epoch": 17.331240188383045,
      "grad_norm": 4.962686061859131,
      "learning_rate": 3.91679748822606e-05,
      "loss": 0.6545,
      "step": 1104000
    },
    {
      "epoch": 17.332810047095762,
      "grad_norm": 3.558624744415283,
      "learning_rate": 3.916699372056515e-05,
      "loss": 0.5871,
      "step": 1104100
    },
    {
      "epoch": 17.334379905808476,
      "grad_norm": 3.905426025390625,
      "learning_rate": 3.91660125588697e-05,
      "loss": 0.5613,
      "step": 1104200
    },
    {
      "epoch": 17.335949764521192,
      "grad_norm": 4.2061872482299805,
      "learning_rate": 3.916503139717426e-05,
      "loss": 0.6013,
      "step": 1104300
    },
    {
      "epoch": 17.33751962323391,
      "grad_norm": 3.1925530433654785,
      "learning_rate": 3.916405023547881e-05,
      "loss": 0.6258,
      "step": 1104400
    },
    {
      "epoch": 17.339089481946626,
      "grad_norm": 2.2800912857055664,
      "learning_rate": 3.916306907378336e-05,
      "loss": 0.6098,
      "step": 1104500
    },
    {
      "epoch": 17.34065934065934,
      "grad_norm": 45.373172760009766,
      "learning_rate": 3.916208791208792e-05,
      "loss": 0.6275,
      "step": 1104600
    },
    {
      "epoch": 17.342229199372056,
      "grad_norm": 4.503050327301025,
      "learning_rate": 3.916110675039247e-05,
      "loss": 0.5861,
      "step": 1104700
    },
    {
      "epoch": 17.343799058084773,
      "grad_norm": 4.383914470672607,
      "learning_rate": 3.916012558869702e-05,
      "loss": 0.6101,
      "step": 1104800
    },
    {
      "epoch": 17.345368916797486,
      "grad_norm": 3.840486526489258,
      "learning_rate": 3.915914442700157e-05,
      "loss": 0.6093,
      "step": 1104900
    },
    {
      "epoch": 17.346938775510203,
      "grad_norm": 3.862142562866211,
      "learning_rate": 3.915816326530613e-05,
      "loss": 0.5994,
      "step": 1105000
    },
    {
      "epoch": 17.34850863422292,
      "grad_norm": 3.568253755569458,
      "learning_rate": 3.915718210361068e-05,
      "loss": 0.6168,
      "step": 1105100
    },
    {
      "epoch": 17.350078492935637,
      "grad_norm": 3.6226930618286133,
      "learning_rate": 3.915620094191523e-05,
      "loss": 0.6414,
      "step": 1105200
    },
    {
      "epoch": 17.35164835164835,
      "grad_norm": 2.6708319187164307,
      "learning_rate": 3.915521978021978e-05,
      "loss": 0.5977,
      "step": 1105300
    },
    {
      "epoch": 17.353218210361067,
      "grad_norm": 4.370911121368408,
      "learning_rate": 3.915423861852434e-05,
      "loss": 0.618,
      "step": 1105400
    },
    {
      "epoch": 17.354788069073784,
      "grad_norm": 3.054482936859131,
      "learning_rate": 3.915325745682888e-05,
      "loss": 0.6317,
      "step": 1105500
    },
    {
      "epoch": 17.356357927786497,
      "grad_norm": 4.582507610321045,
      "learning_rate": 3.915227629513344e-05,
      "loss": 0.605,
      "step": 1105600
    },
    {
      "epoch": 17.357927786499214,
      "grad_norm": 2.7381532192230225,
      "learning_rate": 3.915129513343799e-05,
      "loss": 0.6029,
      "step": 1105700
    },
    {
      "epoch": 17.35949764521193,
      "grad_norm": 4.001704216003418,
      "learning_rate": 3.915031397174255e-05,
      "loss": 0.5865,
      "step": 1105800
    },
    {
      "epoch": 17.361067503924648,
      "grad_norm": 3.966963052749634,
      "learning_rate": 3.914933281004709e-05,
      "loss": 0.6011,
      "step": 1105900
    },
    {
      "epoch": 17.36263736263736,
      "grad_norm": 4.086675643920898,
      "learning_rate": 3.914835164835165e-05,
      "loss": 0.6088,
      "step": 1106000
    },
    {
      "epoch": 17.364207221350078,
      "grad_norm": 4.130178451538086,
      "learning_rate": 3.91473704866562e-05,
      "loss": 0.6053,
      "step": 1106100
    },
    {
      "epoch": 17.365777080062795,
      "grad_norm": 4.254973888397217,
      "learning_rate": 3.914638932496075e-05,
      "loss": 0.6146,
      "step": 1106200
    },
    {
      "epoch": 17.367346938775512,
      "grad_norm": 4.801390647888184,
      "learning_rate": 3.9145408163265304e-05,
      "loss": 0.6127,
      "step": 1106300
    },
    {
      "epoch": 17.368916797488225,
      "grad_norm": 3.1499505043029785,
      "learning_rate": 3.914442700156986e-05,
      "loss": 0.6484,
      "step": 1106400
    },
    {
      "epoch": 17.370486656200942,
      "grad_norm": 3.6321606636047363,
      "learning_rate": 3.914344583987441e-05,
      "loss": 0.5957,
      "step": 1106500
    },
    {
      "epoch": 17.37205651491366,
      "grad_norm": 3.351309299468994,
      "learning_rate": 3.9142464678178964e-05,
      "loss": 0.6338,
      "step": 1106600
    },
    {
      "epoch": 17.373626373626372,
      "grad_norm": 3.8820791244506836,
      "learning_rate": 3.914148351648352e-05,
      "loss": 0.6225,
      "step": 1106700
    },
    {
      "epoch": 17.37519623233909,
      "grad_norm": 3.5343589782714844,
      "learning_rate": 3.914050235478807e-05,
      "loss": 0.6056,
      "step": 1106800
    },
    {
      "epoch": 17.376766091051806,
      "grad_norm": 4.021559238433838,
      "learning_rate": 3.9139521193092623e-05,
      "loss": 0.6621,
      "step": 1106900
    },
    {
      "epoch": 17.378335949764523,
      "grad_norm": 3.826173782348633,
      "learning_rate": 3.9138540031397174e-05,
      "loss": 0.5976,
      "step": 1107000
    },
    {
      "epoch": 17.379905808477236,
      "grad_norm": 3.582601547241211,
      "learning_rate": 3.913755886970173e-05,
      "loss": 0.5999,
      "step": 1107100
    },
    {
      "epoch": 17.381475667189953,
      "grad_norm": 3.968083620071411,
      "learning_rate": 3.913657770800628e-05,
      "loss": 0.5787,
      "step": 1107200
    },
    {
      "epoch": 17.38304552590267,
      "grad_norm": 4.138933181762695,
      "learning_rate": 3.9135596546310834e-05,
      "loss": 0.6011,
      "step": 1107300
    },
    {
      "epoch": 17.384615384615383,
      "grad_norm": 3.1937568187713623,
      "learning_rate": 3.9134615384615385e-05,
      "loss": 0.5877,
      "step": 1107400
    },
    {
      "epoch": 17.3861852433281,
      "grad_norm": 3.96205997467041,
      "learning_rate": 3.913363422291994e-05,
      "loss": 0.5859,
      "step": 1107500
    },
    {
      "epoch": 17.387755102040817,
      "grad_norm": 3.2915377616882324,
      "learning_rate": 3.913265306122449e-05,
      "loss": 0.6132,
      "step": 1107600
    },
    {
      "epoch": 17.389324960753534,
      "grad_norm": 4.20498514175415,
      "learning_rate": 3.9131671899529045e-05,
      "loss": 0.6458,
      "step": 1107700
    },
    {
      "epoch": 17.390894819466247,
      "grad_norm": 3.4063642024993896,
      "learning_rate": 3.9130690737833596e-05,
      "loss": 0.6001,
      "step": 1107800
    },
    {
      "epoch": 17.392464678178964,
      "grad_norm": 2.779802083969116,
      "learning_rate": 3.9129709576138154e-05,
      "loss": 0.5979,
      "step": 1107900
    },
    {
      "epoch": 17.39403453689168,
      "grad_norm": 3.130963087081909,
      "learning_rate": 3.91287284144427e-05,
      "loss": 0.5982,
      "step": 1108000
    },
    {
      "epoch": 17.395604395604394,
      "grad_norm": 3.4469478130340576,
      "learning_rate": 3.9127747252747256e-05,
      "loss": 0.5905,
      "step": 1108100
    },
    {
      "epoch": 17.39717425431711,
      "grad_norm": 3.236896514892578,
      "learning_rate": 3.9126766091051807e-05,
      "loss": 0.5954,
      "step": 1108200
    },
    {
      "epoch": 17.398744113029828,
      "grad_norm": 2.9561567306518555,
      "learning_rate": 3.912578492935636e-05,
      "loss": 0.6109,
      "step": 1108300
    },
    {
      "epoch": 17.400313971742545,
      "grad_norm": 3.5830206871032715,
      "learning_rate": 3.912480376766091e-05,
      "loss": 0.6095,
      "step": 1108400
    },
    {
      "epoch": 17.401883830455258,
      "grad_norm": 3.6860873699188232,
      "learning_rate": 3.9123822605965466e-05,
      "loss": 0.6329,
      "step": 1108500
    },
    {
      "epoch": 17.403453689167975,
      "grad_norm": 3.0281147956848145,
      "learning_rate": 3.912284144427002e-05,
      "loss": 0.6357,
      "step": 1108600
    },
    {
      "epoch": 17.405023547880692,
      "grad_norm": 3.319368600845337,
      "learning_rate": 3.912186028257457e-05,
      "loss": 0.6289,
      "step": 1108700
    },
    {
      "epoch": 17.406593406593405,
      "grad_norm": 4.391334056854248,
      "learning_rate": 3.912087912087912e-05,
      "loss": 0.5698,
      "step": 1108800
    },
    {
      "epoch": 17.408163265306122,
      "grad_norm": 2.7086822986602783,
      "learning_rate": 3.911989795918368e-05,
      "loss": 0.5943,
      "step": 1108900
    },
    {
      "epoch": 17.40973312401884,
      "grad_norm": 4.503158092498779,
      "learning_rate": 3.911891679748823e-05,
      "loss": 0.6001,
      "step": 1109000
    },
    {
      "epoch": 17.411302982731556,
      "grad_norm": 4.143476486206055,
      "learning_rate": 3.911793563579278e-05,
      "loss": 0.6281,
      "step": 1109100
    },
    {
      "epoch": 17.41287284144427,
      "grad_norm": 3.2848026752471924,
      "learning_rate": 3.911695447409734e-05,
      "loss": 0.5873,
      "step": 1109200
    },
    {
      "epoch": 17.414442700156986,
      "grad_norm": 3.3518424034118652,
      "learning_rate": 3.911597331240189e-05,
      "loss": 0.6153,
      "step": 1109300
    },
    {
      "epoch": 17.416012558869703,
      "grad_norm": 4.043524265289307,
      "learning_rate": 3.911499215070644e-05,
      "loss": 0.6415,
      "step": 1109400
    },
    {
      "epoch": 17.417582417582416,
      "grad_norm": 4.077972412109375,
      "learning_rate": 3.911401098901099e-05,
      "loss": 0.5558,
      "step": 1109500
    },
    {
      "epoch": 17.419152276295133,
      "grad_norm": 3.823775053024292,
      "learning_rate": 3.911302982731555e-05,
      "loss": 0.58,
      "step": 1109600
    },
    {
      "epoch": 17.42072213500785,
      "grad_norm": 3.3935670852661133,
      "learning_rate": 3.911204866562009e-05,
      "loss": 0.6371,
      "step": 1109700
    },
    {
      "epoch": 17.422291993720567,
      "grad_norm": 4.064877510070801,
      "learning_rate": 3.911106750392465e-05,
      "loss": 0.6201,
      "step": 1109800
    },
    {
      "epoch": 17.42386185243328,
      "grad_norm": 3.758476734161377,
      "learning_rate": 3.91100863422292e-05,
      "loss": 0.5678,
      "step": 1109900
    },
    {
      "epoch": 17.425431711145997,
      "grad_norm": 4.37597131729126,
      "learning_rate": 3.910910518053376e-05,
      "loss": 0.593,
      "step": 1110000
    },
    {
      "epoch": 17.427001569858714,
      "grad_norm": 4.285313129425049,
      "learning_rate": 3.91081240188383e-05,
      "loss": 0.6142,
      "step": 1110100
    },
    {
      "epoch": 17.428571428571427,
      "grad_norm": 4.1824493408203125,
      "learning_rate": 3.910714285714286e-05,
      "loss": 0.5877,
      "step": 1110200
    },
    {
      "epoch": 17.430141287284144,
      "grad_norm": 3.8694493770599365,
      "learning_rate": 3.910616169544741e-05,
      "loss": 0.6107,
      "step": 1110300
    },
    {
      "epoch": 17.43171114599686,
      "grad_norm": 3.2888307571411133,
      "learning_rate": 3.910518053375196e-05,
      "loss": 0.6019,
      "step": 1110400
    },
    {
      "epoch": 17.433281004709578,
      "grad_norm": 3.9049129486083984,
      "learning_rate": 3.910419937205651e-05,
      "loss": 0.6,
      "step": 1110500
    },
    {
      "epoch": 17.43485086342229,
      "grad_norm": 3.8250460624694824,
      "learning_rate": 3.910321821036107e-05,
      "loss": 0.5995,
      "step": 1110600
    },
    {
      "epoch": 17.436420722135008,
      "grad_norm": 3.0482470989227295,
      "learning_rate": 3.910223704866562e-05,
      "loss": 0.6205,
      "step": 1110700
    },
    {
      "epoch": 17.437990580847725,
      "grad_norm": 4.071128845214844,
      "learning_rate": 3.910125588697017e-05,
      "loss": 0.6221,
      "step": 1110800
    },
    {
      "epoch": 17.439560439560438,
      "grad_norm": 4.421660900115967,
      "learning_rate": 3.9100274725274724e-05,
      "loss": 0.6115,
      "step": 1110900
    },
    {
      "epoch": 17.441130298273155,
      "grad_norm": 4.47487735748291,
      "learning_rate": 3.909929356357928e-05,
      "loss": 0.5983,
      "step": 1111000
    },
    {
      "epoch": 17.44270015698587,
      "grad_norm": 3.923837900161743,
      "learning_rate": 3.909831240188383e-05,
      "loss": 0.5772,
      "step": 1111100
    },
    {
      "epoch": 17.44427001569859,
      "grad_norm": 4.069458961486816,
      "learning_rate": 3.909733124018838e-05,
      "loss": 0.6211,
      "step": 1111200
    },
    {
      "epoch": 17.445839874411302,
      "grad_norm": 3.603114604949951,
      "learning_rate": 3.909635007849294e-05,
      "loss": 0.5723,
      "step": 1111300
    },
    {
      "epoch": 17.44740973312402,
      "grad_norm": 4.8945393562316895,
      "learning_rate": 3.909536891679749e-05,
      "loss": 0.6483,
      "step": 1111400
    },
    {
      "epoch": 17.448979591836736,
      "grad_norm": 3.9410948753356934,
      "learning_rate": 3.909438775510204e-05,
      "loss": 0.5743,
      "step": 1111500
    },
    {
      "epoch": 17.45054945054945,
      "grad_norm": 3.5479683876037598,
      "learning_rate": 3.9093406593406594e-05,
      "loss": 0.5966,
      "step": 1111600
    },
    {
      "epoch": 17.452119309262166,
      "grad_norm": 3.7512195110321045,
      "learning_rate": 3.909242543171115e-05,
      "loss": 0.6205,
      "step": 1111700
    },
    {
      "epoch": 17.453689167974883,
      "grad_norm": 3.435528039932251,
      "learning_rate": 3.9091444270015696e-05,
      "loss": 0.6117,
      "step": 1111800
    },
    {
      "epoch": 17.4552590266876,
      "grad_norm": 2.504580020904541,
      "learning_rate": 3.9090463108320254e-05,
      "loss": 0.6167,
      "step": 1111900
    },
    {
      "epoch": 17.456828885400313,
      "grad_norm": 3.590222120285034,
      "learning_rate": 3.9089481946624805e-05,
      "loss": 0.6016,
      "step": 1112000
    },
    {
      "epoch": 17.45839874411303,
      "grad_norm": 4.2319231033325195,
      "learning_rate": 3.908850078492936e-05,
      "loss": 0.6064,
      "step": 1112100
    },
    {
      "epoch": 17.459968602825747,
      "grad_norm": 3.494412899017334,
      "learning_rate": 3.908751962323391e-05,
      "loss": 0.6282,
      "step": 1112200
    },
    {
      "epoch": 17.46153846153846,
      "grad_norm": 5.262572765350342,
      "learning_rate": 3.9086538461538464e-05,
      "loss": 0.6423,
      "step": 1112300
    },
    {
      "epoch": 17.463108320251177,
      "grad_norm": 3.6011133193969727,
      "learning_rate": 3.9085557299843015e-05,
      "loss": 0.6497,
      "step": 1112400
    },
    {
      "epoch": 17.464678178963894,
      "grad_norm": 4.296894073486328,
      "learning_rate": 3.9084576138147566e-05,
      "loss": 0.5934,
      "step": 1112500
    },
    {
      "epoch": 17.46624803767661,
      "grad_norm": 4.047599792480469,
      "learning_rate": 3.908359497645212e-05,
      "loss": 0.6164,
      "step": 1112600
    },
    {
      "epoch": 17.467817896389324,
      "grad_norm": 4.037300109863281,
      "learning_rate": 3.9082613814756675e-05,
      "loss": 0.6186,
      "step": 1112700
    },
    {
      "epoch": 17.46938775510204,
      "grad_norm": 2.5185651779174805,
      "learning_rate": 3.9081632653061226e-05,
      "loss": 0.6307,
      "step": 1112800
    },
    {
      "epoch": 17.470957613814758,
      "grad_norm": 4.607117176055908,
      "learning_rate": 3.908065149136578e-05,
      "loss": 0.5794,
      "step": 1112900
    },
    {
      "epoch": 17.47252747252747,
      "grad_norm": 4.186953544616699,
      "learning_rate": 3.907967032967033e-05,
      "loss": 0.6242,
      "step": 1113000
    },
    {
      "epoch": 17.474097331240188,
      "grad_norm": 3.883972406387329,
      "learning_rate": 3.9078689167974886e-05,
      "loss": 0.6182,
      "step": 1113100
    },
    {
      "epoch": 17.475667189952905,
      "grad_norm": 3.8770322799682617,
      "learning_rate": 3.907770800627944e-05,
      "loss": 0.6092,
      "step": 1113200
    },
    {
      "epoch": 17.47723704866562,
      "grad_norm": 4.427590370178223,
      "learning_rate": 3.907672684458399e-05,
      "loss": 0.6134,
      "step": 1113300
    },
    {
      "epoch": 17.478806907378335,
      "grad_norm": 4.172593593597412,
      "learning_rate": 3.9075745682888546e-05,
      "loss": 0.5959,
      "step": 1113400
    },
    {
      "epoch": 17.48037676609105,
      "grad_norm": 2.6094183921813965,
      "learning_rate": 3.9074764521193097e-05,
      "loss": 0.6189,
      "step": 1113500
    },
    {
      "epoch": 17.48194662480377,
      "grad_norm": 3.792405605316162,
      "learning_rate": 3.907378335949765e-05,
      "loss": 0.6101,
      "step": 1113600
    },
    {
      "epoch": 17.483516483516482,
      "grad_norm": 3.52955961227417,
      "learning_rate": 3.90728021978022e-05,
      "loss": 0.5826,
      "step": 1113700
    },
    {
      "epoch": 17.4850863422292,
      "grad_norm": 2.209657669067383,
      "learning_rate": 3.9071821036106756e-05,
      "loss": 0.5873,
      "step": 1113800
    },
    {
      "epoch": 17.486656200941916,
      "grad_norm": 3.061767578125,
      "learning_rate": 3.90708398744113e-05,
      "loss": 0.6142,
      "step": 1113900
    },
    {
      "epoch": 17.488226059654632,
      "grad_norm": 4.089900493621826,
      "learning_rate": 3.906985871271586e-05,
      "loss": 0.6398,
      "step": 1114000
    },
    {
      "epoch": 17.489795918367346,
      "grad_norm": 4.036391735076904,
      "learning_rate": 3.906887755102041e-05,
      "loss": 0.6236,
      "step": 1114100
    },
    {
      "epoch": 17.491365777080063,
      "grad_norm": 3.413364887237549,
      "learning_rate": 3.906789638932497e-05,
      "loss": 0.6147,
      "step": 1114200
    },
    {
      "epoch": 17.49293563579278,
      "grad_norm": 2.5654242038726807,
      "learning_rate": 3.906691522762951e-05,
      "loss": 0.5888,
      "step": 1114300
    },
    {
      "epoch": 17.494505494505496,
      "grad_norm": 3.8674933910369873,
      "learning_rate": 3.906593406593407e-05,
      "loss": 0.5883,
      "step": 1114400
    },
    {
      "epoch": 17.49607535321821,
      "grad_norm": 3.9030649662017822,
      "learning_rate": 3.906495290423862e-05,
      "loss": 0.6533,
      "step": 1114500
    },
    {
      "epoch": 17.497645211930926,
      "grad_norm": 3.8227176666259766,
      "learning_rate": 3.906397174254317e-05,
      "loss": 0.6047,
      "step": 1114600
    },
    {
      "epoch": 17.499215070643643,
      "grad_norm": 3.1466667652130127,
      "learning_rate": 3.906299058084772e-05,
      "loss": 0.6403,
      "step": 1114700
    },
    {
      "epoch": 17.500784929356357,
      "grad_norm": 3.523547649383545,
      "learning_rate": 3.906200941915228e-05,
      "loss": 0.61,
      "step": 1114800
    },
    {
      "epoch": 17.502354788069074,
      "grad_norm": 3.6574764251708984,
      "learning_rate": 3.906102825745683e-05,
      "loss": 0.6169,
      "step": 1114900
    },
    {
      "epoch": 17.50392464678179,
      "grad_norm": 4.219527721405029,
      "learning_rate": 3.906004709576138e-05,
      "loss": 0.6277,
      "step": 1115000
    },
    {
      "epoch": 17.505494505494504,
      "grad_norm": 4.408730506896973,
      "learning_rate": 3.905906593406593e-05,
      "loss": 0.5704,
      "step": 1115100
    },
    {
      "epoch": 17.50706436420722,
      "grad_norm": 3.1728110313415527,
      "learning_rate": 3.905808477237049e-05,
      "loss": 0.631,
      "step": 1115200
    },
    {
      "epoch": 17.508634222919937,
      "grad_norm": 4.145878314971924,
      "learning_rate": 3.905710361067504e-05,
      "loss": 0.6153,
      "step": 1115300
    },
    {
      "epoch": 17.510204081632654,
      "grad_norm": 2.9029526710510254,
      "learning_rate": 3.905612244897959e-05,
      "loss": 0.6367,
      "step": 1115400
    },
    {
      "epoch": 17.511773940345368,
      "grad_norm": 2.362238883972168,
      "learning_rate": 3.905514128728415e-05,
      "loss": 0.5937,
      "step": 1115500
    },
    {
      "epoch": 17.513343799058084,
      "grad_norm": 3.7591664791107178,
      "learning_rate": 3.90541601255887e-05,
      "loss": 0.633,
      "step": 1115600
    },
    {
      "epoch": 17.5149136577708,
      "grad_norm": 3.3972721099853516,
      "learning_rate": 3.905317896389325e-05,
      "loss": 0.6078,
      "step": 1115700
    },
    {
      "epoch": 17.516483516483518,
      "grad_norm": 3.368645191192627,
      "learning_rate": 3.90521978021978e-05,
      "loss": 0.65,
      "step": 1115800
    },
    {
      "epoch": 17.51805337519623,
      "grad_norm": 2.9276421070098877,
      "learning_rate": 3.905121664050236e-05,
      "loss": 0.5853,
      "step": 1115900
    },
    {
      "epoch": 17.51962323390895,
      "grad_norm": 2.2513885498046875,
      "learning_rate": 3.9050235478806905e-05,
      "loss": 0.6285,
      "step": 1116000
    },
    {
      "epoch": 17.521193092621665,
      "grad_norm": 3.6244444847106934,
      "learning_rate": 3.904925431711146e-05,
      "loss": 0.6184,
      "step": 1116100
    },
    {
      "epoch": 17.52276295133438,
      "grad_norm": 4.601935386657715,
      "learning_rate": 3.9048273155416014e-05,
      "loss": 0.6552,
      "step": 1116200
    },
    {
      "epoch": 17.524332810047095,
      "grad_norm": 3.6609716415405273,
      "learning_rate": 3.904729199372057e-05,
      "loss": 0.5568,
      "step": 1116300
    },
    {
      "epoch": 17.525902668759812,
      "grad_norm": 3.8810482025146484,
      "learning_rate": 3.9046310832025116e-05,
      "loss": 0.6054,
      "step": 1116400
    },
    {
      "epoch": 17.52747252747253,
      "grad_norm": 4.025224685668945,
      "learning_rate": 3.9045329670329673e-05,
      "loss": 0.5905,
      "step": 1116500
    },
    {
      "epoch": 17.529042386185242,
      "grad_norm": 3.0377426147460938,
      "learning_rate": 3.9044348508634224e-05,
      "loss": 0.5865,
      "step": 1116600
    },
    {
      "epoch": 17.53061224489796,
      "grad_norm": 3.775268793106079,
      "learning_rate": 3.9043367346938775e-05,
      "loss": 0.6349,
      "step": 1116700
    },
    {
      "epoch": 17.532182103610676,
      "grad_norm": 3.9938108921051025,
      "learning_rate": 3.9042386185243326e-05,
      "loss": 0.5745,
      "step": 1116800
    },
    {
      "epoch": 17.53375196232339,
      "grad_norm": 4.38679838180542,
      "learning_rate": 3.9041405023547884e-05,
      "loss": 0.6039,
      "step": 1116900
    },
    {
      "epoch": 17.535321821036106,
      "grad_norm": 3.846593141555786,
      "learning_rate": 3.9040423861852435e-05,
      "loss": 0.6058,
      "step": 1117000
    },
    {
      "epoch": 17.536891679748823,
      "grad_norm": 5.020160675048828,
      "learning_rate": 3.9039442700156986e-05,
      "loss": 0.6459,
      "step": 1117100
    },
    {
      "epoch": 17.53846153846154,
      "grad_norm": 4.186480522155762,
      "learning_rate": 3.903846153846154e-05,
      "loss": 0.5702,
      "step": 1117200
    },
    {
      "epoch": 17.540031397174253,
      "grad_norm": 3.920754909515381,
      "learning_rate": 3.9037480376766095e-05,
      "loss": 0.6134,
      "step": 1117300
    },
    {
      "epoch": 17.54160125588697,
      "grad_norm": 3.861133098602295,
      "learning_rate": 3.9036499215070646e-05,
      "loss": 0.5803,
      "step": 1117400
    },
    {
      "epoch": 17.543171114599687,
      "grad_norm": 3.941166877746582,
      "learning_rate": 3.90355180533752e-05,
      "loss": 0.6322,
      "step": 1117500
    },
    {
      "epoch": 17.5447409733124,
      "grad_norm": 4.186564922332764,
      "learning_rate": 3.9034536891679755e-05,
      "loss": 0.6336,
      "step": 1117600
    },
    {
      "epoch": 17.546310832025117,
      "grad_norm": 3.8716213703155518,
      "learning_rate": 3.9033555729984306e-05,
      "loss": 0.5649,
      "step": 1117700
    },
    {
      "epoch": 17.547880690737834,
      "grad_norm": 3.501328945159912,
      "learning_rate": 3.9032574568288857e-05,
      "loss": 0.606,
      "step": 1117800
    },
    {
      "epoch": 17.54945054945055,
      "grad_norm": 2.946129083633423,
      "learning_rate": 3.903159340659341e-05,
      "loss": 0.6329,
      "step": 1117900
    },
    {
      "epoch": 17.551020408163264,
      "grad_norm": 4.16188907623291,
      "learning_rate": 3.9030612244897965e-05,
      "loss": 0.6028,
      "step": 1118000
    },
    {
      "epoch": 17.55259026687598,
      "grad_norm": 5.167304039001465,
      "learning_rate": 3.902963108320251e-05,
      "loss": 0.5668,
      "step": 1118100
    },
    {
      "epoch": 17.554160125588698,
      "grad_norm": 4.343939781188965,
      "learning_rate": 3.902864992150707e-05,
      "loss": 0.6464,
      "step": 1118200
    },
    {
      "epoch": 17.55572998430141,
      "grad_norm": 4.269657135009766,
      "learning_rate": 3.902766875981162e-05,
      "loss": 0.6107,
      "step": 1118300
    },
    {
      "epoch": 17.55729984301413,
      "grad_norm": 3.82700514793396,
      "learning_rate": 3.9026687598116176e-05,
      "loss": 0.6254,
      "step": 1118400
    },
    {
      "epoch": 17.558869701726845,
      "grad_norm": 5.136186599731445,
      "learning_rate": 3.902570643642072e-05,
      "loss": 0.6166,
      "step": 1118500
    },
    {
      "epoch": 17.560439560439562,
      "grad_norm": 4.060443878173828,
      "learning_rate": 3.902472527472528e-05,
      "loss": 0.6198,
      "step": 1118600
    },
    {
      "epoch": 17.562009419152275,
      "grad_norm": 4.675360679626465,
      "learning_rate": 3.902374411302983e-05,
      "loss": 0.6005,
      "step": 1118700
    },
    {
      "epoch": 17.563579277864992,
      "grad_norm": 3.177098274230957,
      "learning_rate": 3.902276295133438e-05,
      "loss": 0.6156,
      "step": 1118800
    },
    {
      "epoch": 17.56514913657771,
      "grad_norm": 3.8880257606506348,
      "learning_rate": 3.902178178963893e-05,
      "loss": 0.6023,
      "step": 1118900
    },
    {
      "epoch": 17.566718995290422,
      "grad_norm": 3.9264414310455322,
      "learning_rate": 3.902080062794349e-05,
      "loss": 0.6275,
      "step": 1119000
    },
    {
      "epoch": 17.56828885400314,
      "grad_norm": 4.262279987335205,
      "learning_rate": 3.901981946624804e-05,
      "loss": 0.5863,
      "step": 1119100
    },
    {
      "epoch": 17.569858712715856,
      "grad_norm": 3.869159698486328,
      "learning_rate": 3.901883830455259e-05,
      "loss": 0.6356,
      "step": 1119200
    },
    {
      "epoch": 17.571428571428573,
      "grad_norm": 3.188450574874878,
      "learning_rate": 3.901785714285714e-05,
      "loss": 0.6583,
      "step": 1119300
    },
    {
      "epoch": 17.572998430141286,
      "grad_norm": 4.791979789733887,
      "learning_rate": 3.90168759811617e-05,
      "loss": 0.6178,
      "step": 1119400
    },
    {
      "epoch": 17.574568288854003,
      "grad_norm": 3.779468059539795,
      "learning_rate": 3.901589481946625e-05,
      "loss": 0.6,
      "step": 1119500
    },
    {
      "epoch": 17.57613814756672,
      "grad_norm": 4.888011932373047,
      "learning_rate": 3.90149136577708e-05,
      "loss": 0.6453,
      "step": 1119600
    },
    {
      "epoch": 17.577708006279433,
      "grad_norm": 4.650528907775879,
      "learning_rate": 3.901393249607536e-05,
      "loss": 0.6525,
      "step": 1119700
    },
    {
      "epoch": 17.57927786499215,
      "grad_norm": 4.246601104736328,
      "learning_rate": 3.901295133437991e-05,
      "loss": 0.6277,
      "step": 1119800
    },
    {
      "epoch": 17.580847723704867,
      "grad_norm": 4.054541110992432,
      "learning_rate": 3.901197017268446e-05,
      "loss": 0.6096,
      "step": 1119900
    },
    {
      "epoch": 17.582417582417584,
      "grad_norm": 4.526790618896484,
      "learning_rate": 3.901098901098901e-05,
      "loss": 0.601,
      "step": 1120000
    },
    {
      "epoch": 17.583987441130297,
      "grad_norm": 2.320004940032959,
      "learning_rate": 3.901000784929357e-05,
      "loss": 0.6218,
      "step": 1120100
    },
    {
      "epoch": 17.585557299843014,
      "grad_norm": 2.9642157554626465,
      "learning_rate": 3.9009026687598114e-05,
      "loss": 0.577,
      "step": 1120200
    },
    {
      "epoch": 17.58712715855573,
      "grad_norm": 3.7712321281433105,
      "learning_rate": 3.900804552590267e-05,
      "loss": 0.6372,
      "step": 1120300
    },
    {
      "epoch": 17.588697017268444,
      "grad_norm": 2.9601941108703613,
      "learning_rate": 3.900706436420722e-05,
      "loss": 0.6552,
      "step": 1120400
    },
    {
      "epoch": 17.59026687598116,
      "grad_norm": 3.5364179611206055,
      "learning_rate": 3.900608320251178e-05,
      "loss": 0.6124,
      "step": 1120500
    },
    {
      "epoch": 17.591836734693878,
      "grad_norm": 4.448805332183838,
      "learning_rate": 3.9005102040816325e-05,
      "loss": 0.5908,
      "step": 1120600
    },
    {
      "epoch": 17.593406593406595,
      "grad_norm": 3.6641156673431396,
      "learning_rate": 3.900412087912088e-05,
      "loss": 0.6146,
      "step": 1120700
    },
    {
      "epoch": 17.594976452119308,
      "grad_norm": 3.521320343017578,
      "learning_rate": 3.9003139717425433e-05,
      "loss": 0.6059,
      "step": 1120800
    },
    {
      "epoch": 17.596546310832025,
      "grad_norm": 4.074937343597412,
      "learning_rate": 3.9002158555729984e-05,
      "loss": 0.6318,
      "step": 1120900
    },
    {
      "epoch": 17.598116169544742,
      "grad_norm": 4.710938930511475,
      "learning_rate": 3.9001177394034535e-05,
      "loss": 0.649,
      "step": 1121000
    },
    {
      "epoch": 17.599686028257455,
      "grad_norm": 4.763833522796631,
      "learning_rate": 3.900019623233909e-05,
      "loss": 0.6119,
      "step": 1121100
    },
    {
      "epoch": 17.601255886970172,
      "grad_norm": 4.435962677001953,
      "learning_rate": 3.8999215070643644e-05,
      "loss": 0.6287,
      "step": 1121200
    },
    {
      "epoch": 17.60282574568289,
      "grad_norm": 3.810964822769165,
      "learning_rate": 3.8998233908948195e-05,
      "loss": 0.6229,
      "step": 1121300
    },
    {
      "epoch": 17.604395604395606,
      "grad_norm": 5.452297210693359,
      "learning_rate": 3.8997252747252746e-05,
      "loss": 0.6326,
      "step": 1121400
    },
    {
      "epoch": 17.60596546310832,
      "grad_norm": 4.488702774047852,
      "learning_rate": 3.8996271585557304e-05,
      "loss": 0.5768,
      "step": 1121500
    },
    {
      "epoch": 17.607535321821036,
      "grad_norm": 3.9309709072113037,
      "learning_rate": 3.8995290423861855e-05,
      "loss": 0.6271,
      "step": 1121600
    },
    {
      "epoch": 17.609105180533753,
      "grad_norm": 4.04487943649292,
      "learning_rate": 3.8994309262166406e-05,
      "loss": 0.6436,
      "step": 1121700
    },
    {
      "epoch": 17.610675039246466,
      "grad_norm": 4.765319347381592,
      "learning_rate": 3.8993328100470964e-05,
      "loss": 0.6214,
      "step": 1121800
    },
    {
      "epoch": 17.612244897959183,
      "grad_norm": 4.000596523284912,
      "learning_rate": 3.8992346938775515e-05,
      "loss": 0.6127,
      "step": 1121900
    },
    {
      "epoch": 17.6138147566719,
      "grad_norm": 3.4997963905334473,
      "learning_rate": 3.8991365777080066e-05,
      "loss": 0.5855,
      "step": 1122000
    },
    {
      "epoch": 17.615384615384617,
      "grad_norm": 3.7915115356445312,
      "learning_rate": 3.8990384615384617e-05,
      "loss": 0.6091,
      "step": 1122100
    },
    {
      "epoch": 17.61695447409733,
      "grad_norm": 4.771917343139648,
      "learning_rate": 3.8989403453689174e-05,
      "loss": 0.623,
      "step": 1122200
    },
    {
      "epoch": 17.618524332810047,
      "grad_norm": 3.7860162258148193,
      "learning_rate": 3.898842229199372e-05,
      "loss": 0.6443,
      "step": 1122300
    },
    {
      "epoch": 17.620094191522764,
      "grad_norm": 4.471182823181152,
      "learning_rate": 3.8987441130298276e-05,
      "loss": 0.6051,
      "step": 1122400
    },
    {
      "epoch": 17.621664050235477,
      "grad_norm": 5.303216934204102,
      "learning_rate": 3.898645996860283e-05,
      "loss": 0.6295,
      "step": 1122500
    },
    {
      "epoch": 17.623233908948194,
      "grad_norm": 4.7486467361450195,
      "learning_rate": 3.8985478806907385e-05,
      "loss": 0.6318,
      "step": 1122600
    },
    {
      "epoch": 17.62480376766091,
      "grad_norm": 4.1363844871521,
      "learning_rate": 3.898449764521193e-05,
      "loss": 0.6366,
      "step": 1122700
    },
    {
      "epoch": 17.626373626373628,
      "grad_norm": 4.326596260070801,
      "learning_rate": 3.898351648351649e-05,
      "loss": 0.6475,
      "step": 1122800
    },
    {
      "epoch": 17.62794348508634,
      "grad_norm": 2.9934372901916504,
      "learning_rate": 3.898253532182104e-05,
      "loss": 0.6094,
      "step": 1122900
    },
    {
      "epoch": 17.629513343799058,
      "grad_norm": 3.6616435050964355,
      "learning_rate": 3.898155416012559e-05,
      "loss": 0.6518,
      "step": 1123000
    },
    {
      "epoch": 17.631083202511775,
      "grad_norm": 3.448796272277832,
      "learning_rate": 3.898057299843014e-05,
      "loss": 0.6077,
      "step": 1123100
    },
    {
      "epoch": 17.632653061224488,
      "grad_norm": 3.294595718383789,
      "learning_rate": 3.89795918367347e-05,
      "loss": 0.5834,
      "step": 1123200
    },
    {
      "epoch": 17.634222919937205,
      "grad_norm": 3.2400588989257812,
      "learning_rate": 3.897861067503925e-05,
      "loss": 0.6097,
      "step": 1123300
    },
    {
      "epoch": 17.635792778649922,
      "grad_norm": 4.175906181335449,
      "learning_rate": 3.89776295133438e-05,
      "loss": 0.5934,
      "step": 1123400
    },
    {
      "epoch": 17.63736263736264,
      "grad_norm": 4.061769962310791,
      "learning_rate": 3.897664835164835e-05,
      "loss": 0.6281,
      "step": 1123500
    },
    {
      "epoch": 17.638932496075352,
      "grad_norm": 5.16428804397583,
      "learning_rate": 3.897566718995291e-05,
      "loss": 0.5887,
      "step": 1123600
    },
    {
      "epoch": 17.64050235478807,
      "grad_norm": 2.8222713470458984,
      "learning_rate": 3.897468602825746e-05,
      "loss": 0.559,
      "step": 1123700
    },
    {
      "epoch": 17.642072213500786,
      "grad_norm": 4.191417694091797,
      "learning_rate": 3.897370486656201e-05,
      "loss": 0.6149,
      "step": 1123800
    },
    {
      "epoch": 17.643642072213503,
      "grad_norm": 4.126617431640625,
      "learning_rate": 3.897272370486657e-05,
      "loss": 0.5864,
      "step": 1123900
    },
    {
      "epoch": 17.645211930926216,
      "grad_norm": 4.369467735290527,
      "learning_rate": 3.897174254317112e-05,
      "loss": 0.5867,
      "step": 1124000
    },
    {
      "epoch": 17.646781789638933,
      "grad_norm": 3.5557374954223633,
      "learning_rate": 3.897076138147567e-05,
      "loss": 0.5864,
      "step": 1124100
    },
    {
      "epoch": 17.64835164835165,
      "grad_norm": 4.514414310455322,
      "learning_rate": 3.896978021978022e-05,
      "loss": 0.6575,
      "step": 1124200
    },
    {
      "epoch": 17.649921507064363,
      "grad_norm": 4.492495059967041,
      "learning_rate": 3.896879905808478e-05,
      "loss": 0.6003,
      "step": 1124300
    },
    {
      "epoch": 17.65149136577708,
      "grad_norm": 3.812293529510498,
      "learning_rate": 3.896781789638932e-05,
      "loss": 0.5689,
      "step": 1124400
    },
    {
      "epoch": 17.653061224489797,
      "grad_norm": 3.5650134086608887,
      "learning_rate": 3.896683673469388e-05,
      "loss": 0.5853,
      "step": 1124500
    },
    {
      "epoch": 17.65463108320251,
      "grad_norm": 4.674851894378662,
      "learning_rate": 3.896585557299843e-05,
      "loss": 0.6384,
      "step": 1124600
    },
    {
      "epoch": 17.656200941915227,
      "grad_norm": 3.9151813983917236,
      "learning_rate": 3.896487441130299e-05,
      "loss": 0.6545,
      "step": 1124700
    },
    {
      "epoch": 17.657770800627944,
      "grad_norm": 4.525876045227051,
      "learning_rate": 3.8963893249607534e-05,
      "loss": 0.6188,
      "step": 1124800
    },
    {
      "epoch": 17.65934065934066,
      "grad_norm": 3.436603546142578,
      "learning_rate": 3.896291208791209e-05,
      "loss": 0.6251,
      "step": 1124900
    },
    {
      "epoch": 17.660910518053374,
      "grad_norm": 3.5579864978790283,
      "learning_rate": 3.896193092621664e-05,
      "loss": 0.6378,
      "step": 1125000
    },
    {
      "epoch": 17.66248037676609,
      "grad_norm": 2.7488391399383545,
      "learning_rate": 3.8960949764521193e-05,
      "loss": 0.5804,
      "step": 1125100
    },
    {
      "epoch": 17.664050235478808,
      "grad_norm": 3.4819629192352295,
      "learning_rate": 3.8959968602825744e-05,
      "loss": 0.6362,
      "step": 1125200
    },
    {
      "epoch": 17.665620094191524,
      "grad_norm": 5.6863017082214355,
      "learning_rate": 3.89589874411303e-05,
      "loss": 0.6314,
      "step": 1125300
    },
    {
      "epoch": 17.667189952904238,
      "grad_norm": 3.8948540687561035,
      "learning_rate": 3.895800627943485e-05,
      "loss": 0.6262,
      "step": 1125400
    },
    {
      "epoch": 17.668759811616955,
      "grad_norm": 4.123859882354736,
      "learning_rate": 3.8957025117739404e-05,
      "loss": 0.5674,
      "step": 1125500
    },
    {
      "epoch": 17.67032967032967,
      "grad_norm": 4.062071800231934,
      "learning_rate": 3.8956043956043955e-05,
      "loss": 0.6235,
      "step": 1125600
    },
    {
      "epoch": 17.671899529042385,
      "grad_norm": 3.986006259918213,
      "learning_rate": 3.895506279434851e-05,
      "loss": 0.6263,
      "step": 1125700
    },
    {
      "epoch": 17.6734693877551,
      "grad_norm": 2.8925845623016357,
      "learning_rate": 3.8954081632653064e-05,
      "loss": 0.5928,
      "step": 1125800
    },
    {
      "epoch": 17.67503924646782,
      "grad_norm": 4.058326244354248,
      "learning_rate": 3.8953100470957615e-05,
      "loss": 0.6171,
      "step": 1125900
    },
    {
      "epoch": 17.676609105180535,
      "grad_norm": 3.3764262199401855,
      "learning_rate": 3.895211930926217e-05,
      "loss": 0.5898,
      "step": 1126000
    },
    {
      "epoch": 17.67817896389325,
      "grad_norm": 3.960454225540161,
      "learning_rate": 3.8951138147566724e-05,
      "loss": 0.5798,
      "step": 1126100
    },
    {
      "epoch": 17.679748822605966,
      "grad_norm": 3.7479288578033447,
      "learning_rate": 3.8950156985871275e-05,
      "loss": 0.6097,
      "step": 1126200
    },
    {
      "epoch": 17.681318681318682,
      "grad_norm": 4.326648712158203,
      "learning_rate": 3.8949175824175826e-05,
      "loss": 0.6488,
      "step": 1126300
    },
    {
      "epoch": 17.682888540031396,
      "grad_norm": 4.291300296783447,
      "learning_rate": 3.894819466248038e-05,
      "loss": 0.6148,
      "step": 1126400
    },
    {
      "epoch": 17.684458398744113,
      "grad_norm": 5.329039573669434,
      "learning_rate": 3.894721350078493e-05,
      "loss": 0.6094,
      "step": 1126500
    },
    {
      "epoch": 17.68602825745683,
      "grad_norm": 4.120333194732666,
      "learning_rate": 3.8946232339089485e-05,
      "loss": 0.638,
      "step": 1126600
    },
    {
      "epoch": 17.687598116169546,
      "grad_norm": 4.3560614585876465,
      "learning_rate": 3.8945251177394036e-05,
      "loss": 0.6231,
      "step": 1126700
    },
    {
      "epoch": 17.68916797488226,
      "grad_norm": 4.052578449249268,
      "learning_rate": 3.894427001569859e-05,
      "loss": 0.6129,
      "step": 1126800
    },
    {
      "epoch": 17.690737833594977,
      "grad_norm": 4.906250953674316,
      "learning_rate": 3.894328885400314e-05,
      "loss": 0.622,
      "step": 1126900
    },
    {
      "epoch": 17.692307692307693,
      "grad_norm": 4.303584575653076,
      "learning_rate": 3.8942307692307696e-05,
      "loss": 0.6036,
      "step": 1127000
    },
    {
      "epoch": 17.693877551020407,
      "grad_norm": 4.064945220947266,
      "learning_rate": 3.894132653061225e-05,
      "loss": 0.6251,
      "step": 1127100
    },
    {
      "epoch": 17.695447409733124,
      "grad_norm": 4.2996826171875,
      "learning_rate": 3.89403453689168e-05,
      "loss": 0.612,
      "step": 1127200
    },
    {
      "epoch": 17.69701726844584,
      "grad_norm": 2.997692823410034,
      "learning_rate": 3.893936420722135e-05,
      "loss": 0.5689,
      "step": 1127300
    },
    {
      "epoch": 17.698587127158557,
      "grad_norm": 2.260427951812744,
      "learning_rate": 3.893838304552591e-05,
      "loss": 0.602,
      "step": 1127400
    },
    {
      "epoch": 17.70015698587127,
      "grad_norm": 3.7839925289154053,
      "learning_rate": 3.893740188383046e-05,
      "loss": 0.6071,
      "step": 1127500
    },
    {
      "epoch": 17.701726844583987,
      "grad_norm": 2.3354601860046387,
      "learning_rate": 3.893642072213501e-05,
      "loss": 0.6119,
      "step": 1127600
    },
    {
      "epoch": 17.703296703296704,
      "grad_norm": 3.2061665058135986,
      "learning_rate": 3.893543956043956e-05,
      "loss": 0.5944,
      "step": 1127700
    },
    {
      "epoch": 17.704866562009418,
      "grad_norm": 4.180512428283691,
      "learning_rate": 3.893445839874412e-05,
      "loss": 0.6218,
      "step": 1127800
    },
    {
      "epoch": 17.706436420722135,
      "grad_norm": 3.5173704624176025,
      "learning_rate": 3.893347723704867e-05,
      "loss": 0.6049,
      "step": 1127900
    },
    {
      "epoch": 17.70800627943485,
      "grad_norm": 2.46793270111084,
      "learning_rate": 3.893249607535322e-05,
      "loss": 0.6344,
      "step": 1128000
    },
    {
      "epoch": 17.70957613814757,
      "grad_norm": 3.587083101272583,
      "learning_rate": 3.893151491365778e-05,
      "loss": 0.6241,
      "step": 1128100
    },
    {
      "epoch": 17.71114599686028,
      "grad_norm": 3.779721260070801,
      "learning_rate": 3.893053375196232e-05,
      "loss": 0.6017,
      "step": 1128200
    },
    {
      "epoch": 17.712715855573,
      "grad_norm": 4.742814540863037,
      "learning_rate": 3.892955259026688e-05,
      "loss": 0.6131,
      "step": 1128300
    },
    {
      "epoch": 17.714285714285715,
      "grad_norm": 4.258427619934082,
      "learning_rate": 3.892857142857143e-05,
      "loss": 0.6249,
      "step": 1128400
    },
    {
      "epoch": 17.71585557299843,
      "grad_norm": 3.817594051361084,
      "learning_rate": 3.892759026687599e-05,
      "loss": 0.6128,
      "step": 1128500
    },
    {
      "epoch": 17.717425431711145,
      "grad_norm": 4.2953033447265625,
      "learning_rate": 3.892660910518053e-05,
      "loss": 0.6215,
      "step": 1128600
    },
    {
      "epoch": 17.718995290423862,
      "grad_norm": 3.124115467071533,
      "learning_rate": 3.892562794348509e-05,
      "loss": 0.5861,
      "step": 1128700
    },
    {
      "epoch": 17.72056514913658,
      "grad_norm": 4.027671813964844,
      "learning_rate": 3.892464678178964e-05,
      "loss": 0.5954,
      "step": 1128800
    },
    {
      "epoch": 17.722135007849293,
      "grad_norm": 1.767327070236206,
      "learning_rate": 3.892366562009419e-05,
      "loss": 0.5834,
      "step": 1128900
    },
    {
      "epoch": 17.72370486656201,
      "grad_norm": 2.570380449295044,
      "learning_rate": 3.892268445839874e-05,
      "loss": 0.6254,
      "step": 1129000
    },
    {
      "epoch": 17.725274725274726,
      "grad_norm": 3.006788730621338,
      "learning_rate": 3.89217032967033e-05,
      "loss": 0.5817,
      "step": 1129100
    },
    {
      "epoch": 17.72684458398744,
      "grad_norm": 3.0348386764526367,
      "learning_rate": 3.892072213500785e-05,
      "loss": 0.6392,
      "step": 1129200
    },
    {
      "epoch": 17.728414442700156,
      "grad_norm": 2.8219120502471924,
      "learning_rate": 3.89197409733124e-05,
      "loss": 0.5766,
      "step": 1129300
    },
    {
      "epoch": 17.729984301412873,
      "grad_norm": 4.083448886871338,
      "learning_rate": 3.891875981161695e-05,
      "loss": 0.572,
      "step": 1129400
    },
    {
      "epoch": 17.73155416012559,
      "grad_norm": 4.393754005432129,
      "learning_rate": 3.891777864992151e-05,
      "loss": 0.6018,
      "step": 1129500
    },
    {
      "epoch": 17.733124018838303,
      "grad_norm": 2.1281960010528564,
      "learning_rate": 3.891679748822606e-05,
      "loss": 0.6008,
      "step": 1129600
    },
    {
      "epoch": 17.73469387755102,
      "grad_norm": 4.47670841217041,
      "learning_rate": 3.891581632653061e-05,
      "loss": 0.6144,
      "step": 1129700
    },
    {
      "epoch": 17.736263736263737,
      "grad_norm": 4.1639604568481445,
      "learning_rate": 3.8914835164835164e-05,
      "loss": 0.6313,
      "step": 1129800
    },
    {
      "epoch": 17.73783359497645,
      "grad_norm": 4.212522029876709,
      "learning_rate": 3.891385400313972e-05,
      "loss": 0.5724,
      "step": 1129900
    },
    {
      "epoch": 17.739403453689167,
      "grad_norm": 4.718602657318115,
      "learning_rate": 3.891287284144427e-05,
      "loss": 0.6155,
      "step": 1130000
    },
    {
      "epoch": 17.740973312401884,
      "grad_norm": 4.089893341064453,
      "learning_rate": 3.8911891679748824e-05,
      "loss": 0.5838,
      "step": 1130100
    },
    {
      "epoch": 17.7425431711146,
      "grad_norm": 3.3682467937469482,
      "learning_rate": 3.891091051805338e-05,
      "loss": 0.6282,
      "step": 1130200
    },
    {
      "epoch": 17.744113029827314,
      "grad_norm": 4.141172885894775,
      "learning_rate": 3.8909929356357926e-05,
      "loss": 0.6256,
      "step": 1130300
    },
    {
      "epoch": 17.74568288854003,
      "grad_norm": 4.065482139587402,
      "learning_rate": 3.8908948194662484e-05,
      "loss": 0.6192,
      "step": 1130400
    },
    {
      "epoch": 17.747252747252748,
      "grad_norm": 4.0287957191467285,
      "learning_rate": 3.8907967032967034e-05,
      "loss": 0.6248,
      "step": 1130500
    },
    {
      "epoch": 17.74882260596546,
      "grad_norm": 3.3386757373809814,
      "learning_rate": 3.890698587127159e-05,
      "loss": 0.6085,
      "step": 1130600
    },
    {
      "epoch": 17.75039246467818,
      "grad_norm": 3.863806962966919,
      "learning_rate": 3.8906004709576136e-05,
      "loss": 0.597,
      "step": 1130700
    },
    {
      "epoch": 17.751962323390895,
      "grad_norm": 3.6838109493255615,
      "learning_rate": 3.8905023547880694e-05,
      "loss": 0.6644,
      "step": 1130800
    },
    {
      "epoch": 17.753532182103612,
      "grad_norm": 3.4947893619537354,
      "learning_rate": 3.8904042386185245e-05,
      "loss": 0.5978,
      "step": 1130900
    },
    {
      "epoch": 17.755102040816325,
      "grad_norm": 2.4303154945373535,
      "learning_rate": 3.8903061224489796e-05,
      "loss": 0.6423,
      "step": 1131000
    },
    {
      "epoch": 17.756671899529042,
      "grad_norm": 3.129819869995117,
      "learning_rate": 3.890208006279435e-05,
      "loss": 0.5972,
      "step": 1131100
    },
    {
      "epoch": 17.75824175824176,
      "grad_norm": 4.081080436706543,
      "learning_rate": 3.8901098901098905e-05,
      "loss": 0.6132,
      "step": 1131200
    },
    {
      "epoch": 17.759811616954472,
      "grad_norm": 2.7412266731262207,
      "learning_rate": 3.8900117739403456e-05,
      "loss": 0.5896,
      "step": 1131300
    },
    {
      "epoch": 17.76138147566719,
      "grad_norm": 3.9827542304992676,
      "learning_rate": 3.889913657770801e-05,
      "loss": 0.581,
      "step": 1131400
    },
    {
      "epoch": 17.762951334379906,
      "grad_norm": 3.6671955585479736,
      "learning_rate": 3.889815541601256e-05,
      "loss": 0.6574,
      "step": 1131500
    },
    {
      "epoch": 17.764521193092623,
      "grad_norm": 3.8706629276275635,
      "learning_rate": 3.8897174254317116e-05,
      "loss": 0.6093,
      "step": 1131600
    },
    {
      "epoch": 17.766091051805336,
      "grad_norm": 4.096333980560303,
      "learning_rate": 3.8896193092621667e-05,
      "loss": 0.6462,
      "step": 1131700
    },
    {
      "epoch": 17.767660910518053,
      "grad_norm": 3.576958417892456,
      "learning_rate": 3.889521193092622e-05,
      "loss": 0.6227,
      "step": 1131800
    },
    {
      "epoch": 17.76923076923077,
      "grad_norm": 3.832603931427002,
      "learning_rate": 3.889423076923077e-05,
      "loss": 0.6567,
      "step": 1131900
    },
    {
      "epoch": 17.770800627943487,
      "grad_norm": 3.9191033840179443,
      "learning_rate": 3.8893249607535326e-05,
      "loss": 0.5963,
      "step": 1132000
    },
    {
      "epoch": 17.7723704866562,
      "grad_norm": 3.384279727935791,
      "learning_rate": 3.889226844583988e-05,
      "loss": 0.6378,
      "step": 1132100
    },
    {
      "epoch": 17.773940345368917,
      "grad_norm": 3.6267387866973877,
      "learning_rate": 3.889128728414443e-05,
      "loss": 0.5994,
      "step": 1132200
    },
    {
      "epoch": 17.775510204081634,
      "grad_norm": 4.045055389404297,
      "learning_rate": 3.8890306122448986e-05,
      "loss": 0.602,
      "step": 1132300
    },
    {
      "epoch": 17.777080062794347,
      "grad_norm": 4.742859363555908,
      "learning_rate": 3.888932496075353e-05,
      "loss": 0.5624,
      "step": 1132400
    },
    {
      "epoch": 17.778649921507064,
      "grad_norm": 4.171154022216797,
      "learning_rate": 3.888834379905809e-05,
      "loss": 0.5676,
      "step": 1132500
    },
    {
      "epoch": 17.78021978021978,
      "grad_norm": 4.198032855987549,
      "learning_rate": 3.888736263736264e-05,
      "loss": 0.6176,
      "step": 1132600
    },
    {
      "epoch": 17.781789638932494,
      "grad_norm": 3.8412368297576904,
      "learning_rate": 3.88863814756672e-05,
      "loss": 0.6016,
      "step": 1132700
    },
    {
      "epoch": 17.78335949764521,
      "grad_norm": 4.656782150268555,
      "learning_rate": 3.888540031397174e-05,
      "loss": 0.6111,
      "step": 1132800
    },
    {
      "epoch": 17.784929356357928,
      "grad_norm": 3.9111392498016357,
      "learning_rate": 3.88844191522763e-05,
      "loss": 0.6542,
      "step": 1132900
    },
    {
      "epoch": 17.786499215070645,
      "grad_norm": 4.196878433227539,
      "learning_rate": 3.888343799058085e-05,
      "loss": 0.6506,
      "step": 1133000
    },
    {
      "epoch": 17.788069073783358,
      "grad_norm": 3.611767292022705,
      "learning_rate": 3.88824568288854e-05,
      "loss": 0.5809,
      "step": 1133100
    },
    {
      "epoch": 17.789638932496075,
      "grad_norm": 2.9964256286621094,
      "learning_rate": 3.888147566718995e-05,
      "loss": 0.6122,
      "step": 1133200
    },
    {
      "epoch": 17.791208791208792,
      "grad_norm": 5.0889763832092285,
      "learning_rate": 3.888049450549451e-05,
      "loss": 0.5475,
      "step": 1133300
    },
    {
      "epoch": 17.79277864992151,
      "grad_norm": 3.6345784664154053,
      "learning_rate": 3.887951334379906e-05,
      "loss": 0.6186,
      "step": 1133400
    },
    {
      "epoch": 17.794348508634222,
      "grad_norm": 4.07397985458374,
      "learning_rate": 3.887853218210361e-05,
      "loss": 0.6013,
      "step": 1133500
    },
    {
      "epoch": 17.79591836734694,
      "grad_norm": 4.279716968536377,
      "learning_rate": 3.887755102040816e-05,
      "loss": 0.6418,
      "step": 1133600
    },
    {
      "epoch": 17.797488226059656,
      "grad_norm": 5.332067966461182,
      "learning_rate": 3.887656985871272e-05,
      "loss": 0.6026,
      "step": 1133700
    },
    {
      "epoch": 17.79905808477237,
      "grad_norm": 5.217798709869385,
      "learning_rate": 3.887558869701727e-05,
      "loss": 0.5744,
      "step": 1133800
    },
    {
      "epoch": 17.800627943485086,
      "grad_norm": 4.558690071105957,
      "learning_rate": 3.887460753532182e-05,
      "loss": 0.6203,
      "step": 1133900
    },
    {
      "epoch": 17.802197802197803,
      "grad_norm": 4.601155757904053,
      "learning_rate": 3.887362637362637e-05,
      "loss": 0.6284,
      "step": 1134000
    },
    {
      "epoch": 17.80376766091052,
      "grad_norm": 3.3562204837799072,
      "learning_rate": 3.887264521193093e-05,
      "loss": 0.6699,
      "step": 1134100
    },
    {
      "epoch": 17.805337519623233,
      "grad_norm": 3.6416146755218506,
      "learning_rate": 3.887166405023548e-05,
      "loss": 0.5872,
      "step": 1134200
    },
    {
      "epoch": 17.80690737833595,
      "grad_norm": 4.028144836425781,
      "learning_rate": 3.887068288854003e-05,
      "loss": 0.6397,
      "step": 1134300
    },
    {
      "epoch": 17.808477237048667,
      "grad_norm": 3.222463846206665,
      "learning_rate": 3.886970172684459e-05,
      "loss": 0.6308,
      "step": 1134400
    },
    {
      "epoch": 17.81004709576138,
      "grad_norm": 4.387990951538086,
      "learning_rate": 3.8868720565149135e-05,
      "loss": 0.5925,
      "step": 1134500
    },
    {
      "epoch": 17.811616954474097,
      "grad_norm": 4.015994071960449,
      "learning_rate": 3.886773940345369e-05,
      "loss": 0.6142,
      "step": 1134600
    },
    {
      "epoch": 17.813186813186814,
      "grad_norm": 3.074070692062378,
      "learning_rate": 3.8866758241758243e-05,
      "loss": 0.5996,
      "step": 1134700
    },
    {
      "epoch": 17.81475667189953,
      "grad_norm": 4.464764595031738,
      "learning_rate": 3.88657770800628e-05,
      "loss": 0.6345,
      "step": 1134800
    },
    {
      "epoch": 17.816326530612244,
      "grad_norm": 3.8402671813964844,
      "learning_rate": 3.8864795918367345e-05,
      "loss": 0.6348,
      "step": 1134900
    },
    {
      "epoch": 17.81789638932496,
      "grad_norm": 3.3087234497070312,
      "learning_rate": 3.88638147566719e-05,
      "loss": 0.6332,
      "step": 1135000
    },
    {
      "epoch": 17.819466248037678,
      "grad_norm": 2.8872873783111572,
      "learning_rate": 3.8862833594976454e-05,
      "loss": 0.5974,
      "step": 1135100
    },
    {
      "epoch": 17.82103610675039,
      "grad_norm": 4.3665266036987305,
      "learning_rate": 3.8861852433281005e-05,
      "loss": 0.5867,
      "step": 1135200
    },
    {
      "epoch": 17.822605965463108,
      "grad_norm": 4.006422519683838,
      "learning_rate": 3.8860871271585556e-05,
      "loss": 0.5637,
      "step": 1135300
    },
    {
      "epoch": 17.824175824175825,
      "grad_norm": 4.220085620880127,
      "learning_rate": 3.8859890109890114e-05,
      "loss": 0.5948,
      "step": 1135400
    },
    {
      "epoch": 17.82574568288854,
      "grad_norm": 4.600598335266113,
      "learning_rate": 3.8858908948194665e-05,
      "loss": 0.6173,
      "step": 1135500
    },
    {
      "epoch": 17.827315541601255,
      "grad_norm": 4.016515254974365,
      "learning_rate": 3.8857927786499216e-05,
      "loss": 0.5998,
      "step": 1135600
    },
    {
      "epoch": 17.828885400313972,
      "grad_norm": 4.286861419677734,
      "learning_rate": 3.885694662480377e-05,
      "loss": 0.5799,
      "step": 1135700
    },
    {
      "epoch": 17.83045525902669,
      "grad_norm": 3.8423354625701904,
      "learning_rate": 3.8855965463108325e-05,
      "loss": 0.6449,
      "step": 1135800
    },
    {
      "epoch": 17.832025117739402,
      "grad_norm": 3.6889114379882812,
      "learning_rate": 3.8854984301412876e-05,
      "loss": 0.6188,
      "step": 1135900
    },
    {
      "epoch": 17.83359497645212,
      "grad_norm": 4.473911762237549,
      "learning_rate": 3.8854003139717427e-05,
      "loss": 0.6358,
      "step": 1136000
    },
    {
      "epoch": 17.835164835164836,
      "grad_norm": 3.6350860595703125,
      "learning_rate": 3.885302197802198e-05,
      "loss": 0.6304,
      "step": 1136100
    },
    {
      "epoch": 17.836734693877553,
      "grad_norm": 3.2760090827941895,
      "learning_rate": 3.8852040816326535e-05,
      "loss": 0.6385,
      "step": 1136200
    },
    {
      "epoch": 17.838304552590266,
      "grad_norm": 4.2649245262146,
      "learning_rate": 3.8851059654631086e-05,
      "loss": 0.6032,
      "step": 1136300
    },
    {
      "epoch": 17.839874411302983,
      "grad_norm": 3.4565603733062744,
      "learning_rate": 3.885007849293564e-05,
      "loss": 0.6139,
      "step": 1136400
    },
    {
      "epoch": 17.8414442700157,
      "grad_norm": 4.849813938140869,
      "learning_rate": 3.8849097331240195e-05,
      "loss": 0.6205,
      "step": 1136500
    },
    {
      "epoch": 17.843014128728413,
      "grad_norm": 3.6055808067321777,
      "learning_rate": 3.884811616954474e-05,
      "loss": 0.6236,
      "step": 1136600
    },
    {
      "epoch": 17.84458398744113,
      "grad_norm": 2.931337833404541,
      "learning_rate": 3.88471350078493e-05,
      "loss": 0.6026,
      "step": 1136700
    },
    {
      "epoch": 17.846153846153847,
      "grad_norm": 4.037144184112549,
      "learning_rate": 3.884615384615385e-05,
      "loss": 0.6235,
      "step": 1136800
    },
    {
      "epoch": 17.847723704866564,
      "grad_norm": 3.9796347618103027,
      "learning_rate": 3.8845172684458406e-05,
      "loss": 0.6197,
      "step": 1136900
    },
    {
      "epoch": 17.849293563579277,
      "grad_norm": 3.1827492713928223,
      "learning_rate": 3.884419152276295e-05,
      "loss": 0.5786,
      "step": 1137000
    },
    {
      "epoch": 17.850863422291994,
      "grad_norm": 4.053572654724121,
      "learning_rate": 3.884321036106751e-05,
      "loss": 0.5734,
      "step": 1137100
    },
    {
      "epoch": 17.85243328100471,
      "grad_norm": 2.3292996883392334,
      "learning_rate": 3.884222919937206e-05,
      "loss": 0.6079,
      "step": 1137200
    },
    {
      "epoch": 17.854003139717424,
      "grad_norm": 4.5095906257629395,
      "learning_rate": 3.884124803767661e-05,
      "loss": 0.6195,
      "step": 1137300
    },
    {
      "epoch": 17.85557299843014,
      "grad_norm": 4.461194038391113,
      "learning_rate": 3.884026687598116e-05,
      "loss": 0.6419,
      "step": 1137400
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 2.7641847133636475,
      "learning_rate": 3.883928571428572e-05,
      "loss": 0.6233,
      "step": 1137500
    },
    {
      "epoch": 17.858712715855575,
      "grad_norm": 4.012491703033447,
      "learning_rate": 3.883830455259027e-05,
      "loss": 0.5732,
      "step": 1137600
    },
    {
      "epoch": 17.860282574568288,
      "grad_norm": 4.507866859436035,
      "learning_rate": 3.883732339089482e-05,
      "loss": 0.6254,
      "step": 1137700
    },
    {
      "epoch": 17.861852433281005,
      "grad_norm": 4.205492973327637,
      "learning_rate": 3.883634222919937e-05,
      "loss": 0.623,
      "step": 1137800
    },
    {
      "epoch": 17.86342229199372,
      "grad_norm": 4.753232955932617,
      "learning_rate": 3.883536106750393e-05,
      "loss": 0.5994,
      "step": 1137900
    },
    {
      "epoch": 17.864992150706435,
      "grad_norm": 4.658770561218262,
      "learning_rate": 3.883437990580848e-05,
      "loss": 0.5961,
      "step": 1138000
    },
    {
      "epoch": 17.86656200941915,
      "grad_norm": 4.3864426612854,
      "learning_rate": 3.883339874411303e-05,
      "loss": 0.6115,
      "step": 1138100
    },
    {
      "epoch": 17.86813186813187,
      "grad_norm": 3.5745046138763428,
      "learning_rate": 3.883241758241758e-05,
      "loss": 0.594,
      "step": 1138200
    },
    {
      "epoch": 17.869701726844585,
      "grad_norm": 4.174413204193115,
      "learning_rate": 3.883143642072214e-05,
      "loss": 0.6197,
      "step": 1138300
    },
    {
      "epoch": 17.8712715855573,
      "grad_norm": 2.9997336864471436,
      "learning_rate": 3.883045525902669e-05,
      "loss": 0.6305,
      "step": 1138400
    },
    {
      "epoch": 17.872841444270016,
      "grad_norm": 4.408154010772705,
      "learning_rate": 3.882947409733124e-05,
      "loss": 0.6291,
      "step": 1138500
    },
    {
      "epoch": 17.874411302982733,
      "grad_norm": 2.707301139831543,
      "learning_rate": 3.88284929356358e-05,
      "loss": 0.657,
      "step": 1138600
    },
    {
      "epoch": 17.875981161695446,
      "grad_norm": 4.011948108673096,
      "learning_rate": 3.8827511773940344e-05,
      "loss": 0.5988,
      "step": 1138700
    },
    {
      "epoch": 17.877551020408163,
      "grad_norm": 3.5438497066497803,
      "learning_rate": 3.88265306122449e-05,
      "loss": 0.6325,
      "step": 1138800
    },
    {
      "epoch": 17.87912087912088,
      "grad_norm": 4.037668228149414,
      "learning_rate": 3.882554945054945e-05,
      "loss": 0.5908,
      "step": 1138900
    },
    {
      "epoch": 17.880690737833596,
      "grad_norm": 4.1186113357543945,
      "learning_rate": 3.882456828885401e-05,
      "loss": 0.5879,
      "step": 1139000
    },
    {
      "epoch": 17.88226059654631,
      "grad_norm": 4.667982578277588,
      "learning_rate": 3.8823587127158554e-05,
      "loss": 0.6064,
      "step": 1139100
    },
    {
      "epoch": 17.883830455259027,
      "grad_norm": 3.993617534637451,
      "learning_rate": 3.882260596546311e-05,
      "loss": 0.6045,
      "step": 1139200
    },
    {
      "epoch": 17.885400313971743,
      "grad_norm": 2.5032358169555664,
      "learning_rate": 3.882162480376766e-05,
      "loss": 0.5892,
      "step": 1139300
    },
    {
      "epoch": 17.886970172684457,
      "grad_norm": 2.152986526489258,
      "learning_rate": 3.8820643642072214e-05,
      "loss": 0.6197,
      "step": 1139400
    },
    {
      "epoch": 17.888540031397174,
      "grad_norm": 4.3405866622924805,
      "learning_rate": 3.8819662480376765e-05,
      "loss": 0.6298,
      "step": 1139500
    },
    {
      "epoch": 17.89010989010989,
      "grad_norm": 4.893119812011719,
      "learning_rate": 3.881868131868132e-05,
      "loss": 0.6473,
      "step": 1139600
    },
    {
      "epoch": 17.891679748822607,
      "grad_norm": 2.9059197902679443,
      "learning_rate": 3.8817700156985874e-05,
      "loss": 0.5977,
      "step": 1139700
    },
    {
      "epoch": 17.89324960753532,
      "grad_norm": 3.1672136783599854,
      "learning_rate": 3.8816718995290425e-05,
      "loss": 0.6457,
      "step": 1139800
    },
    {
      "epoch": 17.894819466248038,
      "grad_norm": 3.969562292098999,
      "learning_rate": 3.8815737833594976e-05,
      "loss": 0.6191,
      "step": 1139900
    },
    {
      "epoch": 17.896389324960754,
      "grad_norm": 5.038914680480957,
      "learning_rate": 3.8814756671899534e-05,
      "loss": 0.6527,
      "step": 1140000
    },
    {
      "epoch": 17.897959183673468,
      "grad_norm": 3.016516923904419,
      "learning_rate": 3.8813775510204085e-05,
      "loss": 0.6119,
      "step": 1140100
    },
    {
      "epoch": 17.899529042386185,
      "grad_norm": 4.418398380279541,
      "learning_rate": 3.8812794348508636e-05,
      "loss": 0.6311,
      "step": 1140200
    },
    {
      "epoch": 17.9010989010989,
      "grad_norm": 3.33231520652771,
      "learning_rate": 3.8811813186813187e-05,
      "loss": 0.6192,
      "step": 1140300
    },
    {
      "epoch": 17.90266875981162,
      "grad_norm": 4.230448246002197,
      "learning_rate": 3.8810832025117744e-05,
      "loss": 0.6476,
      "step": 1140400
    },
    {
      "epoch": 17.90423861852433,
      "grad_norm": 3.3105645179748535,
      "learning_rate": 3.8809850863422295e-05,
      "loss": 0.6362,
      "step": 1140500
    },
    {
      "epoch": 17.90580847723705,
      "grad_norm": 3.398165225982666,
      "learning_rate": 3.8808869701726846e-05,
      "loss": 0.5786,
      "step": 1140600
    },
    {
      "epoch": 17.907378335949765,
      "grad_norm": 2.4222450256347656,
      "learning_rate": 3.8807888540031404e-05,
      "loss": 0.6377,
      "step": 1140700
    },
    {
      "epoch": 17.90894819466248,
      "grad_norm": 3.586022138595581,
      "learning_rate": 3.880690737833595e-05,
      "loss": 0.6115,
      "step": 1140800
    },
    {
      "epoch": 17.910518053375196,
      "grad_norm": 3.2706260681152344,
      "learning_rate": 3.8805926216640506e-05,
      "loss": 0.5973,
      "step": 1140900
    },
    {
      "epoch": 17.912087912087912,
      "grad_norm": 4.317351818084717,
      "learning_rate": 3.880494505494506e-05,
      "loss": 0.6242,
      "step": 1141000
    },
    {
      "epoch": 17.91365777080063,
      "grad_norm": 4.104681491851807,
      "learning_rate": 3.8803963893249615e-05,
      "loss": 0.6192,
      "step": 1141100
    },
    {
      "epoch": 17.915227629513343,
      "grad_norm": 2.957249641418457,
      "learning_rate": 3.880298273155416e-05,
      "loss": 0.6108,
      "step": 1141200
    },
    {
      "epoch": 17.91679748822606,
      "grad_norm": 3.502716541290283,
      "learning_rate": 3.880200156985872e-05,
      "loss": 0.564,
      "step": 1141300
    },
    {
      "epoch": 17.918367346938776,
      "grad_norm": 3.0277726650238037,
      "learning_rate": 3.880102040816327e-05,
      "loss": 0.6134,
      "step": 1141400
    },
    {
      "epoch": 17.919937205651493,
      "grad_norm": 3.7717158794403076,
      "learning_rate": 3.880003924646782e-05,
      "loss": 0.6302,
      "step": 1141500
    },
    {
      "epoch": 17.921507064364206,
      "grad_norm": 3.660299062728882,
      "learning_rate": 3.879905808477237e-05,
      "loss": 0.573,
      "step": 1141600
    },
    {
      "epoch": 17.923076923076923,
      "grad_norm": 3.745882511138916,
      "learning_rate": 3.879807692307693e-05,
      "loss": 0.6031,
      "step": 1141700
    },
    {
      "epoch": 17.92464678178964,
      "grad_norm": 4.268600940704346,
      "learning_rate": 3.879709576138148e-05,
      "loss": 0.6299,
      "step": 1141800
    },
    {
      "epoch": 17.926216640502354,
      "grad_norm": 4.163695335388184,
      "learning_rate": 3.879611459968603e-05,
      "loss": 0.5572,
      "step": 1141900
    },
    {
      "epoch": 17.92778649921507,
      "grad_norm": 4.451095104217529,
      "learning_rate": 3.879513343799058e-05,
      "loss": 0.6047,
      "step": 1142000
    },
    {
      "epoch": 17.929356357927787,
      "grad_norm": 4.053461074829102,
      "learning_rate": 3.879415227629514e-05,
      "loss": 0.6267,
      "step": 1142100
    },
    {
      "epoch": 17.9309262166405,
      "grad_norm": 4.28375244140625,
      "learning_rate": 3.879317111459969e-05,
      "loss": 0.6066,
      "step": 1142200
    },
    {
      "epoch": 17.932496075353217,
      "grad_norm": 4.060074806213379,
      "learning_rate": 3.879218995290424e-05,
      "loss": 0.6165,
      "step": 1142300
    },
    {
      "epoch": 17.934065934065934,
      "grad_norm": 4.139071464538574,
      "learning_rate": 3.879120879120879e-05,
      "loss": 0.6334,
      "step": 1142400
    },
    {
      "epoch": 17.93563579277865,
      "grad_norm": 4.598051071166992,
      "learning_rate": 3.879022762951335e-05,
      "loss": 0.6554,
      "step": 1142500
    },
    {
      "epoch": 17.937205651491364,
      "grad_norm": 4.547423839569092,
      "learning_rate": 3.87892464678179e-05,
      "loss": 0.6155,
      "step": 1142600
    },
    {
      "epoch": 17.93877551020408,
      "grad_norm": 4.352532386779785,
      "learning_rate": 3.878826530612245e-05,
      "loss": 0.5891,
      "step": 1142700
    },
    {
      "epoch": 17.940345368916798,
      "grad_norm": 3.763523578643799,
      "learning_rate": 3.878728414442701e-05,
      "loss": 0.6088,
      "step": 1142800
    },
    {
      "epoch": 17.941915227629515,
      "grad_norm": 4.038355350494385,
      "learning_rate": 3.878630298273155e-05,
      "loss": 0.5687,
      "step": 1142900
    },
    {
      "epoch": 17.94348508634223,
      "grad_norm": 2.896660804748535,
      "learning_rate": 3.878532182103611e-05,
      "loss": 0.6159,
      "step": 1143000
    },
    {
      "epoch": 17.945054945054945,
      "grad_norm": 3.671074867248535,
      "learning_rate": 3.878434065934066e-05,
      "loss": 0.6049,
      "step": 1143100
    },
    {
      "epoch": 17.946624803767662,
      "grad_norm": 3.7315797805786133,
      "learning_rate": 3.878335949764522e-05,
      "loss": 0.6272,
      "step": 1143200
    },
    {
      "epoch": 17.948194662480375,
      "grad_norm": 2.8475496768951416,
      "learning_rate": 3.8782378335949763e-05,
      "loss": 0.6198,
      "step": 1143300
    },
    {
      "epoch": 17.949764521193092,
      "grad_norm": 3.917689800262451,
      "learning_rate": 3.878139717425432e-05,
      "loss": 0.6236,
      "step": 1143400
    },
    {
      "epoch": 17.95133437990581,
      "grad_norm": 3.7746245861053467,
      "learning_rate": 3.878041601255887e-05,
      "loss": 0.6209,
      "step": 1143500
    },
    {
      "epoch": 17.952904238618526,
      "grad_norm": 4.798568248748779,
      "learning_rate": 3.877943485086342e-05,
      "loss": 0.575,
      "step": 1143600
    },
    {
      "epoch": 17.95447409733124,
      "grad_norm": 4.395484924316406,
      "learning_rate": 3.8778453689167974e-05,
      "loss": 0.6406,
      "step": 1143700
    },
    {
      "epoch": 17.956043956043956,
      "grad_norm": 3.3022797107696533,
      "learning_rate": 3.877747252747253e-05,
      "loss": 0.5995,
      "step": 1143800
    },
    {
      "epoch": 17.957613814756673,
      "grad_norm": 3.6877779960632324,
      "learning_rate": 3.877649136577708e-05,
      "loss": 0.6074,
      "step": 1143900
    },
    {
      "epoch": 17.959183673469386,
      "grad_norm": 3.737426280975342,
      "learning_rate": 3.8775510204081634e-05,
      "loss": 0.6225,
      "step": 1144000
    },
    {
      "epoch": 17.960753532182103,
      "grad_norm": 3.0322558879852295,
      "learning_rate": 3.8774529042386185e-05,
      "loss": 0.6453,
      "step": 1144100
    },
    {
      "epoch": 17.96232339089482,
      "grad_norm": 3.937608003616333,
      "learning_rate": 3.877354788069074e-05,
      "loss": 0.6264,
      "step": 1144200
    },
    {
      "epoch": 17.963893249607537,
      "grad_norm": 3.4907360076904297,
      "learning_rate": 3.877256671899529e-05,
      "loss": 0.6183,
      "step": 1144300
    },
    {
      "epoch": 17.96546310832025,
      "grad_norm": 3.130932569503784,
      "learning_rate": 3.8771585557299845e-05,
      "loss": 0.641,
      "step": 1144400
    },
    {
      "epoch": 17.967032967032967,
      "grad_norm": 3.829031467437744,
      "learning_rate": 3.8770604395604396e-05,
      "loss": 0.6089,
      "step": 1144500
    },
    {
      "epoch": 17.968602825745684,
      "grad_norm": 2.403998851776123,
      "learning_rate": 3.876962323390895e-05,
      "loss": 0.5973,
      "step": 1144600
    },
    {
      "epoch": 17.970172684458397,
      "grad_norm": 2.6808550357818604,
      "learning_rate": 3.8768642072213504e-05,
      "loss": 0.5795,
      "step": 1144700
    },
    {
      "epoch": 17.971742543171114,
      "grad_norm": 4.305638313293457,
      "learning_rate": 3.8767660910518055e-05,
      "loss": 0.6375,
      "step": 1144800
    },
    {
      "epoch": 17.97331240188383,
      "grad_norm": 2.450572967529297,
      "learning_rate": 3.876667974882261e-05,
      "loss": 0.6406,
      "step": 1144900
    },
    {
      "epoch": 17.974882260596548,
      "grad_norm": 3.8833131790161133,
      "learning_rate": 3.876569858712716e-05,
      "loss": 0.647,
      "step": 1145000
    },
    {
      "epoch": 17.97645211930926,
      "grad_norm": 3.428077459335327,
      "learning_rate": 3.8764717425431715e-05,
      "loss": 0.6015,
      "step": 1145100
    },
    {
      "epoch": 17.978021978021978,
      "grad_norm": 4.815937519073486,
      "learning_rate": 3.8763736263736266e-05,
      "loss": 0.5786,
      "step": 1145200
    },
    {
      "epoch": 17.979591836734695,
      "grad_norm": 2.9339418411254883,
      "learning_rate": 3.8762755102040824e-05,
      "loss": 0.6089,
      "step": 1145300
    },
    {
      "epoch": 17.98116169544741,
      "grad_norm": 2.3999760150909424,
      "learning_rate": 3.876177394034537e-05,
      "loss": 0.5977,
      "step": 1145400
    },
    {
      "epoch": 17.982731554160125,
      "grad_norm": 3.7668542861938477,
      "learning_rate": 3.8760792778649926e-05,
      "loss": 0.6427,
      "step": 1145500
    },
    {
      "epoch": 17.984301412872842,
      "grad_norm": 4.230712413787842,
      "learning_rate": 3.875981161695448e-05,
      "loss": 0.6289,
      "step": 1145600
    },
    {
      "epoch": 17.98587127158556,
      "grad_norm": 3.4324417114257812,
      "learning_rate": 3.875883045525903e-05,
      "loss": 0.6177,
      "step": 1145700
    },
    {
      "epoch": 17.987441130298272,
      "grad_norm": 4.597522258758545,
      "learning_rate": 3.875784929356358e-05,
      "loss": 0.5993,
      "step": 1145800
    },
    {
      "epoch": 17.98901098901099,
      "grad_norm": 4.090712070465088,
      "learning_rate": 3.8756868131868136e-05,
      "loss": 0.6418,
      "step": 1145900
    },
    {
      "epoch": 17.990580847723706,
      "grad_norm": 4.608640193939209,
      "learning_rate": 3.875588697017269e-05,
      "loss": 0.5683,
      "step": 1146000
    },
    {
      "epoch": 17.99215070643642,
      "grad_norm": 3.6262335777282715,
      "learning_rate": 3.875490580847724e-05,
      "loss": 0.6253,
      "step": 1146100
    },
    {
      "epoch": 17.993720565149136,
      "grad_norm": 4.233903884887695,
      "learning_rate": 3.875392464678179e-05,
      "loss": 0.5981,
      "step": 1146200
    },
    {
      "epoch": 17.995290423861853,
      "grad_norm": 4.181850910186768,
      "learning_rate": 3.875294348508635e-05,
      "loss": 0.5805,
      "step": 1146300
    },
    {
      "epoch": 17.99686028257457,
      "grad_norm": 2.5431172847747803,
      "learning_rate": 3.875196232339089e-05,
      "loss": 0.6125,
      "step": 1146400
    },
    {
      "epoch": 17.998430141287283,
      "grad_norm": 5.433051586151123,
      "learning_rate": 3.875098116169545e-05,
      "loss": 0.6557,
      "step": 1146500
    },
    {
      "epoch": 18.0,
      "grad_norm": 3.1265785694122314,
      "learning_rate": 3.875e-05,
      "loss": 0.6454,
      "step": 1146600
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.0268595218658447,
      "eval_runtime": 14.71,
      "eval_samples_per_second": 227.94,
      "eval_steps_per_second": 227.94,
      "step": 1146600
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.4725857377052307,
      "eval_runtime": 281.2921,
      "eval_samples_per_second": 226.455,
      "eval_steps_per_second": 226.455,
      "step": 1146600
    },
    {
      "epoch": 18.001569858712717,
      "grad_norm": 4.902028560638428,
      "learning_rate": 3.874901883830456e-05,
      "loss": 0.5602,
      "step": 1146700
    },
    {
      "epoch": 18.00313971742543,
      "grad_norm": 3.2732653617858887,
      "learning_rate": 3.874803767660911e-05,
      "loss": 0.6105,
      "step": 1146800
    },
    {
      "epoch": 18.004709576138147,
      "grad_norm": 4.050643444061279,
      "learning_rate": 3.874705651491366e-05,
      "loss": 0.6296,
      "step": 1146900
    },
    {
      "epoch": 18.006279434850864,
      "grad_norm": 2.219522476196289,
      "learning_rate": 3.874607535321822e-05,
      "loss": 0.6132,
      "step": 1147000
    },
    {
      "epoch": 18.00784929356358,
      "grad_norm": 2.778998851776123,
      "learning_rate": 3.874509419152276e-05,
      "loss": 0.5916,
      "step": 1147100
    },
    {
      "epoch": 18.009419152276294,
      "grad_norm": 3.3811843395233154,
      "learning_rate": 3.874411302982732e-05,
      "loss": 0.5645,
      "step": 1147200
    },
    {
      "epoch": 18.01098901098901,
      "grad_norm": 4.121472358703613,
      "learning_rate": 3.874313186813187e-05,
      "loss": 0.6227,
      "step": 1147300
    },
    {
      "epoch": 18.012558869701728,
      "grad_norm": 3.675011396408081,
      "learning_rate": 3.874215070643643e-05,
      "loss": 0.6003,
      "step": 1147400
    },
    {
      "epoch": 18.01412872841444,
      "grad_norm": 3.4252967834472656,
      "learning_rate": 3.874116954474097e-05,
      "loss": 0.5875,
      "step": 1147500
    },
    {
      "epoch": 18.015698587127158,
      "grad_norm": 2.968111038208008,
      "learning_rate": 3.874018838304553e-05,
      "loss": 0.6228,
      "step": 1147600
    },
    {
      "epoch": 18.017268445839875,
      "grad_norm": 4.159202575683594,
      "learning_rate": 3.873920722135008e-05,
      "loss": 0.6432,
      "step": 1147700
    },
    {
      "epoch": 18.01883830455259,
      "grad_norm": 3.433882236480713,
      "learning_rate": 3.873822605965463e-05,
      "loss": 0.5802,
      "step": 1147800
    },
    {
      "epoch": 18.020408163265305,
      "grad_norm": 4.131646633148193,
      "learning_rate": 3.873724489795918e-05,
      "loss": 0.6166,
      "step": 1147900
    },
    {
      "epoch": 18.021978021978022,
      "grad_norm": 3.348149538040161,
      "learning_rate": 3.873626373626374e-05,
      "loss": 0.6123,
      "step": 1148000
    },
    {
      "epoch": 18.02354788069074,
      "grad_norm": 3.654029130935669,
      "learning_rate": 3.873528257456829e-05,
      "loss": 0.6083,
      "step": 1148100
    },
    {
      "epoch": 18.025117739403452,
      "grad_norm": 4.432048320770264,
      "learning_rate": 3.873430141287284e-05,
      "loss": 0.6086,
      "step": 1148200
    },
    {
      "epoch": 18.02668759811617,
      "grad_norm": 3.552175998687744,
      "learning_rate": 3.8733320251177394e-05,
      "loss": 0.5654,
      "step": 1148300
    },
    {
      "epoch": 18.028257456828886,
      "grad_norm": 3.9175965785980225,
      "learning_rate": 3.873233908948195e-05,
      "loss": 0.5865,
      "step": 1148400
    },
    {
      "epoch": 18.029827315541603,
      "grad_norm": 3.0303382873535156,
      "learning_rate": 3.8731357927786496e-05,
      "loss": 0.6186,
      "step": 1148500
    },
    {
      "epoch": 18.031397174254316,
      "grad_norm": 3.704664468765259,
      "learning_rate": 3.8730376766091054e-05,
      "loss": 0.6195,
      "step": 1148600
    },
    {
      "epoch": 18.032967032967033,
      "grad_norm": 3.892021656036377,
      "learning_rate": 3.8729395604395604e-05,
      "loss": 0.5973,
      "step": 1148700
    },
    {
      "epoch": 18.03453689167975,
      "grad_norm": 2.9396302700042725,
      "learning_rate": 3.872841444270016e-05,
      "loss": 0.6084,
      "step": 1148800
    },
    {
      "epoch": 18.036106750392463,
      "grad_norm": 4.726550579071045,
      "learning_rate": 3.872743328100471e-05,
      "loss": 0.6231,
      "step": 1148900
    },
    {
      "epoch": 18.03767660910518,
      "grad_norm": 3.591099739074707,
      "learning_rate": 3.8726452119309264e-05,
      "loss": 0.617,
      "step": 1149000
    },
    {
      "epoch": 18.039246467817897,
      "grad_norm": 4.3906989097595215,
      "learning_rate": 3.872547095761382e-05,
      "loss": 0.5674,
      "step": 1149100
    },
    {
      "epoch": 18.040816326530614,
      "grad_norm": 4.270225524902344,
      "learning_rate": 3.8724489795918366e-05,
      "loss": 0.6132,
      "step": 1149200
    },
    {
      "epoch": 18.042386185243327,
      "grad_norm": 4.251999855041504,
      "learning_rate": 3.8723508634222924e-05,
      "loss": 0.5944,
      "step": 1149300
    },
    {
      "epoch": 18.043956043956044,
      "grad_norm": 2.5710608959198,
      "learning_rate": 3.8722527472527475e-05,
      "loss": 0.6012,
      "step": 1149400
    },
    {
      "epoch": 18.04552590266876,
      "grad_norm": 3.933389902114868,
      "learning_rate": 3.8721546310832026e-05,
      "loss": 0.6617,
      "step": 1149500
    },
    {
      "epoch": 18.047095761381474,
      "grad_norm": 2.9748432636260986,
      "learning_rate": 3.872056514913658e-05,
      "loss": 0.6024,
      "step": 1149600
    },
    {
      "epoch": 18.04866562009419,
      "grad_norm": 4.820389747619629,
      "learning_rate": 3.8719583987441135e-05,
      "loss": 0.6081,
      "step": 1149700
    },
    {
      "epoch": 18.050235478806908,
      "grad_norm": 4.5640645027160645,
      "learning_rate": 3.8718602825745686e-05,
      "loss": 0.6141,
      "step": 1149800
    },
    {
      "epoch": 18.051805337519625,
      "grad_norm": 3.538790225982666,
      "learning_rate": 3.8717621664050237e-05,
      "loss": 0.6059,
      "step": 1149900
    },
    {
      "epoch": 18.053375196232338,
      "grad_norm": 3.371677875518799,
      "learning_rate": 3.871664050235479e-05,
      "loss": 0.5836,
      "step": 1150000
    },
    {
      "epoch": 18.054945054945055,
      "grad_norm": 3.224882125854492,
      "learning_rate": 3.8715659340659345e-05,
      "loss": 0.6013,
      "step": 1150100
    },
    {
      "epoch": 18.05651491365777,
      "grad_norm": 3.803656578063965,
      "learning_rate": 3.871467817896389e-05,
      "loss": 0.5842,
      "step": 1150200
    },
    {
      "epoch": 18.058084772370485,
      "grad_norm": 3.2571685314178467,
      "learning_rate": 3.871369701726845e-05,
      "loss": 0.6121,
      "step": 1150300
    },
    {
      "epoch": 18.059654631083202,
      "grad_norm": 3.9009006023406982,
      "learning_rate": 3.8712715855573e-05,
      "loss": 0.5837,
      "step": 1150400
    },
    {
      "epoch": 18.06122448979592,
      "grad_norm": 3.7865867614746094,
      "learning_rate": 3.8711734693877556e-05,
      "loss": 0.606,
      "step": 1150500
    },
    {
      "epoch": 18.062794348508636,
      "grad_norm": 4.567677974700928,
      "learning_rate": 3.87107535321821e-05,
      "loss": 0.6009,
      "step": 1150600
    },
    {
      "epoch": 18.06436420722135,
      "grad_norm": 4.015920162200928,
      "learning_rate": 3.870977237048666e-05,
      "loss": 0.6248,
      "step": 1150700
    },
    {
      "epoch": 18.065934065934066,
      "grad_norm": 3.8641364574432373,
      "learning_rate": 3.870879120879121e-05,
      "loss": 0.573,
      "step": 1150800
    },
    {
      "epoch": 18.067503924646783,
      "grad_norm": 3.986964225769043,
      "learning_rate": 3.870781004709576e-05,
      "loss": 0.6224,
      "step": 1150900
    },
    {
      "epoch": 18.069073783359496,
      "grad_norm": 4.337564468383789,
      "learning_rate": 3.870682888540032e-05,
      "loss": 0.5726,
      "step": 1151000
    },
    {
      "epoch": 18.070643642072213,
      "grad_norm": 3.5565907955169678,
      "learning_rate": 3.870584772370487e-05,
      "loss": 0.6165,
      "step": 1151100
    },
    {
      "epoch": 18.07221350078493,
      "grad_norm": 4.1127777099609375,
      "learning_rate": 3.8704866562009426e-05,
      "loss": 0.629,
      "step": 1151200
    },
    {
      "epoch": 18.073783359497646,
      "grad_norm": 3.2746214866638184,
      "learning_rate": 3.870388540031397e-05,
      "loss": 0.6111,
      "step": 1151300
    },
    {
      "epoch": 18.07535321821036,
      "grad_norm": 4.324219703674316,
      "learning_rate": 3.870290423861853e-05,
      "loss": 0.5808,
      "step": 1151400
    },
    {
      "epoch": 18.076923076923077,
      "grad_norm": 3.7735300064086914,
      "learning_rate": 3.870192307692308e-05,
      "loss": 0.5966,
      "step": 1151500
    },
    {
      "epoch": 18.078492935635794,
      "grad_norm": 3.8109285831451416,
      "learning_rate": 3.870094191522763e-05,
      "loss": 0.6391,
      "step": 1151600
    },
    {
      "epoch": 18.08006279434851,
      "grad_norm": 4.581761837005615,
      "learning_rate": 3.869996075353218e-05,
      "loss": 0.5617,
      "step": 1151700
    },
    {
      "epoch": 18.081632653061224,
      "grad_norm": 4.059772491455078,
      "learning_rate": 3.869897959183674e-05,
      "loss": 0.6309,
      "step": 1151800
    },
    {
      "epoch": 18.08320251177394,
      "grad_norm": 3.900533676147461,
      "learning_rate": 3.869799843014129e-05,
      "loss": 0.5792,
      "step": 1151900
    },
    {
      "epoch": 18.084772370486657,
      "grad_norm": 4.2625041007995605,
      "learning_rate": 3.869701726844584e-05,
      "loss": 0.6043,
      "step": 1152000
    },
    {
      "epoch": 18.08634222919937,
      "grad_norm": 3.799004316329956,
      "learning_rate": 3.869603610675039e-05,
      "loss": 0.5997,
      "step": 1152100
    },
    {
      "epoch": 18.087912087912088,
      "grad_norm": 3.89546275138855,
      "learning_rate": 3.869505494505495e-05,
      "loss": 0.576,
      "step": 1152200
    },
    {
      "epoch": 18.089481946624804,
      "grad_norm": 3.212794065475464,
      "learning_rate": 3.8694073783359494e-05,
      "loss": 0.5847,
      "step": 1152300
    },
    {
      "epoch": 18.09105180533752,
      "grad_norm": 3.5563669204711914,
      "learning_rate": 3.869309262166405e-05,
      "loss": 0.6039,
      "step": 1152400
    },
    {
      "epoch": 18.092621664050235,
      "grad_norm": 3.7817752361297607,
      "learning_rate": 3.86921114599686e-05,
      "loss": 0.5921,
      "step": 1152500
    },
    {
      "epoch": 18.09419152276295,
      "grad_norm": 4.560122489929199,
      "learning_rate": 3.869113029827316e-05,
      "loss": 0.6077,
      "step": 1152600
    },
    {
      "epoch": 18.09576138147567,
      "grad_norm": 3.7187459468841553,
      "learning_rate": 3.8690149136577705e-05,
      "loss": 0.5886,
      "step": 1152700
    },
    {
      "epoch": 18.09733124018838,
      "grad_norm": 3.9475483894348145,
      "learning_rate": 3.868916797488226e-05,
      "loss": 0.6386,
      "step": 1152800
    },
    {
      "epoch": 18.0989010989011,
      "grad_norm": 4.2269439697265625,
      "learning_rate": 3.8688186813186813e-05,
      "loss": 0.6042,
      "step": 1152900
    },
    {
      "epoch": 18.100470957613815,
      "grad_norm": 3.361621141433716,
      "learning_rate": 3.8687205651491364e-05,
      "loss": 0.6089,
      "step": 1153000
    },
    {
      "epoch": 18.102040816326532,
      "grad_norm": 4.501515865325928,
      "learning_rate": 3.868622448979592e-05,
      "loss": 0.6232,
      "step": 1153100
    },
    {
      "epoch": 18.103610675039246,
      "grad_norm": 3.8832364082336426,
      "learning_rate": 3.868524332810047e-05,
      "loss": 0.6003,
      "step": 1153200
    },
    {
      "epoch": 18.105180533751962,
      "grad_norm": 4.390923500061035,
      "learning_rate": 3.8684262166405024e-05,
      "loss": 0.6022,
      "step": 1153300
    },
    {
      "epoch": 18.10675039246468,
      "grad_norm": 4.192018032073975,
      "learning_rate": 3.8683281004709575e-05,
      "loss": 0.5837,
      "step": 1153400
    },
    {
      "epoch": 18.108320251177393,
      "grad_norm": 5.132882595062256,
      "learning_rate": 3.868229984301413e-05,
      "loss": 0.6199,
      "step": 1153500
    },
    {
      "epoch": 18.10989010989011,
      "grad_norm": 2.960120439529419,
      "learning_rate": 3.8681318681318684e-05,
      "loss": 0.5927,
      "step": 1153600
    },
    {
      "epoch": 18.111459968602826,
      "grad_norm": 3.9256482124328613,
      "learning_rate": 3.8680337519623235e-05,
      "loss": 0.5977,
      "step": 1153700
    },
    {
      "epoch": 18.113029827315543,
      "grad_norm": 4.125941276550293,
      "learning_rate": 3.8679356357927786e-05,
      "loss": 0.5849,
      "step": 1153800
    },
    {
      "epoch": 18.114599686028257,
      "grad_norm": 3.67858624458313,
      "learning_rate": 3.8678375196232344e-05,
      "loss": 0.6101,
      "step": 1153900
    },
    {
      "epoch": 18.116169544740973,
      "grad_norm": 4.5340895652771,
      "learning_rate": 3.8677394034536895e-05,
      "loss": 0.6349,
      "step": 1154000
    },
    {
      "epoch": 18.11773940345369,
      "grad_norm": 2.8162662982940674,
      "learning_rate": 3.8676412872841446e-05,
      "loss": 0.6415,
      "step": 1154100
    },
    {
      "epoch": 18.119309262166404,
      "grad_norm": 4.37088680267334,
      "learning_rate": 3.8675431711145997e-05,
      "loss": 0.6243,
      "step": 1154200
    },
    {
      "epoch": 18.12087912087912,
      "grad_norm": 2.9741623401641846,
      "learning_rate": 3.8674450549450554e-05,
      "loss": 0.6577,
      "step": 1154300
    },
    {
      "epoch": 18.122448979591837,
      "grad_norm": 3.884711265563965,
      "learning_rate": 3.86734693877551e-05,
      "loss": 0.601,
      "step": 1154400
    },
    {
      "epoch": 18.124018838304554,
      "grad_norm": 3.963655710220337,
      "learning_rate": 3.8672488226059656e-05,
      "loss": 0.6071,
      "step": 1154500
    },
    {
      "epoch": 18.125588697017267,
      "grad_norm": 3.307023286819458,
      "learning_rate": 3.867150706436421e-05,
      "loss": 0.6074,
      "step": 1154600
    },
    {
      "epoch": 18.127158555729984,
      "grad_norm": 2.69183087348938,
      "learning_rate": 3.8670525902668765e-05,
      "loss": 0.6103,
      "step": 1154700
    },
    {
      "epoch": 18.1287284144427,
      "grad_norm": 4.228456974029541,
      "learning_rate": 3.866954474097331e-05,
      "loss": 0.6216,
      "step": 1154800
    },
    {
      "epoch": 18.130298273155415,
      "grad_norm": 3.708336591720581,
      "learning_rate": 3.866856357927787e-05,
      "loss": 0.6178,
      "step": 1154900
    },
    {
      "epoch": 18.13186813186813,
      "grad_norm": 2.8157131671905518,
      "learning_rate": 3.866758241758242e-05,
      "loss": 0.6118,
      "step": 1155000
    },
    {
      "epoch": 18.13343799058085,
      "grad_norm": 4.050940036773682,
      "learning_rate": 3.866660125588697e-05,
      "loss": 0.6096,
      "step": 1155100
    },
    {
      "epoch": 18.135007849293565,
      "grad_norm": 3.821563243865967,
      "learning_rate": 3.866562009419153e-05,
      "loss": 0.5941,
      "step": 1155200
    },
    {
      "epoch": 18.13657770800628,
      "grad_norm": 4.1226677894592285,
      "learning_rate": 3.866463893249608e-05,
      "loss": 0.6072,
      "step": 1155300
    },
    {
      "epoch": 18.138147566718995,
      "grad_norm": 4.149133205413818,
      "learning_rate": 3.866365777080063e-05,
      "loss": 0.6208,
      "step": 1155400
    },
    {
      "epoch": 18.139717425431712,
      "grad_norm": 3.7948660850524902,
      "learning_rate": 3.866267660910518e-05,
      "loss": 0.6097,
      "step": 1155500
    },
    {
      "epoch": 18.141287284144425,
      "grad_norm": 4.98252010345459,
      "learning_rate": 3.866169544740974e-05,
      "loss": 0.631,
      "step": 1155600
    },
    {
      "epoch": 18.142857142857142,
      "grad_norm": 4.186183929443359,
      "learning_rate": 3.866071428571429e-05,
      "loss": 0.6238,
      "step": 1155700
    },
    {
      "epoch": 18.14442700156986,
      "grad_norm": 4.405523300170898,
      "learning_rate": 3.865973312401884e-05,
      "loss": 0.6007,
      "step": 1155800
    },
    {
      "epoch": 18.145996860282576,
      "grad_norm": 3.9402599334716797,
      "learning_rate": 3.865875196232339e-05,
      "loss": 0.6133,
      "step": 1155900
    },
    {
      "epoch": 18.14756671899529,
      "grad_norm": 4.586944103240967,
      "learning_rate": 3.865777080062795e-05,
      "loss": 0.6301,
      "step": 1156000
    },
    {
      "epoch": 18.149136577708006,
      "grad_norm": 2.9334371089935303,
      "learning_rate": 3.86567896389325e-05,
      "loss": 0.6162,
      "step": 1156100
    },
    {
      "epoch": 18.150706436420723,
      "grad_norm": 4.413478851318359,
      "learning_rate": 3.865580847723705e-05,
      "loss": 0.6003,
      "step": 1156200
    },
    {
      "epoch": 18.152276295133436,
      "grad_norm": 4.128787994384766,
      "learning_rate": 3.86548273155416e-05,
      "loss": 0.6113,
      "step": 1156300
    },
    {
      "epoch": 18.153846153846153,
      "grad_norm": 4.019546985626221,
      "learning_rate": 3.865384615384616e-05,
      "loss": 0.5661,
      "step": 1156400
    },
    {
      "epoch": 18.15541601255887,
      "grad_norm": 3.359278440475464,
      "learning_rate": 3.86528649921507e-05,
      "loss": 0.5802,
      "step": 1156500
    },
    {
      "epoch": 18.156985871271587,
      "grad_norm": 3.786297082901001,
      "learning_rate": 3.865188383045526e-05,
      "loss": 0.6264,
      "step": 1156600
    },
    {
      "epoch": 18.1585557299843,
      "grad_norm": 3.2814204692840576,
      "learning_rate": 3.865090266875981e-05,
      "loss": 0.6042,
      "step": 1156700
    },
    {
      "epoch": 18.160125588697017,
      "grad_norm": 3.8387327194213867,
      "learning_rate": 3.864992150706437e-05,
      "loss": 0.61,
      "step": 1156800
    },
    {
      "epoch": 18.161695447409734,
      "grad_norm": 2.646009922027588,
      "learning_rate": 3.8648940345368914e-05,
      "loss": 0.5581,
      "step": 1156900
    },
    {
      "epoch": 18.163265306122447,
      "grad_norm": 4.492892742156982,
      "learning_rate": 3.864795918367347e-05,
      "loss": 0.5825,
      "step": 1157000
    },
    {
      "epoch": 18.164835164835164,
      "grad_norm": 4.28321647644043,
      "learning_rate": 3.864697802197802e-05,
      "loss": 0.6439,
      "step": 1157100
    },
    {
      "epoch": 18.16640502354788,
      "grad_norm": 4.3217973709106445,
      "learning_rate": 3.8645996860282573e-05,
      "loss": 0.5898,
      "step": 1157200
    },
    {
      "epoch": 18.167974882260598,
      "grad_norm": 5.449800968170166,
      "learning_rate": 3.864501569858713e-05,
      "loss": 0.6091,
      "step": 1157300
    },
    {
      "epoch": 18.16954474097331,
      "grad_norm": 4.043837547302246,
      "learning_rate": 3.864403453689168e-05,
      "loss": 0.6483,
      "step": 1157400
    },
    {
      "epoch": 18.171114599686028,
      "grad_norm": 3.8227453231811523,
      "learning_rate": 3.864305337519623e-05,
      "loss": 0.5876,
      "step": 1157500
    },
    {
      "epoch": 18.172684458398745,
      "grad_norm": 4.607327461242676,
      "learning_rate": 3.8642072213500784e-05,
      "loss": 0.5962,
      "step": 1157600
    },
    {
      "epoch": 18.17425431711146,
      "grad_norm": 4.421112537384033,
      "learning_rate": 3.864109105180534e-05,
      "loss": 0.5891,
      "step": 1157700
    },
    {
      "epoch": 18.175824175824175,
      "grad_norm": 3.520826578140259,
      "learning_rate": 3.864010989010989e-05,
      "loss": 0.5904,
      "step": 1157800
    },
    {
      "epoch": 18.177394034536892,
      "grad_norm": 4.285830020904541,
      "learning_rate": 3.8639128728414444e-05,
      "loss": 0.6512,
      "step": 1157900
    },
    {
      "epoch": 18.17896389324961,
      "grad_norm": 2.6708874702453613,
      "learning_rate": 3.8638147566718995e-05,
      "loss": 0.6228,
      "step": 1158000
    },
    {
      "epoch": 18.180533751962322,
      "grad_norm": 3.92175030708313,
      "learning_rate": 3.863716640502355e-05,
      "loss": 0.6388,
      "step": 1158100
    },
    {
      "epoch": 18.18210361067504,
      "grad_norm": 4.47019624710083,
      "learning_rate": 3.8636185243328104e-05,
      "loss": 0.5853,
      "step": 1158200
    },
    {
      "epoch": 18.183673469387756,
      "grad_norm": 3.9044182300567627,
      "learning_rate": 3.8635204081632655e-05,
      "loss": 0.6323,
      "step": 1158300
    },
    {
      "epoch": 18.18524332810047,
      "grad_norm": 3.6157119274139404,
      "learning_rate": 3.8634222919937206e-05,
      "loss": 0.5761,
      "step": 1158400
    },
    {
      "epoch": 18.186813186813186,
      "grad_norm": 4.545523643493652,
      "learning_rate": 3.863324175824176e-05,
      "loss": 0.5898,
      "step": 1158500
    },
    {
      "epoch": 18.188383045525903,
      "grad_norm": 4.528931140899658,
      "learning_rate": 3.863226059654631e-05,
      "loss": 0.6165,
      "step": 1158600
    },
    {
      "epoch": 18.18995290423862,
      "grad_norm": 4.269615650177002,
      "learning_rate": 3.8631279434850865e-05,
      "loss": 0.582,
      "step": 1158700
    },
    {
      "epoch": 18.191522762951333,
      "grad_norm": 4.366621971130371,
      "learning_rate": 3.8630298273155416e-05,
      "loss": 0.6076,
      "step": 1158800
    },
    {
      "epoch": 18.19309262166405,
      "grad_norm": 3.6438510417938232,
      "learning_rate": 3.8629317111459974e-05,
      "loss": 0.6103,
      "step": 1158900
    },
    {
      "epoch": 18.194662480376767,
      "grad_norm": 3.750833034515381,
      "learning_rate": 3.862833594976452e-05,
      "loss": 0.6132,
      "step": 1159000
    },
    {
      "epoch": 18.19623233908948,
      "grad_norm": 3.5681381225585938,
      "learning_rate": 3.8627354788069076e-05,
      "loss": 0.6177,
      "step": 1159100
    },
    {
      "epoch": 18.197802197802197,
      "grad_norm": 4.735739707946777,
      "learning_rate": 3.862637362637363e-05,
      "loss": 0.5763,
      "step": 1159200
    },
    {
      "epoch": 18.199372056514914,
      "grad_norm": 4.309195518493652,
      "learning_rate": 3.862539246467818e-05,
      "loss": 0.6116,
      "step": 1159300
    },
    {
      "epoch": 18.20094191522763,
      "grad_norm": 4.737024307250977,
      "learning_rate": 3.8624411302982736e-05,
      "loss": 0.6195,
      "step": 1159400
    },
    {
      "epoch": 18.202511773940344,
      "grad_norm": 3.6128411293029785,
      "learning_rate": 3.862343014128729e-05,
      "loss": 0.554,
      "step": 1159500
    },
    {
      "epoch": 18.20408163265306,
      "grad_norm": 3.9686646461486816,
      "learning_rate": 3.862244897959184e-05,
      "loss": 0.6124,
      "step": 1159600
    },
    {
      "epoch": 18.205651491365778,
      "grad_norm": 3.078322649002075,
      "learning_rate": 3.862146781789639e-05,
      "loss": 0.5823,
      "step": 1159700
    },
    {
      "epoch": 18.20722135007849,
      "grad_norm": 4.210809230804443,
      "learning_rate": 3.8620486656200946e-05,
      "loss": 0.6531,
      "step": 1159800
    },
    {
      "epoch": 18.208791208791208,
      "grad_norm": 4.189146041870117,
      "learning_rate": 3.86195054945055e-05,
      "loss": 0.6201,
      "step": 1159900
    },
    {
      "epoch": 18.210361067503925,
      "grad_norm": 3.645791530609131,
      "learning_rate": 3.861852433281005e-05,
      "loss": 0.5885,
      "step": 1160000
    },
    {
      "epoch": 18.211930926216642,
      "grad_norm": 4.093164920806885,
      "learning_rate": 3.86175431711146e-05,
      "loss": 0.5795,
      "step": 1160100
    },
    {
      "epoch": 18.213500784929355,
      "grad_norm": 3.0038278102874756,
      "learning_rate": 3.861656200941916e-05,
      "loss": 0.6172,
      "step": 1160200
    },
    {
      "epoch": 18.215070643642072,
      "grad_norm": 2.377601146697998,
      "learning_rate": 3.861558084772371e-05,
      "loss": 0.5885,
      "step": 1160300
    },
    {
      "epoch": 18.21664050235479,
      "grad_norm": 3.864297389984131,
      "learning_rate": 3.861459968602826e-05,
      "loss": 0.59,
      "step": 1160400
    },
    {
      "epoch": 18.218210361067506,
      "grad_norm": 4.390764236450195,
      "learning_rate": 3.861361852433281e-05,
      "loss": 0.6015,
      "step": 1160500
    },
    {
      "epoch": 18.21978021978022,
      "grad_norm": 3.3470754623413086,
      "learning_rate": 3.861263736263737e-05,
      "loss": 0.6198,
      "step": 1160600
    },
    {
      "epoch": 18.221350078492936,
      "grad_norm": 4.265140056610107,
      "learning_rate": 3.861165620094191e-05,
      "loss": 0.6263,
      "step": 1160700
    },
    {
      "epoch": 18.222919937205653,
      "grad_norm": 4.297624588012695,
      "learning_rate": 3.861067503924647e-05,
      "loss": 0.5798,
      "step": 1160800
    },
    {
      "epoch": 18.224489795918366,
      "grad_norm": 3.580387830734253,
      "learning_rate": 3.860969387755102e-05,
      "loss": 0.5929,
      "step": 1160900
    },
    {
      "epoch": 18.226059654631083,
      "grad_norm": 3.9912149906158447,
      "learning_rate": 3.860871271585558e-05,
      "loss": 0.5849,
      "step": 1161000
    },
    {
      "epoch": 18.2276295133438,
      "grad_norm": 4.316645622253418,
      "learning_rate": 3.860773155416012e-05,
      "loss": 0.6221,
      "step": 1161100
    },
    {
      "epoch": 18.229199372056517,
      "grad_norm": 4.080477714538574,
      "learning_rate": 3.860675039246468e-05,
      "loss": 0.6012,
      "step": 1161200
    },
    {
      "epoch": 18.23076923076923,
      "grad_norm": 4.235805988311768,
      "learning_rate": 3.860576923076923e-05,
      "loss": 0.5739,
      "step": 1161300
    },
    {
      "epoch": 18.232339089481947,
      "grad_norm": 4.7888898849487305,
      "learning_rate": 3.860478806907378e-05,
      "loss": 0.5918,
      "step": 1161400
    },
    {
      "epoch": 18.233908948194664,
      "grad_norm": 4.332433700561523,
      "learning_rate": 3.860380690737834e-05,
      "loss": 0.6065,
      "step": 1161500
    },
    {
      "epoch": 18.235478806907377,
      "grad_norm": 3.746307134628296,
      "learning_rate": 3.860282574568289e-05,
      "loss": 0.614,
      "step": 1161600
    },
    {
      "epoch": 18.237048665620094,
      "grad_norm": 3.8683221340179443,
      "learning_rate": 3.860184458398744e-05,
      "loss": 0.5908,
      "step": 1161700
    },
    {
      "epoch": 18.23861852433281,
      "grad_norm": 1.8230078220367432,
      "learning_rate": 3.860086342229199e-05,
      "loss": 0.5853,
      "step": 1161800
    },
    {
      "epoch": 18.240188383045528,
      "grad_norm": 2.768517017364502,
      "learning_rate": 3.859988226059655e-05,
      "loss": 0.5956,
      "step": 1161900
    },
    {
      "epoch": 18.24175824175824,
      "grad_norm": 3.9461112022399902,
      "learning_rate": 3.85989010989011e-05,
      "loss": 0.6027,
      "step": 1162000
    },
    {
      "epoch": 18.243328100470958,
      "grad_norm": 2.3505403995513916,
      "learning_rate": 3.859791993720565e-05,
      "loss": 0.584,
      "step": 1162100
    },
    {
      "epoch": 18.244897959183675,
      "grad_norm": 3.1302239894866943,
      "learning_rate": 3.8596938775510204e-05,
      "loss": 0.6166,
      "step": 1162200
    },
    {
      "epoch": 18.246467817896388,
      "grad_norm": 3.7121121883392334,
      "learning_rate": 3.859595761381476e-05,
      "loss": 0.6227,
      "step": 1162300
    },
    {
      "epoch": 18.248037676609105,
      "grad_norm": 3.1305792331695557,
      "learning_rate": 3.859497645211931e-05,
      "loss": 0.5915,
      "step": 1162400
    },
    {
      "epoch": 18.24960753532182,
      "grad_norm": 3.5648045539855957,
      "learning_rate": 3.8593995290423864e-05,
      "loss": 0.616,
      "step": 1162500
    },
    {
      "epoch": 18.25117739403454,
      "grad_norm": 4.186057090759277,
      "learning_rate": 3.8593014128728415e-05,
      "loss": 0.5751,
      "step": 1162600
    },
    {
      "epoch": 18.252747252747252,
      "grad_norm": 3.5622620582580566,
      "learning_rate": 3.859203296703297e-05,
      "loss": 0.6359,
      "step": 1162700
    },
    {
      "epoch": 18.25431711145997,
      "grad_norm": 2.870678424835205,
      "learning_rate": 3.8591051805337516e-05,
      "loss": 0.6021,
      "step": 1162800
    },
    {
      "epoch": 18.255886970172686,
      "grad_norm": 3.3413681983947754,
      "learning_rate": 3.8590070643642074e-05,
      "loss": 0.5592,
      "step": 1162900
    },
    {
      "epoch": 18.2574568288854,
      "grad_norm": 4.546273231506348,
      "learning_rate": 3.8589089481946625e-05,
      "loss": 0.6255,
      "step": 1163000
    },
    {
      "epoch": 18.259026687598116,
      "grad_norm": 3.3107552528381348,
      "learning_rate": 3.858810832025118e-05,
      "loss": 0.6035,
      "step": 1163100
    },
    {
      "epoch": 18.260596546310833,
      "grad_norm": 4.476245403289795,
      "learning_rate": 3.858712715855573e-05,
      "loss": 0.5953,
      "step": 1163200
    },
    {
      "epoch": 18.26216640502355,
      "grad_norm": 2.874621629714966,
      "learning_rate": 3.8586145996860285e-05,
      "loss": 0.6104,
      "step": 1163300
    },
    {
      "epoch": 18.263736263736263,
      "grad_norm": 3.148103952407837,
      "learning_rate": 3.8585164835164836e-05,
      "loss": 0.6014,
      "step": 1163400
    },
    {
      "epoch": 18.26530612244898,
      "grad_norm": 3.745884895324707,
      "learning_rate": 3.858418367346939e-05,
      "loss": 0.6245,
      "step": 1163500
    },
    {
      "epoch": 18.266875981161697,
      "grad_norm": 3.733879327774048,
      "learning_rate": 3.8583202511773945e-05,
      "loss": 0.6265,
      "step": 1163600
    },
    {
      "epoch": 18.26844583987441,
      "grad_norm": 3.1324679851531982,
      "learning_rate": 3.8582221350078496e-05,
      "loss": 0.6619,
      "step": 1163700
    },
    {
      "epoch": 18.270015698587127,
      "grad_norm": 3.7725653648376465,
      "learning_rate": 3.858124018838305e-05,
      "loss": 0.6255,
      "step": 1163800
    },
    {
      "epoch": 18.271585557299844,
      "grad_norm": 4.623006820678711,
      "learning_rate": 3.85802590266876e-05,
      "loss": 0.6179,
      "step": 1163900
    },
    {
      "epoch": 18.27315541601256,
      "grad_norm": 4.207984447479248,
      "learning_rate": 3.8579277864992155e-05,
      "loss": 0.5966,
      "step": 1164000
    },
    {
      "epoch": 18.274725274725274,
      "grad_norm": 4.734418869018555,
      "learning_rate": 3.8578296703296706e-05,
      "loss": 0.6001,
      "step": 1164100
    },
    {
      "epoch": 18.27629513343799,
      "grad_norm": 3.980147123336792,
      "learning_rate": 3.857731554160126e-05,
      "loss": 0.6245,
      "step": 1164200
    },
    {
      "epoch": 18.277864992150707,
      "grad_norm": 3.4593374729156494,
      "learning_rate": 3.857633437990581e-05,
      "loss": 0.6074,
      "step": 1164300
    },
    {
      "epoch": 18.27943485086342,
      "grad_norm": 3.8210887908935547,
      "learning_rate": 3.8575353218210366e-05,
      "loss": 0.6239,
      "step": 1164400
    },
    {
      "epoch": 18.281004709576138,
      "grad_norm": 4.0417585372924805,
      "learning_rate": 3.857437205651492e-05,
      "loss": 0.6201,
      "step": 1164500
    },
    {
      "epoch": 18.282574568288855,
      "grad_norm": 4.143115520477295,
      "learning_rate": 3.857339089481947e-05,
      "loss": 0.6063,
      "step": 1164600
    },
    {
      "epoch": 18.28414442700157,
      "grad_norm": 3.7347025871276855,
      "learning_rate": 3.857240973312402e-05,
      "loss": 0.5624,
      "step": 1164700
    },
    {
      "epoch": 18.285714285714285,
      "grad_norm": 3.156255006790161,
      "learning_rate": 3.857142857142858e-05,
      "loss": 0.6577,
      "step": 1164800
    },
    {
      "epoch": 18.287284144427,
      "grad_norm": 3.425633192062378,
      "learning_rate": 3.857044740973312e-05,
      "loss": 0.6351,
      "step": 1164900
    },
    {
      "epoch": 18.28885400313972,
      "grad_norm": 3.21600341796875,
      "learning_rate": 3.856946624803768e-05,
      "loss": 0.6003,
      "step": 1165000
    },
    {
      "epoch": 18.29042386185243,
      "grad_norm": 3.628983497619629,
      "learning_rate": 3.856848508634223e-05,
      "loss": 0.6,
      "step": 1165100
    },
    {
      "epoch": 18.29199372056515,
      "grad_norm": 3.463931083679199,
      "learning_rate": 3.856750392464679e-05,
      "loss": 0.5858,
      "step": 1165200
    },
    {
      "epoch": 18.293563579277865,
      "grad_norm": 3.974565267562866,
      "learning_rate": 3.856652276295133e-05,
      "loss": 0.6056,
      "step": 1165300
    },
    {
      "epoch": 18.295133437990582,
      "grad_norm": 3.7849953174591064,
      "learning_rate": 3.856554160125589e-05,
      "loss": 0.6217,
      "step": 1165400
    },
    {
      "epoch": 18.296703296703296,
      "grad_norm": 3.8849284648895264,
      "learning_rate": 3.856456043956044e-05,
      "loss": 0.6115,
      "step": 1165500
    },
    {
      "epoch": 18.298273155416013,
      "grad_norm": 4.205157279968262,
      "learning_rate": 3.856357927786499e-05,
      "loss": 0.6086,
      "step": 1165600
    },
    {
      "epoch": 18.29984301412873,
      "grad_norm": 5.349961280822754,
      "learning_rate": 3.856259811616955e-05,
      "loss": 0.57,
      "step": 1165700
    },
    {
      "epoch": 18.301412872841443,
      "grad_norm": 4.3338942527771,
      "learning_rate": 3.85616169544741e-05,
      "loss": 0.6038,
      "step": 1165800
    },
    {
      "epoch": 18.30298273155416,
      "grad_norm": 3.2305827140808105,
      "learning_rate": 3.856063579277865e-05,
      "loss": 0.6064,
      "step": 1165900
    },
    {
      "epoch": 18.304552590266876,
      "grad_norm": 4.493518829345703,
      "learning_rate": 3.85596546310832e-05,
      "loss": 0.605,
      "step": 1166000
    },
    {
      "epoch": 18.306122448979593,
      "grad_norm": 4.3838605880737305,
      "learning_rate": 3.855867346938776e-05,
      "loss": 0.6432,
      "step": 1166100
    },
    {
      "epoch": 18.307692307692307,
      "grad_norm": 4.517828941345215,
      "learning_rate": 3.855769230769231e-05,
      "loss": 0.6032,
      "step": 1166200
    },
    {
      "epoch": 18.309262166405023,
      "grad_norm": 4.065165996551514,
      "learning_rate": 3.855671114599686e-05,
      "loss": 0.6287,
      "step": 1166300
    },
    {
      "epoch": 18.31083202511774,
      "grad_norm": 4.509836196899414,
      "learning_rate": 3.855572998430141e-05,
      "loss": 0.5931,
      "step": 1166400
    },
    {
      "epoch": 18.312401883830454,
      "grad_norm": 4.0435309410095215,
      "learning_rate": 3.855474882260597e-05,
      "loss": 0.6256,
      "step": 1166500
    },
    {
      "epoch": 18.31397174254317,
      "grad_norm": 4.3116865158081055,
      "learning_rate": 3.855376766091052e-05,
      "loss": 0.6045,
      "step": 1166600
    },
    {
      "epoch": 18.315541601255887,
      "grad_norm": 3.1707959175109863,
      "learning_rate": 3.855278649921507e-05,
      "loss": 0.6055,
      "step": 1166700
    },
    {
      "epoch": 18.317111459968604,
      "grad_norm": 3.915018081665039,
      "learning_rate": 3.8551805337519623e-05,
      "loss": 0.6173,
      "step": 1166800
    },
    {
      "epoch": 18.318681318681318,
      "grad_norm": 4.290986061096191,
      "learning_rate": 3.855082417582418e-05,
      "loss": 0.606,
      "step": 1166900
    },
    {
      "epoch": 18.320251177394034,
      "grad_norm": 4.428864479064941,
      "learning_rate": 3.8549843014128725e-05,
      "loss": 0.5793,
      "step": 1167000
    },
    {
      "epoch": 18.32182103610675,
      "grad_norm": 3.780184268951416,
      "learning_rate": 3.854886185243328e-05,
      "loss": 0.6115,
      "step": 1167100
    },
    {
      "epoch": 18.323390894819465,
      "grad_norm": 4.41436243057251,
      "learning_rate": 3.8547880690737834e-05,
      "loss": 0.6114,
      "step": 1167200
    },
    {
      "epoch": 18.32496075353218,
      "grad_norm": 3.2307164669036865,
      "learning_rate": 3.854689952904239e-05,
      "loss": 0.6574,
      "step": 1167300
    },
    {
      "epoch": 18.3265306122449,
      "grad_norm": 3.686274766921997,
      "learning_rate": 3.8545918367346936e-05,
      "loss": 0.6104,
      "step": 1167400
    },
    {
      "epoch": 18.328100470957615,
      "grad_norm": 3.8663623332977295,
      "learning_rate": 3.8544937205651494e-05,
      "loss": 0.6094,
      "step": 1167500
    },
    {
      "epoch": 18.32967032967033,
      "grad_norm": 3.9863126277923584,
      "learning_rate": 3.8543956043956045e-05,
      "loss": 0.6022,
      "step": 1167600
    },
    {
      "epoch": 18.331240188383045,
      "grad_norm": 4.031998157501221,
      "learning_rate": 3.8542974882260596e-05,
      "loss": 0.6122,
      "step": 1167700
    },
    {
      "epoch": 18.332810047095762,
      "grad_norm": 4.6485490798950195,
      "learning_rate": 3.8541993720565154e-05,
      "loss": 0.6205,
      "step": 1167800
    },
    {
      "epoch": 18.334379905808476,
      "grad_norm": 3.153848886489868,
      "learning_rate": 3.8541012558869705e-05,
      "loss": 0.6214,
      "step": 1167900
    },
    {
      "epoch": 18.335949764521192,
      "grad_norm": 4.19741153717041,
      "learning_rate": 3.8540031397174256e-05,
      "loss": 0.5893,
      "step": 1168000
    },
    {
      "epoch": 18.33751962323391,
      "grad_norm": 3.5367271900177,
      "learning_rate": 3.8539050235478807e-05,
      "loss": 0.6075,
      "step": 1168100
    },
    {
      "epoch": 18.339089481946626,
      "grad_norm": 4.040196418762207,
      "learning_rate": 3.8538069073783364e-05,
      "loss": 0.5791,
      "step": 1168200
    },
    {
      "epoch": 18.34065934065934,
      "grad_norm": 5.0289483070373535,
      "learning_rate": 3.8537087912087915e-05,
      "loss": 0.6184,
      "step": 1168300
    },
    {
      "epoch": 18.342229199372056,
      "grad_norm": 3.2416281700134277,
      "learning_rate": 3.8536106750392466e-05,
      "loss": 0.5844,
      "step": 1168400
    },
    {
      "epoch": 18.343799058084773,
      "grad_norm": 3.9082658290863037,
      "learning_rate": 3.853512558869702e-05,
      "loss": 0.6344,
      "step": 1168500
    },
    {
      "epoch": 18.345368916797486,
      "grad_norm": 3.7118582725524902,
      "learning_rate": 3.8534144427001575e-05,
      "loss": 0.6484,
      "step": 1168600
    },
    {
      "epoch": 18.346938775510203,
      "grad_norm": 3.3354570865631104,
      "learning_rate": 3.8533163265306126e-05,
      "loss": 0.5838,
      "step": 1168700
    },
    {
      "epoch": 18.34850863422292,
      "grad_norm": 4.482194900512695,
      "learning_rate": 3.853218210361068e-05,
      "loss": 0.6251,
      "step": 1168800
    },
    {
      "epoch": 18.350078492935637,
      "grad_norm": 3.9465017318725586,
      "learning_rate": 3.853120094191523e-05,
      "loss": 0.614,
      "step": 1168900
    },
    {
      "epoch": 18.35164835164835,
      "grad_norm": 3.9013257026672363,
      "learning_rate": 3.8530219780219786e-05,
      "loss": 0.6081,
      "step": 1169000
    },
    {
      "epoch": 18.353218210361067,
      "grad_norm": 3.490948438644409,
      "learning_rate": 3.852923861852433e-05,
      "loss": 0.5834,
      "step": 1169100
    },
    {
      "epoch": 18.354788069073784,
      "grad_norm": 3.725780963897705,
      "learning_rate": 3.852825745682889e-05,
      "loss": 0.6086,
      "step": 1169200
    },
    {
      "epoch": 18.356357927786497,
      "grad_norm": 3.3356175422668457,
      "learning_rate": 3.852727629513344e-05,
      "loss": 0.645,
      "step": 1169300
    },
    {
      "epoch": 18.357927786499214,
      "grad_norm": 3.5826966762542725,
      "learning_rate": 3.8526295133437996e-05,
      "loss": 0.6298,
      "step": 1169400
    },
    {
      "epoch": 18.35949764521193,
      "grad_norm": 3.494450330734253,
      "learning_rate": 3.852531397174254e-05,
      "loss": 0.6391,
      "step": 1169500
    },
    {
      "epoch": 18.361067503924648,
      "grad_norm": 3.059866189956665,
      "learning_rate": 3.85243328100471e-05,
      "loss": 0.5679,
      "step": 1169600
    },
    {
      "epoch": 18.36263736263736,
      "grad_norm": 3.7903831005096436,
      "learning_rate": 3.852335164835165e-05,
      "loss": 0.6062,
      "step": 1169700
    },
    {
      "epoch": 18.364207221350078,
      "grad_norm": 5.11016845703125,
      "learning_rate": 3.85223704866562e-05,
      "loss": 0.6346,
      "step": 1169800
    },
    {
      "epoch": 18.365777080062795,
      "grad_norm": 4.042714595794678,
      "learning_rate": 3.852138932496076e-05,
      "loss": 0.5977,
      "step": 1169900
    },
    {
      "epoch": 18.367346938775512,
      "grad_norm": 4.471413612365723,
      "learning_rate": 3.852040816326531e-05,
      "loss": 0.6139,
      "step": 1170000
    },
    {
      "epoch": 18.368916797488225,
      "grad_norm": 3.398127317428589,
      "learning_rate": 3.851942700156986e-05,
      "loss": 0.6142,
      "step": 1170100
    },
    {
      "epoch": 18.370486656200942,
      "grad_norm": 5.051486492156982,
      "learning_rate": 3.851844583987441e-05,
      "loss": 0.6362,
      "step": 1170200
    },
    {
      "epoch": 18.37205651491366,
      "grad_norm": 3.933614730834961,
      "learning_rate": 3.851746467817897e-05,
      "loss": 0.6537,
      "step": 1170300
    },
    {
      "epoch": 18.373626373626372,
      "grad_norm": 3.227888822555542,
      "learning_rate": 3.851648351648352e-05,
      "loss": 0.6049,
      "step": 1170400
    },
    {
      "epoch": 18.37519623233909,
      "grad_norm": 4.654229640960693,
      "learning_rate": 3.851550235478807e-05,
      "loss": 0.5913,
      "step": 1170500
    },
    {
      "epoch": 18.376766091051806,
      "grad_norm": 3.3013548851013184,
      "learning_rate": 3.851452119309262e-05,
      "loss": 0.5709,
      "step": 1170600
    },
    {
      "epoch": 18.378335949764523,
      "grad_norm": 3.675022602081299,
      "learning_rate": 3.851354003139718e-05,
      "loss": 0.5936,
      "step": 1170700
    },
    {
      "epoch": 18.379905808477236,
      "grad_norm": 3.3379721641540527,
      "learning_rate": 3.851255886970173e-05,
      "loss": 0.6426,
      "step": 1170800
    },
    {
      "epoch": 18.381475667189953,
      "grad_norm": 3.3401331901550293,
      "learning_rate": 3.851157770800628e-05,
      "loss": 0.6418,
      "step": 1170900
    },
    {
      "epoch": 18.38304552590267,
      "grad_norm": 4.04580545425415,
      "learning_rate": 3.851059654631083e-05,
      "loss": 0.6099,
      "step": 1171000
    },
    {
      "epoch": 18.384615384615383,
      "grad_norm": 3.2215802669525146,
      "learning_rate": 3.850961538461539e-05,
      "loss": 0.6116,
      "step": 1171100
    },
    {
      "epoch": 18.3861852433281,
      "grad_norm": 2.6113626956939697,
      "learning_rate": 3.8508634222919934e-05,
      "loss": 0.6153,
      "step": 1171200
    },
    {
      "epoch": 18.387755102040817,
      "grad_norm": 3.2713708877563477,
      "learning_rate": 3.850765306122449e-05,
      "loss": 0.6686,
      "step": 1171300
    },
    {
      "epoch": 18.389324960753534,
      "grad_norm": 3.716376543045044,
      "learning_rate": 3.850667189952904e-05,
      "loss": 0.581,
      "step": 1171400
    },
    {
      "epoch": 18.390894819466247,
      "grad_norm": 2.645390272140503,
      "learning_rate": 3.85056907378336e-05,
      "loss": 0.6417,
      "step": 1171500
    },
    {
      "epoch": 18.392464678178964,
      "grad_norm": 3.5712666511535645,
      "learning_rate": 3.8504709576138145e-05,
      "loss": 0.6053,
      "step": 1171600
    },
    {
      "epoch": 18.39403453689168,
      "grad_norm": 4.3911542892456055,
      "learning_rate": 3.85037284144427e-05,
      "loss": 0.5855,
      "step": 1171700
    },
    {
      "epoch": 18.395604395604394,
      "grad_norm": 3.852766275405884,
      "learning_rate": 3.8502747252747254e-05,
      "loss": 0.6242,
      "step": 1171800
    },
    {
      "epoch": 18.39717425431711,
      "grad_norm": 4.237832546234131,
      "learning_rate": 3.8501766091051805e-05,
      "loss": 0.6106,
      "step": 1171900
    },
    {
      "epoch": 18.398744113029828,
      "grad_norm": 3.5406455993652344,
      "learning_rate": 3.850078492935636e-05,
      "loss": 0.6138,
      "step": 1172000
    },
    {
      "epoch": 18.400313971742545,
      "grad_norm": 3.478029727935791,
      "learning_rate": 3.8499803767660914e-05,
      "loss": 0.6375,
      "step": 1172100
    },
    {
      "epoch": 18.401883830455258,
      "grad_norm": 4.000338554382324,
      "learning_rate": 3.8498822605965465e-05,
      "loss": 0.6555,
      "step": 1172200
    },
    {
      "epoch": 18.403453689167975,
      "grad_norm": 3.687180995941162,
      "learning_rate": 3.8497841444270016e-05,
      "loss": 0.6121,
      "step": 1172300
    },
    {
      "epoch": 18.405023547880692,
      "grad_norm": 5.69191837310791,
      "learning_rate": 3.849686028257457e-05,
      "loss": 0.6414,
      "step": 1172400
    },
    {
      "epoch": 18.406593406593405,
      "grad_norm": 2.4868459701538086,
      "learning_rate": 3.8495879120879124e-05,
      "loss": 0.6132,
      "step": 1172500
    },
    {
      "epoch": 18.408163265306122,
      "grad_norm": 4.234446048736572,
      "learning_rate": 3.8494897959183675e-05,
      "loss": 0.6238,
      "step": 1172600
    },
    {
      "epoch": 18.40973312401884,
      "grad_norm": 3.3839056491851807,
      "learning_rate": 3.8493916797488226e-05,
      "loss": 0.5948,
      "step": 1172700
    },
    {
      "epoch": 18.411302982731556,
      "grad_norm": 2.9806406497955322,
      "learning_rate": 3.8492935635792784e-05,
      "loss": 0.603,
      "step": 1172800
    },
    {
      "epoch": 18.41287284144427,
      "grad_norm": 2.5940206050872803,
      "learning_rate": 3.849195447409733e-05,
      "loss": 0.6205,
      "step": 1172900
    },
    {
      "epoch": 18.414442700156986,
      "grad_norm": 3.8552944660186768,
      "learning_rate": 3.8490973312401886e-05,
      "loss": 0.6283,
      "step": 1173000
    },
    {
      "epoch": 18.416012558869703,
      "grad_norm": 2.8039703369140625,
      "learning_rate": 3.848999215070644e-05,
      "loss": 0.6014,
      "step": 1173100
    },
    {
      "epoch": 18.417582417582416,
      "grad_norm": 3.44704532623291,
      "learning_rate": 3.8489010989010995e-05,
      "loss": 0.6209,
      "step": 1173200
    },
    {
      "epoch": 18.419152276295133,
      "grad_norm": 3.688898801803589,
      "learning_rate": 3.848802982731554e-05,
      "loss": 0.6288,
      "step": 1173300
    },
    {
      "epoch": 18.42072213500785,
      "grad_norm": 4.746547222137451,
      "learning_rate": 3.84870486656201e-05,
      "loss": 0.6403,
      "step": 1173400
    },
    {
      "epoch": 18.422291993720567,
      "grad_norm": 3.1263444423675537,
      "learning_rate": 3.848606750392465e-05,
      "loss": 0.586,
      "step": 1173500
    },
    {
      "epoch": 18.42386185243328,
      "grad_norm": 3.4929771423339844,
      "learning_rate": 3.84850863422292e-05,
      "loss": 0.6304,
      "step": 1173600
    },
    {
      "epoch": 18.425431711145997,
      "grad_norm": 3.8548338413238525,
      "learning_rate": 3.848410518053375e-05,
      "loss": 0.6298,
      "step": 1173700
    },
    {
      "epoch": 18.427001569858714,
      "grad_norm": 2.4020252227783203,
      "learning_rate": 3.848312401883831e-05,
      "loss": 0.6375,
      "step": 1173800
    },
    {
      "epoch": 18.428571428571427,
      "grad_norm": 4.5298566818237305,
      "learning_rate": 3.848214285714286e-05,
      "loss": 0.5936,
      "step": 1173900
    },
    {
      "epoch": 18.430141287284144,
      "grad_norm": 3.405419111251831,
      "learning_rate": 3.848116169544741e-05,
      "loss": 0.6023,
      "step": 1174000
    },
    {
      "epoch": 18.43171114599686,
      "grad_norm": 4.152820587158203,
      "learning_rate": 3.848018053375197e-05,
      "loss": 0.5884,
      "step": 1174100
    },
    {
      "epoch": 18.433281004709578,
      "grad_norm": 3.753276824951172,
      "learning_rate": 3.847919937205652e-05,
      "loss": 0.6303,
      "step": 1174200
    },
    {
      "epoch": 18.43485086342229,
      "grad_norm": 3.3860368728637695,
      "learning_rate": 3.847821821036107e-05,
      "loss": 0.5968,
      "step": 1174300
    },
    {
      "epoch": 18.436420722135008,
      "grad_norm": 3.738964319229126,
      "learning_rate": 3.847723704866562e-05,
      "loss": 0.5765,
      "step": 1174400
    },
    {
      "epoch": 18.437990580847725,
      "grad_norm": 4.023591995239258,
      "learning_rate": 3.847625588697018e-05,
      "loss": 0.585,
      "step": 1174500
    },
    {
      "epoch": 18.439560439560438,
      "grad_norm": 4.000659942626953,
      "learning_rate": 3.847527472527473e-05,
      "loss": 0.6196,
      "step": 1174600
    },
    {
      "epoch": 18.441130298273155,
      "grad_norm": 3.3551812171936035,
      "learning_rate": 3.847429356357928e-05,
      "loss": 0.5903,
      "step": 1174700
    },
    {
      "epoch": 18.44270015698587,
      "grad_norm": 4.104404449462891,
      "learning_rate": 3.847331240188383e-05,
      "loss": 0.5473,
      "step": 1174800
    },
    {
      "epoch": 18.44427001569859,
      "grad_norm": 3.414257526397705,
      "learning_rate": 3.847233124018839e-05,
      "loss": 0.6194,
      "step": 1174900
    },
    {
      "epoch": 18.445839874411302,
      "grad_norm": 3.7262048721313477,
      "learning_rate": 3.847135007849293e-05,
      "loss": 0.6161,
      "step": 1175000
    },
    {
      "epoch": 18.44740973312402,
      "grad_norm": 3.6196446418762207,
      "learning_rate": 3.847036891679749e-05,
      "loss": 0.6025,
      "step": 1175100
    },
    {
      "epoch": 18.448979591836736,
      "grad_norm": 3.911851406097412,
      "learning_rate": 3.846938775510204e-05,
      "loss": 0.6443,
      "step": 1175200
    },
    {
      "epoch": 18.45054945054945,
      "grad_norm": 4.466725826263428,
      "learning_rate": 3.84684065934066e-05,
      "loss": 0.5829,
      "step": 1175300
    },
    {
      "epoch": 18.452119309262166,
      "grad_norm": 4.021188735961914,
      "learning_rate": 3.8467425431711143e-05,
      "loss": 0.6329,
      "step": 1175400
    },
    {
      "epoch": 18.453689167974883,
      "grad_norm": 4.195682048797607,
      "learning_rate": 3.84664442700157e-05,
      "loss": 0.5807,
      "step": 1175500
    },
    {
      "epoch": 18.4552590266876,
      "grad_norm": 3.79229736328125,
      "learning_rate": 3.846546310832025e-05,
      "loss": 0.651,
      "step": 1175600
    },
    {
      "epoch": 18.456828885400313,
      "grad_norm": 2.2757937908172607,
      "learning_rate": 3.84644819466248e-05,
      "loss": 0.6181,
      "step": 1175700
    },
    {
      "epoch": 18.45839874411303,
      "grad_norm": 4.146231651306152,
      "learning_rate": 3.8463500784929354e-05,
      "loss": 0.6106,
      "step": 1175800
    },
    {
      "epoch": 18.459968602825747,
      "grad_norm": 3.678199291229248,
      "learning_rate": 3.846251962323391e-05,
      "loss": 0.6119,
      "step": 1175900
    },
    {
      "epoch": 18.46153846153846,
      "grad_norm": 3.676575183868408,
      "learning_rate": 3.846153846153846e-05,
      "loss": 0.5818,
      "step": 1176000
    },
    {
      "epoch": 18.463108320251177,
      "grad_norm": 2.599360942840576,
      "learning_rate": 3.8460557299843014e-05,
      "loss": 0.6058,
      "step": 1176100
    },
    {
      "epoch": 18.464678178963894,
      "grad_norm": 3.696026086807251,
      "learning_rate": 3.845957613814757e-05,
      "loss": 0.6209,
      "step": 1176200
    },
    {
      "epoch": 18.46624803767661,
      "grad_norm": 5.165064334869385,
      "learning_rate": 3.845859497645212e-05,
      "loss": 0.6619,
      "step": 1176300
    },
    {
      "epoch": 18.467817896389324,
      "grad_norm": 3.602475643157959,
      "learning_rate": 3.8457613814756674e-05,
      "loss": 0.597,
      "step": 1176400
    },
    {
      "epoch": 18.46938775510204,
      "grad_norm": 2.540344476699829,
      "learning_rate": 3.8456632653061225e-05,
      "loss": 0.6093,
      "step": 1176500
    },
    {
      "epoch": 18.470957613814758,
      "grad_norm": 3.5787079334259033,
      "learning_rate": 3.845565149136578e-05,
      "loss": 0.6124,
      "step": 1176600
    },
    {
      "epoch": 18.47252747252747,
      "grad_norm": 3.3190832138061523,
      "learning_rate": 3.845467032967033e-05,
      "loss": 0.6325,
      "step": 1176700
    },
    {
      "epoch": 18.474097331240188,
      "grad_norm": 4.621532917022705,
      "learning_rate": 3.8453689167974884e-05,
      "loss": 0.5941,
      "step": 1176800
    },
    {
      "epoch": 18.475667189952905,
      "grad_norm": 3.241131544113159,
      "learning_rate": 3.8452708006279435e-05,
      "loss": 0.6219,
      "step": 1176900
    },
    {
      "epoch": 18.47723704866562,
      "grad_norm": 2.6426548957824707,
      "learning_rate": 3.845172684458399e-05,
      "loss": 0.5989,
      "step": 1177000
    },
    {
      "epoch": 18.478806907378335,
      "grad_norm": 3.0585641860961914,
      "learning_rate": 3.845074568288854e-05,
      "loss": 0.57,
      "step": 1177100
    },
    {
      "epoch": 18.48037676609105,
      "grad_norm": 3.4166948795318604,
      "learning_rate": 3.8449764521193095e-05,
      "loss": 0.6221,
      "step": 1177200
    },
    {
      "epoch": 18.48194662480377,
      "grad_norm": 4.012842655181885,
      "learning_rate": 3.8448783359497646e-05,
      "loss": 0.633,
      "step": 1177300
    },
    {
      "epoch": 18.483516483516482,
      "grad_norm": 4.430439472198486,
      "learning_rate": 3.8447802197802204e-05,
      "loss": 0.6208,
      "step": 1177400
    },
    {
      "epoch": 18.4850863422292,
      "grad_norm": 3.7154951095581055,
      "learning_rate": 3.844682103610675e-05,
      "loss": 0.5963,
      "step": 1177500
    },
    {
      "epoch": 18.486656200941916,
      "grad_norm": 3.0514719486236572,
      "learning_rate": 3.8445839874411306e-05,
      "loss": 0.6062,
      "step": 1177600
    },
    {
      "epoch": 18.488226059654632,
      "grad_norm": 4.496368885040283,
      "learning_rate": 3.844485871271586e-05,
      "loss": 0.6015,
      "step": 1177700
    },
    {
      "epoch": 18.489795918367346,
      "grad_norm": 4.800639629364014,
      "learning_rate": 3.844387755102041e-05,
      "loss": 0.6211,
      "step": 1177800
    },
    {
      "epoch": 18.491365777080063,
      "grad_norm": 1.9734598398208618,
      "learning_rate": 3.844289638932496e-05,
      "loss": 0.5563,
      "step": 1177900
    },
    {
      "epoch": 18.49293563579278,
      "grad_norm": 3.7838282585144043,
      "learning_rate": 3.8441915227629516e-05,
      "loss": 0.6086,
      "step": 1178000
    },
    {
      "epoch": 18.494505494505496,
      "grad_norm": 5.023614883422852,
      "learning_rate": 3.844093406593407e-05,
      "loss": 0.6126,
      "step": 1178100
    },
    {
      "epoch": 18.49607535321821,
      "grad_norm": 4.4335784912109375,
      "learning_rate": 3.843995290423862e-05,
      "loss": 0.6113,
      "step": 1178200
    },
    {
      "epoch": 18.497645211930926,
      "grad_norm": 3.914280414581299,
      "learning_rate": 3.8438971742543176e-05,
      "loss": 0.5775,
      "step": 1178300
    },
    {
      "epoch": 18.499215070643643,
      "grad_norm": 3.9418814182281494,
      "learning_rate": 3.843799058084773e-05,
      "loss": 0.5873,
      "step": 1178400
    },
    {
      "epoch": 18.500784929356357,
      "grad_norm": 3.646104574203491,
      "learning_rate": 3.843700941915228e-05,
      "loss": 0.6252,
      "step": 1178500
    },
    {
      "epoch": 18.502354788069074,
      "grad_norm": 3.475952625274658,
      "learning_rate": 3.843602825745683e-05,
      "loss": 0.6224,
      "step": 1178600
    },
    {
      "epoch": 18.50392464678179,
      "grad_norm": 2.9955954551696777,
      "learning_rate": 3.843504709576139e-05,
      "loss": 0.611,
      "step": 1178700
    },
    {
      "epoch": 18.505494505494504,
      "grad_norm": 3.3659520149230957,
      "learning_rate": 3.843406593406594e-05,
      "loss": 0.5914,
      "step": 1178800
    },
    {
      "epoch": 18.50706436420722,
      "grad_norm": 4.460586071014404,
      "learning_rate": 3.843308477237049e-05,
      "loss": 0.6254,
      "step": 1178900
    },
    {
      "epoch": 18.508634222919937,
      "grad_norm": 3.1413991451263428,
      "learning_rate": 3.843210361067504e-05,
      "loss": 0.5975,
      "step": 1179000
    },
    {
      "epoch": 18.510204081632654,
      "grad_norm": 4.710160255432129,
      "learning_rate": 3.84311224489796e-05,
      "loss": 0.6012,
      "step": 1179100
    },
    {
      "epoch": 18.511773940345368,
      "grad_norm": 4.409731864929199,
      "learning_rate": 3.843014128728414e-05,
      "loss": 0.5626,
      "step": 1179200
    },
    {
      "epoch": 18.513343799058084,
      "grad_norm": 3.847951650619507,
      "learning_rate": 3.84291601255887e-05,
      "loss": 0.6175,
      "step": 1179300
    },
    {
      "epoch": 18.5149136577708,
      "grad_norm": 2.2987191677093506,
      "learning_rate": 3.842817896389325e-05,
      "loss": 0.6176,
      "step": 1179400
    },
    {
      "epoch": 18.516483516483518,
      "grad_norm": 2.8872764110565186,
      "learning_rate": 3.842719780219781e-05,
      "loss": 0.6066,
      "step": 1179500
    },
    {
      "epoch": 18.51805337519623,
      "grad_norm": 3.3532800674438477,
      "learning_rate": 3.842621664050235e-05,
      "loss": 0.5562,
      "step": 1179600
    },
    {
      "epoch": 18.51962323390895,
      "grad_norm": 3.866056203842163,
      "learning_rate": 3.842523547880691e-05,
      "loss": 0.6277,
      "step": 1179700
    },
    {
      "epoch": 18.521193092621665,
      "grad_norm": 4.5033745765686035,
      "learning_rate": 3.842425431711146e-05,
      "loss": 0.5618,
      "step": 1179800
    },
    {
      "epoch": 18.52276295133438,
      "grad_norm": 3.9134621620178223,
      "learning_rate": 3.842327315541601e-05,
      "loss": 0.6353,
      "step": 1179900
    },
    {
      "epoch": 18.524332810047095,
      "grad_norm": 4.424439430236816,
      "learning_rate": 3.842229199372056e-05,
      "loss": 0.6159,
      "step": 1180000
    },
    {
      "epoch": 18.525902668759812,
      "grad_norm": 3.510963201522827,
      "learning_rate": 3.842131083202512e-05,
      "loss": 0.6445,
      "step": 1180100
    },
    {
      "epoch": 18.52747252747253,
      "grad_norm": 3.9137957096099854,
      "learning_rate": 3.842032967032967e-05,
      "loss": 0.6614,
      "step": 1180200
    },
    {
      "epoch": 18.529042386185242,
      "grad_norm": 3.630052089691162,
      "learning_rate": 3.841934850863422e-05,
      "loss": 0.6356,
      "step": 1180300
    },
    {
      "epoch": 18.53061224489796,
      "grad_norm": 4.259570598602295,
      "learning_rate": 3.841836734693878e-05,
      "loss": 0.5887,
      "step": 1180400
    },
    {
      "epoch": 18.532182103610676,
      "grad_norm": 3.2326419353485107,
      "learning_rate": 3.841738618524333e-05,
      "loss": 0.6396,
      "step": 1180500
    },
    {
      "epoch": 18.53375196232339,
      "grad_norm": 3.47807240486145,
      "learning_rate": 3.841640502354788e-05,
      "loss": 0.6311,
      "step": 1180600
    },
    {
      "epoch": 18.535321821036106,
      "grad_norm": 3.785183906555176,
      "learning_rate": 3.8415423861852434e-05,
      "loss": 0.635,
      "step": 1180700
    },
    {
      "epoch": 18.536891679748823,
      "grad_norm": 3.979957342147827,
      "learning_rate": 3.841444270015699e-05,
      "loss": 0.5972,
      "step": 1180800
    },
    {
      "epoch": 18.53846153846154,
      "grad_norm": 3.151761531829834,
      "learning_rate": 3.841346153846154e-05,
      "loss": 0.623,
      "step": 1180900
    },
    {
      "epoch": 18.540031397174253,
      "grad_norm": 3.1299548149108887,
      "learning_rate": 3.841248037676609e-05,
      "loss": 0.5817,
      "step": 1181000
    },
    {
      "epoch": 18.54160125588697,
      "grad_norm": 3.5990138053894043,
      "learning_rate": 3.8411499215070644e-05,
      "loss": 0.6043,
      "step": 1181100
    },
    {
      "epoch": 18.543171114599687,
      "grad_norm": 1.9519777297973633,
      "learning_rate": 3.84105180533752e-05,
      "loss": 0.65,
      "step": 1181200
    },
    {
      "epoch": 18.5447409733124,
      "grad_norm": 5.295146465301514,
      "learning_rate": 3.8409536891679746e-05,
      "loss": 0.5967,
      "step": 1181300
    },
    {
      "epoch": 18.546310832025117,
      "grad_norm": 4.217931747436523,
      "learning_rate": 3.8408555729984304e-05,
      "loss": 0.6029,
      "step": 1181400
    },
    {
      "epoch": 18.547880690737834,
      "grad_norm": 4.279810428619385,
      "learning_rate": 3.8407574568288855e-05,
      "loss": 0.6043,
      "step": 1181500
    },
    {
      "epoch": 18.54945054945055,
      "grad_norm": 4.670173645019531,
      "learning_rate": 3.840659340659341e-05,
      "loss": 0.6296,
      "step": 1181600
    },
    {
      "epoch": 18.551020408163264,
      "grad_norm": 3.755516767501831,
      "learning_rate": 3.840561224489796e-05,
      "loss": 0.6114,
      "step": 1181700
    },
    {
      "epoch": 18.55259026687598,
      "grad_norm": 4.42317008972168,
      "learning_rate": 3.8404631083202515e-05,
      "loss": 0.6102,
      "step": 1181800
    },
    {
      "epoch": 18.554160125588698,
      "grad_norm": 4.378042221069336,
      "learning_rate": 3.8403649921507066e-05,
      "loss": 0.6171,
      "step": 1181900
    },
    {
      "epoch": 18.55572998430141,
      "grad_norm": 3.7979965209960938,
      "learning_rate": 3.8402668759811617e-05,
      "loss": 0.6202,
      "step": 1182000
    },
    {
      "epoch": 18.55729984301413,
      "grad_norm": 4.311962127685547,
      "learning_rate": 3.840168759811617e-05,
      "loss": 0.5989,
      "step": 1182100
    },
    {
      "epoch": 18.558869701726845,
      "grad_norm": 3.146343469619751,
      "learning_rate": 3.8400706436420725e-05,
      "loss": 0.6212,
      "step": 1182200
    },
    {
      "epoch": 18.560439560439562,
      "grad_norm": 2.952332019805908,
      "learning_rate": 3.8399725274725276e-05,
      "loss": 0.6171,
      "step": 1182300
    },
    {
      "epoch": 18.562009419152275,
      "grad_norm": 4.167517185211182,
      "learning_rate": 3.839874411302983e-05,
      "loss": 0.6339,
      "step": 1182400
    },
    {
      "epoch": 18.563579277864992,
      "grad_norm": 4.58998966217041,
      "learning_rate": 3.8397762951334385e-05,
      "loss": 0.5856,
      "step": 1182500
    },
    {
      "epoch": 18.56514913657771,
      "grad_norm": 4.183865070343018,
      "learning_rate": 3.8396781789638936e-05,
      "loss": 0.5822,
      "step": 1182600
    },
    {
      "epoch": 18.566718995290422,
      "grad_norm": 2.862649917602539,
      "learning_rate": 3.839580062794349e-05,
      "loss": 0.6191,
      "step": 1182700
    },
    {
      "epoch": 18.56828885400314,
      "grad_norm": 3.6015892028808594,
      "learning_rate": 3.839481946624804e-05,
      "loss": 0.6407,
      "step": 1182800
    },
    {
      "epoch": 18.569858712715856,
      "grad_norm": 3.328803300857544,
      "learning_rate": 3.8393838304552596e-05,
      "loss": 0.6178,
      "step": 1182900
    },
    {
      "epoch": 18.571428571428573,
      "grad_norm": 3.1250853538513184,
      "learning_rate": 3.839285714285715e-05,
      "loss": 0.621,
      "step": 1183000
    },
    {
      "epoch": 18.572998430141286,
      "grad_norm": 4.16347599029541,
      "learning_rate": 3.83918759811617e-05,
      "loss": 0.6555,
      "step": 1183100
    },
    {
      "epoch": 18.574568288854003,
      "grad_norm": 4.303194046020508,
      "learning_rate": 3.839089481946625e-05,
      "loss": 0.6097,
      "step": 1183200
    },
    {
      "epoch": 18.57613814756672,
      "grad_norm": 3.2146694660186768,
      "learning_rate": 3.8389913657770806e-05,
      "loss": 0.6562,
      "step": 1183300
    },
    {
      "epoch": 18.577708006279433,
      "grad_norm": 2.9112112522125244,
      "learning_rate": 3.838893249607535e-05,
      "loss": 0.5727,
      "step": 1183400
    },
    {
      "epoch": 18.57927786499215,
      "grad_norm": 3.08528733253479,
      "learning_rate": 3.838795133437991e-05,
      "loss": 0.6048,
      "step": 1183500
    },
    {
      "epoch": 18.580847723704867,
      "grad_norm": 3.7474939823150635,
      "learning_rate": 3.838697017268446e-05,
      "loss": 0.5724,
      "step": 1183600
    },
    {
      "epoch": 18.582417582417584,
      "grad_norm": 4.017163276672363,
      "learning_rate": 3.838598901098902e-05,
      "loss": 0.598,
      "step": 1183700
    },
    {
      "epoch": 18.583987441130297,
      "grad_norm": 2.6068999767303467,
      "learning_rate": 3.838500784929356e-05,
      "loss": 0.5665,
      "step": 1183800
    },
    {
      "epoch": 18.585557299843014,
      "grad_norm": 3.091989517211914,
      "learning_rate": 3.838402668759812e-05,
      "loss": 0.5825,
      "step": 1183900
    },
    {
      "epoch": 18.58712715855573,
      "grad_norm": 4.3513360023498535,
      "learning_rate": 3.838304552590267e-05,
      "loss": 0.568,
      "step": 1184000
    },
    {
      "epoch": 18.588697017268444,
      "grad_norm": 3.2626943588256836,
      "learning_rate": 3.838206436420722e-05,
      "loss": 0.6218,
      "step": 1184100
    },
    {
      "epoch": 18.59026687598116,
      "grad_norm": 4.198731899261475,
      "learning_rate": 3.838108320251177e-05,
      "loss": 0.5831,
      "step": 1184200
    },
    {
      "epoch": 18.591836734693878,
      "grad_norm": 3.929297685623169,
      "learning_rate": 3.838010204081633e-05,
      "loss": 0.623,
      "step": 1184300
    },
    {
      "epoch": 18.593406593406595,
      "grad_norm": 3.5921621322631836,
      "learning_rate": 3.837912087912088e-05,
      "loss": 0.6141,
      "step": 1184400
    },
    {
      "epoch": 18.594976452119308,
      "grad_norm": 4.250699043273926,
      "learning_rate": 3.837813971742543e-05,
      "loss": 0.6036,
      "step": 1184500
    },
    {
      "epoch": 18.596546310832025,
      "grad_norm": 4.327305316925049,
      "learning_rate": 3.837715855572999e-05,
      "loss": 0.5887,
      "step": 1184600
    },
    {
      "epoch": 18.598116169544742,
      "grad_norm": 4.582242488861084,
      "learning_rate": 3.837617739403454e-05,
      "loss": 0.5985,
      "step": 1184700
    },
    {
      "epoch": 18.599686028257455,
      "grad_norm": 5.056443691253662,
      "learning_rate": 3.837519623233909e-05,
      "loss": 0.5472,
      "step": 1184800
    },
    {
      "epoch": 18.601255886970172,
      "grad_norm": 3.35473370552063,
      "learning_rate": 3.837421507064364e-05,
      "loss": 0.6267,
      "step": 1184900
    },
    {
      "epoch": 18.60282574568289,
      "grad_norm": 4.542970180511475,
      "learning_rate": 3.83732339089482e-05,
      "loss": 0.5953,
      "step": 1185000
    },
    {
      "epoch": 18.604395604395606,
      "grad_norm": 2.185781955718994,
      "learning_rate": 3.837225274725275e-05,
      "loss": 0.6299,
      "step": 1185100
    },
    {
      "epoch": 18.60596546310832,
      "grad_norm": 3.8997020721435547,
      "learning_rate": 3.83712715855573e-05,
      "loss": 0.6482,
      "step": 1185200
    },
    {
      "epoch": 18.607535321821036,
      "grad_norm": 3.344788074493408,
      "learning_rate": 3.837029042386185e-05,
      "loss": 0.635,
      "step": 1185300
    },
    {
      "epoch": 18.609105180533753,
      "grad_norm": 2.953134775161743,
      "learning_rate": 3.836930926216641e-05,
      "loss": 0.5998,
      "step": 1185400
    },
    {
      "epoch": 18.610675039246466,
      "grad_norm": 2.9604506492614746,
      "learning_rate": 3.8368328100470955e-05,
      "loss": 0.59,
      "step": 1185500
    },
    {
      "epoch": 18.612244897959183,
      "grad_norm": 2.8622870445251465,
      "learning_rate": 3.836734693877551e-05,
      "loss": 0.5939,
      "step": 1185600
    },
    {
      "epoch": 18.6138147566719,
      "grad_norm": 3.6175692081451416,
      "learning_rate": 3.8366365777080064e-05,
      "loss": 0.6412,
      "step": 1185700
    },
    {
      "epoch": 18.615384615384617,
      "grad_norm": 3.2342517375946045,
      "learning_rate": 3.836538461538462e-05,
      "loss": 0.5959,
      "step": 1185800
    },
    {
      "epoch": 18.61695447409733,
      "grad_norm": 4.879739761352539,
      "learning_rate": 3.8364403453689166e-05,
      "loss": 0.6659,
      "step": 1185900
    },
    {
      "epoch": 18.618524332810047,
      "grad_norm": 3.3682639598846436,
      "learning_rate": 3.8363422291993724e-05,
      "loss": 0.6003,
      "step": 1186000
    },
    {
      "epoch": 18.620094191522764,
      "grad_norm": 5.296908378601074,
      "learning_rate": 3.8362441130298275e-05,
      "loss": 0.6087,
      "step": 1186100
    },
    {
      "epoch": 18.621664050235477,
      "grad_norm": 2.9621026515960693,
      "learning_rate": 3.8361459968602826e-05,
      "loss": 0.5946,
      "step": 1186200
    },
    {
      "epoch": 18.623233908948194,
      "grad_norm": 4.334831237792969,
      "learning_rate": 3.8360478806907377e-05,
      "loss": 0.586,
      "step": 1186300
    },
    {
      "epoch": 18.62480376766091,
      "grad_norm": 5.680431842803955,
      "learning_rate": 3.8359497645211934e-05,
      "loss": 0.6155,
      "step": 1186400
    },
    {
      "epoch": 18.626373626373628,
      "grad_norm": 3.921232223510742,
      "learning_rate": 3.8358516483516485e-05,
      "loss": 0.6244,
      "step": 1186500
    },
    {
      "epoch": 18.62794348508634,
      "grad_norm": 5.122059345245361,
      "learning_rate": 3.8357535321821036e-05,
      "loss": 0.6603,
      "step": 1186600
    },
    {
      "epoch": 18.629513343799058,
      "grad_norm": 3.175422191619873,
      "learning_rate": 3.8356554160125594e-05,
      "loss": 0.5933,
      "step": 1186700
    },
    {
      "epoch": 18.631083202511775,
      "grad_norm": 4.884359836578369,
      "learning_rate": 3.8355572998430145e-05,
      "loss": 0.6404,
      "step": 1186800
    },
    {
      "epoch": 18.632653061224488,
      "grad_norm": 3.3750126361846924,
      "learning_rate": 3.8354591836734696e-05,
      "loss": 0.6099,
      "step": 1186900
    },
    {
      "epoch": 18.634222919937205,
      "grad_norm": 2.9962315559387207,
      "learning_rate": 3.835361067503925e-05,
      "loss": 0.6542,
      "step": 1187000
    },
    {
      "epoch": 18.635792778649922,
      "grad_norm": 4.406124591827393,
      "learning_rate": 3.8352629513343805e-05,
      "loss": 0.5791,
      "step": 1187100
    },
    {
      "epoch": 18.63736263736264,
      "grad_norm": 3.640087604522705,
      "learning_rate": 3.8351648351648356e-05,
      "loss": 0.662,
      "step": 1187200
    },
    {
      "epoch": 18.638932496075352,
      "grad_norm": 3.254542112350464,
      "learning_rate": 3.835066718995291e-05,
      "loss": 0.6403,
      "step": 1187300
    },
    {
      "epoch": 18.64050235478807,
      "grad_norm": 3.236855983734131,
      "learning_rate": 3.834968602825746e-05,
      "loss": 0.5664,
      "step": 1187400
    },
    {
      "epoch": 18.642072213500786,
      "grad_norm": 3.677036762237549,
      "learning_rate": 3.8348704866562015e-05,
      "loss": 0.5712,
      "step": 1187500
    },
    {
      "epoch": 18.643642072213503,
      "grad_norm": 4.118476867675781,
      "learning_rate": 3.834772370486656e-05,
      "loss": 0.6207,
      "step": 1187600
    },
    {
      "epoch": 18.645211930926216,
      "grad_norm": 4.248356342315674,
      "learning_rate": 3.834674254317112e-05,
      "loss": 0.65,
      "step": 1187700
    },
    {
      "epoch": 18.646781789638933,
      "grad_norm": 3.7374560832977295,
      "learning_rate": 3.834576138147567e-05,
      "loss": 0.6273,
      "step": 1187800
    },
    {
      "epoch": 18.64835164835165,
      "grad_norm": 3.4189653396606445,
      "learning_rate": 3.8344780219780226e-05,
      "loss": 0.6063,
      "step": 1187900
    },
    {
      "epoch": 18.649921507064363,
      "grad_norm": 4.007369518280029,
      "learning_rate": 3.834379905808477e-05,
      "loss": 0.5845,
      "step": 1188000
    },
    {
      "epoch": 18.65149136577708,
      "grad_norm": 4.4666619300842285,
      "learning_rate": 3.834281789638933e-05,
      "loss": 0.6171,
      "step": 1188100
    },
    {
      "epoch": 18.653061224489797,
      "grad_norm": 3.6912989616394043,
      "learning_rate": 3.834183673469388e-05,
      "loss": 0.6181,
      "step": 1188200
    },
    {
      "epoch": 18.65463108320251,
      "grad_norm": 3.536892890930176,
      "learning_rate": 3.834085557299843e-05,
      "loss": 0.6235,
      "step": 1188300
    },
    {
      "epoch": 18.656200941915227,
      "grad_norm": 4.168670177459717,
      "learning_rate": 3.833987441130298e-05,
      "loss": 0.6204,
      "step": 1188400
    },
    {
      "epoch": 18.657770800627944,
      "grad_norm": 3.8673243522644043,
      "learning_rate": 3.833889324960754e-05,
      "loss": 0.5919,
      "step": 1188500
    },
    {
      "epoch": 18.65934065934066,
      "grad_norm": 4.155098915100098,
      "learning_rate": 3.833791208791209e-05,
      "loss": 0.6419,
      "step": 1188600
    },
    {
      "epoch": 18.660910518053374,
      "grad_norm": 4.084372043609619,
      "learning_rate": 3.833693092621664e-05,
      "loss": 0.5912,
      "step": 1188700
    },
    {
      "epoch": 18.66248037676609,
      "grad_norm": 3.858131170272827,
      "learning_rate": 3.833594976452119e-05,
      "loss": 0.6306,
      "step": 1188800
    },
    {
      "epoch": 18.664050235478808,
      "grad_norm": 3.271040678024292,
      "learning_rate": 3.833496860282575e-05,
      "loss": 0.6392,
      "step": 1188900
    },
    {
      "epoch": 18.665620094191524,
      "grad_norm": 4.533327102661133,
      "learning_rate": 3.83339874411303e-05,
      "loss": 0.586,
      "step": 1189000
    },
    {
      "epoch": 18.667189952904238,
      "grad_norm": 4.2713117599487305,
      "learning_rate": 3.833300627943485e-05,
      "loss": 0.6536,
      "step": 1189100
    },
    {
      "epoch": 18.668759811616955,
      "grad_norm": 4.8914618492126465,
      "learning_rate": 3.833202511773941e-05,
      "loss": 0.6123,
      "step": 1189200
    },
    {
      "epoch": 18.67032967032967,
      "grad_norm": 2.71172833442688,
      "learning_rate": 3.833104395604396e-05,
      "loss": 0.6117,
      "step": 1189300
    },
    {
      "epoch": 18.671899529042385,
      "grad_norm": 4.786922931671143,
      "learning_rate": 3.833006279434851e-05,
      "loss": 0.6192,
      "step": 1189400
    },
    {
      "epoch": 18.6734693877551,
      "grad_norm": 3.6788768768310547,
      "learning_rate": 3.832908163265306e-05,
      "loss": 0.603,
      "step": 1189500
    },
    {
      "epoch": 18.67503924646782,
      "grad_norm": 3.199983835220337,
      "learning_rate": 3.832810047095762e-05,
      "loss": 0.6072,
      "step": 1189600
    },
    {
      "epoch": 18.676609105180535,
      "grad_norm": 3.522646903991699,
      "learning_rate": 3.8327119309262164e-05,
      "loss": 0.5656,
      "step": 1189700
    },
    {
      "epoch": 18.67817896389325,
      "grad_norm": 3.925999402999878,
      "learning_rate": 3.832613814756672e-05,
      "loss": 0.6058,
      "step": 1189800
    },
    {
      "epoch": 18.679748822605966,
      "grad_norm": 3.7525317668914795,
      "learning_rate": 3.832515698587127e-05,
      "loss": 0.5549,
      "step": 1189900
    },
    {
      "epoch": 18.681318681318682,
      "grad_norm": 3.6081650257110596,
      "learning_rate": 3.832417582417583e-05,
      "loss": 0.6139,
      "step": 1190000
    },
    {
      "epoch": 18.682888540031396,
      "grad_norm": 4.531327247619629,
      "learning_rate": 3.8323194662480375e-05,
      "loss": 0.5856,
      "step": 1190100
    },
    {
      "epoch": 18.684458398744113,
      "grad_norm": 3.393798828125,
      "learning_rate": 3.832221350078493e-05,
      "loss": 0.6333,
      "step": 1190200
    },
    {
      "epoch": 18.68602825745683,
      "grad_norm": 4.245148658752441,
      "learning_rate": 3.8321232339089484e-05,
      "loss": 0.6086,
      "step": 1190300
    },
    {
      "epoch": 18.687598116169546,
      "grad_norm": 3.9980549812316895,
      "learning_rate": 3.8320251177394035e-05,
      "loss": 0.6065,
      "step": 1190400
    },
    {
      "epoch": 18.68916797488226,
      "grad_norm": 3.309629440307617,
      "learning_rate": 3.8319270015698586e-05,
      "loss": 0.5828,
      "step": 1190500
    },
    {
      "epoch": 18.690737833594977,
      "grad_norm": 3.294569969177246,
      "learning_rate": 3.831828885400314e-05,
      "loss": 0.6122,
      "step": 1190600
    },
    {
      "epoch": 18.692307692307693,
      "grad_norm": 3.798380136489868,
      "learning_rate": 3.8317307692307694e-05,
      "loss": 0.5845,
      "step": 1190700
    },
    {
      "epoch": 18.693877551020407,
      "grad_norm": 3.555948257446289,
      "learning_rate": 3.8316326530612245e-05,
      "loss": 0.5925,
      "step": 1190800
    },
    {
      "epoch": 18.695447409733124,
      "grad_norm": 4.144213676452637,
      "learning_rate": 3.8315345368916796e-05,
      "loss": 0.6254,
      "step": 1190900
    },
    {
      "epoch": 18.69701726844584,
      "grad_norm": 3.839197874069214,
      "learning_rate": 3.8314364207221354e-05,
      "loss": 0.6203,
      "step": 1191000
    },
    {
      "epoch": 18.698587127158557,
      "grad_norm": 3.0135915279388428,
      "learning_rate": 3.8313383045525905e-05,
      "loss": 0.6397,
      "step": 1191100
    },
    {
      "epoch": 18.70015698587127,
      "grad_norm": 4.869098663330078,
      "learning_rate": 3.8312401883830456e-05,
      "loss": 0.6076,
      "step": 1191200
    },
    {
      "epoch": 18.701726844583987,
      "grad_norm": 4.423994064331055,
      "learning_rate": 3.8311420722135014e-05,
      "loss": 0.6074,
      "step": 1191300
    },
    {
      "epoch": 18.703296703296704,
      "grad_norm": 4.2392802238464355,
      "learning_rate": 3.8310439560439565e-05,
      "loss": 0.5994,
      "step": 1191400
    },
    {
      "epoch": 18.704866562009418,
      "grad_norm": 3.906919002532959,
      "learning_rate": 3.8309458398744116e-05,
      "loss": 0.6491,
      "step": 1191500
    },
    {
      "epoch": 18.706436420722135,
      "grad_norm": 4.427881717681885,
      "learning_rate": 3.830847723704867e-05,
      "loss": 0.5988,
      "step": 1191600
    },
    {
      "epoch": 18.70800627943485,
      "grad_norm": 3.492810010910034,
      "learning_rate": 3.8307496075353224e-05,
      "loss": 0.6238,
      "step": 1191700
    },
    {
      "epoch": 18.70957613814757,
      "grad_norm": 4.059502124786377,
      "learning_rate": 3.830651491365777e-05,
      "loss": 0.6231,
      "step": 1191800
    },
    {
      "epoch": 18.71114599686028,
      "grad_norm": 2.967008590698242,
      "learning_rate": 3.8305533751962326e-05,
      "loss": 0.5607,
      "step": 1191900
    },
    {
      "epoch": 18.712715855573,
      "grad_norm": 3.6927664279937744,
      "learning_rate": 3.830455259026688e-05,
      "loss": 0.6207,
      "step": 1192000
    },
    {
      "epoch": 18.714285714285715,
      "grad_norm": 3.047098159790039,
      "learning_rate": 3.8303571428571435e-05,
      "loss": 0.5882,
      "step": 1192100
    },
    {
      "epoch": 18.71585557299843,
      "grad_norm": 4.116419315338135,
      "learning_rate": 3.830259026687598e-05,
      "loss": 0.5968,
      "step": 1192200
    },
    {
      "epoch": 18.717425431711145,
      "grad_norm": 4.887107849121094,
      "learning_rate": 3.830160910518054e-05,
      "loss": 0.5983,
      "step": 1192300
    },
    {
      "epoch": 18.718995290423862,
      "grad_norm": 3.971367359161377,
      "learning_rate": 3.830062794348509e-05,
      "loss": 0.6429,
      "step": 1192400
    },
    {
      "epoch": 18.72056514913658,
      "grad_norm": 3.416015863418579,
      "learning_rate": 3.829964678178964e-05,
      "loss": 0.6061,
      "step": 1192500
    },
    {
      "epoch": 18.722135007849293,
      "grad_norm": 4.753291130065918,
      "learning_rate": 3.829866562009419e-05,
      "loss": 0.5959,
      "step": 1192600
    },
    {
      "epoch": 18.72370486656201,
      "grad_norm": 3.519235372543335,
      "learning_rate": 3.829768445839875e-05,
      "loss": 0.6034,
      "step": 1192700
    },
    {
      "epoch": 18.725274725274726,
      "grad_norm": 4.915491580963135,
      "learning_rate": 3.82967032967033e-05,
      "loss": 0.5684,
      "step": 1192800
    },
    {
      "epoch": 18.72684458398744,
      "grad_norm": 3.467827558517456,
      "learning_rate": 3.829572213500785e-05,
      "loss": 0.6233,
      "step": 1192900
    },
    {
      "epoch": 18.728414442700156,
      "grad_norm": 3.062483072280884,
      "learning_rate": 3.82947409733124e-05,
      "loss": 0.5827,
      "step": 1193000
    },
    {
      "epoch": 18.729984301412873,
      "grad_norm": 3.7508108615875244,
      "learning_rate": 3.829375981161696e-05,
      "loss": 0.5839,
      "step": 1193100
    },
    {
      "epoch": 18.73155416012559,
      "grad_norm": 3.134917974472046,
      "learning_rate": 3.829277864992151e-05,
      "loss": 0.6191,
      "step": 1193200
    },
    {
      "epoch": 18.733124018838303,
      "grad_norm": 3.1001484394073486,
      "learning_rate": 3.829179748822606e-05,
      "loss": 0.624,
      "step": 1193300
    },
    {
      "epoch": 18.73469387755102,
      "grad_norm": 3.4594266414642334,
      "learning_rate": 3.829081632653062e-05,
      "loss": 0.6397,
      "step": 1193400
    },
    {
      "epoch": 18.736263736263737,
      "grad_norm": 3.9935851097106934,
      "learning_rate": 3.828983516483517e-05,
      "loss": 0.5823,
      "step": 1193500
    },
    {
      "epoch": 18.73783359497645,
      "grad_norm": 3.4264304637908936,
      "learning_rate": 3.828885400313972e-05,
      "loss": 0.6383,
      "step": 1193600
    },
    {
      "epoch": 18.739403453689167,
      "grad_norm": 4.116109848022461,
      "learning_rate": 3.828787284144427e-05,
      "loss": 0.6128,
      "step": 1193700
    },
    {
      "epoch": 18.740973312401884,
      "grad_norm": 3.8984155654907227,
      "learning_rate": 3.828689167974883e-05,
      "loss": 0.5876,
      "step": 1193800
    },
    {
      "epoch": 18.7425431711146,
      "grad_norm": 3.5346310138702393,
      "learning_rate": 3.828591051805337e-05,
      "loss": 0.5997,
      "step": 1193900
    },
    {
      "epoch": 18.744113029827314,
      "grad_norm": 2.694944143295288,
      "learning_rate": 3.828492935635793e-05,
      "loss": 0.601,
      "step": 1194000
    },
    {
      "epoch": 18.74568288854003,
      "grad_norm": 3.0024619102478027,
      "learning_rate": 3.828394819466248e-05,
      "loss": 0.5922,
      "step": 1194100
    },
    {
      "epoch": 18.747252747252748,
      "grad_norm": 3.9877874851226807,
      "learning_rate": 3.828296703296704e-05,
      "loss": 0.6374,
      "step": 1194200
    },
    {
      "epoch": 18.74882260596546,
      "grad_norm": 3.952110528945923,
      "learning_rate": 3.8281985871271584e-05,
      "loss": 0.5926,
      "step": 1194300
    },
    {
      "epoch": 18.75039246467818,
      "grad_norm": 4.516107082366943,
      "learning_rate": 3.828100470957614e-05,
      "loss": 0.6206,
      "step": 1194400
    },
    {
      "epoch": 18.751962323390895,
      "grad_norm": 3.2628800868988037,
      "learning_rate": 3.828002354788069e-05,
      "loss": 0.6241,
      "step": 1194500
    },
    {
      "epoch": 18.753532182103612,
      "grad_norm": 4.826218605041504,
      "learning_rate": 3.8279042386185244e-05,
      "loss": 0.6123,
      "step": 1194600
    },
    {
      "epoch": 18.755102040816325,
      "grad_norm": 2.529114007949829,
      "learning_rate": 3.8278061224489795e-05,
      "loss": 0.6123,
      "step": 1194700
    },
    {
      "epoch": 18.756671899529042,
      "grad_norm": 3.7219831943511963,
      "learning_rate": 3.827708006279435e-05,
      "loss": 0.6231,
      "step": 1194800
    },
    {
      "epoch": 18.75824175824176,
      "grad_norm": 3.527212619781494,
      "learning_rate": 3.82760989010989e-05,
      "loss": 0.6196,
      "step": 1194900
    },
    {
      "epoch": 18.759811616954472,
      "grad_norm": 3.6896915435791016,
      "learning_rate": 3.8275117739403454e-05,
      "loss": 0.6098,
      "step": 1195000
    },
    {
      "epoch": 18.76138147566719,
      "grad_norm": 4.874230861663818,
      "learning_rate": 3.8274136577708005e-05,
      "loss": 0.5874,
      "step": 1195100
    },
    {
      "epoch": 18.762951334379906,
      "grad_norm": 3.914781093597412,
      "learning_rate": 3.827315541601256e-05,
      "loss": 0.5811,
      "step": 1195200
    },
    {
      "epoch": 18.764521193092623,
      "grad_norm": 2.9551916122436523,
      "learning_rate": 3.8272174254317114e-05,
      "loss": 0.6021,
      "step": 1195300
    },
    {
      "epoch": 18.766091051805336,
      "grad_norm": 4.504894733428955,
      "learning_rate": 3.8271193092621665e-05,
      "loss": 0.6331,
      "step": 1195400
    },
    {
      "epoch": 18.767660910518053,
      "grad_norm": 3.857644557952881,
      "learning_rate": 3.827021193092622e-05,
      "loss": 0.6189,
      "step": 1195500
    },
    {
      "epoch": 18.76923076923077,
      "grad_norm": 4.247335433959961,
      "learning_rate": 3.826923076923077e-05,
      "loss": 0.6288,
      "step": 1195600
    },
    {
      "epoch": 18.770800627943487,
      "grad_norm": 4.285769462585449,
      "learning_rate": 3.8268249607535325e-05,
      "loss": 0.6139,
      "step": 1195700
    },
    {
      "epoch": 18.7723704866562,
      "grad_norm": 3.5731043815612793,
      "learning_rate": 3.8267268445839876e-05,
      "loss": 0.5679,
      "step": 1195800
    },
    {
      "epoch": 18.773940345368917,
      "grad_norm": 3.295823097229004,
      "learning_rate": 3.8266287284144433e-05,
      "loss": 0.6235,
      "step": 1195900
    },
    {
      "epoch": 18.775510204081634,
      "grad_norm": 5.378678798675537,
      "learning_rate": 3.826530612244898e-05,
      "loss": 0.6077,
      "step": 1196000
    },
    {
      "epoch": 18.777080062794347,
      "grad_norm": 3.817110300064087,
      "learning_rate": 3.8264324960753535e-05,
      "loss": 0.6479,
      "step": 1196100
    },
    {
      "epoch": 18.778649921507064,
      "grad_norm": 4.284853458404541,
      "learning_rate": 3.8263343799058086e-05,
      "loss": 0.6312,
      "step": 1196200
    },
    {
      "epoch": 18.78021978021978,
      "grad_norm": 4.14567232131958,
      "learning_rate": 3.826236263736264e-05,
      "loss": 0.5768,
      "step": 1196300
    },
    {
      "epoch": 18.781789638932494,
      "grad_norm": 2.8567957878112793,
      "learning_rate": 3.826138147566719e-05,
      "loss": 0.6004,
      "step": 1196400
    },
    {
      "epoch": 18.78335949764521,
      "grad_norm": 2.969238519668579,
      "learning_rate": 3.8260400313971746e-05,
      "loss": 0.6193,
      "step": 1196500
    },
    {
      "epoch": 18.784929356357928,
      "grad_norm": 4.549492835998535,
      "learning_rate": 3.82594191522763e-05,
      "loss": 0.637,
      "step": 1196600
    },
    {
      "epoch": 18.786499215070645,
      "grad_norm": 2.905250310897827,
      "learning_rate": 3.825843799058085e-05,
      "loss": 0.6149,
      "step": 1196700
    },
    {
      "epoch": 18.788069073783358,
      "grad_norm": 4.312470436096191,
      "learning_rate": 3.82574568288854e-05,
      "loss": 0.6009,
      "step": 1196800
    },
    {
      "epoch": 18.789638932496075,
      "grad_norm": 5.189326286315918,
      "learning_rate": 3.825647566718996e-05,
      "loss": 0.6606,
      "step": 1196900
    },
    {
      "epoch": 18.791208791208792,
      "grad_norm": 4.005884647369385,
      "learning_rate": 3.825549450549451e-05,
      "loss": 0.663,
      "step": 1197000
    },
    {
      "epoch": 18.79277864992151,
      "grad_norm": 3.4469752311706543,
      "learning_rate": 3.825451334379906e-05,
      "loss": 0.598,
      "step": 1197100
    },
    {
      "epoch": 18.794348508634222,
      "grad_norm": 3.3824996948242188,
      "learning_rate": 3.825353218210361e-05,
      "loss": 0.5964,
      "step": 1197200
    },
    {
      "epoch": 18.79591836734694,
      "grad_norm": 4.8682780265808105,
      "learning_rate": 3.825255102040817e-05,
      "loss": 0.6323,
      "step": 1197300
    },
    {
      "epoch": 18.797488226059656,
      "grad_norm": 3.705819606781006,
      "learning_rate": 3.825156985871272e-05,
      "loss": 0.6204,
      "step": 1197400
    },
    {
      "epoch": 18.79905808477237,
      "grad_norm": 3.712979316711426,
      "learning_rate": 3.825058869701727e-05,
      "loss": 0.5851,
      "step": 1197500
    },
    {
      "epoch": 18.800627943485086,
      "grad_norm": 3.6898984909057617,
      "learning_rate": 3.824960753532183e-05,
      "loss": 0.6404,
      "step": 1197600
    },
    {
      "epoch": 18.802197802197803,
      "grad_norm": 4.5331339836120605,
      "learning_rate": 3.824862637362637e-05,
      "loss": 0.6082,
      "step": 1197700
    },
    {
      "epoch": 18.80376766091052,
      "grad_norm": 4.865333557128906,
      "learning_rate": 3.824764521193093e-05,
      "loss": 0.6211,
      "step": 1197800
    },
    {
      "epoch": 18.805337519623233,
      "grad_norm": 4.052743911743164,
      "learning_rate": 3.824666405023548e-05,
      "loss": 0.6126,
      "step": 1197900
    },
    {
      "epoch": 18.80690737833595,
      "grad_norm": 3.6491549015045166,
      "learning_rate": 3.824568288854004e-05,
      "loss": 0.6381,
      "step": 1198000
    },
    {
      "epoch": 18.808477237048667,
      "grad_norm": 3.2548604011535645,
      "learning_rate": 3.824470172684458e-05,
      "loss": 0.6409,
      "step": 1198100
    },
    {
      "epoch": 18.81004709576138,
      "grad_norm": 4.457373142242432,
      "learning_rate": 3.824372056514914e-05,
      "loss": 0.588,
      "step": 1198200
    },
    {
      "epoch": 18.811616954474097,
      "grad_norm": 5.128990173339844,
      "learning_rate": 3.824273940345369e-05,
      "loss": 0.6186,
      "step": 1198300
    },
    {
      "epoch": 18.813186813186814,
      "grad_norm": 4.212259292602539,
      "learning_rate": 3.824175824175824e-05,
      "loss": 0.6019,
      "step": 1198400
    },
    {
      "epoch": 18.81475667189953,
      "grad_norm": 3.959737777709961,
      "learning_rate": 3.824077708006279e-05,
      "loss": 0.6303,
      "step": 1198500
    },
    {
      "epoch": 18.816326530612244,
      "grad_norm": 3.9834885597229004,
      "learning_rate": 3.823979591836735e-05,
      "loss": 0.62,
      "step": 1198600
    },
    {
      "epoch": 18.81789638932496,
      "grad_norm": 2.4768660068511963,
      "learning_rate": 3.82388147566719e-05,
      "loss": 0.6191,
      "step": 1198700
    },
    {
      "epoch": 18.819466248037678,
      "grad_norm": 4.544938564300537,
      "learning_rate": 3.823783359497645e-05,
      "loss": 0.6095,
      "step": 1198800
    },
    {
      "epoch": 18.82103610675039,
      "grad_norm": 4.033902168273926,
      "learning_rate": 3.8236852433281004e-05,
      "loss": 0.6083,
      "step": 1198900
    },
    {
      "epoch": 18.822605965463108,
      "grad_norm": 3.91538667678833,
      "learning_rate": 3.823587127158556e-05,
      "loss": 0.6195,
      "step": 1199000
    },
    {
      "epoch": 18.824175824175825,
      "grad_norm": 3.762035846710205,
      "learning_rate": 3.823489010989011e-05,
      "loss": 0.5887,
      "step": 1199100
    },
    {
      "epoch": 18.82574568288854,
      "grad_norm": 2.497612953186035,
      "learning_rate": 3.823390894819466e-05,
      "loss": 0.6195,
      "step": 1199200
    },
    {
      "epoch": 18.827315541601255,
      "grad_norm": 3.5142266750335693,
      "learning_rate": 3.8232927786499214e-05,
      "loss": 0.6477,
      "step": 1199300
    },
    {
      "epoch": 18.828885400313972,
      "grad_norm": 4.147842884063721,
      "learning_rate": 3.823194662480377e-05,
      "loss": 0.6018,
      "step": 1199400
    },
    {
      "epoch": 18.83045525902669,
      "grad_norm": 3.6434438228607178,
      "learning_rate": 3.823096546310832e-05,
      "loss": 0.6117,
      "step": 1199500
    },
    {
      "epoch": 18.832025117739402,
      "grad_norm": 4.291444778442383,
      "learning_rate": 3.8229984301412874e-05,
      "loss": 0.6119,
      "step": 1199600
    },
    {
      "epoch": 18.83359497645212,
      "grad_norm": 4.320022106170654,
      "learning_rate": 3.822900313971743e-05,
      "loss": 0.6025,
      "step": 1199700
    },
    {
      "epoch": 18.835164835164836,
      "grad_norm": 3.9387595653533936,
      "learning_rate": 3.8228021978021976e-05,
      "loss": 0.6207,
      "step": 1199800
    },
    {
      "epoch": 18.836734693877553,
      "grad_norm": 4.152953147888184,
      "learning_rate": 3.8227040816326534e-05,
      "loss": 0.6127,
      "step": 1199900
    },
    {
      "epoch": 18.838304552590266,
      "grad_norm": 3.7950539588928223,
      "learning_rate": 3.8226059654631085e-05,
      "loss": 0.6272,
      "step": 1200000
    },
    {
      "epoch": 18.839874411302983,
      "grad_norm": 3.922896385192871,
      "learning_rate": 3.822507849293564e-05,
      "loss": 0.6168,
      "step": 1200100
    },
    {
      "epoch": 18.8414442700157,
      "grad_norm": 4.206241607666016,
      "learning_rate": 3.8224097331240187e-05,
      "loss": 0.611,
      "step": 1200200
    },
    {
      "epoch": 18.843014128728413,
      "grad_norm": 4.488431930541992,
      "learning_rate": 3.8223116169544744e-05,
      "loss": 0.6166,
      "step": 1200300
    },
    {
      "epoch": 18.84458398744113,
      "grad_norm": 4.0632758140563965,
      "learning_rate": 3.8222135007849295e-05,
      "loss": 0.6045,
      "step": 1200400
    },
    {
      "epoch": 18.846153846153847,
      "grad_norm": 4.382113456726074,
      "learning_rate": 3.8221153846153846e-05,
      "loss": 0.605,
      "step": 1200500
    },
    {
      "epoch": 18.847723704866564,
      "grad_norm": 2.832392930984497,
      "learning_rate": 3.82201726844584e-05,
      "loss": 0.5853,
      "step": 1200600
    },
    {
      "epoch": 18.849293563579277,
      "grad_norm": 3.654716968536377,
      "learning_rate": 3.8219191522762955e-05,
      "loss": 0.6198,
      "step": 1200700
    },
    {
      "epoch": 18.850863422291994,
      "grad_norm": 4.098766803741455,
      "learning_rate": 3.8218210361067506e-05,
      "loss": 0.6189,
      "step": 1200800
    },
    {
      "epoch": 18.85243328100471,
      "grad_norm": 4.2642951011657715,
      "learning_rate": 3.821722919937206e-05,
      "loss": 0.586,
      "step": 1200900
    },
    {
      "epoch": 18.854003139717424,
      "grad_norm": 3.495792865753174,
      "learning_rate": 3.821624803767661e-05,
      "loss": 0.6076,
      "step": 1201000
    },
    {
      "epoch": 18.85557299843014,
      "grad_norm": 3.260890483856201,
      "learning_rate": 3.8215266875981166e-05,
      "loss": 0.6134,
      "step": 1201100
    },
    {
      "epoch": 18.857142857142858,
      "grad_norm": 4.1439995765686035,
      "learning_rate": 3.821428571428572e-05,
      "loss": 0.6291,
      "step": 1201200
    },
    {
      "epoch": 18.858712715855575,
      "grad_norm": 3.5329971313476562,
      "learning_rate": 3.821330455259027e-05,
      "loss": 0.6009,
      "step": 1201300
    },
    {
      "epoch": 18.860282574568288,
      "grad_norm": 3.1747989654541016,
      "learning_rate": 3.821232339089482e-05,
      "loss": 0.5689,
      "step": 1201400
    },
    {
      "epoch": 18.861852433281005,
      "grad_norm": 3.383619785308838,
      "learning_rate": 3.8211342229199376e-05,
      "loss": 0.6508,
      "step": 1201500
    },
    {
      "epoch": 18.86342229199372,
      "grad_norm": 3.63712739944458,
      "learning_rate": 3.821036106750393e-05,
      "loss": 0.5905,
      "step": 1201600
    },
    {
      "epoch": 18.864992150706435,
      "grad_norm": 3.6660633087158203,
      "learning_rate": 3.820937990580848e-05,
      "loss": 0.6107,
      "step": 1201700
    },
    {
      "epoch": 18.86656200941915,
      "grad_norm": 3.6438961029052734,
      "learning_rate": 3.8208398744113036e-05,
      "loss": 0.6111,
      "step": 1201800
    },
    {
      "epoch": 18.86813186813187,
      "grad_norm": 3.619102716445923,
      "learning_rate": 3.820741758241758e-05,
      "loss": 0.5691,
      "step": 1201900
    },
    {
      "epoch": 18.869701726844585,
      "grad_norm": 5.6370391845703125,
      "learning_rate": 3.820643642072214e-05,
      "loss": 0.6045,
      "step": 1202000
    },
    {
      "epoch": 18.8712715855573,
      "grad_norm": 3.5747880935668945,
      "learning_rate": 3.820545525902669e-05,
      "loss": 0.5942,
      "step": 1202100
    },
    {
      "epoch": 18.872841444270016,
      "grad_norm": 3.734119176864624,
      "learning_rate": 3.820447409733125e-05,
      "loss": 0.5768,
      "step": 1202200
    },
    {
      "epoch": 18.874411302982733,
      "grad_norm": 3.682044744491577,
      "learning_rate": 3.820349293563579e-05,
      "loss": 0.6223,
      "step": 1202300
    },
    {
      "epoch": 18.875981161695446,
      "grad_norm": 3.7326948642730713,
      "learning_rate": 3.820251177394035e-05,
      "loss": 0.621,
      "step": 1202400
    },
    {
      "epoch": 18.877551020408163,
      "grad_norm": 3.852774143218994,
      "learning_rate": 3.82015306122449e-05,
      "loss": 0.6089,
      "step": 1202500
    },
    {
      "epoch": 18.87912087912088,
      "grad_norm": 4.3965163230896,
      "learning_rate": 3.820054945054945e-05,
      "loss": 0.6278,
      "step": 1202600
    },
    {
      "epoch": 18.880690737833596,
      "grad_norm": 2.7829220294952393,
      "learning_rate": 3.8199568288854e-05,
      "loss": 0.6191,
      "step": 1202700
    },
    {
      "epoch": 18.88226059654631,
      "grad_norm": 4.150230884552002,
      "learning_rate": 3.819858712715856e-05,
      "loss": 0.6454,
      "step": 1202800
    },
    {
      "epoch": 18.883830455259027,
      "grad_norm": 4.182549476623535,
      "learning_rate": 3.819760596546311e-05,
      "loss": 0.6243,
      "step": 1202900
    },
    {
      "epoch": 18.885400313971743,
      "grad_norm": 3.5571749210357666,
      "learning_rate": 3.819662480376766e-05,
      "loss": 0.6153,
      "step": 1203000
    },
    {
      "epoch": 18.886970172684457,
      "grad_norm": 4.057097434997559,
      "learning_rate": 3.819564364207221e-05,
      "loss": 0.6378,
      "step": 1203100
    },
    {
      "epoch": 18.888540031397174,
      "grad_norm": 3.8907389640808105,
      "learning_rate": 3.819466248037677e-05,
      "loss": 0.6291,
      "step": 1203200
    },
    {
      "epoch": 18.89010989010989,
      "grad_norm": 4.289527416229248,
      "learning_rate": 3.819368131868132e-05,
      "loss": 0.64,
      "step": 1203300
    },
    {
      "epoch": 18.891679748822607,
      "grad_norm": 3.8088598251342773,
      "learning_rate": 3.819270015698587e-05,
      "loss": 0.5784,
      "step": 1203400
    },
    {
      "epoch": 18.89324960753532,
      "grad_norm": 3.970313549041748,
      "learning_rate": 3.819171899529042e-05,
      "loss": 0.6204,
      "step": 1203500
    },
    {
      "epoch": 18.894819466248038,
      "grad_norm": 4.508934020996094,
      "learning_rate": 3.819073783359498e-05,
      "loss": 0.6138,
      "step": 1203600
    },
    {
      "epoch": 18.896389324960754,
      "grad_norm": 3.5682966709136963,
      "learning_rate": 3.818975667189953e-05,
      "loss": 0.5913,
      "step": 1203700
    },
    {
      "epoch": 18.897959183673468,
      "grad_norm": 3.9308784008026123,
      "learning_rate": 3.818877551020408e-05,
      "loss": 0.6122,
      "step": 1203800
    },
    {
      "epoch": 18.899529042386185,
      "grad_norm": 3.290900707244873,
      "learning_rate": 3.818779434850864e-05,
      "loss": 0.6475,
      "step": 1203900
    },
    {
      "epoch": 18.9010989010989,
      "grad_norm": 3.771859884262085,
      "learning_rate": 3.8186813186813185e-05,
      "loss": 0.6494,
      "step": 1204000
    },
    {
      "epoch": 18.90266875981162,
      "grad_norm": 3.1727969646453857,
      "learning_rate": 3.818583202511774e-05,
      "loss": 0.5932,
      "step": 1204100
    },
    {
      "epoch": 18.90423861852433,
      "grad_norm": 3.942523956298828,
      "learning_rate": 3.8184850863422294e-05,
      "loss": 0.6101,
      "step": 1204200
    },
    {
      "epoch": 18.90580847723705,
      "grad_norm": 5.217241287231445,
      "learning_rate": 3.818386970172685e-05,
      "loss": 0.6103,
      "step": 1204300
    },
    {
      "epoch": 18.907378335949765,
      "grad_norm": 3.4802634716033936,
      "learning_rate": 3.8182888540031396e-05,
      "loss": 0.5509,
      "step": 1204400
    },
    {
      "epoch": 18.90894819466248,
      "grad_norm": 2.5836079120635986,
      "learning_rate": 3.818190737833595e-05,
      "loss": 0.5958,
      "step": 1204500
    },
    {
      "epoch": 18.910518053375196,
      "grad_norm": 3.8072967529296875,
      "learning_rate": 3.8180926216640504e-05,
      "loss": 0.5475,
      "step": 1204600
    },
    {
      "epoch": 18.912087912087912,
      "grad_norm": 4.2196173667907715,
      "learning_rate": 3.8179945054945055e-05,
      "loss": 0.6062,
      "step": 1204700
    },
    {
      "epoch": 18.91365777080063,
      "grad_norm": 4.589784622192383,
      "learning_rate": 3.8178963893249606e-05,
      "loss": 0.6699,
      "step": 1204800
    },
    {
      "epoch": 18.915227629513343,
      "grad_norm": 4.010770320892334,
      "learning_rate": 3.8177982731554164e-05,
      "loss": 0.6568,
      "step": 1204900
    },
    {
      "epoch": 18.91679748822606,
      "grad_norm": 3.134917974472046,
      "learning_rate": 3.8177001569858715e-05,
      "loss": 0.5765,
      "step": 1205000
    },
    {
      "epoch": 18.918367346938776,
      "grad_norm": 4.350070953369141,
      "learning_rate": 3.8176020408163266e-05,
      "loss": 0.6017,
      "step": 1205100
    },
    {
      "epoch": 18.919937205651493,
      "grad_norm": 3.171433210372925,
      "learning_rate": 3.817503924646782e-05,
      "loss": 0.6165,
      "step": 1205200
    },
    {
      "epoch": 18.921507064364206,
      "grad_norm": 3.4982500076293945,
      "learning_rate": 3.8174058084772375e-05,
      "loss": 0.6272,
      "step": 1205300
    },
    {
      "epoch": 18.923076923076923,
      "grad_norm": 3.501333475112915,
      "learning_rate": 3.8173076923076926e-05,
      "loss": 0.6252,
      "step": 1205400
    },
    {
      "epoch": 18.92464678178964,
      "grad_norm": 2.9442129135131836,
      "learning_rate": 3.817209576138148e-05,
      "loss": 0.6138,
      "step": 1205500
    },
    {
      "epoch": 18.926216640502354,
      "grad_norm": 3.4586145877838135,
      "learning_rate": 3.817111459968603e-05,
      "loss": 0.572,
      "step": 1205600
    },
    {
      "epoch": 18.92778649921507,
      "grad_norm": 4.133987903594971,
      "learning_rate": 3.8170133437990585e-05,
      "loss": 0.5895,
      "step": 1205700
    },
    {
      "epoch": 18.929356357927787,
      "grad_norm": 2.3186771869659424,
      "learning_rate": 3.8169152276295136e-05,
      "loss": 0.6421,
      "step": 1205800
    },
    {
      "epoch": 18.9309262166405,
      "grad_norm": 2.6052920818328857,
      "learning_rate": 3.816817111459969e-05,
      "loss": 0.6342,
      "step": 1205900
    },
    {
      "epoch": 18.932496075353217,
      "grad_norm": 3.0242013931274414,
      "learning_rate": 3.8167189952904245e-05,
      "loss": 0.5904,
      "step": 1206000
    },
    {
      "epoch": 18.934065934065934,
      "grad_norm": 4.333207607269287,
      "learning_rate": 3.816620879120879e-05,
      "loss": 0.5884,
      "step": 1206100
    },
    {
      "epoch": 18.93563579277865,
      "grad_norm": 3.4590394496917725,
      "learning_rate": 3.816522762951335e-05,
      "loss": 0.6134,
      "step": 1206200
    },
    {
      "epoch": 18.937205651491364,
      "grad_norm": 3.3084704875946045,
      "learning_rate": 3.81642464678179e-05,
      "loss": 0.6366,
      "step": 1206300
    },
    {
      "epoch": 18.93877551020408,
      "grad_norm": 3.848065137863159,
      "learning_rate": 3.8163265306122456e-05,
      "loss": 0.6212,
      "step": 1206400
    },
    {
      "epoch": 18.940345368916798,
      "grad_norm": 2.8278703689575195,
      "learning_rate": 3.8162284144427e-05,
      "loss": 0.6229,
      "step": 1206500
    },
    {
      "epoch": 18.941915227629515,
      "grad_norm": 3.3054122924804688,
      "learning_rate": 3.816130298273156e-05,
      "loss": 0.6219,
      "step": 1206600
    },
    {
      "epoch": 18.94348508634223,
      "grad_norm": 3.7538251876831055,
      "learning_rate": 3.816032182103611e-05,
      "loss": 0.5699,
      "step": 1206700
    },
    {
      "epoch": 18.945054945054945,
      "grad_norm": 6.696791648864746,
      "learning_rate": 3.815934065934066e-05,
      "loss": 0.6111,
      "step": 1206800
    },
    {
      "epoch": 18.946624803767662,
      "grad_norm": 4.813737392425537,
      "learning_rate": 3.815835949764521e-05,
      "loss": 0.6195,
      "step": 1206900
    },
    {
      "epoch": 18.948194662480375,
      "grad_norm": 2.355168104171753,
      "learning_rate": 3.815737833594977e-05,
      "loss": 0.5943,
      "step": 1207000
    },
    {
      "epoch": 18.949764521193092,
      "grad_norm": 3.1957435607910156,
      "learning_rate": 3.815639717425432e-05,
      "loss": 0.644,
      "step": 1207100
    },
    {
      "epoch": 18.95133437990581,
      "grad_norm": 4.094753742218018,
      "learning_rate": 3.815541601255887e-05,
      "loss": 0.5908,
      "step": 1207200
    },
    {
      "epoch": 18.952904238618526,
      "grad_norm": 2.758899211883545,
      "learning_rate": 3.815443485086342e-05,
      "loss": 0.6107,
      "step": 1207300
    },
    {
      "epoch": 18.95447409733124,
      "grad_norm": 3.391885995864868,
      "learning_rate": 3.815345368916798e-05,
      "loss": 0.5807,
      "step": 1207400
    },
    {
      "epoch": 18.956043956043956,
      "grad_norm": 2.4013452529907227,
      "learning_rate": 3.815247252747253e-05,
      "loss": 0.6314,
      "step": 1207500
    },
    {
      "epoch": 18.957613814756673,
      "grad_norm": 3.779745101928711,
      "learning_rate": 3.815149136577708e-05,
      "loss": 0.5767,
      "step": 1207600
    },
    {
      "epoch": 18.959183673469386,
      "grad_norm": 4.855216979980469,
      "learning_rate": 3.815051020408163e-05,
      "loss": 0.5797,
      "step": 1207700
    },
    {
      "epoch": 18.960753532182103,
      "grad_norm": 4.17867374420166,
      "learning_rate": 3.814952904238619e-05,
      "loss": 0.6152,
      "step": 1207800
    },
    {
      "epoch": 18.96232339089482,
      "grad_norm": 4.658595085144043,
      "learning_rate": 3.814854788069074e-05,
      "loss": 0.5963,
      "step": 1207900
    },
    {
      "epoch": 18.963893249607537,
      "grad_norm": 2.7636940479278564,
      "learning_rate": 3.814756671899529e-05,
      "loss": 0.6075,
      "step": 1208000
    },
    {
      "epoch": 18.96546310832025,
      "grad_norm": 4.435537338256836,
      "learning_rate": 3.814658555729985e-05,
      "loss": 0.5848,
      "step": 1208100
    },
    {
      "epoch": 18.967032967032967,
      "grad_norm": 4.575384140014648,
      "learning_rate": 3.8145604395604394e-05,
      "loss": 0.6221,
      "step": 1208200
    },
    {
      "epoch": 18.968602825745684,
      "grad_norm": 3.079237222671509,
      "learning_rate": 3.814462323390895e-05,
      "loss": 0.6033,
      "step": 1208300
    },
    {
      "epoch": 18.970172684458397,
      "grad_norm": 2.9833219051361084,
      "learning_rate": 3.81436420722135e-05,
      "loss": 0.6183,
      "step": 1208400
    },
    {
      "epoch": 18.971742543171114,
      "grad_norm": 4.724390029907227,
      "learning_rate": 3.814266091051806e-05,
      "loss": 0.6054,
      "step": 1208500
    },
    {
      "epoch": 18.97331240188383,
      "grad_norm": 5.03834342956543,
      "learning_rate": 3.8141679748822605e-05,
      "loss": 0.6051,
      "step": 1208600
    },
    {
      "epoch": 18.974882260596548,
      "grad_norm": 4.759524345397949,
      "learning_rate": 3.814069858712716e-05,
      "loss": 0.6172,
      "step": 1208700
    },
    {
      "epoch": 18.97645211930926,
      "grad_norm": 4.27164363861084,
      "learning_rate": 3.813971742543171e-05,
      "loss": 0.6075,
      "step": 1208800
    },
    {
      "epoch": 18.978021978021978,
      "grad_norm": 3.730752468109131,
      "learning_rate": 3.8138736263736264e-05,
      "loss": 0.6099,
      "step": 1208900
    },
    {
      "epoch": 18.979591836734695,
      "grad_norm": 3.7484827041625977,
      "learning_rate": 3.8137755102040815e-05,
      "loss": 0.5733,
      "step": 1209000
    },
    {
      "epoch": 18.98116169544741,
      "grad_norm": 3.9115560054779053,
      "learning_rate": 3.813677394034537e-05,
      "loss": 0.6099,
      "step": 1209100
    },
    {
      "epoch": 18.982731554160125,
      "grad_norm": 2.7433691024780273,
      "learning_rate": 3.8135792778649924e-05,
      "loss": 0.6009,
      "step": 1209200
    },
    {
      "epoch": 18.984301412872842,
      "grad_norm": 3.064521312713623,
      "learning_rate": 3.8134811616954475e-05,
      "loss": 0.6439,
      "step": 1209300
    },
    {
      "epoch": 18.98587127158556,
      "grad_norm": 3.786365509033203,
      "learning_rate": 3.8133830455259026e-05,
      "loss": 0.6321,
      "step": 1209400
    },
    {
      "epoch": 18.987441130298272,
      "grad_norm": 2.8071343898773193,
      "learning_rate": 3.8132849293563584e-05,
      "loss": 0.5661,
      "step": 1209500
    },
    {
      "epoch": 18.98901098901099,
      "grad_norm": 3.7182044982910156,
      "learning_rate": 3.8131868131868135e-05,
      "loss": 0.5946,
      "step": 1209600
    },
    {
      "epoch": 18.990580847723706,
      "grad_norm": 3.8885157108306885,
      "learning_rate": 3.8130886970172686e-05,
      "loss": 0.6002,
      "step": 1209700
    },
    {
      "epoch": 18.99215070643642,
      "grad_norm": 3.904103994369507,
      "learning_rate": 3.812990580847724e-05,
      "loss": 0.6497,
      "step": 1209800
    },
    {
      "epoch": 18.993720565149136,
      "grad_norm": 2.8640222549438477,
      "learning_rate": 3.8128924646781794e-05,
      "loss": 0.6111,
      "step": 1209900
    },
    {
      "epoch": 18.995290423861853,
      "grad_norm": 3.3780758380889893,
      "learning_rate": 3.8127943485086345e-05,
      "loss": 0.6051,
      "step": 1210000
    },
    {
      "epoch": 18.99686028257457,
      "grad_norm": 3.929992914199829,
      "learning_rate": 3.8126962323390896e-05,
      "loss": 0.6395,
      "step": 1210100
    },
    {
      "epoch": 18.998430141287283,
      "grad_norm": 2.900779962539673,
      "learning_rate": 3.8125981161695454e-05,
      "loss": 0.6389,
      "step": 1210200
    },
    {
      "epoch": 19.0,
      "grad_norm": 3.4383323192596436,
      "learning_rate": 3.8125e-05,
      "loss": 0.5636,
      "step": 1210300
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.0355162620544434,
      "eval_runtime": 14.7055,
      "eval_samples_per_second": 228.009,
      "eval_steps_per_second": 228.009,
      "step": 1210300
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.47225552797317505,
      "eval_runtime": 282.2535,
      "eval_samples_per_second": 225.684,
      "eval_steps_per_second": 225.684,
      "step": 1210300
    },
    {
      "epoch": 19.001569858712717,
      "grad_norm": 4.1368408203125,
      "learning_rate": 3.8124018838304556e-05,
      "loss": 0.5903,
      "step": 1210400
    },
    {
      "epoch": 19.00313971742543,
      "grad_norm": 4.355050086975098,
      "learning_rate": 3.812303767660911e-05,
      "loss": 0.6398,
      "step": 1210500
    },
    {
      "epoch": 19.004709576138147,
      "grad_norm": 3.8835318088531494,
      "learning_rate": 3.8122056514913665e-05,
      "loss": 0.6141,
      "step": 1210600
    },
    {
      "epoch": 19.006279434850864,
      "grad_norm": 2.8580055236816406,
      "learning_rate": 3.812107535321821e-05,
      "loss": 0.6231,
      "step": 1210700
    },
    {
      "epoch": 19.00784929356358,
      "grad_norm": 4.4897332191467285,
      "learning_rate": 3.812009419152277e-05,
      "loss": 0.5859,
      "step": 1210800
    },
    {
      "epoch": 19.009419152276294,
      "grad_norm": 4.710933685302734,
      "learning_rate": 3.811911302982732e-05,
      "loss": 0.6332,
      "step": 1210900
    },
    {
      "epoch": 19.01098901098901,
      "grad_norm": 3.920689344406128,
      "learning_rate": 3.811813186813187e-05,
      "loss": 0.6667,
      "step": 1211000
    },
    {
      "epoch": 19.012558869701728,
      "grad_norm": 2.9869253635406494,
      "learning_rate": 3.811715070643642e-05,
      "loss": 0.5921,
      "step": 1211100
    },
    {
      "epoch": 19.01412872841444,
      "grad_norm": 3.5206515789031982,
      "learning_rate": 3.811616954474098e-05,
      "loss": 0.5881,
      "step": 1211200
    },
    {
      "epoch": 19.015698587127158,
      "grad_norm": 4.909332275390625,
      "learning_rate": 3.811518838304553e-05,
      "loss": 0.6073,
      "step": 1211300
    },
    {
      "epoch": 19.017268445839875,
      "grad_norm": 3.2339839935302734,
      "learning_rate": 3.811420722135008e-05,
      "loss": 0.6373,
      "step": 1211400
    },
    {
      "epoch": 19.01883830455259,
      "grad_norm": 3.872380256652832,
      "learning_rate": 3.811322605965463e-05,
      "loss": 0.6139,
      "step": 1211500
    },
    {
      "epoch": 19.020408163265305,
      "grad_norm": 4.005204677581787,
      "learning_rate": 3.811224489795919e-05,
      "loss": 0.5931,
      "step": 1211600
    },
    {
      "epoch": 19.021978021978022,
      "grad_norm": 3.8738901615142822,
      "learning_rate": 3.811126373626374e-05,
      "loss": 0.6448,
      "step": 1211700
    },
    {
      "epoch": 19.02354788069074,
      "grad_norm": 3.6798133850097656,
      "learning_rate": 3.811028257456829e-05,
      "loss": 0.6489,
      "step": 1211800
    },
    {
      "epoch": 19.025117739403452,
      "grad_norm": 4.4740447998046875,
      "learning_rate": 3.810930141287284e-05,
      "loss": 0.585,
      "step": 1211900
    },
    {
      "epoch": 19.02668759811617,
      "grad_norm": 4.059591293334961,
      "learning_rate": 3.81083202511774e-05,
      "loss": 0.5959,
      "step": 1212000
    },
    {
      "epoch": 19.028257456828886,
      "grad_norm": 4.358952045440674,
      "learning_rate": 3.810733908948195e-05,
      "loss": 0.5858,
      "step": 1212100
    },
    {
      "epoch": 19.029827315541603,
      "grad_norm": 4.329319953918457,
      "learning_rate": 3.81063579277865e-05,
      "loss": 0.6094,
      "step": 1212200
    },
    {
      "epoch": 19.031397174254316,
      "grad_norm": 3.2906651496887207,
      "learning_rate": 3.810537676609106e-05,
      "loss": 0.5935,
      "step": 1212300
    },
    {
      "epoch": 19.032967032967033,
      "grad_norm": 4.113304138183594,
      "learning_rate": 3.81043956043956e-05,
      "loss": 0.62,
      "step": 1212400
    },
    {
      "epoch": 19.03453689167975,
      "grad_norm": 4.131540298461914,
      "learning_rate": 3.810341444270016e-05,
      "loss": 0.595,
      "step": 1212500
    },
    {
      "epoch": 19.036106750392463,
      "grad_norm": 5.611502170562744,
      "learning_rate": 3.810243328100471e-05,
      "loss": 0.5868,
      "step": 1212600
    },
    {
      "epoch": 19.03767660910518,
      "grad_norm": 4.695623874664307,
      "learning_rate": 3.810145211930927e-05,
      "loss": 0.5839,
      "step": 1212700
    },
    {
      "epoch": 19.039246467817897,
      "grad_norm": 4.446121692657471,
      "learning_rate": 3.8100470957613814e-05,
      "loss": 0.6069,
      "step": 1212800
    },
    {
      "epoch": 19.040816326530614,
      "grad_norm": 3.539581775665283,
      "learning_rate": 3.809948979591837e-05,
      "loss": 0.644,
      "step": 1212900
    },
    {
      "epoch": 19.042386185243327,
      "grad_norm": 2.9074645042419434,
      "learning_rate": 3.809850863422292e-05,
      "loss": 0.5914,
      "step": 1213000
    },
    {
      "epoch": 19.043956043956044,
      "grad_norm": 3.3861894607543945,
      "learning_rate": 3.809752747252747e-05,
      "loss": 0.572,
      "step": 1213100
    },
    {
      "epoch": 19.04552590266876,
      "grad_norm": 3.3403947353363037,
      "learning_rate": 3.8096546310832024e-05,
      "loss": 0.6202,
      "step": 1213200
    },
    {
      "epoch": 19.047095761381474,
      "grad_norm": 3.8500022888183594,
      "learning_rate": 3.809556514913658e-05,
      "loss": 0.6031,
      "step": 1213300
    },
    {
      "epoch": 19.04866562009419,
      "grad_norm": 3.8156094551086426,
      "learning_rate": 3.809458398744113e-05,
      "loss": 0.6102,
      "step": 1213400
    },
    {
      "epoch": 19.050235478806908,
      "grad_norm": 3.3639745712280273,
      "learning_rate": 3.8093602825745684e-05,
      "loss": 0.557,
      "step": 1213500
    },
    {
      "epoch": 19.051805337519625,
      "grad_norm": 4.430666923522949,
      "learning_rate": 3.8092621664050235e-05,
      "loss": 0.5824,
      "step": 1213600
    },
    {
      "epoch": 19.053375196232338,
      "grad_norm": 2.12988543510437,
      "learning_rate": 3.809164050235479e-05,
      "loss": 0.5731,
      "step": 1213700
    },
    {
      "epoch": 19.054945054945055,
      "grad_norm": 4.472163677215576,
      "learning_rate": 3.8090659340659344e-05,
      "loss": 0.5959,
      "step": 1213800
    },
    {
      "epoch": 19.05651491365777,
      "grad_norm": 3.491931676864624,
      "learning_rate": 3.8089678178963895e-05,
      "loss": 0.581,
      "step": 1213900
    },
    {
      "epoch": 19.058084772370485,
      "grad_norm": 3.8876118659973145,
      "learning_rate": 3.8088697017268446e-05,
      "loss": 0.631,
      "step": 1214000
    },
    {
      "epoch": 19.059654631083202,
      "grad_norm": 4.272710800170898,
      "learning_rate": 3.8087715855573003e-05,
      "loss": 0.6047,
      "step": 1214100
    },
    {
      "epoch": 19.06122448979592,
      "grad_norm": 4.333923816680908,
      "learning_rate": 3.8086734693877554e-05,
      "loss": 0.636,
      "step": 1214200
    },
    {
      "epoch": 19.062794348508636,
      "grad_norm": 3.339202880859375,
      "learning_rate": 3.8085753532182105e-05,
      "loss": 0.5941,
      "step": 1214300
    },
    {
      "epoch": 19.06436420722135,
      "grad_norm": 5.8354387283325195,
      "learning_rate": 3.808477237048666e-05,
      "loss": 0.6282,
      "step": 1214400
    },
    {
      "epoch": 19.065934065934066,
      "grad_norm": 3.9221384525299072,
      "learning_rate": 3.808379120879121e-05,
      "loss": 0.6152,
      "step": 1214500
    },
    {
      "epoch": 19.067503924646783,
      "grad_norm": 2.6582436561584473,
      "learning_rate": 3.8082810047095765e-05,
      "loss": 0.6314,
      "step": 1214600
    },
    {
      "epoch": 19.069073783359496,
      "grad_norm": 5.046427249908447,
      "learning_rate": 3.8081828885400316e-05,
      "loss": 0.6025,
      "step": 1214700
    },
    {
      "epoch": 19.070643642072213,
      "grad_norm": 3.272056818008423,
      "learning_rate": 3.8080847723704874e-05,
      "loss": 0.6289,
      "step": 1214800
    },
    {
      "epoch": 19.07221350078493,
      "grad_norm": 3.4532642364501953,
      "learning_rate": 3.807986656200942e-05,
      "loss": 0.6113,
      "step": 1214900
    },
    {
      "epoch": 19.073783359497646,
      "grad_norm": 4.5029473304748535,
      "learning_rate": 3.8078885400313976e-05,
      "loss": 0.5912,
      "step": 1215000
    },
    {
      "epoch": 19.07535321821036,
      "grad_norm": 3.999833106994629,
      "learning_rate": 3.807790423861853e-05,
      "loss": 0.5852,
      "step": 1215100
    },
    {
      "epoch": 19.076923076923077,
      "grad_norm": 3.7719430923461914,
      "learning_rate": 3.807692307692308e-05,
      "loss": 0.5875,
      "step": 1215200
    },
    {
      "epoch": 19.078492935635794,
      "grad_norm": 4.159170627593994,
      "learning_rate": 3.807594191522763e-05,
      "loss": 0.6441,
      "step": 1215300
    },
    {
      "epoch": 19.08006279434851,
      "grad_norm": 3.663209915161133,
      "learning_rate": 3.8074960753532186e-05,
      "loss": 0.5913,
      "step": 1215400
    },
    {
      "epoch": 19.081632653061224,
      "grad_norm": 4.200409412384033,
      "learning_rate": 3.807397959183674e-05,
      "loss": 0.5715,
      "step": 1215500
    },
    {
      "epoch": 19.08320251177394,
      "grad_norm": 3.7223942279815674,
      "learning_rate": 3.807299843014129e-05,
      "loss": 0.5663,
      "step": 1215600
    },
    {
      "epoch": 19.084772370486657,
      "grad_norm": 3.3899483680725098,
      "learning_rate": 3.807201726844584e-05,
      "loss": 0.5767,
      "step": 1215700
    },
    {
      "epoch": 19.08634222919937,
      "grad_norm": 3.442857027053833,
      "learning_rate": 3.80710361067504e-05,
      "loss": 0.5752,
      "step": 1215800
    },
    {
      "epoch": 19.087912087912088,
      "grad_norm": 4.4865593910217285,
      "learning_rate": 3.807005494505495e-05,
      "loss": 0.5817,
      "step": 1215900
    },
    {
      "epoch": 19.089481946624804,
      "grad_norm": 5.076561450958252,
      "learning_rate": 3.80690737833595e-05,
      "loss": 0.6199,
      "step": 1216000
    },
    {
      "epoch": 19.09105180533752,
      "grad_norm": 4.4453511238098145,
      "learning_rate": 3.806809262166405e-05,
      "loss": 0.6008,
      "step": 1216100
    },
    {
      "epoch": 19.092621664050235,
      "grad_norm": 4.42396354675293,
      "learning_rate": 3.806711145996861e-05,
      "loss": 0.6337,
      "step": 1216200
    },
    {
      "epoch": 19.09419152276295,
      "grad_norm": 3.7947065830230713,
      "learning_rate": 3.806613029827316e-05,
      "loss": 0.6107,
      "step": 1216300
    },
    {
      "epoch": 19.09576138147567,
      "grad_norm": 4.612131118774414,
      "learning_rate": 3.806514913657771e-05,
      "loss": 0.6099,
      "step": 1216400
    },
    {
      "epoch": 19.09733124018838,
      "grad_norm": 3.819627285003662,
      "learning_rate": 3.806416797488227e-05,
      "loss": 0.5819,
      "step": 1216500
    },
    {
      "epoch": 19.0989010989011,
      "grad_norm": 4.674370288848877,
      "learning_rate": 3.806318681318681e-05,
      "loss": 0.5778,
      "step": 1216600
    },
    {
      "epoch": 19.100470957613815,
      "grad_norm": 3.9028918743133545,
      "learning_rate": 3.806220565149137e-05,
      "loss": 0.6168,
      "step": 1216700
    },
    {
      "epoch": 19.102040816326532,
      "grad_norm": 2.680570602416992,
      "learning_rate": 3.806122448979592e-05,
      "loss": 0.6143,
      "step": 1216800
    },
    {
      "epoch": 19.103610675039246,
      "grad_norm": 5.138695240020752,
      "learning_rate": 3.806024332810048e-05,
      "loss": 0.5974,
      "step": 1216900
    },
    {
      "epoch": 19.105180533751962,
      "grad_norm": 3.3847055435180664,
      "learning_rate": 3.805926216640502e-05,
      "loss": 0.5962,
      "step": 1217000
    },
    {
      "epoch": 19.10675039246468,
      "grad_norm": 3.1644792556762695,
      "learning_rate": 3.805828100470958e-05,
      "loss": 0.6404,
      "step": 1217100
    },
    {
      "epoch": 19.108320251177393,
      "grad_norm": 3.5859603881835938,
      "learning_rate": 3.805729984301413e-05,
      "loss": 0.6016,
      "step": 1217200
    },
    {
      "epoch": 19.10989010989011,
      "grad_norm": 2.9707374572753906,
      "learning_rate": 3.805631868131868e-05,
      "loss": 0.6138,
      "step": 1217300
    },
    {
      "epoch": 19.111459968602826,
      "grad_norm": 3.2879443168640137,
      "learning_rate": 3.805533751962323e-05,
      "loss": 0.5919,
      "step": 1217400
    },
    {
      "epoch": 19.113029827315543,
      "grad_norm": 3.810434103012085,
      "learning_rate": 3.805435635792779e-05,
      "loss": 0.6123,
      "step": 1217500
    },
    {
      "epoch": 19.114599686028257,
      "grad_norm": 3.8898189067840576,
      "learning_rate": 3.8053375196232335e-05,
      "loss": 0.6053,
      "step": 1217600
    },
    {
      "epoch": 19.116169544740973,
      "grad_norm": 4.40689754486084,
      "learning_rate": 3.805239403453689e-05,
      "loss": 0.5937,
      "step": 1217700
    },
    {
      "epoch": 19.11773940345369,
      "grad_norm": 3.7925779819488525,
      "learning_rate": 3.8051412872841444e-05,
      "loss": 0.6087,
      "step": 1217800
    },
    {
      "epoch": 19.119309262166404,
      "grad_norm": 4.538469314575195,
      "learning_rate": 3.8050431711146e-05,
      "loss": 0.6074,
      "step": 1217900
    },
    {
      "epoch": 19.12087912087912,
      "grad_norm": 3.541395664215088,
      "learning_rate": 3.804945054945055e-05,
      "loss": 0.6078,
      "step": 1218000
    },
    {
      "epoch": 19.122448979591837,
      "grad_norm": 3.5211117267608643,
      "learning_rate": 3.8048469387755104e-05,
      "loss": 0.575,
      "step": 1218100
    },
    {
      "epoch": 19.124018838304554,
      "grad_norm": 4.137115478515625,
      "learning_rate": 3.8047488226059655e-05,
      "loss": 0.6064,
      "step": 1218200
    },
    {
      "epoch": 19.125588697017267,
      "grad_norm": 3.997689723968506,
      "learning_rate": 3.8046507064364206e-05,
      "loss": 0.5981,
      "step": 1218300
    },
    {
      "epoch": 19.127158555729984,
      "grad_norm": 5.401329040527344,
      "learning_rate": 3.804552590266876e-05,
      "loss": 0.5785,
      "step": 1218400
    },
    {
      "epoch": 19.1287284144427,
      "grad_norm": 4.352939128875732,
      "learning_rate": 3.8044544740973314e-05,
      "loss": 0.5738,
      "step": 1218500
    },
    {
      "epoch": 19.130298273155415,
      "grad_norm": 3.8919477462768555,
      "learning_rate": 3.804356357927787e-05,
      "loss": 0.6112,
      "step": 1218600
    },
    {
      "epoch": 19.13186813186813,
      "grad_norm": 4.07786750793457,
      "learning_rate": 3.8042582417582416e-05,
      "loss": 0.615,
      "step": 1218700
    },
    {
      "epoch": 19.13343799058085,
      "grad_norm": 3.573693037033081,
      "learning_rate": 3.8041601255886974e-05,
      "loss": 0.6112,
      "step": 1218800
    },
    {
      "epoch": 19.135007849293565,
      "grad_norm": 4.809084892272949,
      "learning_rate": 3.8040620094191525e-05,
      "loss": 0.6259,
      "step": 1218900
    },
    {
      "epoch": 19.13657770800628,
      "grad_norm": 3.6298811435699463,
      "learning_rate": 3.8039638932496076e-05,
      "loss": 0.6439,
      "step": 1219000
    },
    {
      "epoch": 19.138147566718995,
      "grad_norm": 3.711406946182251,
      "learning_rate": 3.803865777080063e-05,
      "loss": 0.5836,
      "step": 1219100
    },
    {
      "epoch": 19.139717425431712,
      "grad_norm": 3.447775363922119,
      "learning_rate": 3.8037676609105185e-05,
      "loss": 0.5812,
      "step": 1219200
    },
    {
      "epoch": 19.141287284144425,
      "grad_norm": 3.7302472591400146,
      "learning_rate": 3.8036695447409736e-05,
      "loss": 0.6119,
      "step": 1219300
    },
    {
      "epoch": 19.142857142857142,
      "grad_norm": 4.781587600708008,
      "learning_rate": 3.803571428571429e-05,
      "loss": 0.6061,
      "step": 1219400
    },
    {
      "epoch": 19.14442700156986,
      "grad_norm": 3.473893165588379,
      "learning_rate": 3.803473312401884e-05,
      "loss": 0.6266,
      "step": 1219500
    },
    {
      "epoch": 19.145996860282576,
      "grad_norm": 3.777923583984375,
      "learning_rate": 3.8033751962323395e-05,
      "loss": 0.6189,
      "step": 1219600
    },
    {
      "epoch": 19.14756671899529,
      "grad_norm": 3.556877374649048,
      "learning_rate": 3.803277080062794e-05,
      "loss": 0.6212,
      "step": 1219700
    },
    {
      "epoch": 19.149136577708006,
      "grad_norm": 3.106119394302368,
      "learning_rate": 3.80317896389325e-05,
      "loss": 0.5825,
      "step": 1219800
    },
    {
      "epoch": 19.150706436420723,
      "grad_norm": 1.9837874174118042,
      "learning_rate": 3.803080847723705e-05,
      "loss": 0.6631,
      "step": 1219900
    },
    {
      "epoch": 19.152276295133436,
      "grad_norm": 2.8372962474823,
      "learning_rate": 3.8029827315541606e-05,
      "loss": 0.5687,
      "step": 1220000
    },
    {
      "epoch": 19.153846153846153,
      "grad_norm": 2.4881279468536377,
      "learning_rate": 3.802884615384616e-05,
      "loss": 0.6156,
      "step": 1220100
    },
    {
      "epoch": 19.15541601255887,
      "grad_norm": 2.736274480819702,
      "learning_rate": 3.802786499215071e-05,
      "loss": 0.5797,
      "step": 1220200
    },
    {
      "epoch": 19.156985871271587,
      "grad_norm": 4.154717445373535,
      "learning_rate": 3.802688383045526e-05,
      "loss": 0.6365,
      "step": 1220300
    },
    {
      "epoch": 19.1585557299843,
      "grad_norm": 3.339323043823242,
      "learning_rate": 3.802590266875981e-05,
      "loss": 0.6342,
      "step": 1220400
    },
    {
      "epoch": 19.160125588697017,
      "grad_norm": 3.401413917541504,
      "learning_rate": 3.802492150706437e-05,
      "loss": 0.6397,
      "step": 1220500
    },
    {
      "epoch": 19.161695447409734,
      "grad_norm": 3.370210886001587,
      "learning_rate": 3.802394034536892e-05,
      "loss": 0.6154,
      "step": 1220600
    },
    {
      "epoch": 19.163265306122447,
      "grad_norm": 3.663076400756836,
      "learning_rate": 3.8022959183673477e-05,
      "loss": 0.6075,
      "step": 1220700
    },
    {
      "epoch": 19.164835164835164,
      "grad_norm": 2.9672563076019287,
      "learning_rate": 3.802197802197802e-05,
      "loss": 0.5826,
      "step": 1220800
    },
    {
      "epoch": 19.16640502354788,
      "grad_norm": 3.9657442569732666,
      "learning_rate": 3.802099686028258e-05,
      "loss": 0.5941,
      "step": 1220900
    },
    {
      "epoch": 19.167974882260598,
      "grad_norm": 3.9390926361083984,
      "learning_rate": 3.802001569858713e-05,
      "loss": 0.6437,
      "step": 1221000
    },
    {
      "epoch": 19.16954474097331,
      "grad_norm": 3.962613344192505,
      "learning_rate": 3.801903453689168e-05,
      "loss": 0.6074,
      "step": 1221100
    },
    {
      "epoch": 19.171114599686028,
      "grad_norm": 3.2118611335754395,
      "learning_rate": 3.801805337519623e-05,
      "loss": 0.5916,
      "step": 1221200
    },
    {
      "epoch": 19.172684458398745,
      "grad_norm": 4.340915679931641,
      "learning_rate": 3.801707221350079e-05,
      "loss": 0.6457,
      "step": 1221300
    },
    {
      "epoch": 19.17425431711146,
      "grad_norm": 4.157924175262451,
      "learning_rate": 3.801609105180534e-05,
      "loss": 0.6041,
      "step": 1221400
    },
    {
      "epoch": 19.175824175824175,
      "grad_norm": 3.9273078441619873,
      "learning_rate": 3.801510989010989e-05,
      "loss": 0.6375,
      "step": 1221500
    },
    {
      "epoch": 19.177394034536892,
      "grad_norm": 5.242659568786621,
      "learning_rate": 3.801412872841444e-05,
      "loss": 0.5932,
      "step": 1221600
    },
    {
      "epoch": 19.17896389324961,
      "grad_norm": 3.555774211883545,
      "learning_rate": 3.8013147566719e-05,
      "loss": 0.5993,
      "step": 1221700
    },
    {
      "epoch": 19.180533751962322,
      "grad_norm": 3.1442785263061523,
      "learning_rate": 3.8012166405023544e-05,
      "loss": 0.5681,
      "step": 1221800
    },
    {
      "epoch": 19.18210361067504,
      "grad_norm": 3.42563533782959,
      "learning_rate": 3.80111852433281e-05,
      "loss": 0.5867,
      "step": 1221900
    },
    {
      "epoch": 19.183673469387756,
      "grad_norm": 4.28119421005249,
      "learning_rate": 3.801020408163265e-05,
      "loss": 0.618,
      "step": 1222000
    },
    {
      "epoch": 19.18524332810047,
      "grad_norm": 3.9750595092773438,
      "learning_rate": 3.800922291993721e-05,
      "loss": 0.6034,
      "step": 1222100
    },
    {
      "epoch": 19.186813186813186,
      "grad_norm": 3.9117751121520996,
      "learning_rate": 3.8008241758241755e-05,
      "loss": 0.6087,
      "step": 1222200
    },
    {
      "epoch": 19.188383045525903,
      "grad_norm": 4.1888427734375,
      "learning_rate": 3.800726059654631e-05,
      "loss": 0.6231,
      "step": 1222300
    },
    {
      "epoch": 19.18995290423862,
      "grad_norm": 3.212756395339966,
      "learning_rate": 3.8006279434850864e-05,
      "loss": 0.6553,
      "step": 1222400
    },
    {
      "epoch": 19.191522762951333,
      "grad_norm": 2.8486437797546387,
      "learning_rate": 3.8005298273155415e-05,
      "loss": 0.591,
      "step": 1222500
    },
    {
      "epoch": 19.19309262166405,
      "grad_norm": 3.632532835006714,
      "learning_rate": 3.800431711145997e-05,
      "loss": 0.6103,
      "step": 1222600
    },
    {
      "epoch": 19.194662480376767,
      "grad_norm": 2.9778568744659424,
      "learning_rate": 3.800333594976452e-05,
      "loss": 0.6248,
      "step": 1222700
    },
    {
      "epoch": 19.19623233908948,
      "grad_norm": 3.1123034954071045,
      "learning_rate": 3.800235478806908e-05,
      "loss": 0.5852,
      "step": 1222800
    },
    {
      "epoch": 19.197802197802197,
      "grad_norm": 3.72074556350708,
      "learning_rate": 3.8001373626373625e-05,
      "loss": 0.5989,
      "step": 1222900
    },
    {
      "epoch": 19.199372056514914,
      "grad_norm": 4.140166759490967,
      "learning_rate": 3.800039246467818e-05,
      "loss": 0.6158,
      "step": 1223000
    },
    {
      "epoch": 19.20094191522763,
      "grad_norm": 3.4606456756591797,
      "learning_rate": 3.7999411302982734e-05,
      "loss": 0.634,
      "step": 1223100
    },
    {
      "epoch": 19.202511773940344,
      "grad_norm": 3.956535577774048,
      "learning_rate": 3.7998430141287285e-05,
      "loss": 0.5902,
      "step": 1223200
    },
    {
      "epoch": 19.20408163265306,
      "grad_norm": 5.127357482910156,
      "learning_rate": 3.7997448979591836e-05,
      "loss": 0.5973,
      "step": 1223300
    },
    {
      "epoch": 19.205651491365778,
      "grad_norm": 3.7780258655548096,
      "learning_rate": 3.7996467817896394e-05,
      "loss": 0.6417,
      "step": 1223400
    },
    {
      "epoch": 19.20722135007849,
      "grad_norm": 3.899475574493408,
      "learning_rate": 3.7995486656200945e-05,
      "loss": 0.6291,
      "step": 1223500
    },
    {
      "epoch": 19.208791208791208,
      "grad_norm": 1.9915083646774292,
      "learning_rate": 3.7994505494505496e-05,
      "loss": 0.6404,
      "step": 1223600
    },
    {
      "epoch": 19.210361067503925,
      "grad_norm": 4.131754398345947,
      "learning_rate": 3.799352433281005e-05,
      "loss": 0.5953,
      "step": 1223700
    },
    {
      "epoch": 19.211930926216642,
      "grad_norm": 4.802354335784912,
      "learning_rate": 3.7992543171114604e-05,
      "loss": 0.6169,
      "step": 1223800
    },
    {
      "epoch": 19.213500784929355,
      "grad_norm": 3.868838310241699,
      "learning_rate": 3.799156200941915e-05,
      "loss": 0.5746,
      "step": 1223900
    },
    {
      "epoch": 19.215070643642072,
      "grad_norm": 4.35807466506958,
      "learning_rate": 3.7990580847723706e-05,
      "loss": 0.5625,
      "step": 1224000
    },
    {
      "epoch": 19.21664050235479,
      "grad_norm": 3.208324909210205,
      "learning_rate": 3.798959968602826e-05,
      "loss": 0.6091,
      "step": 1224100
    },
    {
      "epoch": 19.218210361067506,
      "grad_norm": 3.475184440612793,
      "learning_rate": 3.7988618524332815e-05,
      "loss": 0.601,
      "step": 1224200
    },
    {
      "epoch": 19.21978021978022,
      "grad_norm": 3.459312915802002,
      "learning_rate": 3.798763736263736e-05,
      "loss": 0.5869,
      "step": 1224300
    },
    {
      "epoch": 19.221350078492936,
      "grad_norm": 4.083259582519531,
      "learning_rate": 3.798665620094192e-05,
      "loss": 0.6338,
      "step": 1224400
    },
    {
      "epoch": 19.222919937205653,
      "grad_norm": 4.916376113891602,
      "learning_rate": 3.798567503924647e-05,
      "loss": 0.6109,
      "step": 1224500
    },
    {
      "epoch": 19.224489795918366,
      "grad_norm": 3.5612633228302,
      "learning_rate": 3.798469387755102e-05,
      "loss": 0.5896,
      "step": 1224600
    },
    {
      "epoch": 19.226059654631083,
      "grad_norm": 3.8246912956237793,
      "learning_rate": 3.798371271585558e-05,
      "loss": 0.5948,
      "step": 1224700
    },
    {
      "epoch": 19.2276295133438,
      "grad_norm": 3.887225866317749,
      "learning_rate": 3.798273155416013e-05,
      "loss": 0.5996,
      "step": 1224800
    },
    {
      "epoch": 19.229199372056517,
      "grad_norm": 4.374935150146484,
      "learning_rate": 3.7981750392464686e-05,
      "loss": 0.5815,
      "step": 1224900
    },
    {
      "epoch": 19.23076923076923,
      "grad_norm": 3.569387435913086,
      "learning_rate": 3.798076923076923e-05,
      "loss": 0.6083,
      "step": 1225000
    },
    {
      "epoch": 19.232339089481947,
      "grad_norm": 2.9408490657806396,
      "learning_rate": 3.797978806907379e-05,
      "loss": 0.6199,
      "step": 1225100
    },
    {
      "epoch": 19.233908948194664,
      "grad_norm": 3.868260622024536,
      "learning_rate": 3.797880690737834e-05,
      "loss": 0.5932,
      "step": 1225200
    },
    {
      "epoch": 19.235478806907377,
      "grad_norm": 3.7316133975982666,
      "learning_rate": 3.797782574568289e-05,
      "loss": 0.6163,
      "step": 1225300
    },
    {
      "epoch": 19.237048665620094,
      "grad_norm": 2.9751999378204346,
      "learning_rate": 3.797684458398744e-05,
      "loss": 0.5918,
      "step": 1225400
    },
    {
      "epoch": 19.23861852433281,
      "grad_norm": 3.4331774711608887,
      "learning_rate": 3.7975863422292e-05,
      "loss": 0.6001,
      "step": 1225500
    },
    {
      "epoch": 19.240188383045528,
      "grad_norm": 4.010054588317871,
      "learning_rate": 3.797488226059655e-05,
      "loss": 0.6182,
      "step": 1225600
    },
    {
      "epoch": 19.24175824175824,
      "grad_norm": 4.114856243133545,
      "learning_rate": 3.79739010989011e-05,
      "loss": 0.5875,
      "step": 1225700
    },
    {
      "epoch": 19.243328100470958,
      "grad_norm": 4.450204849243164,
      "learning_rate": 3.797291993720565e-05,
      "loss": 0.6245,
      "step": 1225800
    },
    {
      "epoch": 19.244897959183675,
      "grad_norm": 3.9489235877990723,
      "learning_rate": 3.797193877551021e-05,
      "loss": 0.5844,
      "step": 1225900
    },
    {
      "epoch": 19.246467817896388,
      "grad_norm": 2.634704828262329,
      "learning_rate": 3.797095761381475e-05,
      "loss": 0.6142,
      "step": 1226000
    },
    {
      "epoch": 19.248037676609105,
      "grad_norm": 3.6739959716796875,
      "learning_rate": 3.796997645211931e-05,
      "loss": 0.5698,
      "step": 1226100
    },
    {
      "epoch": 19.24960753532182,
      "grad_norm": 3.7846429347991943,
      "learning_rate": 3.796899529042386e-05,
      "loss": 0.6072,
      "step": 1226200
    },
    {
      "epoch": 19.25117739403454,
      "grad_norm": 3.770869493484497,
      "learning_rate": 3.796801412872842e-05,
      "loss": 0.6188,
      "step": 1226300
    },
    {
      "epoch": 19.252747252747252,
      "grad_norm": 4.035941123962402,
      "learning_rate": 3.7967032967032964e-05,
      "loss": 0.6104,
      "step": 1226400
    },
    {
      "epoch": 19.25431711145997,
      "grad_norm": 4.404880046844482,
      "learning_rate": 3.796605180533752e-05,
      "loss": 0.6234,
      "step": 1226500
    },
    {
      "epoch": 19.255886970172686,
      "grad_norm": 5.06187105178833,
      "learning_rate": 3.796507064364207e-05,
      "loss": 0.6299,
      "step": 1226600
    },
    {
      "epoch": 19.2574568288854,
      "grad_norm": 6.111336708068848,
      "learning_rate": 3.7964089481946624e-05,
      "loss": 0.617,
      "step": 1226700
    },
    {
      "epoch": 19.259026687598116,
      "grad_norm": 4.52168607711792,
      "learning_rate": 3.796310832025118e-05,
      "loss": 0.6259,
      "step": 1226800
    },
    {
      "epoch": 19.260596546310833,
      "grad_norm": 3.611605644226074,
      "learning_rate": 3.796212715855573e-05,
      "loss": 0.5802,
      "step": 1226900
    },
    {
      "epoch": 19.26216640502355,
      "grad_norm": 4.988986968994141,
      "learning_rate": 3.796114599686029e-05,
      "loss": 0.625,
      "step": 1227000
    },
    {
      "epoch": 19.263736263736263,
      "grad_norm": 3.6713027954101562,
      "learning_rate": 3.7960164835164834e-05,
      "loss": 0.6516,
      "step": 1227100
    },
    {
      "epoch": 19.26530612244898,
      "grad_norm": 3.850322961807251,
      "learning_rate": 3.795918367346939e-05,
      "loss": 0.5785,
      "step": 1227200
    },
    {
      "epoch": 19.266875981161697,
      "grad_norm": 3.3312690258026123,
      "learning_rate": 3.795820251177394e-05,
      "loss": 0.6117,
      "step": 1227300
    },
    {
      "epoch": 19.26844583987441,
      "grad_norm": 3.6220736503601074,
      "learning_rate": 3.7957221350078494e-05,
      "loss": 0.5983,
      "step": 1227400
    },
    {
      "epoch": 19.270015698587127,
      "grad_norm": 4.285383701324463,
      "learning_rate": 3.7956240188383045e-05,
      "loss": 0.5816,
      "step": 1227500
    },
    {
      "epoch": 19.271585557299844,
      "grad_norm": 4.619045257568359,
      "learning_rate": 3.79552590266876e-05,
      "loss": 0.6055,
      "step": 1227600
    },
    {
      "epoch": 19.27315541601256,
      "grad_norm": 4.709415435791016,
      "learning_rate": 3.7954277864992154e-05,
      "loss": 0.6441,
      "step": 1227700
    },
    {
      "epoch": 19.274725274725274,
      "grad_norm": 3.1646368503570557,
      "learning_rate": 3.7953296703296705e-05,
      "loss": 0.5593,
      "step": 1227800
    },
    {
      "epoch": 19.27629513343799,
      "grad_norm": 3.0713911056518555,
      "learning_rate": 3.7952315541601256e-05,
      "loss": 0.5822,
      "step": 1227900
    },
    {
      "epoch": 19.277864992150707,
      "grad_norm": 3.0854339599609375,
      "learning_rate": 3.7951334379905813e-05,
      "loss": 0.5843,
      "step": 1228000
    },
    {
      "epoch": 19.27943485086342,
      "grad_norm": 4.20229434967041,
      "learning_rate": 3.795035321821036e-05,
      "loss": 0.5916,
      "step": 1228100
    },
    {
      "epoch": 19.281004709576138,
      "grad_norm": 4.261311054229736,
      "learning_rate": 3.7949372056514915e-05,
      "loss": 0.6302,
      "step": 1228200
    },
    {
      "epoch": 19.282574568288855,
      "grad_norm": 3.577176094055176,
      "learning_rate": 3.7948390894819466e-05,
      "loss": 0.5907,
      "step": 1228300
    },
    {
      "epoch": 19.28414442700157,
      "grad_norm": 2.574000835418701,
      "learning_rate": 3.7947409733124024e-05,
      "loss": 0.6009,
      "step": 1228400
    },
    {
      "epoch": 19.285714285714285,
      "grad_norm": 2.0731444358825684,
      "learning_rate": 3.794642857142857e-05,
      "loss": 0.6326,
      "step": 1228500
    },
    {
      "epoch": 19.287284144427,
      "grad_norm": 4.027377128601074,
      "learning_rate": 3.7945447409733126e-05,
      "loss": 0.6121,
      "step": 1228600
    },
    {
      "epoch": 19.28885400313972,
      "grad_norm": 2.5590620040893555,
      "learning_rate": 3.794446624803768e-05,
      "loss": 0.6049,
      "step": 1228700
    },
    {
      "epoch": 19.29042386185243,
      "grad_norm": 4.030065059661865,
      "learning_rate": 3.794348508634223e-05,
      "loss": 0.575,
      "step": 1228800
    },
    {
      "epoch": 19.29199372056515,
      "grad_norm": 3.9334771633148193,
      "learning_rate": 3.7942503924646786e-05,
      "loss": 0.5845,
      "step": 1228900
    },
    {
      "epoch": 19.293563579277865,
      "grad_norm": 4.433375835418701,
      "learning_rate": 3.794152276295134e-05,
      "loss": 0.5926,
      "step": 1229000
    },
    {
      "epoch": 19.295133437990582,
      "grad_norm": 3.9560182094573975,
      "learning_rate": 3.7940541601255895e-05,
      "loss": 0.614,
      "step": 1229100
    },
    {
      "epoch": 19.296703296703296,
      "grad_norm": 3.807741403579712,
      "learning_rate": 3.793956043956044e-05,
      "loss": 0.5955,
      "step": 1229200
    },
    {
      "epoch": 19.298273155416013,
      "grad_norm": 3.5758578777313232,
      "learning_rate": 3.7938579277864997e-05,
      "loss": 0.6008,
      "step": 1229300
    },
    {
      "epoch": 19.29984301412873,
      "grad_norm": 3.7970852851867676,
      "learning_rate": 3.793759811616955e-05,
      "loss": 0.6065,
      "step": 1229400
    },
    {
      "epoch": 19.301412872841443,
      "grad_norm": 3.702211618423462,
      "learning_rate": 3.79366169544741e-05,
      "loss": 0.5721,
      "step": 1229500
    },
    {
      "epoch": 19.30298273155416,
      "grad_norm": 3.3148555755615234,
      "learning_rate": 3.793563579277865e-05,
      "loss": 0.605,
      "step": 1229600
    },
    {
      "epoch": 19.304552590266876,
      "grad_norm": 3.5623960494995117,
      "learning_rate": 3.793465463108321e-05,
      "loss": 0.6223,
      "step": 1229700
    },
    {
      "epoch": 19.306122448979593,
      "grad_norm": 3.142404794692993,
      "learning_rate": 3.793367346938776e-05,
      "loss": 0.5895,
      "step": 1229800
    },
    {
      "epoch": 19.307692307692307,
      "grad_norm": 4.002394676208496,
      "learning_rate": 3.793269230769231e-05,
      "loss": 0.6174,
      "step": 1229900
    },
    {
      "epoch": 19.309262166405023,
      "grad_norm": 3.27760648727417,
      "learning_rate": 3.793171114599686e-05,
      "loss": 0.6246,
      "step": 1230000
    },
    {
      "epoch": 19.31083202511774,
      "grad_norm": 3.990083694458008,
      "learning_rate": 3.793072998430142e-05,
      "loss": 0.584,
      "step": 1230100
    },
    {
      "epoch": 19.312401883830454,
      "grad_norm": 3.9830093383789062,
      "learning_rate": 3.792974882260596e-05,
      "loss": 0.5631,
      "step": 1230200
    },
    {
      "epoch": 19.31397174254317,
      "grad_norm": 3.8366544246673584,
      "learning_rate": 3.792876766091052e-05,
      "loss": 0.6053,
      "step": 1230300
    },
    {
      "epoch": 19.315541601255887,
      "grad_norm": 4.902726650238037,
      "learning_rate": 3.792778649921507e-05,
      "loss": 0.6383,
      "step": 1230400
    },
    {
      "epoch": 19.317111459968604,
      "grad_norm": 3.074148416519165,
      "learning_rate": 3.792680533751963e-05,
      "loss": 0.6045,
      "step": 1230500
    },
    {
      "epoch": 19.318681318681318,
      "grad_norm": 3.7957606315612793,
      "learning_rate": 3.792582417582417e-05,
      "loss": 0.5981,
      "step": 1230600
    },
    {
      "epoch": 19.320251177394034,
      "grad_norm": 4.239269256591797,
      "learning_rate": 3.792484301412873e-05,
      "loss": 0.6158,
      "step": 1230700
    },
    {
      "epoch": 19.32182103610675,
      "grad_norm": 3.4017117023468018,
      "learning_rate": 3.792386185243328e-05,
      "loss": 0.6133,
      "step": 1230800
    },
    {
      "epoch": 19.323390894819465,
      "grad_norm": 3.5912415981292725,
      "learning_rate": 3.792288069073783e-05,
      "loss": 0.5863,
      "step": 1230900
    },
    {
      "epoch": 19.32496075353218,
      "grad_norm": 4.171750068664551,
      "learning_rate": 3.792189952904239e-05,
      "loss": 0.604,
      "step": 1231000
    },
    {
      "epoch": 19.3265306122449,
      "grad_norm": 2.467003107070923,
      "learning_rate": 3.792091836734694e-05,
      "loss": 0.6382,
      "step": 1231100
    },
    {
      "epoch": 19.328100470957615,
      "grad_norm": 3.0635342597961426,
      "learning_rate": 3.791993720565149e-05,
      "loss": 0.5781,
      "step": 1231200
    },
    {
      "epoch": 19.32967032967033,
      "grad_norm": 3.806260347366333,
      "learning_rate": 3.791895604395604e-05,
      "loss": 0.6156,
      "step": 1231300
    },
    {
      "epoch": 19.331240188383045,
      "grad_norm": 3.5104963779449463,
      "learning_rate": 3.79179748822606e-05,
      "loss": 0.5903,
      "step": 1231400
    },
    {
      "epoch": 19.332810047095762,
      "grad_norm": 3.8132588863372803,
      "learning_rate": 3.791699372056515e-05,
      "loss": 0.6534,
      "step": 1231500
    },
    {
      "epoch": 19.334379905808476,
      "grad_norm": 3.4990036487579346,
      "learning_rate": 3.79160125588697e-05,
      "loss": 0.6008,
      "step": 1231600
    },
    {
      "epoch": 19.335949764521192,
      "grad_norm": 3.216437578201294,
      "learning_rate": 3.7915031397174254e-05,
      "loss": 0.5884,
      "step": 1231700
    },
    {
      "epoch": 19.33751962323391,
      "grad_norm": 3.561652660369873,
      "learning_rate": 3.791405023547881e-05,
      "loss": 0.5693,
      "step": 1231800
    },
    {
      "epoch": 19.339089481946626,
      "grad_norm": 4.017709732055664,
      "learning_rate": 3.791306907378336e-05,
      "loss": 0.6351,
      "step": 1231900
    },
    {
      "epoch": 19.34065934065934,
      "grad_norm": 4.9262309074401855,
      "learning_rate": 3.7912087912087914e-05,
      "loss": 0.6127,
      "step": 1232000
    },
    {
      "epoch": 19.342229199372056,
      "grad_norm": 4.344097137451172,
      "learning_rate": 3.7911106750392465e-05,
      "loss": 0.5821,
      "step": 1232100
    },
    {
      "epoch": 19.343799058084773,
      "grad_norm": 4.213165283203125,
      "learning_rate": 3.791012558869702e-05,
      "loss": 0.594,
      "step": 1232200
    },
    {
      "epoch": 19.345368916797486,
      "grad_norm": 2.196035861968994,
      "learning_rate": 3.790914442700157e-05,
      "loss": 0.5738,
      "step": 1232300
    },
    {
      "epoch": 19.346938775510203,
      "grad_norm": 2.2907042503356934,
      "learning_rate": 3.7908163265306124e-05,
      "loss": 0.5722,
      "step": 1232400
    },
    {
      "epoch": 19.34850863422292,
      "grad_norm": 3.8193275928497314,
      "learning_rate": 3.7907182103610675e-05,
      "loss": 0.5947,
      "step": 1232500
    },
    {
      "epoch": 19.350078492935637,
      "grad_norm": 3.3623645305633545,
      "learning_rate": 3.790620094191523e-05,
      "loss": 0.6069,
      "step": 1232600
    },
    {
      "epoch": 19.35164835164835,
      "grad_norm": 3.2210919857025146,
      "learning_rate": 3.790521978021978e-05,
      "loss": 0.621,
      "step": 1232700
    },
    {
      "epoch": 19.353218210361067,
      "grad_norm": 4.163195610046387,
      "learning_rate": 3.7904238618524335e-05,
      "loss": 0.6195,
      "step": 1232800
    },
    {
      "epoch": 19.354788069073784,
      "grad_norm": 4.618981838226318,
      "learning_rate": 3.7903257456828886e-05,
      "loss": 0.5852,
      "step": 1232900
    },
    {
      "epoch": 19.356357927786497,
      "grad_norm": 3.921110153198242,
      "learning_rate": 3.790227629513344e-05,
      "loss": 0.5992,
      "step": 1233000
    },
    {
      "epoch": 19.357927786499214,
      "grad_norm": 2.4561500549316406,
      "learning_rate": 3.7901295133437995e-05,
      "loss": 0.613,
      "step": 1233100
    },
    {
      "epoch": 19.35949764521193,
      "grad_norm": 3.7287871837615967,
      "learning_rate": 3.7900313971742546e-05,
      "loss": 0.5806,
      "step": 1233200
    },
    {
      "epoch": 19.361067503924648,
      "grad_norm": 2.5593109130859375,
      "learning_rate": 3.78993328100471e-05,
      "loss": 0.5843,
      "step": 1233300
    },
    {
      "epoch": 19.36263736263736,
      "grad_norm": 3.769711971282959,
      "learning_rate": 3.789835164835165e-05,
      "loss": 0.6186,
      "step": 1233400
    },
    {
      "epoch": 19.364207221350078,
      "grad_norm": 4.24257230758667,
      "learning_rate": 3.7897370486656206e-05,
      "loss": 0.6205,
      "step": 1233500
    },
    {
      "epoch": 19.365777080062795,
      "grad_norm": 3.928088426589966,
      "learning_rate": 3.7896389324960756e-05,
      "loss": 0.6337,
      "step": 1233600
    },
    {
      "epoch": 19.367346938775512,
      "grad_norm": 4.121380805969238,
      "learning_rate": 3.789540816326531e-05,
      "loss": 0.5682,
      "step": 1233700
    },
    {
      "epoch": 19.368916797488225,
      "grad_norm": 4.116261959075928,
      "learning_rate": 3.789442700156986e-05,
      "loss": 0.5994,
      "step": 1233800
    },
    {
      "epoch": 19.370486656200942,
      "grad_norm": 3.15362548828125,
      "learning_rate": 3.7893445839874416e-05,
      "loss": 0.6481,
      "step": 1233900
    },
    {
      "epoch": 19.37205651491366,
      "grad_norm": 3.0727317333221436,
      "learning_rate": 3.789246467817897e-05,
      "loss": 0.6263,
      "step": 1234000
    },
    {
      "epoch": 19.373626373626372,
      "grad_norm": 4.043079853057861,
      "learning_rate": 3.789148351648352e-05,
      "loss": 0.6327,
      "step": 1234100
    },
    {
      "epoch": 19.37519623233909,
      "grad_norm": 3.9759511947631836,
      "learning_rate": 3.789050235478807e-05,
      "loss": 0.6105,
      "step": 1234200
    },
    {
      "epoch": 19.376766091051806,
      "grad_norm": 3.617946147918701,
      "learning_rate": 3.788952119309263e-05,
      "loss": 0.6182,
      "step": 1234300
    },
    {
      "epoch": 19.378335949764523,
      "grad_norm": 2.1440987586975098,
      "learning_rate": 3.788854003139717e-05,
      "loss": 0.5883,
      "step": 1234400
    },
    {
      "epoch": 19.379905808477236,
      "grad_norm": 3.2269248962402344,
      "learning_rate": 3.788755886970173e-05,
      "loss": 0.6346,
      "step": 1234500
    },
    {
      "epoch": 19.381475667189953,
      "grad_norm": 3.3108811378479004,
      "learning_rate": 3.788657770800628e-05,
      "loss": 0.6528,
      "step": 1234600
    },
    {
      "epoch": 19.38304552590267,
      "grad_norm": 4.058504104614258,
      "learning_rate": 3.788559654631084e-05,
      "loss": 0.608,
      "step": 1234700
    },
    {
      "epoch": 19.384615384615383,
      "grad_norm": 3.9877734184265137,
      "learning_rate": 3.788461538461538e-05,
      "loss": 0.6289,
      "step": 1234800
    },
    {
      "epoch": 19.3861852433281,
      "grad_norm": 3.269638776779175,
      "learning_rate": 3.788363422291994e-05,
      "loss": 0.6439,
      "step": 1234900
    },
    {
      "epoch": 19.387755102040817,
      "grad_norm": 4.256027698516846,
      "learning_rate": 3.788265306122449e-05,
      "loss": 0.6553,
      "step": 1235000
    },
    {
      "epoch": 19.389324960753534,
      "grad_norm": 3.41068959236145,
      "learning_rate": 3.788167189952904e-05,
      "loss": 0.5676,
      "step": 1235100
    },
    {
      "epoch": 19.390894819466247,
      "grad_norm": 5.170997619628906,
      "learning_rate": 3.78806907378336e-05,
      "loss": 0.5996,
      "step": 1235200
    },
    {
      "epoch": 19.392464678178964,
      "grad_norm": 4.40210485458374,
      "learning_rate": 3.787970957613815e-05,
      "loss": 0.6024,
      "step": 1235300
    },
    {
      "epoch": 19.39403453689168,
      "grad_norm": 4.00464391708374,
      "learning_rate": 3.78787284144427e-05,
      "loss": 0.5722,
      "step": 1235400
    },
    {
      "epoch": 19.395604395604394,
      "grad_norm": 3.729552984237671,
      "learning_rate": 3.787774725274725e-05,
      "loss": 0.6178,
      "step": 1235500
    },
    {
      "epoch": 19.39717425431711,
      "grad_norm": 3.645358085632324,
      "learning_rate": 3.787676609105181e-05,
      "loss": 0.628,
      "step": 1235600
    },
    {
      "epoch": 19.398744113029828,
      "grad_norm": 3.898489475250244,
      "learning_rate": 3.787578492935636e-05,
      "loss": 0.5994,
      "step": 1235700
    },
    {
      "epoch": 19.400313971742545,
      "grad_norm": 2.6959903240203857,
      "learning_rate": 3.787480376766091e-05,
      "loss": 0.603,
      "step": 1235800
    },
    {
      "epoch": 19.401883830455258,
      "grad_norm": 4.2203192710876465,
      "learning_rate": 3.787382260596546e-05,
      "loss": 0.5985,
      "step": 1235900
    },
    {
      "epoch": 19.403453689167975,
      "grad_norm": 3.737190008163452,
      "learning_rate": 3.787284144427002e-05,
      "loss": 0.6382,
      "step": 1236000
    },
    {
      "epoch": 19.405023547880692,
      "grad_norm": 2.925055742263794,
      "learning_rate": 3.787186028257457e-05,
      "loss": 0.6152,
      "step": 1236100
    },
    {
      "epoch": 19.406593406593405,
      "grad_norm": 3.378774404525757,
      "learning_rate": 3.787087912087912e-05,
      "loss": 0.6129,
      "step": 1236200
    },
    {
      "epoch": 19.408163265306122,
      "grad_norm": 4.0456318855285645,
      "learning_rate": 3.7869897959183674e-05,
      "loss": 0.6054,
      "step": 1236300
    },
    {
      "epoch": 19.40973312401884,
      "grad_norm": 3.3617467880249023,
      "learning_rate": 3.786891679748823e-05,
      "loss": 0.6198,
      "step": 1236400
    },
    {
      "epoch": 19.411302982731556,
      "grad_norm": 4.092105388641357,
      "learning_rate": 3.7867935635792776e-05,
      "loss": 0.6104,
      "step": 1236500
    },
    {
      "epoch": 19.41287284144427,
      "grad_norm": 4.206581115722656,
      "learning_rate": 3.786695447409733e-05,
      "loss": 0.6007,
      "step": 1236600
    },
    {
      "epoch": 19.414442700156986,
      "grad_norm": 5.002748489379883,
      "learning_rate": 3.7865973312401884e-05,
      "loss": 0.625,
      "step": 1236700
    },
    {
      "epoch": 19.416012558869703,
      "grad_norm": 2.7774124145507812,
      "learning_rate": 3.786499215070644e-05,
      "loss": 0.5689,
      "step": 1236800
    },
    {
      "epoch": 19.417582417582416,
      "grad_norm": 4.263518810272217,
      "learning_rate": 3.7864010989010986e-05,
      "loss": 0.6212,
      "step": 1236900
    },
    {
      "epoch": 19.419152276295133,
      "grad_norm": 3.651789903640747,
      "learning_rate": 3.7863029827315544e-05,
      "loss": 0.6244,
      "step": 1237000
    },
    {
      "epoch": 19.42072213500785,
      "grad_norm": 3.8946733474731445,
      "learning_rate": 3.7862048665620095e-05,
      "loss": 0.661,
      "step": 1237100
    },
    {
      "epoch": 19.422291993720567,
      "grad_norm": 3.977278709411621,
      "learning_rate": 3.7861067503924646e-05,
      "loss": 0.6498,
      "step": 1237200
    },
    {
      "epoch": 19.42386185243328,
      "grad_norm": 4.430781364440918,
      "learning_rate": 3.7860086342229204e-05,
      "loss": 0.6143,
      "step": 1237300
    },
    {
      "epoch": 19.425431711145997,
      "grad_norm": 3.575444221496582,
      "learning_rate": 3.7859105180533755e-05,
      "loss": 0.6172,
      "step": 1237400
    },
    {
      "epoch": 19.427001569858714,
      "grad_norm": 3.844418525695801,
      "learning_rate": 3.7858124018838306e-05,
      "loss": 0.5797,
      "step": 1237500
    },
    {
      "epoch": 19.428571428571427,
      "grad_norm": 4.480759143829346,
      "learning_rate": 3.785714285714286e-05,
      "loss": 0.5828,
      "step": 1237600
    },
    {
      "epoch": 19.430141287284144,
      "grad_norm": 5.707581996917725,
      "learning_rate": 3.7856161695447414e-05,
      "loss": 0.5989,
      "step": 1237700
    },
    {
      "epoch": 19.43171114599686,
      "grad_norm": 4.30674934387207,
      "learning_rate": 3.7855180533751965e-05,
      "loss": 0.6317,
      "step": 1237800
    },
    {
      "epoch": 19.433281004709578,
      "grad_norm": 4.004364490509033,
      "learning_rate": 3.7854199372056516e-05,
      "loss": 0.6117,
      "step": 1237900
    },
    {
      "epoch": 19.43485086342229,
      "grad_norm": 4.043080806732178,
      "learning_rate": 3.785321821036107e-05,
      "loss": 0.6023,
      "step": 1238000
    },
    {
      "epoch": 19.436420722135008,
      "grad_norm": 4.380982875823975,
      "learning_rate": 3.7852237048665625e-05,
      "loss": 0.5635,
      "step": 1238100
    },
    {
      "epoch": 19.437990580847725,
      "grad_norm": 4.365274906158447,
      "learning_rate": 3.7851255886970176e-05,
      "loss": 0.6215,
      "step": 1238200
    },
    {
      "epoch": 19.439560439560438,
      "grad_norm": 4.669280529022217,
      "learning_rate": 3.785027472527473e-05,
      "loss": 0.5589,
      "step": 1238300
    },
    {
      "epoch": 19.441130298273155,
      "grad_norm": 3.928140878677368,
      "learning_rate": 3.784929356357928e-05,
      "loss": 0.602,
      "step": 1238400
    },
    {
      "epoch": 19.44270015698587,
      "grad_norm": 4.118873119354248,
      "learning_rate": 3.7848312401883836e-05,
      "loss": 0.582,
      "step": 1238500
    },
    {
      "epoch": 19.44427001569859,
      "grad_norm": 4.095106601715088,
      "learning_rate": 3.784733124018838e-05,
      "loss": 0.633,
      "step": 1238600
    },
    {
      "epoch": 19.445839874411302,
      "grad_norm": 2.0946567058563232,
      "learning_rate": 3.784635007849294e-05,
      "loss": 0.5817,
      "step": 1238700
    },
    {
      "epoch": 19.44740973312402,
      "grad_norm": 3.6562328338623047,
      "learning_rate": 3.784536891679749e-05,
      "loss": 0.6027,
      "step": 1238800
    },
    {
      "epoch": 19.448979591836736,
      "grad_norm": 3.9893860816955566,
      "learning_rate": 3.7844387755102047e-05,
      "loss": 0.5641,
      "step": 1238900
    },
    {
      "epoch": 19.45054945054945,
      "grad_norm": 4.6070356369018555,
      "learning_rate": 3.784340659340659e-05,
      "loss": 0.6014,
      "step": 1239000
    },
    {
      "epoch": 19.452119309262166,
      "grad_norm": 4.113406181335449,
      "learning_rate": 3.784242543171115e-05,
      "loss": 0.5911,
      "step": 1239100
    },
    {
      "epoch": 19.453689167974883,
      "grad_norm": 3.304034471511841,
      "learning_rate": 3.78414442700157e-05,
      "loss": 0.6207,
      "step": 1239200
    },
    {
      "epoch": 19.4552590266876,
      "grad_norm": 3.162297010421753,
      "learning_rate": 3.784046310832025e-05,
      "loss": 0.5762,
      "step": 1239300
    },
    {
      "epoch": 19.456828885400313,
      "grad_norm": 2.9775702953338623,
      "learning_rate": 3.783948194662481e-05,
      "loss": 0.612,
      "step": 1239400
    },
    {
      "epoch": 19.45839874411303,
      "grad_norm": 3.288109302520752,
      "learning_rate": 3.783850078492936e-05,
      "loss": 0.592,
      "step": 1239500
    },
    {
      "epoch": 19.459968602825747,
      "grad_norm": 4.329830646514893,
      "learning_rate": 3.783751962323391e-05,
      "loss": 0.573,
      "step": 1239600
    },
    {
      "epoch": 19.46153846153846,
      "grad_norm": 3.587038516998291,
      "learning_rate": 3.783653846153846e-05,
      "loss": 0.6074,
      "step": 1239700
    },
    {
      "epoch": 19.463108320251177,
      "grad_norm": 4.027094841003418,
      "learning_rate": 3.783555729984302e-05,
      "loss": 0.6212,
      "step": 1239800
    },
    {
      "epoch": 19.464678178963894,
      "grad_norm": 3.4092938899993896,
      "learning_rate": 3.783457613814757e-05,
      "loss": 0.6007,
      "step": 1239900
    },
    {
      "epoch": 19.46624803767661,
      "grad_norm": 4.089128494262695,
      "learning_rate": 3.783359497645212e-05,
      "loss": 0.6258,
      "step": 1240000
    },
    {
      "epoch": 19.467817896389324,
      "grad_norm": 4.735284805297852,
      "learning_rate": 3.783261381475667e-05,
      "loss": 0.5947,
      "step": 1240100
    },
    {
      "epoch": 19.46938775510204,
      "grad_norm": 4.5655131340026855,
      "learning_rate": 3.783163265306123e-05,
      "loss": 0.5929,
      "step": 1240200
    },
    {
      "epoch": 19.470957613814758,
      "grad_norm": 4.429511547088623,
      "learning_rate": 3.7830651491365774e-05,
      "loss": 0.6229,
      "step": 1240300
    },
    {
      "epoch": 19.47252747252747,
      "grad_norm": 3.5680716037750244,
      "learning_rate": 3.782967032967033e-05,
      "loss": 0.6501,
      "step": 1240400
    },
    {
      "epoch": 19.474097331240188,
      "grad_norm": 3.138237237930298,
      "learning_rate": 3.782868916797488e-05,
      "loss": 0.5951,
      "step": 1240500
    },
    {
      "epoch": 19.475667189952905,
      "grad_norm": 4.58920955657959,
      "learning_rate": 3.782770800627944e-05,
      "loss": 0.5895,
      "step": 1240600
    },
    {
      "epoch": 19.47723704866562,
      "grad_norm": 3.9101037979125977,
      "learning_rate": 3.7826726844583985e-05,
      "loss": 0.5958,
      "step": 1240700
    },
    {
      "epoch": 19.478806907378335,
      "grad_norm": 2.6363513469696045,
      "learning_rate": 3.782574568288854e-05,
      "loss": 0.6121,
      "step": 1240800
    },
    {
      "epoch": 19.48037676609105,
      "grad_norm": 3.7637364864349365,
      "learning_rate": 3.782476452119309e-05,
      "loss": 0.605,
      "step": 1240900
    },
    {
      "epoch": 19.48194662480377,
      "grad_norm": 3.496304512023926,
      "learning_rate": 3.7823783359497644e-05,
      "loss": 0.6099,
      "step": 1241000
    },
    {
      "epoch": 19.483516483516482,
      "grad_norm": 3.7598307132720947,
      "learning_rate": 3.7822802197802195e-05,
      "loss": 0.6648,
      "step": 1241100
    },
    {
      "epoch": 19.4850863422292,
      "grad_norm": 3.569281816482544,
      "learning_rate": 3.782182103610675e-05,
      "loss": 0.626,
      "step": 1241200
    },
    {
      "epoch": 19.486656200941916,
      "grad_norm": 3.2365362644195557,
      "learning_rate": 3.7820839874411304e-05,
      "loss": 0.6137,
      "step": 1241300
    },
    {
      "epoch": 19.488226059654632,
      "grad_norm": 2.8222873210906982,
      "learning_rate": 3.7819858712715855e-05,
      "loss": 0.5719,
      "step": 1241400
    },
    {
      "epoch": 19.489795918367346,
      "grad_norm": 3.5362987518310547,
      "learning_rate": 3.781887755102041e-05,
      "loss": 0.6512,
      "step": 1241500
    },
    {
      "epoch": 19.491365777080063,
      "grad_norm": 4.21359920501709,
      "learning_rate": 3.7817896389324964e-05,
      "loss": 0.6238,
      "step": 1241600
    },
    {
      "epoch": 19.49293563579278,
      "grad_norm": 2.88032603263855,
      "learning_rate": 3.7816915227629515e-05,
      "loss": 0.6005,
      "step": 1241700
    },
    {
      "epoch": 19.494505494505496,
      "grad_norm": 3.875814437866211,
      "learning_rate": 3.7815934065934066e-05,
      "loss": 0.5961,
      "step": 1241800
    },
    {
      "epoch": 19.49607535321821,
      "grad_norm": 3.454192876815796,
      "learning_rate": 3.7814952904238623e-05,
      "loss": 0.6012,
      "step": 1241900
    },
    {
      "epoch": 19.497645211930926,
      "grad_norm": 4.606579780578613,
      "learning_rate": 3.7813971742543174e-05,
      "loss": 0.6189,
      "step": 1242000
    },
    {
      "epoch": 19.499215070643643,
      "grad_norm": 3.94915771484375,
      "learning_rate": 3.7812990580847725e-05,
      "loss": 0.6153,
      "step": 1242100
    },
    {
      "epoch": 19.500784929356357,
      "grad_norm": 4.103848457336426,
      "learning_rate": 3.7812009419152276e-05,
      "loss": 0.5965,
      "step": 1242200
    },
    {
      "epoch": 19.502354788069074,
      "grad_norm": 4.567671775817871,
      "learning_rate": 3.7811028257456834e-05,
      "loss": 0.639,
      "step": 1242300
    },
    {
      "epoch": 19.50392464678179,
      "grad_norm": 4.394737243652344,
      "learning_rate": 3.781004709576138e-05,
      "loss": 0.6313,
      "step": 1242400
    },
    {
      "epoch": 19.505494505494504,
      "grad_norm": 3.799509048461914,
      "learning_rate": 3.7809065934065936e-05,
      "loss": 0.5718,
      "step": 1242500
    },
    {
      "epoch": 19.50706436420722,
      "grad_norm": 4.1381940841674805,
      "learning_rate": 3.780808477237049e-05,
      "loss": 0.6564,
      "step": 1242600
    },
    {
      "epoch": 19.508634222919937,
      "grad_norm": 3.129405975341797,
      "learning_rate": 3.7807103610675045e-05,
      "loss": 0.6093,
      "step": 1242700
    },
    {
      "epoch": 19.510204081632654,
      "grad_norm": 4.384207725524902,
      "learning_rate": 3.780612244897959e-05,
      "loss": 0.6054,
      "step": 1242800
    },
    {
      "epoch": 19.511773940345368,
      "grad_norm": 3.4992921352386475,
      "learning_rate": 3.780514128728415e-05,
      "loss": 0.6403,
      "step": 1242900
    },
    {
      "epoch": 19.513343799058084,
      "grad_norm": 2.3002138137817383,
      "learning_rate": 3.78041601255887e-05,
      "loss": 0.6223,
      "step": 1243000
    },
    {
      "epoch": 19.5149136577708,
      "grad_norm": 4.2361674308776855,
      "learning_rate": 3.780317896389325e-05,
      "loss": 0.5482,
      "step": 1243100
    },
    {
      "epoch": 19.516483516483518,
      "grad_norm": 3.4529621601104736,
      "learning_rate": 3.78021978021978e-05,
      "loss": 0.6379,
      "step": 1243200
    },
    {
      "epoch": 19.51805337519623,
      "grad_norm": 3.110010862350464,
      "learning_rate": 3.780121664050236e-05,
      "loss": 0.5842,
      "step": 1243300
    },
    {
      "epoch": 19.51962323390895,
      "grad_norm": 3.6470746994018555,
      "learning_rate": 3.780023547880691e-05,
      "loss": 0.6356,
      "step": 1243400
    },
    {
      "epoch": 19.521193092621665,
      "grad_norm": 4.184436321258545,
      "learning_rate": 3.779925431711146e-05,
      "loss": 0.5854,
      "step": 1243500
    },
    {
      "epoch": 19.52276295133438,
      "grad_norm": 3.5425913333892822,
      "learning_rate": 3.779827315541602e-05,
      "loss": 0.6154,
      "step": 1243600
    },
    {
      "epoch": 19.524332810047095,
      "grad_norm": 3.4783499240875244,
      "learning_rate": 3.779729199372057e-05,
      "loss": 0.5738,
      "step": 1243700
    },
    {
      "epoch": 19.525902668759812,
      "grad_norm": 3.9088027477264404,
      "learning_rate": 3.779631083202512e-05,
      "loss": 0.6181,
      "step": 1243800
    },
    {
      "epoch": 19.52747252747253,
      "grad_norm": 4.062182426452637,
      "learning_rate": 3.779532967032967e-05,
      "loss": 0.5892,
      "step": 1243900
    },
    {
      "epoch": 19.529042386185242,
      "grad_norm": 3.7662267684936523,
      "learning_rate": 3.779434850863423e-05,
      "loss": 0.6265,
      "step": 1244000
    },
    {
      "epoch": 19.53061224489796,
      "grad_norm": 3.929823875427246,
      "learning_rate": 3.779336734693878e-05,
      "loss": 0.6284,
      "step": 1244100
    },
    {
      "epoch": 19.532182103610676,
      "grad_norm": 4.120014190673828,
      "learning_rate": 3.779238618524333e-05,
      "loss": 0.5775,
      "step": 1244200
    },
    {
      "epoch": 19.53375196232339,
      "grad_norm": 4.522655487060547,
      "learning_rate": 3.779140502354788e-05,
      "loss": 0.5975,
      "step": 1244300
    },
    {
      "epoch": 19.535321821036106,
      "grad_norm": 3.903296709060669,
      "learning_rate": 3.779042386185244e-05,
      "loss": 0.6634,
      "step": 1244400
    },
    {
      "epoch": 19.536891679748823,
      "grad_norm": 4.114072799682617,
      "learning_rate": 3.778944270015698e-05,
      "loss": 0.6085,
      "step": 1244500
    },
    {
      "epoch": 19.53846153846154,
      "grad_norm": 4.682892799377441,
      "learning_rate": 3.778846153846154e-05,
      "loss": 0.609,
      "step": 1244600
    },
    {
      "epoch": 19.540031397174253,
      "grad_norm": 4.241145610809326,
      "learning_rate": 3.778748037676609e-05,
      "loss": 0.6567,
      "step": 1244700
    },
    {
      "epoch": 19.54160125588697,
      "grad_norm": 4.834805965423584,
      "learning_rate": 3.778649921507065e-05,
      "loss": 0.6041,
      "step": 1244800
    },
    {
      "epoch": 19.543171114599687,
      "grad_norm": 4.458837032318115,
      "learning_rate": 3.7785518053375194e-05,
      "loss": 0.5849,
      "step": 1244900
    },
    {
      "epoch": 19.5447409733124,
      "grad_norm": 4.580751419067383,
      "learning_rate": 3.778453689167975e-05,
      "loss": 0.6728,
      "step": 1245000
    },
    {
      "epoch": 19.546310832025117,
      "grad_norm": 3.994166612625122,
      "learning_rate": 3.77835557299843e-05,
      "loss": 0.6448,
      "step": 1245100
    },
    {
      "epoch": 19.547880690737834,
      "grad_norm": 4.545910835266113,
      "learning_rate": 3.778257456828885e-05,
      "loss": 0.5945,
      "step": 1245200
    },
    {
      "epoch": 19.54945054945055,
      "grad_norm": 5.417375564575195,
      "learning_rate": 3.7781593406593404e-05,
      "loss": 0.6027,
      "step": 1245300
    },
    {
      "epoch": 19.551020408163264,
      "grad_norm": 3.860109567642212,
      "learning_rate": 3.778061224489796e-05,
      "loss": 0.6016,
      "step": 1245400
    },
    {
      "epoch": 19.55259026687598,
      "grad_norm": 3.7582461833953857,
      "learning_rate": 3.777963108320251e-05,
      "loss": 0.6133,
      "step": 1245500
    },
    {
      "epoch": 19.554160125588698,
      "grad_norm": 3.980271816253662,
      "learning_rate": 3.7778649921507064e-05,
      "loss": 0.5854,
      "step": 1245600
    },
    {
      "epoch": 19.55572998430141,
      "grad_norm": 3.7627079486846924,
      "learning_rate": 3.777766875981162e-05,
      "loss": 0.586,
      "step": 1245700
    },
    {
      "epoch": 19.55729984301413,
      "grad_norm": 3.131864309310913,
      "learning_rate": 3.777668759811617e-05,
      "loss": 0.6411,
      "step": 1245800
    },
    {
      "epoch": 19.558869701726845,
      "grad_norm": 4.139559268951416,
      "learning_rate": 3.7775706436420724e-05,
      "loss": 0.6208,
      "step": 1245900
    },
    {
      "epoch": 19.560439560439562,
      "grad_norm": 3.655341386795044,
      "learning_rate": 3.7774725274725275e-05,
      "loss": 0.5874,
      "step": 1246000
    },
    {
      "epoch": 19.562009419152275,
      "grad_norm": 4.328346252441406,
      "learning_rate": 3.777374411302983e-05,
      "loss": 0.5989,
      "step": 1246100
    },
    {
      "epoch": 19.563579277864992,
      "grad_norm": 4.3511576652526855,
      "learning_rate": 3.7772762951334383e-05,
      "loss": 0.6054,
      "step": 1246200
    },
    {
      "epoch": 19.56514913657771,
      "grad_norm": 3.9295198917388916,
      "learning_rate": 3.7771781789638934e-05,
      "loss": 0.6201,
      "step": 1246300
    },
    {
      "epoch": 19.566718995290422,
      "grad_norm": 4.2296295166015625,
      "learning_rate": 3.7770800627943485e-05,
      "loss": 0.6029,
      "step": 1246400
    },
    {
      "epoch": 19.56828885400314,
      "grad_norm": 2.842538833618164,
      "learning_rate": 3.776981946624804e-05,
      "loss": 0.5822,
      "step": 1246500
    },
    {
      "epoch": 19.569858712715856,
      "grad_norm": 2.880342960357666,
      "learning_rate": 3.776883830455259e-05,
      "loss": 0.6019,
      "step": 1246600
    },
    {
      "epoch": 19.571428571428573,
      "grad_norm": 3.578911542892456,
      "learning_rate": 3.7767857142857145e-05,
      "loss": 0.59,
      "step": 1246700
    },
    {
      "epoch": 19.572998430141286,
      "grad_norm": 4.103226661682129,
      "learning_rate": 3.7766875981161696e-05,
      "loss": 0.595,
      "step": 1246800
    },
    {
      "epoch": 19.574568288854003,
      "grad_norm": 3.7295844554901123,
      "learning_rate": 3.7765894819466254e-05,
      "loss": 0.5941,
      "step": 1246900
    },
    {
      "epoch": 19.57613814756672,
      "grad_norm": 2.8086631298065186,
      "learning_rate": 3.77649136577708e-05,
      "loss": 0.6188,
      "step": 1247000
    },
    {
      "epoch": 19.577708006279433,
      "grad_norm": 4.398367881774902,
      "learning_rate": 3.7763932496075356e-05,
      "loss": 0.607,
      "step": 1247100
    },
    {
      "epoch": 19.57927786499215,
      "grad_norm": 3.8225414752960205,
      "learning_rate": 3.776295133437991e-05,
      "loss": 0.5748,
      "step": 1247200
    },
    {
      "epoch": 19.580847723704867,
      "grad_norm": 3.4491302967071533,
      "learning_rate": 3.776197017268446e-05,
      "loss": 0.6336,
      "step": 1247300
    },
    {
      "epoch": 19.582417582417584,
      "grad_norm": 5.0241007804870605,
      "learning_rate": 3.776098901098901e-05,
      "loss": 0.5993,
      "step": 1247400
    },
    {
      "epoch": 19.583987441130297,
      "grad_norm": 4.066188812255859,
      "learning_rate": 3.7760007849293567e-05,
      "loss": 0.6,
      "step": 1247500
    },
    {
      "epoch": 19.585557299843014,
      "grad_norm": 5.035603046417236,
      "learning_rate": 3.775902668759812e-05,
      "loss": 0.6005,
      "step": 1247600
    },
    {
      "epoch": 19.58712715855573,
      "grad_norm": 3.6352715492248535,
      "learning_rate": 3.775804552590267e-05,
      "loss": 0.6284,
      "step": 1247700
    },
    {
      "epoch": 19.588697017268444,
      "grad_norm": 2.7718682289123535,
      "learning_rate": 3.7757064364207226e-05,
      "loss": 0.5871,
      "step": 1247800
    },
    {
      "epoch": 19.59026687598116,
      "grad_norm": 3.9078612327575684,
      "learning_rate": 3.775608320251178e-05,
      "loss": 0.563,
      "step": 1247900
    },
    {
      "epoch": 19.591836734693878,
      "grad_norm": 3.2777321338653564,
      "learning_rate": 3.775510204081633e-05,
      "loss": 0.5724,
      "step": 1248000
    },
    {
      "epoch": 19.593406593406595,
      "grad_norm": 4.004796981811523,
      "learning_rate": 3.775412087912088e-05,
      "loss": 0.5883,
      "step": 1248100
    },
    {
      "epoch": 19.594976452119308,
      "grad_norm": 3.6086909770965576,
      "learning_rate": 3.775313971742544e-05,
      "loss": 0.6044,
      "step": 1248200
    },
    {
      "epoch": 19.596546310832025,
      "grad_norm": 3.3970112800598145,
      "learning_rate": 3.775215855572999e-05,
      "loss": 0.649,
      "step": 1248300
    },
    {
      "epoch": 19.598116169544742,
      "grad_norm": 3.747594118118286,
      "learning_rate": 3.775117739403454e-05,
      "loss": 0.6234,
      "step": 1248400
    },
    {
      "epoch": 19.599686028257455,
      "grad_norm": 3.7586441040039062,
      "learning_rate": 3.775019623233909e-05,
      "loss": 0.5969,
      "step": 1248500
    },
    {
      "epoch": 19.601255886970172,
      "grad_norm": 2.7531025409698486,
      "learning_rate": 3.774921507064365e-05,
      "loss": 0.5772,
      "step": 1248600
    },
    {
      "epoch": 19.60282574568289,
      "grad_norm": 4.482022285461426,
      "learning_rate": 3.774823390894819e-05,
      "loss": 0.6228,
      "step": 1248700
    },
    {
      "epoch": 19.604395604395606,
      "grad_norm": 4.212533950805664,
      "learning_rate": 3.774725274725275e-05,
      "loss": 0.5996,
      "step": 1248800
    },
    {
      "epoch": 19.60596546310832,
      "grad_norm": 4.105931758880615,
      "learning_rate": 3.77462715855573e-05,
      "loss": 0.6493,
      "step": 1248900
    },
    {
      "epoch": 19.607535321821036,
      "grad_norm": 3.5654687881469727,
      "learning_rate": 3.774529042386186e-05,
      "loss": 0.615,
      "step": 1249000
    },
    {
      "epoch": 19.609105180533753,
      "grad_norm": 4.309971332550049,
      "learning_rate": 3.77443092621664e-05,
      "loss": 0.6395,
      "step": 1249100
    },
    {
      "epoch": 19.610675039246466,
      "grad_norm": 3.861987590789795,
      "learning_rate": 3.774332810047096e-05,
      "loss": 0.6145,
      "step": 1249200
    },
    {
      "epoch": 19.612244897959183,
      "grad_norm": 4.490279197692871,
      "learning_rate": 3.774234693877551e-05,
      "loss": 0.6557,
      "step": 1249300
    },
    {
      "epoch": 19.6138147566719,
      "grad_norm": 3.5130157470703125,
      "learning_rate": 3.774136577708006e-05,
      "loss": 0.6231,
      "step": 1249400
    },
    {
      "epoch": 19.615384615384617,
      "grad_norm": 3.9274351596832275,
      "learning_rate": 3.774038461538461e-05,
      "loss": 0.6155,
      "step": 1249500
    },
    {
      "epoch": 19.61695447409733,
      "grad_norm": 4.280693531036377,
      "learning_rate": 3.773940345368917e-05,
      "loss": 0.6126,
      "step": 1249600
    },
    {
      "epoch": 19.618524332810047,
      "grad_norm": 4.8886332511901855,
      "learning_rate": 3.773842229199372e-05,
      "loss": 0.6135,
      "step": 1249700
    },
    {
      "epoch": 19.620094191522764,
      "grad_norm": 3.8067092895507812,
      "learning_rate": 3.773744113029827e-05,
      "loss": 0.6083,
      "step": 1249800
    },
    {
      "epoch": 19.621664050235477,
      "grad_norm": 3.187922716140747,
      "learning_rate": 3.773645996860283e-05,
      "loss": 0.5964,
      "step": 1249900
    },
    {
      "epoch": 19.623233908948194,
      "grad_norm": 4.379863262176514,
      "learning_rate": 3.773547880690738e-05,
      "loss": 0.5896,
      "step": 1250000
    },
    {
      "epoch": 19.62480376766091,
      "grad_norm": 3.6145009994506836,
      "learning_rate": 3.773449764521193e-05,
      "loss": 0.5767,
      "step": 1250100
    },
    {
      "epoch": 19.626373626373628,
      "grad_norm": 3.2912561893463135,
      "learning_rate": 3.7733516483516484e-05,
      "loss": 0.6045,
      "step": 1250200
    },
    {
      "epoch": 19.62794348508634,
      "grad_norm": 4.1984052658081055,
      "learning_rate": 3.773253532182104e-05,
      "loss": 0.5753,
      "step": 1250300
    },
    {
      "epoch": 19.629513343799058,
      "grad_norm": 4.103270530700684,
      "learning_rate": 3.773155416012559e-05,
      "loss": 0.6095,
      "step": 1250400
    },
    {
      "epoch": 19.631083202511775,
      "grad_norm": 4.416474342346191,
      "learning_rate": 3.7730572998430143e-05,
      "loss": 0.6016,
      "step": 1250500
    },
    {
      "epoch": 19.632653061224488,
      "grad_norm": 3.5337119102478027,
      "learning_rate": 3.7729591836734694e-05,
      "loss": 0.5941,
      "step": 1250600
    },
    {
      "epoch": 19.634222919937205,
      "grad_norm": 3.7036750316619873,
      "learning_rate": 3.772861067503925e-05,
      "loss": 0.6097,
      "step": 1250700
    },
    {
      "epoch": 19.635792778649922,
      "grad_norm": 4.463865756988525,
      "learning_rate": 3.7727629513343796e-05,
      "loss": 0.6226,
      "step": 1250800
    },
    {
      "epoch": 19.63736263736264,
      "grad_norm": 4.255849838256836,
      "learning_rate": 3.7726648351648354e-05,
      "loss": 0.6515,
      "step": 1250900
    },
    {
      "epoch": 19.638932496075352,
      "grad_norm": 4.020117282867432,
      "learning_rate": 3.7725667189952905e-05,
      "loss": 0.615,
      "step": 1251000
    },
    {
      "epoch": 19.64050235478807,
      "grad_norm": 3.8624303340911865,
      "learning_rate": 3.772468602825746e-05,
      "loss": 0.6342,
      "step": 1251100
    },
    {
      "epoch": 19.642072213500786,
      "grad_norm": 3.03859281539917,
      "learning_rate": 3.772370486656201e-05,
      "loss": 0.6097,
      "step": 1251200
    },
    {
      "epoch": 19.643642072213503,
      "grad_norm": 3.6741812229156494,
      "learning_rate": 3.7722723704866565e-05,
      "loss": 0.5891,
      "step": 1251300
    },
    {
      "epoch": 19.645211930926216,
      "grad_norm": 3.3819267749786377,
      "learning_rate": 3.7721742543171116e-05,
      "loss": 0.6149,
      "step": 1251400
    },
    {
      "epoch": 19.646781789638933,
      "grad_norm": 4.359961986541748,
      "learning_rate": 3.772076138147567e-05,
      "loss": 0.6224,
      "step": 1251500
    },
    {
      "epoch": 19.64835164835165,
      "grad_norm": 3.4431841373443604,
      "learning_rate": 3.771978021978022e-05,
      "loss": 0.5935,
      "step": 1251600
    },
    {
      "epoch": 19.649921507064363,
      "grad_norm": 3.6340603828430176,
      "learning_rate": 3.7718799058084775e-05,
      "loss": 0.6015,
      "step": 1251700
    },
    {
      "epoch": 19.65149136577708,
      "grad_norm": 4.215424060821533,
      "learning_rate": 3.7717817896389326e-05,
      "loss": 0.5954,
      "step": 1251800
    },
    {
      "epoch": 19.653061224489797,
      "grad_norm": 3.4300475120544434,
      "learning_rate": 3.771683673469388e-05,
      "loss": 0.5749,
      "step": 1251900
    },
    {
      "epoch": 19.65463108320251,
      "grad_norm": 4.140792369842529,
      "learning_rate": 3.7715855572998435e-05,
      "loss": 0.6629,
      "step": 1252000
    },
    {
      "epoch": 19.656200941915227,
      "grad_norm": 3.652395725250244,
      "learning_rate": 3.7714874411302986e-05,
      "loss": 0.6714,
      "step": 1252100
    },
    {
      "epoch": 19.657770800627944,
      "grad_norm": 3.7748289108276367,
      "learning_rate": 3.771389324960754e-05,
      "loss": 0.5687,
      "step": 1252200
    },
    {
      "epoch": 19.65934065934066,
      "grad_norm": 4.886989593505859,
      "learning_rate": 3.771291208791209e-05,
      "loss": 0.6237,
      "step": 1252300
    },
    {
      "epoch": 19.660910518053374,
      "grad_norm": 3.636019229888916,
      "learning_rate": 3.7711930926216646e-05,
      "loss": 0.6206,
      "step": 1252400
    },
    {
      "epoch": 19.66248037676609,
      "grad_norm": 3.326043128967285,
      "learning_rate": 3.77109497645212e-05,
      "loss": 0.5563,
      "step": 1252500
    },
    {
      "epoch": 19.664050235478808,
      "grad_norm": 3.6460211277008057,
      "learning_rate": 3.770996860282575e-05,
      "loss": 0.6498,
      "step": 1252600
    },
    {
      "epoch": 19.665620094191524,
      "grad_norm": 4.747972011566162,
      "learning_rate": 3.77089874411303e-05,
      "loss": 0.5734,
      "step": 1252700
    },
    {
      "epoch": 19.667189952904238,
      "grad_norm": 5.0744218826293945,
      "learning_rate": 3.770800627943486e-05,
      "loss": 0.6124,
      "step": 1252800
    },
    {
      "epoch": 19.668759811616955,
      "grad_norm": 3.808058023452759,
      "learning_rate": 3.77070251177394e-05,
      "loss": 0.5866,
      "step": 1252900
    },
    {
      "epoch": 19.67032967032967,
      "grad_norm": 2.7705795764923096,
      "learning_rate": 3.770604395604396e-05,
      "loss": 0.5828,
      "step": 1253000
    },
    {
      "epoch": 19.671899529042385,
      "grad_norm": 4.244904518127441,
      "learning_rate": 3.770506279434851e-05,
      "loss": 0.6081,
      "step": 1253100
    },
    {
      "epoch": 19.6734693877551,
      "grad_norm": 3.764324903488159,
      "learning_rate": 3.770408163265307e-05,
      "loss": 0.6051,
      "step": 1253200
    },
    {
      "epoch": 19.67503924646782,
      "grad_norm": 3.1627097129821777,
      "learning_rate": 3.770310047095761e-05,
      "loss": 0.5994,
      "step": 1253300
    },
    {
      "epoch": 19.676609105180535,
      "grad_norm": 3.8430488109588623,
      "learning_rate": 3.770211930926217e-05,
      "loss": 0.6168,
      "step": 1253400
    },
    {
      "epoch": 19.67817896389325,
      "grad_norm": 3.5795106887817383,
      "learning_rate": 3.770113814756672e-05,
      "loss": 0.5948,
      "step": 1253500
    },
    {
      "epoch": 19.679748822605966,
      "grad_norm": 3.3754756450653076,
      "learning_rate": 3.770015698587127e-05,
      "loss": 0.5684,
      "step": 1253600
    },
    {
      "epoch": 19.681318681318682,
      "grad_norm": 4.265766143798828,
      "learning_rate": 3.769917582417582e-05,
      "loss": 0.6207,
      "step": 1253700
    },
    {
      "epoch": 19.682888540031396,
      "grad_norm": 3.361858606338501,
      "learning_rate": 3.769819466248038e-05,
      "loss": 0.6087,
      "step": 1253800
    },
    {
      "epoch": 19.684458398744113,
      "grad_norm": 2.852982521057129,
      "learning_rate": 3.769721350078493e-05,
      "loss": 0.6098,
      "step": 1253900
    },
    {
      "epoch": 19.68602825745683,
      "grad_norm": 3.539278984069824,
      "learning_rate": 3.769623233908948e-05,
      "loss": 0.6338,
      "step": 1254000
    },
    {
      "epoch": 19.687598116169546,
      "grad_norm": 3.3594608306884766,
      "learning_rate": 3.769525117739404e-05,
      "loss": 0.5885,
      "step": 1254100
    },
    {
      "epoch": 19.68916797488226,
      "grad_norm": 2.663738250732422,
      "learning_rate": 3.769427001569859e-05,
      "loss": 0.6054,
      "step": 1254200
    },
    {
      "epoch": 19.690737833594977,
      "grad_norm": 2.9758620262145996,
      "learning_rate": 3.769328885400314e-05,
      "loss": 0.628,
      "step": 1254300
    },
    {
      "epoch": 19.692307692307693,
      "grad_norm": 2.924776792526245,
      "learning_rate": 3.769230769230769e-05,
      "loss": 0.6371,
      "step": 1254400
    },
    {
      "epoch": 19.693877551020407,
      "grad_norm": 4.478027820587158,
      "learning_rate": 3.769132653061225e-05,
      "loss": 0.6145,
      "step": 1254500
    },
    {
      "epoch": 19.695447409733124,
      "grad_norm": 3.940234899520874,
      "learning_rate": 3.76903453689168e-05,
      "loss": 0.603,
      "step": 1254600
    },
    {
      "epoch": 19.69701726844584,
      "grad_norm": 5.345085144042969,
      "learning_rate": 3.768936420722135e-05,
      "loss": 0.6053,
      "step": 1254700
    },
    {
      "epoch": 19.698587127158557,
      "grad_norm": 4.1377058029174805,
      "learning_rate": 3.76883830455259e-05,
      "loss": 0.6265,
      "step": 1254800
    },
    {
      "epoch": 19.70015698587127,
      "grad_norm": 3.9338858127593994,
      "learning_rate": 3.768740188383046e-05,
      "loss": 0.6038,
      "step": 1254900
    },
    {
      "epoch": 19.701726844583987,
      "grad_norm": 3.67567777633667,
      "learning_rate": 3.7686420722135005e-05,
      "loss": 0.6429,
      "step": 1255000
    },
    {
      "epoch": 19.703296703296704,
      "grad_norm": 4.208931922912598,
      "learning_rate": 3.768543956043956e-05,
      "loss": 0.6268,
      "step": 1255100
    },
    {
      "epoch": 19.704866562009418,
      "grad_norm": 3.3209781646728516,
      "learning_rate": 3.7684458398744114e-05,
      "loss": 0.608,
      "step": 1255200
    },
    {
      "epoch": 19.706436420722135,
      "grad_norm": 2.895115375518799,
      "learning_rate": 3.768347723704867e-05,
      "loss": 0.6165,
      "step": 1255300
    },
    {
      "epoch": 19.70800627943485,
      "grad_norm": 3.289346218109131,
      "learning_rate": 3.7682496075353216e-05,
      "loss": 0.6154,
      "step": 1255400
    },
    {
      "epoch": 19.70957613814757,
      "grad_norm": 4.187231540679932,
      "learning_rate": 3.7681514913657774e-05,
      "loss": 0.6031,
      "step": 1255500
    },
    {
      "epoch": 19.71114599686028,
      "grad_norm": 3.7702348232269287,
      "learning_rate": 3.7680533751962325e-05,
      "loss": 0.5994,
      "step": 1255600
    },
    {
      "epoch": 19.712715855573,
      "grad_norm": 3.779576063156128,
      "learning_rate": 3.7679552590266876e-05,
      "loss": 0.5844,
      "step": 1255700
    },
    {
      "epoch": 19.714285714285715,
      "grad_norm": 3.101609706878662,
      "learning_rate": 3.767857142857143e-05,
      "loss": 0.5708,
      "step": 1255800
    },
    {
      "epoch": 19.71585557299843,
      "grad_norm": 3.336221694946289,
      "learning_rate": 3.7677590266875984e-05,
      "loss": 0.6321,
      "step": 1255900
    },
    {
      "epoch": 19.717425431711145,
      "grad_norm": 2.863351345062256,
      "learning_rate": 3.7676609105180535e-05,
      "loss": 0.6441,
      "step": 1256000
    },
    {
      "epoch": 19.718995290423862,
      "grad_norm": 3.9784085750579834,
      "learning_rate": 3.7675627943485086e-05,
      "loss": 0.6024,
      "step": 1256100
    },
    {
      "epoch": 19.72056514913658,
      "grad_norm": 4.667926788330078,
      "learning_rate": 3.7674646781789644e-05,
      "loss": 0.603,
      "step": 1256200
    },
    {
      "epoch": 19.722135007849293,
      "grad_norm": 3.4588210582733154,
      "learning_rate": 3.7673665620094195e-05,
      "loss": 0.6016,
      "step": 1256300
    },
    {
      "epoch": 19.72370486656201,
      "grad_norm": 3.9110279083251953,
      "learning_rate": 3.7672684458398746e-05,
      "loss": 0.5847,
      "step": 1256400
    },
    {
      "epoch": 19.725274725274726,
      "grad_norm": 2.3824691772460938,
      "learning_rate": 3.76717032967033e-05,
      "loss": 0.6162,
      "step": 1256500
    },
    {
      "epoch": 19.72684458398744,
      "grad_norm": 3.543970823287964,
      "learning_rate": 3.7670722135007855e-05,
      "loss": 0.6235,
      "step": 1256600
    },
    {
      "epoch": 19.728414442700156,
      "grad_norm": 4.289394378662109,
      "learning_rate": 3.7669740973312406e-05,
      "loss": 0.6198,
      "step": 1256700
    },
    {
      "epoch": 19.729984301412873,
      "grad_norm": 3.355788230895996,
      "learning_rate": 3.766875981161696e-05,
      "loss": 0.6305,
      "step": 1256800
    },
    {
      "epoch": 19.73155416012559,
      "grad_norm": 3.9137163162231445,
      "learning_rate": 3.766777864992151e-05,
      "loss": 0.5924,
      "step": 1256900
    },
    {
      "epoch": 19.733124018838303,
      "grad_norm": 4.020567893981934,
      "learning_rate": 3.7666797488226066e-05,
      "loss": 0.6227,
      "step": 1257000
    },
    {
      "epoch": 19.73469387755102,
      "grad_norm": 3.7554094791412354,
      "learning_rate": 3.766581632653061e-05,
      "loss": 0.5976,
      "step": 1257100
    },
    {
      "epoch": 19.736263736263737,
      "grad_norm": 4.200350761413574,
      "learning_rate": 3.766483516483517e-05,
      "loss": 0.6495,
      "step": 1257200
    },
    {
      "epoch": 19.73783359497645,
      "grad_norm": 3.7901511192321777,
      "learning_rate": 3.766385400313972e-05,
      "loss": 0.6033,
      "step": 1257300
    },
    {
      "epoch": 19.739403453689167,
      "grad_norm": 3.053682565689087,
      "learning_rate": 3.7662872841444276e-05,
      "loss": 0.5719,
      "step": 1257400
    },
    {
      "epoch": 19.740973312401884,
      "grad_norm": 3.2901246547698975,
      "learning_rate": 3.766189167974882e-05,
      "loss": 0.5991,
      "step": 1257500
    },
    {
      "epoch": 19.7425431711146,
      "grad_norm": 3.942282199859619,
      "learning_rate": 3.766091051805338e-05,
      "loss": 0.595,
      "step": 1257600
    },
    {
      "epoch": 19.744113029827314,
      "grad_norm": 3.665764570236206,
      "learning_rate": 3.765992935635793e-05,
      "loss": 0.5763,
      "step": 1257700
    },
    {
      "epoch": 19.74568288854003,
      "grad_norm": 3.7219278812408447,
      "learning_rate": 3.765894819466248e-05,
      "loss": 0.5877,
      "step": 1257800
    },
    {
      "epoch": 19.747252747252748,
      "grad_norm": 2.992788314819336,
      "learning_rate": 3.765796703296703e-05,
      "loss": 0.593,
      "step": 1257900
    },
    {
      "epoch": 19.74882260596546,
      "grad_norm": 4.483489990234375,
      "learning_rate": 3.765698587127159e-05,
      "loss": 0.5874,
      "step": 1258000
    },
    {
      "epoch": 19.75039246467818,
      "grad_norm": 4.193185329437256,
      "learning_rate": 3.765600470957614e-05,
      "loss": 0.5819,
      "step": 1258100
    },
    {
      "epoch": 19.751962323390895,
      "grad_norm": 3.8068315982818604,
      "learning_rate": 3.765502354788069e-05,
      "loss": 0.5837,
      "step": 1258200
    },
    {
      "epoch": 19.753532182103612,
      "grad_norm": 4.266019821166992,
      "learning_rate": 3.765404238618525e-05,
      "loss": 0.6488,
      "step": 1258300
    },
    {
      "epoch": 19.755102040816325,
      "grad_norm": 3.836120367050171,
      "learning_rate": 3.76530612244898e-05,
      "loss": 0.6038,
      "step": 1258400
    },
    {
      "epoch": 19.756671899529042,
      "grad_norm": 3.9259395599365234,
      "learning_rate": 3.765208006279435e-05,
      "loss": 0.5906,
      "step": 1258500
    },
    {
      "epoch": 19.75824175824176,
      "grad_norm": 2.1229166984558105,
      "learning_rate": 3.76510989010989e-05,
      "loss": 0.6103,
      "step": 1258600
    },
    {
      "epoch": 19.759811616954472,
      "grad_norm": 3.0351722240448,
      "learning_rate": 3.765011773940346e-05,
      "loss": 0.61,
      "step": 1258700
    },
    {
      "epoch": 19.76138147566719,
      "grad_norm": 4.329221725463867,
      "learning_rate": 3.764913657770801e-05,
      "loss": 0.5972,
      "step": 1258800
    },
    {
      "epoch": 19.762951334379906,
      "grad_norm": 3.4156103134155273,
      "learning_rate": 3.764815541601256e-05,
      "loss": 0.5909,
      "step": 1258900
    },
    {
      "epoch": 19.764521193092623,
      "grad_norm": 3.6077115535736084,
      "learning_rate": 3.764717425431711e-05,
      "loss": 0.586,
      "step": 1259000
    },
    {
      "epoch": 19.766091051805336,
      "grad_norm": 4.696043014526367,
      "learning_rate": 3.764619309262167e-05,
      "loss": 0.6348,
      "step": 1259100
    },
    {
      "epoch": 19.767660910518053,
      "grad_norm": 4.95132303237915,
      "learning_rate": 3.7645211930926214e-05,
      "loss": 0.6234,
      "step": 1259200
    },
    {
      "epoch": 19.76923076923077,
      "grad_norm": 4.963318824768066,
      "learning_rate": 3.764423076923077e-05,
      "loss": 0.6323,
      "step": 1259300
    },
    {
      "epoch": 19.770800627943487,
      "grad_norm": 4.133034706115723,
      "learning_rate": 3.764324960753532e-05,
      "loss": 0.6276,
      "step": 1259400
    },
    {
      "epoch": 19.7723704866562,
      "grad_norm": 4.491880893707275,
      "learning_rate": 3.764226844583988e-05,
      "loss": 0.5443,
      "step": 1259500
    },
    {
      "epoch": 19.773940345368917,
      "grad_norm": 4.2267537117004395,
      "learning_rate": 3.7641287284144425e-05,
      "loss": 0.6232,
      "step": 1259600
    },
    {
      "epoch": 19.775510204081634,
      "grad_norm": 3.1321322917938232,
      "learning_rate": 3.764030612244898e-05,
      "loss": 0.6114,
      "step": 1259700
    },
    {
      "epoch": 19.777080062794347,
      "grad_norm": 3.9422693252563477,
      "learning_rate": 3.7639324960753534e-05,
      "loss": 0.5922,
      "step": 1259800
    },
    {
      "epoch": 19.778649921507064,
      "grad_norm": 3.827561855316162,
      "learning_rate": 3.7638343799058085e-05,
      "loss": 0.626,
      "step": 1259900
    },
    {
      "epoch": 19.78021978021978,
      "grad_norm": 3.0332024097442627,
      "learning_rate": 3.7637362637362636e-05,
      "loss": 0.6059,
      "step": 1260000
    },
    {
      "epoch": 19.781789638932494,
      "grad_norm": 3.304325819015503,
      "learning_rate": 3.7636381475667193e-05,
      "loss": 0.622,
      "step": 1260100
    },
    {
      "epoch": 19.78335949764521,
      "grad_norm": 3.208953857421875,
      "learning_rate": 3.7635400313971744e-05,
      "loss": 0.6156,
      "step": 1260200
    },
    {
      "epoch": 19.784929356357928,
      "grad_norm": 4.019304275512695,
      "learning_rate": 3.7634419152276295e-05,
      "loss": 0.6014,
      "step": 1260300
    },
    {
      "epoch": 19.786499215070645,
      "grad_norm": 3.4196078777313232,
      "learning_rate": 3.763343799058085e-05,
      "loss": 0.5996,
      "step": 1260400
    },
    {
      "epoch": 19.788069073783358,
      "grad_norm": 3.7457454204559326,
      "learning_rate": 3.7632456828885404e-05,
      "loss": 0.6021,
      "step": 1260500
    },
    {
      "epoch": 19.789638932496075,
      "grad_norm": 4.048464775085449,
      "learning_rate": 3.7631475667189955e-05,
      "loss": 0.6101,
      "step": 1260600
    },
    {
      "epoch": 19.791208791208792,
      "grad_norm": 4.3037896156311035,
      "learning_rate": 3.7630494505494506e-05,
      "loss": 0.6613,
      "step": 1260700
    },
    {
      "epoch": 19.79277864992151,
      "grad_norm": 4.76769495010376,
      "learning_rate": 3.7629513343799064e-05,
      "loss": 0.5933,
      "step": 1260800
    },
    {
      "epoch": 19.794348508634222,
      "grad_norm": 3.9182887077331543,
      "learning_rate": 3.7628532182103615e-05,
      "loss": 0.6176,
      "step": 1260900
    },
    {
      "epoch": 19.79591836734694,
      "grad_norm": 3.793628454208374,
      "learning_rate": 3.7627551020408166e-05,
      "loss": 0.6144,
      "step": 1261000
    },
    {
      "epoch": 19.797488226059656,
      "grad_norm": 4.406522750854492,
      "learning_rate": 3.762656985871272e-05,
      "loss": 0.5919,
      "step": 1261100
    },
    {
      "epoch": 19.79905808477237,
      "grad_norm": 3.951892614364624,
      "learning_rate": 3.7625588697017275e-05,
      "loss": 0.6036,
      "step": 1261200
    },
    {
      "epoch": 19.800627943485086,
      "grad_norm": 3.6962873935699463,
      "learning_rate": 3.762460753532182e-05,
      "loss": 0.6022,
      "step": 1261300
    },
    {
      "epoch": 19.802197802197803,
      "grad_norm": 3.7875380516052246,
      "learning_rate": 3.7623626373626377e-05,
      "loss": 0.5815,
      "step": 1261400
    },
    {
      "epoch": 19.80376766091052,
      "grad_norm": 4.13423490524292,
      "learning_rate": 3.762264521193093e-05,
      "loss": 0.5893,
      "step": 1261500
    },
    {
      "epoch": 19.805337519623233,
      "grad_norm": 4.360344409942627,
      "learning_rate": 3.7621664050235485e-05,
      "loss": 0.6374,
      "step": 1261600
    },
    {
      "epoch": 19.80690737833595,
      "grad_norm": 3.2889394760131836,
      "learning_rate": 3.762068288854003e-05,
      "loss": 0.5791,
      "step": 1261700
    },
    {
      "epoch": 19.808477237048667,
      "grad_norm": 3.560800552368164,
      "learning_rate": 3.761970172684459e-05,
      "loss": 0.5953,
      "step": 1261800
    },
    {
      "epoch": 19.81004709576138,
      "grad_norm": 5.94007682800293,
      "learning_rate": 3.761872056514914e-05,
      "loss": 0.6011,
      "step": 1261900
    },
    {
      "epoch": 19.811616954474097,
      "grad_norm": 4.298278331756592,
      "learning_rate": 3.761773940345369e-05,
      "loss": 0.6051,
      "step": 1262000
    },
    {
      "epoch": 19.813186813186814,
      "grad_norm": 2.706328868865967,
      "learning_rate": 3.761675824175824e-05,
      "loss": 0.6197,
      "step": 1262100
    },
    {
      "epoch": 19.81475667189953,
      "grad_norm": 3.8602454662323,
      "learning_rate": 3.76157770800628e-05,
      "loss": 0.5997,
      "step": 1262200
    },
    {
      "epoch": 19.816326530612244,
      "grad_norm": 2.7611618041992188,
      "learning_rate": 3.761479591836735e-05,
      "loss": 0.6278,
      "step": 1262300
    },
    {
      "epoch": 19.81789638932496,
      "grad_norm": 4.541514873504639,
      "learning_rate": 3.76138147566719e-05,
      "loss": 0.6404,
      "step": 1262400
    },
    {
      "epoch": 19.819466248037678,
      "grad_norm": 3.3811194896698,
      "learning_rate": 3.761283359497646e-05,
      "loss": 0.6093,
      "step": 1262500
    },
    {
      "epoch": 19.82103610675039,
      "grad_norm": 3.2283010482788086,
      "learning_rate": 3.761185243328101e-05,
      "loss": 0.6689,
      "step": 1262600
    },
    {
      "epoch": 19.822605965463108,
      "grad_norm": 4.312839508056641,
      "learning_rate": 3.761087127158556e-05,
      "loss": 0.6268,
      "step": 1262700
    },
    {
      "epoch": 19.824175824175825,
      "grad_norm": 3.900965452194214,
      "learning_rate": 3.760989010989011e-05,
      "loss": 0.6013,
      "step": 1262800
    },
    {
      "epoch": 19.82574568288854,
      "grad_norm": 3.583846092224121,
      "learning_rate": 3.760890894819467e-05,
      "loss": 0.6077,
      "step": 1262900
    },
    {
      "epoch": 19.827315541601255,
      "grad_norm": 4.027797698974609,
      "learning_rate": 3.760792778649921e-05,
      "loss": 0.6386,
      "step": 1263000
    },
    {
      "epoch": 19.828885400313972,
      "grad_norm": 2.4638280868530273,
      "learning_rate": 3.760694662480377e-05,
      "loss": 0.5763,
      "step": 1263100
    },
    {
      "epoch": 19.83045525902669,
      "grad_norm": 3.4995241165161133,
      "learning_rate": 3.760596546310832e-05,
      "loss": 0.6143,
      "step": 1263200
    },
    {
      "epoch": 19.832025117739402,
      "grad_norm": 4.424698829650879,
      "learning_rate": 3.760498430141288e-05,
      "loss": 0.6073,
      "step": 1263300
    },
    {
      "epoch": 19.83359497645212,
      "grad_norm": 3.0727429389953613,
      "learning_rate": 3.760400313971742e-05,
      "loss": 0.5923,
      "step": 1263400
    },
    {
      "epoch": 19.835164835164836,
      "grad_norm": 3.4902124404907227,
      "learning_rate": 3.760302197802198e-05,
      "loss": 0.6013,
      "step": 1263500
    },
    {
      "epoch": 19.836734693877553,
      "grad_norm": 2.983173370361328,
      "learning_rate": 3.760204081632653e-05,
      "loss": 0.6051,
      "step": 1263600
    },
    {
      "epoch": 19.838304552590266,
      "grad_norm": 3.9643611907958984,
      "learning_rate": 3.760105965463108e-05,
      "loss": 0.6418,
      "step": 1263700
    },
    {
      "epoch": 19.839874411302983,
      "grad_norm": 4.229036331176758,
      "learning_rate": 3.7600078492935634e-05,
      "loss": 0.5783,
      "step": 1263800
    },
    {
      "epoch": 19.8414442700157,
      "grad_norm": 4.132236957550049,
      "learning_rate": 3.759909733124019e-05,
      "loss": 0.6318,
      "step": 1263900
    },
    {
      "epoch": 19.843014128728413,
      "grad_norm": 4.99806022644043,
      "learning_rate": 3.759811616954474e-05,
      "loss": 0.5977,
      "step": 1264000
    },
    {
      "epoch": 19.84458398744113,
      "grad_norm": 4.115429878234863,
      "learning_rate": 3.7597135007849294e-05,
      "loss": 0.631,
      "step": 1264100
    },
    {
      "epoch": 19.846153846153847,
      "grad_norm": 3.719573497772217,
      "learning_rate": 3.7596153846153845e-05,
      "loss": 0.5504,
      "step": 1264200
    },
    {
      "epoch": 19.847723704866564,
      "grad_norm": 2.867046356201172,
      "learning_rate": 3.75951726844584e-05,
      "loss": 0.6176,
      "step": 1264300
    },
    {
      "epoch": 19.849293563579277,
      "grad_norm": 4.297046661376953,
      "learning_rate": 3.7594191522762953e-05,
      "loss": 0.6129,
      "step": 1264400
    },
    {
      "epoch": 19.850863422291994,
      "grad_norm": 4.457104682922363,
      "learning_rate": 3.7593210361067504e-05,
      "loss": 0.6019,
      "step": 1264500
    },
    {
      "epoch": 19.85243328100471,
      "grad_norm": 3.554004669189453,
      "learning_rate": 3.759222919937206e-05,
      "loss": 0.6128,
      "step": 1264600
    },
    {
      "epoch": 19.854003139717424,
      "grad_norm": 4.115577697753906,
      "learning_rate": 3.759124803767661e-05,
      "loss": 0.6598,
      "step": 1264700
    },
    {
      "epoch": 19.85557299843014,
      "grad_norm": 3.202805757522583,
      "learning_rate": 3.7590266875981164e-05,
      "loss": 0.5792,
      "step": 1264800
    },
    {
      "epoch": 19.857142857142858,
      "grad_norm": 2.9363627433776855,
      "learning_rate": 3.7589285714285715e-05,
      "loss": 0.6216,
      "step": 1264900
    },
    {
      "epoch": 19.858712715855575,
      "grad_norm": 3.7180867195129395,
      "learning_rate": 3.758830455259027e-05,
      "loss": 0.638,
      "step": 1265000
    },
    {
      "epoch": 19.860282574568288,
      "grad_norm": 2.5944759845733643,
      "learning_rate": 3.758732339089482e-05,
      "loss": 0.6422,
      "step": 1265100
    },
    {
      "epoch": 19.861852433281005,
      "grad_norm": 3.7076449394226074,
      "learning_rate": 3.7586342229199375e-05,
      "loss": 0.6067,
      "step": 1265200
    },
    {
      "epoch": 19.86342229199372,
      "grad_norm": 2.7915198802948,
      "learning_rate": 3.7585361067503926e-05,
      "loss": 0.5493,
      "step": 1265300
    },
    {
      "epoch": 19.864992150706435,
      "grad_norm": 3.6199331283569336,
      "learning_rate": 3.7584379905808484e-05,
      "loss": 0.5845,
      "step": 1265400
    },
    {
      "epoch": 19.86656200941915,
      "grad_norm": 4.285961151123047,
      "learning_rate": 3.758339874411303e-05,
      "loss": 0.614,
      "step": 1265500
    },
    {
      "epoch": 19.86813186813187,
      "grad_norm": 4.5192551612854,
      "learning_rate": 3.7582417582417586e-05,
      "loss": 0.5927,
      "step": 1265600
    },
    {
      "epoch": 19.869701726844585,
      "grad_norm": 4.243954181671143,
      "learning_rate": 3.7581436420722137e-05,
      "loss": 0.6077,
      "step": 1265700
    },
    {
      "epoch": 19.8712715855573,
      "grad_norm": 3.6856510639190674,
      "learning_rate": 3.758045525902669e-05,
      "loss": 0.6218,
      "step": 1265800
    },
    {
      "epoch": 19.872841444270016,
      "grad_norm": 3.941913366317749,
      "learning_rate": 3.757947409733124e-05,
      "loss": 0.6172,
      "step": 1265900
    },
    {
      "epoch": 19.874411302982733,
      "grad_norm": 3.8434009552001953,
      "learning_rate": 3.7578492935635796e-05,
      "loss": 0.6265,
      "step": 1266000
    },
    {
      "epoch": 19.875981161695446,
      "grad_norm": 3.2565975189208984,
      "learning_rate": 3.757751177394035e-05,
      "loss": 0.5757,
      "step": 1266100
    },
    {
      "epoch": 19.877551020408163,
      "grad_norm": 3.6343142986297607,
      "learning_rate": 3.75765306122449e-05,
      "loss": 0.6154,
      "step": 1266200
    },
    {
      "epoch": 19.87912087912088,
      "grad_norm": 3.4149131774902344,
      "learning_rate": 3.757554945054945e-05,
      "loss": 0.5888,
      "step": 1266300
    },
    {
      "epoch": 19.880690737833596,
      "grad_norm": 4.21403694152832,
      "learning_rate": 3.757456828885401e-05,
      "loss": 0.622,
      "step": 1266400
    },
    {
      "epoch": 19.88226059654631,
      "grad_norm": 3.1654672622680664,
      "learning_rate": 3.757358712715856e-05,
      "loss": 0.5744,
      "step": 1266500
    },
    {
      "epoch": 19.883830455259027,
      "grad_norm": 3.248333215713501,
      "learning_rate": 3.757260596546311e-05,
      "loss": 0.6006,
      "step": 1266600
    },
    {
      "epoch": 19.885400313971743,
      "grad_norm": 3.5078437328338623,
      "learning_rate": 3.757162480376766e-05,
      "loss": 0.6179,
      "step": 1266700
    },
    {
      "epoch": 19.886970172684457,
      "grad_norm": 3.0083181858062744,
      "learning_rate": 3.757064364207222e-05,
      "loss": 0.6057,
      "step": 1266800
    },
    {
      "epoch": 19.888540031397174,
      "grad_norm": 3.252234697341919,
      "learning_rate": 3.756966248037677e-05,
      "loss": 0.5907,
      "step": 1266900
    },
    {
      "epoch": 19.89010989010989,
      "grad_norm": 2.873612880706787,
      "learning_rate": 3.756868131868132e-05,
      "loss": 0.6187,
      "step": 1267000
    },
    {
      "epoch": 19.891679748822607,
      "grad_norm": 5.06167459487915,
      "learning_rate": 3.756770015698588e-05,
      "loss": 0.5882,
      "step": 1267100
    },
    {
      "epoch": 19.89324960753532,
      "grad_norm": 3.3543641567230225,
      "learning_rate": 3.756671899529042e-05,
      "loss": 0.6062,
      "step": 1267200
    },
    {
      "epoch": 19.894819466248038,
      "grad_norm": 4.93068265914917,
      "learning_rate": 3.756573783359498e-05,
      "loss": 0.6062,
      "step": 1267300
    },
    {
      "epoch": 19.896389324960754,
      "grad_norm": 5.229237079620361,
      "learning_rate": 3.756475667189953e-05,
      "loss": 0.6109,
      "step": 1267400
    },
    {
      "epoch": 19.897959183673468,
      "grad_norm": 3.86619234085083,
      "learning_rate": 3.756377551020409e-05,
      "loss": 0.6202,
      "step": 1267500
    },
    {
      "epoch": 19.899529042386185,
      "grad_norm": 3.734586715698242,
      "learning_rate": 3.756279434850863e-05,
      "loss": 0.567,
      "step": 1267600
    },
    {
      "epoch": 19.9010989010989,
      "grad_norm": 3.349574327468872,
      "learning_rate": 3.756181318681319e-05,
      "loss": 0.5946,
      "step": 1267700
    },
    {
      "epoch": 19.90266875981162,
      "grad_norm": 3.3137145042419434,
      "learning_rate": 3.756083202511774e-05,
      "loss": 0.6215,
      "step": 1267800
    },
    {
      "epoch": 19.90423861852433,
      "grad_norm": 3.3109049797058105,
      "learning_rate": 3.755985086342229e-05,
      "loss": 0.5937,
      "step": 1267900
    },
    {
      "epoch": 19.90580847723705,
      "grad_norm": 3.485894203186035,
      "learning_rate": 3.755886970172684e-05,
      "loss": 0.633,
      "step": 1268000
    },
    {
      "epoch": 19.907378335949765,
      "grad_norm": 4.400091171264648,
      "learning_rate": 3.75578885400314e-05,
      "loss": 0.5823,
      "step": 1268100
    },
    {
      "epoch": 19.90894819466248,
      "grad_norm": 3.5137245655059814,
      "learning_rate": 3.755690737833595e-05,
      "loss": 0.593,
      "step": 1268200
    },
    {
      "epoch": 19.910518053375196,
      "grad_norm": 4.456088542938232,
      "learning_rate": 3.75559262166405e-05,
      "loss": 0.6064,
      "step": 1268300
    },
    {
      "epoch": 19.912087912087912,
      "grad_norm": 3.4849183559417725,
      "learning_rate": 3.7554945054945054e-05,
      "loss": 0.6236,
      "step": 1268400
    },
    {
      "epoch": 19.91365777080063,
      "grad_norm": 3.061920166015625,
      "learning_rate": 3.755396389324961e-05,
      "loss": 0.593,
      "step": 1268500
    },
    {
      "epoch": 19.915227629513343,
      "grad_norm": 4.087400913238525,
      "learning_rate": 3.755298273155416e-05,
      "loss": 0.5985,
      "step": 1268600
    },
    {
      "epoch": 19.91679748822606,
      "grad_norm": 3.5377914905548096,
      "learning_rate": 3.755200156985871e-05,
      "loss": 0.6066,
      "step": 1268700
    },
    {
      "epoch": 19.918367346938776,
      "grad_norm": 2.4096102714538574,
      "learning_rate": 3.7551020408163264e-05,
      "loss": 0.6153,
      "step": 1268800
    },
    {
      "epoch": 19.919937205651493,
      "grad_norm": 3.502262592315674,
      "learning_rate": 3.755003924646782e-05,
      "loss": 0.5937,
      "step": 1268900
    },
    {
      "epoch": 19.921507064364206,
      "grad_norm": 3.4659552574157715,
      "learning_rate": 3.754905808477237e-05,
      "loss": 0.6112,
      "step": 1269000
    },
    {
      "epoch": 19.923076923076923,
      "grad_norm": 3.1517508029937744,
      "learning_rate": 3.7548076923076924e-05,
      "loss": 0.5984,
      "step": 1269100
    },
    {
      "epoch": 19.92464678178964,
      "grad_norm": 3.965763807296753,
      "learning_rate": 3.754709576138148e-05,
      "loss": 0.5893,
      "step": 1269200
    },
    {
      "epoch": 19.926216640502354,
      "grad_norm": 2.892155885696411,
      "learning_rate": 3.7546114599686026e-05,
      "loss": 0.5984,
      "step": 1269300
    },
    {
      "epoch": 19.92778649921507,
      "grad_norm": 4.116424560546875,
      "learning_rate": 3.7545133437990584e-05,
      "loss": 0.6205,
      "step": 1269400
    },
    {
      "epoch": 19.929356357927787,
      "grad_norm": 4.087400913238525,
      "learning_rate": 3.7544152276295135e-05,
      "loss": 0.5514,
      "step": 1269500
    },
    {
      "epoch": 19.9309262166405,
      "grad_norm": 4.235557556152344,
      "learning_rate": 3.754317111459969e-05,
      "loss": 0.6132,
      "step": 1269600
    },
    {
      "epoch": 19.932496075353217,
      "grad_norm": 3.7834393978118896,
      "learning_rate": 3.754218995290424e-05,
      "loss": 0.6109,
      "step": 1269700
    },
    {
      "epoch": 19.934065934065934,
      "grad_norm": 4.588034152984619,
      "learning_rate": 3.7541208791208795e-05,
      "loss": 0.6575,
      "step": 1269800
    },
    {
      "epoch": 19.93563579277865,
      "grad_norm": 4.707427024841309,
      "learning_rate": 3.7540227629513345e-05,
      "loss": 0.6263,
      "step": 1269900
    },
    {
      "epoch": 19.937205651491364,
      "grad_norm": 4.366358280181885,
      "learning_rate": 3.7539246467817896e-05,
      "loss": 0.6031,
      "step": 1270000
    },
    {
      "epoch": 19.93877551020408,
      "grad_norm": 3.6680867671966553,
      "learning_rate": 3.753826530612245e-05,
      "loss": 0.6138,
      "step": 1270100
    },
    {
      "epoch": 19.940345368916798,
      "grad_norm": 4.274901866912842,
      "learning_rate": 3.7537284144427005e-05,
      "loss": 0.5845,
      "step": 1270200
    },
    {
      "epoch": 19.941915227629515,
      "grad_norm": 3.0664851665496826,
      "learning_rate": 3.7536302982731556e-05,
      "loss": 0.606,
      "step": 1270300
    },
    {
      "epoch": 19.94348508634223,
      "grad_norm": 3.7420923709869385,
      "learning_rate": 3.753532182103611e-05,
      "loss": 0.624,
      "step": 1270400
    },
    {
      "epoch": 19.945054945054945,
      "grad_norm": 5.07004451751709,
      "learning_rate": 3.753434065934066e-05,
      "loss": 0.5773,
      "step": 1270500
    },
    {
      "epoch": 19.946624803767662,
      "grad_norm": 3.635195732116699,
      "learning_rate": 3.7533359497645216e-05,
      "loss": 0.5786,
      "step": 1270600
    },
    {
      "epoch": 19.948194662480375,
      "grad_norm": 3.2656142711639404,
      "learning_rate": 3.753237833594977e-05,
      "loss": 0.5934,
      "step": 1270700
    },
    {
      "epoch": 19.949764521193092,
      "grad_norm": 3.877507448196411,
      "learning_rate": 3.753139717425432e-05,
      "loss": 0.6032,
      "step": 1270800
    },
    {
      "epoch": 19.95133437990581,
      "grad_norm": 2.1811273097991943,
      "learning_rate": 3.753041601255887e-05,
      "loss": 0.5992,
      "step": 1270900
    },
    {
      "epoch": 19.952904238618526,
      "grad_norm": 4.303061008453369,
      "learning_rate": 3.752943485086343e-05,
      "loss": 0.6053,
      "step": 1271000
    },
    {
      "epoch": 19.95447409733124,
      "grad_norm": 4.005082607269287,
      "learning_rate": 3.752845368916798e-05,
      "loss": 0.6418,
      "step": 1271100
    },
    {
      "epoch": 19.956043956043956,
      "grad_norm": 3.3862428665161133,
      "learning_rate": 3.752747252747253e-05,
      "loss": 0.653,
      "step": 1271200
    },
    {
      "epoch": 19.957613814756673,
      "grad_norm": 4.032967567443848,
      "learning_rate": 3.7526491365777086e-05,
      "loss": 0.5997,
      "step": 1271300
    },
    {
      "epoch": 19.959183673469386,
      "grad_norm": 3.3666296005249023,
      "learning_rate": 3.752551020408163e-05,
      "loss": 0.6534,
      "step": 1271400
    },
    {
      "epoch": 19.960753532182103,
      "grad_norm": 4.406066417694092,
      "learning_rate": 3.752452904238619e-05,
      "loss": 0.6362,
      "step": 1271500
    },
    {
      "epoch": 19.96232339089482,
      "grad_norm": 3.7993340492248535,
      "learning_rate": 3.752354788069074e-05,
      "loss": 0.629,
      "step": 1271600
    },
    {
      "epoch": 19.963893249607537,
      "grad_norm": 3.4883172512054443,
      "learning_rate": 3.75225667189953e-05,
      "loss": 0.6455,
      "step": 1271700
    },
    {
      "epoch": 19.96546310832025,
      "grad_norm": 3.8332839012145996,
      "learning_rate": 3.752158555729984e-05,
      "loss": 0.6189,
      "step": 1271800
    },
    {
      "epoch": 19.967032967032967,
      "grad_norm": 3.521125316619873,
      "learning_rate": 3.75206043956044e-05,
      "loss": 0.6592,
      "step": 1271900
    },
    {
      "epoch": 19.968602825745684,
      "grad_norm": 2.9044928550720215,
      "learning_rate": 3.751962323390895e-05,
      "loss": 0.6157,
      "step": 1272000
    },
    {
      "epoch": 19.970172684458397,
      "grad_norm": 4.082690238952637,
      "learning_rate": 3.75186420722135e-05,
      "loss": 0.6013,
      "step": 1272100
    },
    {
      "epoch": 19.971742543171114,
      "grad_norm": 3.249924898147583,
      "learning_rate": 3.751766091051805e-05,
      "loss": 0.5963,
      "step": 1272200
    },
    {
      "epoch": 19.97331240188383,
      "grad_norm": 2.972804307937622,
      "learning_rate": 3.751667974882261e-05,
      "loss": 0.5993,
      "step": 1272300
    },
    {
      "epoch": 19.974882260596548,
      "grad_norm": 2.747558116912842,
      "learning_rate": 3.751569858712716e-05,
      "loss": 0.6435,
      "step": 1272400
    },
    {
      "epoch": 19.97645211930926,
      "grad_norm": 3.6803038120269775,
      "learning_rate": 3.751471742543171e-05,
      "loss": 0.5969,
      "step": 1272500
    },
    {
      "epoch": 19.978021978021978,
      "grad_norm": 3.6552793979644775,
      "learning_rate": 3.751373626373626e-05,
      "loss": 0.6167,
      "step": 1272600
    },
    {
      "epoch": 19.979591836734695,
      "grad_norm": 4.049358367919922,
      "learning_rate": 3.751275510204082e-05,
      "loss": 0.5971,
      "step": 1272700
    },
    {
      "epoch": 19.98116169544741,
      "grad_norm": 4.074724197387695,
      "learning_rate": 3.751177394034537e-05,
      "loss": 0.5695,
      "step": 1272800
    },
    {
      "epoch": 19.982731554160125,
      "grad_norm": 4.323297500610352,
      "learning_rate": 3.751079277864992e-05,
      "loss": 0.5714,
      "step": 1272900
    },
    {
      "epoch": 19.984301412872842,
      "grad_norm": 2.9841325283050537,
      "learning_rate": 3.750981161695447e-05,
      "loss": 0.6209,
      "step": 1273000
    },
    {
      "epoch": 19.98587127158556,
      "grad_norm": 5.346075057983398,
      "learning_rate": 3.750883045525903e-05,
      "loss": 0.6158,
      "step": 1273100
    },
    {
      "epoch": 19.987441130298272,
      "grad_norm": 3.1839988231658936,
      "learning_rate": 3.750784929356358e-05,
      "loss": 0.6001,
      "step": 1273200
    },
    {
      "epoch": 19.98901098901099,
      "grad_norm": 2.8759305477142334,
      "learning_rate": 3.750686813186813e-05,
      "loss": 0.5845,
      "step": 1273300
    },
    {
      "epoch": 19.990580847723706,
      "grad_norm": 4.4180426597595215,
      "learning_rate": 3.750588697017269e-05,
      "loss": 0.5842,
      "step": 1273400
    },
    {
      "epoch": 19.99215070643642,
      "grad_norm": 3.5862064361572266,
      "learning_rate": 3.7504905808477235e-05,
      "loss": 0.5815,
      "step": 1273500
    },
    {
      "epoch": 19.993720565149136,
      "grad_norm": 2.9324114322662354,
      "learning_rate": 3.750392464678179e-05,
      "loss": 0.6226,
      "step": 1273600
    },
    {
      "epoch": 19.995290423861853,
      "grad_norm": 3.0636885166168213,
      "learning_rate": 3.7502943485086344e-05,
      "loss": 0.5835,
      "step": 1273700
    },
    {
      "epoch": 19.99686028257457,
      "grad_norm": 4.009927272796631,
      "learning_rate": 3.75019623233909e-05,
      "loss": 0.6456,
      "step": 1273800
    },
    {
      "epoch": 19.998430141287283,
      "grad_norm": 3.265240430831909,
      "learning_rate": 3.7500981161695446e-05,
      "loss": 0.623,
      "step": 1273900
    },
    {
      "epoch": 20.0,
      "grad_norm": 4.771679878234863,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.5881,
      "step": 1274000
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.031293511390686,
      "eval_runtime": 14.8402,
      "eval_samples_per_second": 225.941,
      "eval_steps_per_second": 225.941,
      "step": 1274000
    },
    {
      "epoch": 20.0,
      "eval_loss": 0.4694415032863617,
      "eval_runtime": 281.2972,
      "eval_samples_per_second": 226.451,
      "eval_steps_per_second": 226.451,
      "step": 1274000
    },
    {
      "epoch": 20.001569858712717,
      "grad_norm": 3.336045742034912,
      "learning_rate": 3.7499018838304554e-05,
      "loss": 0.5832,
      "step": 1274100
    },
    {
      "epoch": 20.00313971742543,
      "grad_norm": 3.8592982292175293,
      "learning_rate": 3.7498037676609105e-05,
      "loss": 0.57,
      "step": 1274200
    },
    {
      "epoch": 20.004709576138147,
      "grad_norm": 4.126112937927246,
      "learning_rate": 3.7497056514913656e-05,
      "loss": 0.5471,
      "step": 1274300
    },
    {
      "epoch": 20.006279434850864,
      "grad_norm": 3.005554676055908,
      "learning_rate": 3.7496075353218214e-05,
      "loss": 0.6004,
      "step": 1274400
    },
    {
      "epoch": 20.00784929356358,
      "grad_norm": 4.686141490936279,
      "learning_rate": 3.7495094191522765e-05,
      "loss": 0.6276,
      "step": 1274500
    },
    {
      "epoch": 20.009419152276294,
      "grad_norm": 3.8811299800872803,
      "learning_rate": 3.7494113029827316e-05,
      "loss": 0.5563,
      "step": 1274600
    },
    {
      "epoch": 20.01098901098901,
      "grad_norm": 5.5887837409973145,
      "learning_rate": 3.749313186813187e-05,
      "loss": 0.642,
      "step": 1274700
    },
    {
      "epoch": 20.012558869701728,
      "grad_norm": 4.168117046356201,
      "learning_rate": 3.7492150706436425e-05,
      "loss": 0.6296,
      "step": 1274800
    },
    {
      "epoch": 20.01412872841444,
      "grad_norm": 4.634943008422852,
      "learning_rate": 3.7491169544740976e-05,
      "loss": 0.6108,
      "step": 1274900
    },
    {
      "epoch": 20.015698587127158,
      "grad_norm": 3.693472146987915,
      "learning_rate": 3.749018838304553e-05,
      "loss": 0.5883,
      "step": 1275000
    },
    {
      "epoch": 20.017268445839875,
      "grad_norm": 4.868142604827881,
      "learning_rate": 3.748920722135008e-05,
      "loss": 0.5869,
      "step": 1275100
    },
    {
      "epoch": 20.01883830455259,
      "grad_norm": 3.6360721588134766,
      "learning_rate": 3.7488226059654636e-05,
      "loss": 0.5782,
      "step": 1275200
    },
    {
      "epoch": 20.020408163265305,
      "grad_norm": 3.878234624862671,
      "learning_rate": 3.7487244897959187e-05,
      "loss": 0.5817,
      "step": 1275300
    },
    {
      "epoch": 20.021978021978022,
      "grad_norm": 3.66768479347229,
      "learning_rate": 3.748626373626374e-05,
      "loss": 0.589,
      "step": 1275400
    },
    {
      "epoch": 20.02354788069074,
      "grad_norm": 4.939799785614014,
      "learning_rate": 3.7485282574568295e-05,
      "loss": 0.6204,
      "step": 1275500
    },
    {
      "epoch": 20.025117739403452,
      "grad_norm": 3.5004172325134277,
      "learning_rate": 3.748430141287284e-05,
      "loss": 0.5848,
      "step": 1275600
    },
    {
      "epoch": 20.02668759811617,
      "grad_norm": 4.297558307647705,
      "learning_rate": 3.74833202511774e-05,
      "loss": 0.628,
      "step": 1275700
    },
    {
      "epoch": 20.028257456828886,
      "grad_norm": 4.125213146209717,
      "learning_rate": 3.748233908948195e-05,
      "loss": 0.6201,
      "step": 1275800
    },
    {
      "epoch": 20.029827315541603,
      "grad_norm": 3.6761045455932617,
      "learning_rate": 3.7481357927786506e-05,
      "loss": 0.5711,
      "step": 1275900
    },
    {
      "epoch": 20.031397174254316,
      "grad_norm": 3.4190614223480225,
      "learning_rate": 3.748037676609105e-05,
      "loss": 0.6,
      "step": 1276000
    },
    {
      "epoch": 20.032967032967033,
      "grad_norm": 2.8317718505859375,
      "learning_rate": 3.747939560439561e-05,
      "loss": 0.6422,
      "step": 1276100
    },
    {
      "epoch": 20.03453689167975,
      "grad_norm": 3.937479019165039,
      "learning_rate": 3.747841444270016e-05,
      "loss": 0.6168,
      "step": 1276200
    },
    {
      "epoch": 20.036106750392463,
      "grad_norm": 3.7396419048309326,
      "learning_rate": 3.747743328100471e-05,
      "loss": 0.6379,
      "step": 1276300
    },
    {
      "epoch": 20.03767660910518,
      "grad_norm": 3.947664976119995,
      "learning_rate": 3.747645211930926e-05,
      "loss": 0.5763,
      "step": 1276400
    },
    {
      "epoch": 20.039246467817897,
      "grad_norm": 3.8470475673675537,
      "learning_rate": 3.747547095761382e-05,
      "loss": 0.5983,
      "step": 1276500
    },
    {
      "epoch": 20.040816326530614,
      "grad_norm": 5.162896156311035,
      "learning_rate": 3.747448979591837e-05,
      "loss": 0.5964,
      "step": 1276600
    },
    {
      "epoch": 20.042386185243327,
      "grad_norm": 4.103103160858154,
      "learning_rate": 3.747350863422292e-05,
      "loss": 0.6252,
      "step": 1276700
    },
    {
      "epoch": 20.043956043956044,
      "grad_norm": 3.236642599105835,
      "learning_rate": 3.747252747252747e-05,
      "loss": 0.5837,
      "step": 1276800
    },
    {
      "epoch": 20.04552590266876,
      "grad_norm": 2.554596424102783,
      "learning_rate": 3.747154631083203e-05,
      "loss": 0.5962,
      "step": 1276900
    },
    {
      "epoch": 20.047095761381474,
      "grad_norm": 5.180960655212402,
      "learning_rate": 3.747056514913658e-05,
      "loss": 0.6256,
      "step": 1277000
    },
    {
      "epoch": 20.04866562009419,
      "grad_norm": 3.7896459102630615,
      "learning_rate": 3.746958398744113e-05,
      "loss": 0.589,
      "step": 1277100
    },
    {
      "epoch": 20.050235478806908,
      "grad_norm": 4.423722267150879,
      "learning_rate": 3.746860282574568e-05,
      "loss": 0.5919,
      "step": 1277200
    },
    {
      "epoch": 20.051805337519625,
      "grad_norm": 1.6886249780654907,
      "learning_rate": 3.746762166405024e-05,
      "loss": 0.5882,
      "step": 1277300
    },
    {
      "epoch": 20.053375196232338,
      "grad_norm": 3.6610357761383057,
      "learning_rate": 3.746664050235479e-05,
      "loss": 0.5839,
      "step": 1277400
    },
    {
      "epoch": 20.054945054945055,
      "grad_norm": 4.001188278198242,
      "learning_rate": 3.746565934065934e-05,
      "loss": 0.5972,
      "step": 1277500
    },
    {
      "epoch": 20.05651491365777,
      "grad_norm": 4.808066368103027,
      "learning_rate": 3.74646781789639e-05,
      "loss": 0.6288,
      "step": 1277600
    },
    {
      "epoch": 20.058084772370485,
      "grad_norm": 4.246776103973389,
      "learning_rate": 3.7463697017268444e-05,
      "loss": 0.5994,
      "step": 1277700
    },
    {
      "epoch": 20.059654631083202,
      "grad_norm": 4.606417655944824,
      "learning_rate": 3.7462715855573e-05,
      "loss": 0.6033,
      "step": 1277800
    },
    {
      "epoch": 20.06122448979592,
      "grad_norm": 4.3874030113220215,
      "learning_rate": 3.746173469387755e-05,
      "loss": 0.6144,
      "step": 1277900
    },
    {
      "epoch": 20.062794348508636,
      "grad_norm": 3.9628796577453613,
      "learning_rate": 3.746075353218211e-05,
      "loss": 0.6219,
      "step": 1278000
    },
    {
      "epoch": 20.06436420722135,
      "grad_norm": 3.7966396808624268,
      "learning_rate": 3.7459772370486655e-05,
      "loss": 0.5846,
      "step": 1278100
    },
    {
      "epoch": 20.065934065934066,
      "grad_norm": 4.146098613739014,
      "learning_rate": 3.745879120879121e-05,
      "loss": 0.637,
      "step": 1278200
    },
    {
      "epoch": 20.067503924646783,
      "grad_norm": 4.077296257019043,
      "learning_rate": 3.7457810047095763e-05,
      "loss": 0.6054,
      "step": 1278300
    },
    {
      "epoch": 20.069073783359496,
      "grad_norm": 3.6425576210021973,
      "learning_rate": 3.7456828885400314e-05,
      "loss": 0.61,
      "step": 1278400
    },
    {
      "epoch": 20.070643642072213,
      "grad_norm": 3.9988791942596436,
      "learning_rate": 3.7455847723704865e-05,
      "loss": 0.6225,
      "step": 1278500
    },
    {
      "epoch": 20.07221350078493,
      "grad_norm": 4.046322345733643,
      "learning_rate": 3.745486656200942e-05,
      "loss": 0.6035,
      "step": 1278600
    },
    {
      "epoch": 20.073783359497646,
      "grad_norm": 2.4751274585723877,
      "learning_rate": 3.7453885400313974e-05,
      "loss": 0.5994,
      "step": 1278700
    },
    {
      "epoch": 20.07535321821036,
      "grad_norm": 4.6281890869140625,
      "learning_rate": 3.7452904238618525e-05,
      "loss": 0.5729,
      "step": 1278800
    },
    {
      "epoch": 20.076923076923077,
      "grad_norm": 3.9483301639556885,
      "learning_rate": 3.7451923076923076e-05,
      "loss": 0.5725,
      "step": 1278900
    },
    {
      "epoch": 20.078492935635794,
      "grad_norm": 3.59578275680542,
      "learning_rate": 3.7450941915227634e-05,
      "loss": 0.6205,
      "step": 1279000
    },
    {
      "epoch": 20.08006279434851,
      "grad_norm": 3.6049580574035645,
      "learning_rate": 3.7449960753532185e-05,
      "loss": 0.5973,
      "step": 1279100
    },
    {
      "epoch": 20.081632653061224,
      "grad_norm": 4.654317855834961,
      "learning_rate": 3.7448979591836736e-05,
      "loss": 0.6041,
      "step": 1279200
    },
    {
      "epoch": 20.08320251177394,
      "grad_norm": 4.353622913360596,
      "learning_rate": 3.744799843014129e-05,
      "loss": 0.5896,
      "step": 1279300
    },
    {
      "epoch": 20.084772370486657,
      "grad_norm": 3.8692586421966553,
      "learning_rate": 3.7447017268445845e-05,
      "loss": 0.604,
      "step": 1279400
    },
    {
      "epoch": 20.08634222919937,
      "grad_norm": 3.16416597366333,
      "learning_rate": 3.7446036106750396e-05,
      "loss": 0.614,
      "step": 1279500
    },
    {
      "epoch": 20.087912087912088,
      "grad_norm": 3.2673373222351074,
      "learning_rate": 3.7445054945054947e-05,
      "loss": 0.5997,
      "step": 1279600
    },
    {
      "epoch": 20.089481946624804,
      "grad_norm": 3.2781999111175537,
      "learning_rate": 3.7444073783359504e-05,
      "loss": 0.6152,
      "step": 1279700
    },
    {
      "epoch": 20.09105180533752,
      "grad_norm": 4.103906154632568,
      "learning_rate": 3.744309262166405e-05,
      "loss": 0.6148,
      "step": 1279800
    },
    {
      "epoch": 20.092621664050235,
      "grad_norm": 2.4510531425476074,
      "learning_rate": 3.7442111459968606e-05,
      "loss": 0.5915,
      "step": 1279900
    },
    {
      "epoch": 20.09419152276295,
      "grad_norm": 4.257292747497559,
      "learning_rate": 3.744113029827316e-05,
      "loss": 0.5871,
      "step": 1280000
    },
    {
      "epoch": 20.09576138147567,
      "grad_norm": 3.6953587532043457,
      "learning_rate": 3.7440149136577715e-05,
      "loss": 0.5942,
      "step": 1280100
    },
    {
      "epoch": 20.09733124018838,
      "grad_norm": 2.794528007507324,
      "learning_rate": 3.743916797488226e-05,
      "loss": 0.6178,
      "step": 1280200
    },
    {
      "epoch": 20.0989010989011,
      "grad_norm": 3.6180572509765625,
      "learning_rate": 3.743818681318682e-05,
      "loss": 0.5972,
      "step": 1280300
    },
    {
      "epoch": 20.100470957613815,
      "grad_norm": 3.0892069339752197,
      "learning_rate": 3.743720565149137e-05,
      "loss": 0.5948,
      "step": 1280400
    },
    {
      "epoch": 20.102040816326532,
      "grad_norm": 4.370118618011475,
      "learning_rate": 3.743622448979592e-05,
      "loss": 0.5823,
      "step": 1280500
    },
    {
      "epoch": 20.103610675039246,
      "grad_norm": 4.778594970703125,
      "learning_rate": 3.743524332810047e-05,
      "loss": 0.6294,
      "step": 1280600
    },
    {
      "epoch": 20.105180533751962,
      "grad_norm": 4.705931186676025,
      "learning_rate": 3.743426216640503e-05,
      "loss": 0.6008,
      "step": 1280700
    },
    {
      "epoch": 20.10675039246468,
      "grad_norm": 2.9945521354675293,
      "learning_rate": 3.743328100470958e-05,
      "loss": 0.5718,
      "step": 1280800
    },
    {
      "epoch": 20.108320251177393,
      "grad_norm": 4.663204669952393,
      "learning_rate": 3.743229984301413e-05,
      "loss": 0.5781,
      "step": 1280900
    },
    {
      "epoch": 20.10989010989011,
      "grad_norm": 3.389723062515259,
      "learning_rate": 3.743131868131868e-05,
      "loss": 0.5954,
      "step": 1281000
    },
    {
      "epoch": 20.111459968602826,
      "grad_norm": 4.162299156188965,
      "learning_rate": 3.743033751962324e-05,
      "loss": 0.598,
      "step": 1281100
    },
    {
      "epoch": 20.113029827315543,
      "grad_norm": 2.744572401046753,
      "learning_rate": 3.742935635792779e-05,
      "loss": 0.5853,
      "step": 1281200
    },
    {
      "epoch": 20.114599686028257,
      "grad_norm": 4.043247222900391,
      "learning_rate": 3.742837519623234e-05,
      "loss": 0.6211,
      "step": 1281300
    },
    {
      "epoch": 20.116169544740973,
      "grad_norm": 2.639193058013916,
      "learning_rate": 3.742739403453689e-05,
      "loss": 0.5921,
      "step": 1281400
    },
    {
      "epoch": 20.11773940345369,
      "grad_norm": 3.4432833194732666,
      "learning_rate": 3.742641287284145e-05,
      "loss": 0.5979,
      "step": 1281500
    },
    {
      "epoch": 20.119309262166404,
      "grad_norm": 4.753706932067871,
      "learning_rate": 3.7425431711146e-05,
      "loss": 0.6045,
      "step": 1281600
    },
    {
      "epoch": 20.12087912087912,
      "grad_norm": 4.423935413360596,
      "learning_rate": 3.742445054945055e-05,
      "loss": 0.6292,
      "step": 1281700
    },
    {
      "epoch": 20.122448979591837,
      "grad_norm": 4.0656633377075195,
      "learning_rate": 3.742346938775511e-05,
      "loss": 0.5975,
      "step": 1281800
    },
    {
      "epoch": 20.124018838304554,
      "grad_norm": 3.920820474624634,
      "learning_rate": 3.742248822605965e-05,
      "loss": 0.5915,
      "step": 1281900
    },
    {
      "epoch": 20.125588697017267,
      "grad_norm": 3.1867823600769043,
      "learning_rate": 3.742150706436421e-05,
      "loss": 0.5857,
      "step": 1282000
    },
    {
      "epoch": 20.127158555729984,
      "grad_norm": 3.991107225418091,
      "learning_rate": 3.742052590266876e-05,
      "loss": 0.592,
      "step": 1282100
    },
    {
      "epoch": 20.1287284144427,
      "grad_norm": 4.762015342712402,
      "learning_rate": 3.741954474097332e-05,
      "loss": 0.6361,
      "step": 1282200
    },
    {
      "epoch": 20.130298273155415,
      "grad_norm": 4.031003952026367,
      "learning_rate": 3.7418563579277864e-05,
      "loss": 0.6439,
      "step": 1282300
    },
    {
      "epoch": 20.13186813186813,
      "grad_norm": 3.223400354385376,
      "learning_rate": 3.741758241758242e-05,
      "loss": 0.5999,
      "step": 1282400
    },
    {
      "epoch": 20.13343799058085,
      "grad_norm": 4.442818641662598,
      "learning_rate": 3.741660125588697e-05,
      "loss": 0.5735,
      "step": 1282500
    },
    {
      "epoch": 20.135007849293565,
      "grad_norm": 3.8963332176208496,
      "learning_rate": 3.7415620094191523e-05,
      "loss": 0.6168,
      "step": 1282600
    },
    {
      "epoch": 20.13657770800628,
      "grad_norm": 3.793780565261841,
      "learning_rate": 3.7414638932496074e-05,
      "loss": 0.6386,
      "step": 1282700
    },
    {
      "epoch": 20.138147566718995,
      "grad_norm": 4.98195219039917,
      "learning_rate": 3.741365777080063e-05,
      "loss": 0.6443,
      "step": 1282800
    },
    {
      "epoch": 20.139717425431712,
      "grad_norm": 3.846414566040039,
      "learning_rate": 3.741267660910518e-05,
      "loss": 0.5892,
      "step": 1282900
    },
    {
      "epoch": 20.141287284144425,
      "grad_norm": 3.7193515300750732,
      "learning_rate": 3.7411695447409734e-05,
      "loss": 0.582,
      "step": 1283000
    },
    {
      "epoch": 20.142857142857142,
      "grad_norm": 3.6654069423675537,
      "learning_rate": 3.7410714285714285e-05,
      "loss": 0.6097,
      "step": 1283100
    },
    {
      "epoch": 20.14442700156986,
      "grad_norm": 4.127026081085205,
      "learning_rate": 3.740973312401884e-05,
      "loss": 0.6198,
      "step": 1283200
    },
    {
      "epoch": 20.145996860282576,
      "grad_norm": 3.8489890098571777,
      "learning_rate": 3.7408751962323394e-05,
      "loss": 0.5908,
      "step": 1283300
    },
    {
      "epoch": 20.14756671899529,
      "grad_norm": 3.139120101928711,
      "learning_rate": 3.7407770800627945e-05,
      "loss": 0.6379,
      "step": 1283400
    },
    {
      "epoch": 20.149136577708006,
      "grad_norm": 3.82426118850708,
      "learning_rate": 3.7406789638932496e-05,
      "loss": 0.6236,
      "step": 1283500
    },
    {
      "epoch": 20.150706436420723,
      "grad_norm": 4.480960845947266,
      "learning_rate": 3.7405808477237054e-05,
      "loss": 0.6212,
      "step": 1283600
    },
    {
      "epoch": 20.152276295133436,
      "grad_norm": 4.661326885223389,
      "learning_rate": 3.7404827315541605e-05,
      "loss": 0.5836,
      "step": 1283700
    },
    {
      "epoch": 20.153846153846153,
      "grad_norm": 2.665890693664551,
      "learning_rate": 3.7403846153846156e-05,
      "loss": 0.6274,
      "step": 1283800
    },
    {
      "epoch": 20.15541601255887,
      "grad_norm": 3.816746234893799,
      "learning_rate": 3.740286499215071e-05,
      "loss": 0.5874,
      "step": 1283900
    },
    {
      "epoch": 20.156985871271587,
      "grad_norm": 2.8254897594451904,
      "learning_rate": 3.740188383045526e-05,
      "loss": 0.5978,
      "step": 1284000
    },
    {
      "epoch": 20.1585557299843,
      "grad_norm": 4.102610111236572,
      "learning_rate": 3.7400902668759815e-05,
      "loss": 0.5978,
      "step": 1284100
    },
    {
      "epoch": 20.160125588697017,
      "grad_norm": 3.090378999710083,
      "learning_rate": 3.7399921507064366e-05,
      "loss": 0.6181,
      "step": 1284200
    },
    {
      "epoch": 20.161695447409734,
      "grad_norm": 4.571456432342529,
      "learning_rate": 3.7398940345368924e-05,
      "loss": 0.6245,
      "step": 1284300
    },
    {
      "epoch": 20.163265306122447,
      "grad_norm": 3.614194631576538,
      "learning_rate": 3.739795918367347e-05,
      "loss": 0.6143,
      "step": 1284400
    },
    {
      "epoch": 20.164835164835164,
      "grad_norm": 3.3126773834228516,
      "learning_rate": 3.7396978021978026e-05,
      "loss": 0.5953,
      "step": 1284500
    },
    {
      "epoch": 20.16640502354788,
      "grad_norm": 5.031543731689453,
      "learning_rate": 3.739599686028258e-05,
      "loss": 0.6045,
      "step": 1284600
    },
    {
      "epoch": 20.167974882260598,
      "grad_norm": 3.783071279525757,
      "learning_rate": 3.739501569858713e-05,
      "loss": 0.5958,
      "step": 1284700
    },
    {
      "epoch": 20.16954474097331,
      "grad_norm": 4.033364772796631,
      "learning_rate": 3.739403453689168e-05,
      "loss": 0.5639,
      "step": 1284800
    },
    {
      "epoch": 20.171114599686028,
      "grad_norm": 4.458150863647461,
      "learning_rate": 3.739305337519624e-05,
      "loss": 0.5817,
      "step": 1284900
    },
    {
      "epoch": 20.172684458398745,
      "grad_norm": 3.340433359146118,
      "learning_rate": 3.739207221350079e-05,
      "loss": 0.5943,
      "step": 1285000
    },
    {
      "epoch": 20.17425431711146,
      "grad_norm": 3.8070316314697266,
      "learning_rate": 3.739109105180534e-05,
      "loss": 0.6154,
      "step": 1285100
    },
    {
      "epoch": 20.175824175824175,
      "grad_norm": 4.337639808654785,
      "learning_rate": 3.739010989010989e-05,
      "loss": 0.5824,
      "step": 1285200
    },
    {
      "epoch": 20.177394034536892,
      "grad_norm": 3.2028701305389404,
      "learning_rate": 3.738912872841445e-05,
      "loss": 0.5937,
      "step": 1285300
    },
    {
      "epoch": 20.17896389324961,
      "grad_norm": 3.531590223312378,
      "learning_rate": 3.7388147566719e-05,
      "loss": 0.6339,
      "step": 1285400
    },
    {
      "epoch": 20.180533751962322,
      "grad_norm": 4.155632019042969,
      "learning_rate": 3.738716640502355e-05,
      "loss": 0.6131,
      "step": 1285500
    },
    {
      "epoch": 20.18210361067504,
      "grad_norm": 3.86421275138855,
      "learning_rate": 3.73861852433281e-05,
      "loss": 0.5702,
      "step": 1285600
    },
    {
      "epoch": 20.183673469387756,
      "grad_norm": 3.9530975818634033,
      "learning_rate": 3.738520408163265e-05,
      "loss": 0.5999,
      "step": 1285700
    },
    {
      "epoch": 20.18524332810047,
      "grad_norm": 3.470538854598999,
      "learning_rate": 3.738422291993721e-05,
      "loss": 0.5578,
      "step": 1285800
    },
    {
      "epoch": 20.186813186813186,
      "grad_norm": 4.243477821350098,
      "learning_rate": 3.738324175824176e-05,
      "loss": 0.6052,
      "step": 1285900
    },
    {
      "epoch": 20.188383045525903,
      "grad_norm": 3.5735158920288086,
      "learning_rate": 3.738226059654632e-05,
      "loss": 0.6263,
      "step": 1286000
    },
    {
      "epoch": 20.18995290423862,
      "grad_norm": 3.89816951751709,
      "learning_rate": 3.738127943485086e-05,
      "loss": 0.5698,
      "step": 1286100
    },
    {
      "epoch": 20.191522762951333,
      "grad_norm": 4.559335231781006,
      "learning_rate": 3.738029827315542e-05,
      "loss": 0.5867,
      "step": 1286200
    },
    {
      "epoch": 20.19309262166405,
      "grad_norm": 3.807276725769043,
      "learning_rate": 3.737931711145997e-05,
      "loss": 0.6023,
      "step": 1286300
    },
    {
      "epoch": 20.194662480376767,
      "grad_norm": 4.002686977386475,
      "learning_rate": 3.737833594976452e-05,
      "loss": 0.638,
      "step": 1286400
    },
    {
      "epoch": 20.19623233908948,
      "grad_norm": 4.553243160247803,
      "learning_rate": 3.737735478806907e-05,
      "loss": 0.6023,
      "step": 1286500
    },
    {
      "epoch": 20.197802197802197,
      "grad_norm": 3.7130160331726074,
      "learning_rate": 3.737637362637363e-05,
      "loss": 0.658,
      "step": 1286600
    },
    {
      "epoch": 20.199372056514914,
      "grad_norm": 3.211129903793335,
      "learning_rate": 3.737539246467818e-05,
      "loss": 0.6281,
      "step": 1286700
    },
    {
      "epoch": 20.20094191522763,
      "grad_norm": 3.8847732543945312,
      "learning_rate": 3.737441130298273e-05,
      "loss": 0.6022,
      "step": 1286800
    },
    {
      "epoch": 20.202511773940344,
      "grad_norm": 4.481027603149414,
      "learning_rate": 3.737343014128728e-05,
      "loss": 0.6057,
      "step": 1286900
    },
    {
      "epoch": 20.20408163265306,
      "grad_norm": 3.957327365875244,
      "learning_rate": 3.737244897959184e-05,
      "loss": 0.5897,
      "step": 1287000
    },
    {
      "epoch": 20.205651491365778,
      "grad_norm": 3.507319450378418,
      "learning_rate": 3.7371467817896385e-05,
      "loss": 0.6058,
      "step": 1287100
    },
    {
      "epoch": 20.20722135007849,
      "grad_norm": 3.302478075027466,
      "learning_rate": 3.737048665620094e-05,
      "loss": 0.6026,
      "step": 1287200
    },
    {
      "epoch": 20.208791208791208,
      "grad_norm": 3.5661845207214355,
      "learning_rate": 3.7369505494505494e-05,
      "loss": 0.6056,
      "step": 1287300
    },
    {
      "epoch": 20.210361067503925,
      "grad_norm": 4.266245365142822,
      "learning_rate": 3.736852433281005e-05,
      "loss": 0.589,
      "step": 1287400
    },
    {
      "epoch": 20.211930926216642,
      "grad_norm": 4.147321701049805,
      "learning_rate": 3.73675431711146e-05,
      "loss": 0.6111,
      "step": 1287500
    },
    {
      "epoch": 20.213500784929355,
      "grad_norm": 4.327630519866943,
      "learning_rate": 3.7366562009419154e-05,
      "loss": 0.5816,
      "step": 1287600
    },
    {
      "epoch": 20.215070643642072,
      "grad_norm": 4.3020405769348145,
      "learning_rate": 3.7365580847723705e-05,
      "loss": 0.608,
      "step": 1287700
    },
    {
      "epoch": 20.21664050235479,
      "grad_norm": 4.164017200469971,
      "learning_rate": 3.7364599686028256e-05,
      "loss": 0.623,
      "step": 1287800
    },
    {
      "epoch": 20.218210361067506,
      "grad_norm": 3.605423927307129,
      "learning_rate": 3.7363618524332814e-05,
      "loss": 0.5928,
      "step": 1287900
    },
    {
      "epoch": 20.21978021978022,
      "grad_norm": 3.4076781272888184,
      "learning_rate": 3.7362637362637365e-05,
      "loss": 0.6073,
      "step": 1288000
    },
    {
      "epoch": 20.221350078492936,
      "grad_norm": 3.7873547077178955,
      "learning_rate": 3.736165620094192e-05,
      "loss": 0.6031,
      "step": 1288100
    },
    {
      "epoch": 20.222919937205653,
      "grad_norm": 4.147928714752197,
      "learning_rate": 3.7360675039246466e-05,
      "loss": 0.596,
      "step": 1288200
    },
    {
      "epoch": 20.224489795918366,
      "grad_norm": 4.019822120666504,
      "learning_rate": 3.7359693877551024e-05,
      "loss": 0.594,
      "step": 1288300
    },
    {
      "epoch": 20.226059654631083,
      "grad_norm": 3.17104434967041,
      "learning_rate": 3.7358712715855575e-05,
      "loss": 0.5837,
      "step": 1288400
    },
    {
      "epoch": 20.2276295133438,
      "grad_norm": 3.4268996715545654,
      "learning_rate": 3.7357731554160126e-05,
      "loss": 0.6252,
      "step": 1288500
    },
    {
      "epoch": 20.229199372056517,
      "grad_norm": 4.400686740875244,
      "learning_rate": 3.735675039246468e-05,
      "loss": 0.5816,
      "step": 1288600
    },
    {
      "epoch": 20.23076923076923,
      "grad_norm": 3.2863354682922363,
      "learning_rate": 3.7355769230769235e-05,
      "loss": 0.5886,
      "step": 1288700
    },
    {
      "epoch": 20.232339089481947,
      "grad_norm": 3.0264546871185303,
      "learning_rate": 3.7354788069073786e-05,
      "loss": 0.6166,
      "step": 1288800
    },
    {
      "epoch": 20.233908948194664,
      "grad_norm": 3.8959853649139404,
      "learning_rate": 3.735380690737834e-05,
      "loss": 0.6099,
      "step": 1288900
    },
    {
      "epoch": 20.235478806907377,
      "grad_norm": 3.5176024436950684,
      "learning_rate": 3.735282574568289e-05,
      "loss": 0.5932,
      "step": 1289000
    },
    {
      "epoch": 20.237048665620094,
      "grad_norm": 4.279162883758545,
      "learning_rate": 3.7351844583987446e-05,
      "loss": 0.617,
      "step": 1289100
    },
    {
      "epoch": 20.23861852433281,
      "grad_norm": 4.642745018005371,
      "learning_rate": 3.735086342229199e-05,
      "loss": 0.6252,
      "step": 1289200
    },
    {
      "epoch": 20.240188383045528,
      "grad_norm": 3.9063940048217773,
      "learning_rate": 3.734988226059655e-05,
      "loss": 0.6212,
      "step": 1289300
    },
    {
      "epoch": 20.24175824175824,
      "grad_norm": 4.326878547668457,
      "learning_rate": 3.73489010989011e-05,
      "loss": 0.5681,
      "step": 1289400
    },
    {
      "epoch": 20.243328100470958,
      "grad_norm": 2.7435643672943115,
      "learning_rate": 3.7347919937205656e-05,
      "loss": 0.6045,
      "step": 1289500
    },
    {
      "epoch": 20.244897959183675,
      "grad_norm": 5.148113250732422,
      "learning_rate": 3.734693877551021e-05,
      "loss": 0.6147,
      "step": 1289600
    },
    {
      "epoch": 20.246467817896388,
      "grad_norm": 4.688928604125977,
      "learning_rate": 3.734595761381476e-05,
      "loss": 0.5794,
      "step": 1289700
    },
    {
      "epoch": 20.248037676609105,
      "grad_norm": 4.219893932342529,
      "learning_rate": 3.734497645211931e-05,
      "loss": 0.5808,
      "step": 1289800
    },
    {
      "epoch": 20.24960753532182,
      "grad_norm": 3.460338592529297,
      "learning_rate": 3.734399529042386e-05,
      "loss": 0.5902,
      "step": 1289900
    },
    {
      "epoch": 20.25117739403454,
      "grad_norm": 2.93096661567688,
      "learning_rate": 3.734301412872842e-05,
      "loss": 0.609,
      "step": 1290000
    },
    {
      "epoch": 20.252747252747252,
      "grad_norm": 3.1991307735443115,
      "learning_rate": 3.734203296703297e-05,
      "loss": 0.5576,
      "step": 1290100
    },
    {
      "epoch": 20.25431711145997,
      "grad_norm": 3.0901215076446533,
      "learning_rate": 3.734105180533753e-05,
      "loss": 0.5738,
      "step": 1290200
    },
    {
      "epoch": 20.255886970172686,
      "grad_norm": 3.2163877487182617,
      "learning_rate": 3.734007064364207e-05,
      "loss": 0.5803,
      "step": 1290300
    },
    {
      "epoch": 20.2574568288854,
      "grad_norm": 4.273787975311279,
      "learning_rate": 3.733908948194663e-05,
      "loss": 0.5723,
      "step": 1290400
    },
    {
      "epoch": 20.259026687598116,
      "grad_norm": 3.3894882202148438,
      "learning_rate": 3.733810832025118e-05,
      "loss": 0.6012,
      "step": 1290500
    },
    {
      "epoch": 20.260596546310833,
      "grad_norm": 3.950690984725952,
      "learning_rate": 3.733712715855573e-05,
      "loss": 0.592,
      "step": 1290600
    },
    {
      "epoch": 20.26216640502355,
      "grad_norm": 3.6646084785461426,
      "learning_rate": 3.733614599686028e-05,
      "loss": 0.6256,
      "step": 1290700
    },
    {
      "epoch": 20.263736263736263,
      "grad_norm": 4.171580791473389,
      "learning_rate": 3.733516483516484e-05,
      "loss": 0.5784,
      "step": 1290800
    },
    {
      "epoch": 20.26530612244898,
      "grad_norm": 3.852182149887085,
      "learning_rate": 3.733418367346939e-05,
      "loss": 0.5867,
      "step": 1290900
    },
    {
      "epoch": 20.266875981161697,
      "grad_norm": 4.206003665924072,
      "learning_rate": 3.733320251177394e-05,
      "loss": 0.6179,
      "step": 1291000
    },
    {
      "epoch": 20.26844583987441,
      "grad_norm": 3.2032270431518555,
      "learning_rate": 3.733222135007849e-05,
      "loss": 0.5607,
      "step": 1291100
    },
    {
      "epoch": 20.270015698587127,
      "grad_norm": 2.5983364582061768,
      "learning_rate": 3.733124018838305e-05,
      "loss": 0.5998,
      "step": 1291200
    },
    {
      "epoch": 20.271585557299844,
      "grad_norm": 3.8829994201660156,
      "learning_rate": 3.7330259026687594e-05,
      "loss": 0.6153,
      "step": 1291300
    },
    {
      "epoch": 20.27315541601256,
      "grad_norm": 3.8364903926849365,
      "learning_rate": 3.732927786499215e-05,
      "loss": 0.6062,
      "step": 1291400
    },
    {
      "epoch": 20.274725274725274,
      "grad_norm": 3.9873762130737305,
      "learning_rate": 3.73282967032967e-05,
      "loss": 0.6127,
      "step": 1291500
    },
    {
      "epoch": 20.27629513343799,
      "grad_norm": 3.2318506240844727,
      "learning_rate": 3.732731554160126e-05,
      "loss": 0.5281,
      "step": 1291600
    },
    {
      "epoch": 20.277864992150707,
      "grad_norm": 4.99452543258667,
      "learning_rate": 3.732633437990581e-05,
      "loss": 0.5427,
      "step": 1291700
    },
    {
      "epoch": 20.27943485086342,
      "grad_norm": 3.325033187866211,
      "learning_rate": 3.732535321821036e-05,
      "loss": 0.5985,
      "step": 1291800
    },
    {
      "epoch": 20.281004709576138,
      "grad_norm": 3.3572773933410645,
      "learning_rate": 3.7324372056514914e-05,
      "loss": 0.6349,
      "step": 1291900
    },
    {
      "epoch": 20.282574568288855,
      "grad_norm": 3.8296241760253906,
      "learning_rate": 3.7323390894819465e-05,
      "loss": 0.6035,
      "step": 1292000
    },
    {
      "epoch": 20.28414442700157,
      "grad_norm": 3.970661163330078,
      "learning_rate": 3.732240973312402e-05,
      "loss": 0.5747,
      "step": 1292100
    },
    {
      "epoch": 20.285714285714285,
      "grad_norm": 3.456580400466919,
      "learning_rate": 3.7321428571428573e-05,
      "loss": 0.6214,
      "step": 1292200
    },
    {
      "epoch": 20.287284144427,
      "grad_norm": 5.233783721923828,
      "learning_rate": 3.732044740973313e-05,
      "loss": 0.6033,
      "step": 1292300
    },
    {
      "epoch": 20.28885400313972,
      "grad_norm": 3.558457374572754,
      "learning_rate": 3.7319466248037675e-05,
      "loss": 0.6385,
      "step": 1292400
    },
    {
      "epoch": 20.29042386185243,
      "grad_norm": 2.9318230152130127,
      "learning_rate": 3.731848508634223e-05,
      "loss": 0.5981,
      "step": 1292500
    },
    {
      "epoch": 20.29199372056515,
      "grad_norm": 3.621818780899048,
      "learning_rate": 3.7317503924646784e-05,
      "loss": 0.6071,
      "step": 1292600
    },
    {
      "epoch": 20.293563579277865,
      "grad_norm": 3.461500883102417,
      "learning_rate": 3.7316522762951335e-05,
      "loss": 0.5879,
      "step": 1292700
    },
    {
      "epoch": 20.295133437990582,
      "grad_norm": 2.59093976020813,
      "learning_rate": 3.7315541601255886e-05,
      "loss": 0.6107,
      "step": 1292800
    },
    {
      "epoch": 20.296703296703296,
      "grad_norm": 2.8561673164367676,
      "learning_rate": 3.7314560439560444e-05,
      "loss": 0.5631,
      "step": 1292900
    },
    {
      "epoch": 20.298273155416013,
      "grad_norm": 2.171759843826294,
      "learning_rate": 3.7313579277864995e-05,
      "loss": 0.5816,
      "step": 1293000
    },
    {
      "epoch": 20.29984301412873,
      "grad_norm": 6.046575546264648,
      "learning_rate": 3.7312598116169546e-05,
      "loss": 0.6209,
      "step": 1293100
    },
    {
      "epoch": 20.301412872841443,
      "grad_norm": 3.898322582244873,
      "learning_rate": 3.73116169544741e-05,
      "loss": 0.6058,
      "step": 1293200
    },
    {
      "epoch": 20.30298273155416,
      "grad_norm": 4.441353797912598,
      "learning_rate": 3.7310635792778655e-05,
      "loss": 0.6029,
      "step": 1293300
    },
    {
      "epoch": 20.304552590266876,
      "grad_norm": 5.163459300994873,
      "learning_rate": 3.73096546310832e-05,
      "loss": 0.6045,
      "step": 1293400
    },
    {
      "epoch": 20.306122448979593,
      "grad_norm": 4.258686542510986,
      "learning_rate": 3.7308673469387757e-05,
      "loss": 0.6393,
      "step": 1293500
    },
    {
      "epoch": 20.307692307692307,
      "grad_norm": 4.360804557800293,
      "learning_rate": 3.730769230769231e-05,
      "loss": 0.5935,
      "step": 1293600
    },
    {
      "epoch": 20.309262166405023,
      "grad_norm": 3.9312806129455566,
      "learning_rate": 3.7306711145996865e-05,
      "loss": 0.5699,
      "step": 1293700
    },
    {
      "epoch": 20.31083202511774,
      "grad_norm": 4.173640251159668,
      "learning_rate": 3.7305729984301416e-05,
      "loss": 0.6286,
      "step": 1293800
    },
    {
      "epoch": 20.312401883830454,
      "grad_norm": 4.562523365020752,
      "learning_rate": 3.730474882260597e-05,
      "loss": 0.5906,
      "step": 1293900
    },
    {
      "epoch": 20.31397174254317,
      "grad_norm": 4.735644340515137,
      "learning_rate": 3.730376766091052e-05,
      "loss": 0.6233,
      "step": 1294000
    },
    {
      "epoch": 20.315541601255887,
      "grad_norm": 3.9426662921905518,
      "learning_rate": 3.730278649921507e-05,
      "loss": 0.6288,
      "step": 1294100
    },
    {
      "epoch": 20.317111459968604,
      "grad_norm": 3.586233139038086,
      "learning_rate": 3.730180533751963e-05,
      "loss": 0.5987,
      "step": 1294200
    },
    {
      "epoch": 20.318681318681318,
      "grad_norm": 4.018279552459717,
      "learning_rate": 3.730082417582418e-05,
      "loss": 0.6848,
      "step": 1294300
    },
    {
      "epoch": 20.320251177394034,
      "grad_norm": 4.622284412384033,
      "learning_rate": 3.7299843014128736e-05,
      "loss": 0.6236,
      "step": 1294400
    },
    {
      "epoch": 20.32182103610675,
      "grad_norm": 2.073242425918579,
      "learning_rate": 3.729886185243328e-05,
      "loss": 0.5909,
      "step": 1294500
    },
    {
      "epoch": 20.323390894819465,
      "grad_norm": 4.016432762145996,
      "learning_rate": 3.729788069073784e-05,
      "loss": 0.647,
      "step": 1294600
    },
    {
      "epoch": 20.32496075353218,
      "grad_norm": 2.332109212875366,
      "learning_rate": 3.729689952904239e-05,
      "loss": 0.5898,
      "step": 1294700
    },
    {
      "epoch": 20.3265306122449,
      "grad_norm": 3.3054721355438232,
      "learning_rate": 3.729591836734694e-05,
      "loss": 0.559,
      "step": 1294800
    },
    {
      "epoch": 20.328100470957615,
      "grad_norm": 3.3180654048919678,
      "learning_rate": 3.729493720565149e-05,
      "loss": 0.6272,
      "step": 1294900
    },
    {
      "epoch": 20.32967032967033,
      "grad_norm": 3.3382415771484375,
      "learning_rate": 3.729395604395605e-05,
      "loss": 0.5949,
      "step": 1295000
    },
    {
      "epoch": 20.331240188383045,
      "grad_norm": 5.037826061248779,
      "learning_rate": 3.72929748822606e-05,
      "loss": 0.5541,
      "step": 1295100
    },
    {
      "epoch": 20.332810047095762,
      "grad_norm": 3.4596455097198486,
      "learning_rate": 3.729199372056515e-05,
      "loss": 0.5518,
      "step": 1295200
    },
    {
      "epoch": 20.334379905808476,
      "grad_norm": 3.621427536010742,
      "learning_rate": 3.72910125588697e-05,
      "loss": 0.6069,
      "step": 1295300
    },
    {
      "epoch": 20.335949764521192,
      "grad_norm": 2.4528205394744873,
      "learning_rate": 3.729003139717426e-05,
      "loss": 0.5955,
      "step": 1295400
    },
    {
      "epoch": 20.33751962323391,
      "grad_norm": 2.967336416244507,
      "learning_rate": 3.72890502354788e-05,
      "loss": 0.5965,
      "step": 1295500
    },
    {
      "epoch": 20.339089481946626,
      "grad_norm": 4.005867958068848,
      "learning_rate": 3.728806907378336e-05,
      "loss": 0.6433,
      "step": 1295600
    },
    {
      "epoch": 20.34065934065934,
      "grad_norm": 4.070275783538818,
      "learning_rate": 3.728708791208791e-05,
      "loss": 0.6242,
      "step": 1295700
    },
    {
      "epoch": 20.342229199372056,
      "grad_norm": 3.856311321258545,
      "learning_rate": 3.728610675039247e-05,
      "loss": 0.5982,
      "step": 1295800
    },
    {
      "epoch": 20.343799058084773,
      "grad_norm": 3.2626359462738037,
      "learning_rate": 3.728512558869702e-05,
      "loss": 0.6481,
      "step": 1295900
    },
    {
      "epoch": 20.345368916797486,
      "grad_norm": 3.2558765411376953,
      "learning_rate": 3.728414442700157e-05,
      "loss": 0.6549,
      "step": 1296000
    },
    {
      "epoch": 20.346938775510203,
      "grad_norm": 4.052002906799316,
      "learning_rate": 3.728316326530612e-05,
      "loss": 0.6226,
      "step": 1296100
    },
    {
      "epoch": 20.34850863422292,
      "grad_norm": 4.3085856437683105,
      "learning_rate": 3.7282182103610674e-05,
      "loss": 0.6087,
      "step": 1296200
    },
    {
      "epoch": 20.350078492935637,
      "grad_norm": 4.031887054443359,
      "learning_rate": 3.728120094191523e-05,
      "loss": 0.6367,
      "step": 1296300
    },
    {
      "epoch": 20.35164835164835,
      "grad_norm": 2.8618292808532715,
      "learning_rate": 3.728021978021978e-05,
      "loss": 0.6108,
      "step": 1296400
    },
    {
      "epoch": 20.353218210361067,
      "grad_norm": 3.8659508228302,
      "learning_rate": 3.727923861852434e-05,
      "loss": 0.5653,
      "step": 1296500
    },
    {
      "epoch": 20.354788069073784,
      "grad_norm": 3.5118014812469482,
      "learning_rate": 3.7278257456828884e-05,
      "loss": 0.6209,
      "step": 1296600
    },
    {
      "epoch": 20.356357927786497,
      "grad_norm": 4.259437561035156,
      "learning_rate": 3.727727629513344e-05,
      "loss": 0.6122,
      "step": 1296700
    },
    {
      "epoch": 20.357927786499214,
      "grad_norm": 3.378984212875366,
      "learning_rate": 3.727629513343799e-05,
      "loss": 0.5898,
      "step": 1296800
    },
    {
      "epoch": 20.35949764521193,
      "grad_norm": 3.643125295639038,
      "learning_rate": 3.7275313971742544e-05,
      "loss": 0.589,
      "step": 1296900
    },
    {
      "epoch": 20.361067503924648,
      "grad_norm": 4.191689968109131,
      "learning_rate": 3.7274332810047095e-05,
      "loss": 0.5993,
      "step": 1297000
    },
    {
      "epoch": 20.36263736263736,
      "grad_norm": 4.558409690856934,
      "learning_rate": 3.727335164835165e-05,
      "loss": 0.6412,
      "step": 1297100
    },
    {
      "epoch": 20.364207221350078,
      "grad_norm": 2.583127021789551,
      "learning_rate": 3.7272370486656204e-05,
      "loss": 0.5715,
      "step": 1297200
    },
    {
      "epoch": 20.365777080062795,
      "grad_norm": 4.379836559295654,
      "learning_rate": 3.7271389324960755e-05,
      "loss": 0.5953,
      "step": 1297300
    },
    {
      "epoch": 20.367346938775512,
      "grad_norm": 5.258159637451172,
      "learning_rate": 3.7270408163265306e-05,
      "loss": 0.6211,
      "step": 1297400
    },
    {
      "epoch": 20.368916797488225,
      "grad_norm": 3.7094457149505615,
      "learning_rate": 3.7269427001569864e-05,
      "loss": 0.6091,
      "step": 1297500
    },
    {
      "epoch": 20.370486656200942,
      "grad_norm": 4.25564432144165,
      "learning_rate": 3.726844583987441e-05,
      "loss": 0.6236,
      "step": 1297600
    },
    {
      "epoch": 20.37205651491366,
      "grad_norm": 3.4382097721099854,
      "learning_rate": 3.7267464678178966e-05,
      "loss": 0.6209,
      "step": 1297700
    },
    {
      "epoch": 20.373626373626372,
      "grad_norm": 4.218665599822998,
      "learning_rate": 3.7266483516483517e-05,
      "loss": 0.6121,
      "step": 1297800
    },
    {
      "epoch": 20.37519623233909,
      "grad_norm": 3.6002538204193115,
      "learning_rate": 3.7265502354788074e-05,
      "loss": 0.6051,
      "step": 1297900
    },
    {
      "epoch": 20.376766091051806,
      "grad_norm": 3.9582722187042236,
      "learning_rate": 3.7264521193092625e-05,
      "loss": 0.6073,
      "step": 1298000
    },
    {
      "epoch": 20.378335949764523,
      "grad_norm": 2.9529600143432617,
      "learning_rate": 3.7263540031397176e-05,
      "loss": 0.5865,
      "step": 1298100
    },
    {
      "epoch": 20.379905808477236,
      "grad_norm": 3.8582353591918945,
      "learning_rate": 3.726255886970173e-05,
      "loss": 0.6661,
      "step": 1298200
    },
    {
      "epoch": 20.381475667189953,
      "grad_norm": 3.350571870803833,
      "learning_rate": 3.726157770800628e-05,
      "loss": 0.6556,
      "step": 1298300
    },
    {
      "epoch": 20.38304552590267,
      "grad_norm": 4.009039402008057,
      "learning_rate": 3.7260596546310836e-05,
      "loss": 0.5987,
      "step": 1298400
    },
    {
      "epoch": 20.384615384615383,
      "grad_norm": 3.4328489303588867,
      "learning_rate": 3.725961538461539e-05,
      "loss": 0.6297,
      "step": 1298500
    },
    {
      "epoch": 20.3861852433281,
      "grad_norm": 4.346551418304443,
      "learning_rate": 3.7258634222919945e-05,
      "loss": 0.6339,
      "step": 1298600
    },
    {
      "epoch": 20.387755102040817,
      "grad_norm": 3.909698486328125,
      "learning_rate": 3.725765306122449e-05,
      "loss": 0.613,
      "step": 1298700
    },
    {
      "epoch": 20.389324960753534,
      "grad_norm": 3.072404623031616,
      "learning_rate": 3.725667189952905e-05,
      "loss": 0.6283,
      "step": 1298800
    },
    {
      "epoch": 20.390894819466247,
      "grad_norm": 4.279853820800781,
      "learning_rate": 3.72556907378336e-05,
      "loss": 0.5898,
      "step": 1298900
    },
    {
      "epoch": 20.392464678178964,
      "grad_norm": 5.106023788452148,
      "learning_rate": 3.725470957613815e-05,
      "loss": 0.6305,
      "step": 1299000
    },
    {
      "epoch": 20.39403453689168,
      "grad_norm": 3.637308120727539,
      "learning_rate": 3.72537284144427e-05,
      "loss": 0.5662,
      "step": 1299100
    },
    {
      "epoch": 20.395604395604394,
      "grad_norm": 4.822934627532959,
      "learning_rate": 3.725274725274726e-05,
      "loss": 0.5753,
      "step": 1299200
    },
    {
      "epoch": 20.39717425431711,
      "grad_norm": 4.071641445159912,
      "learning_rate": 3.725176609105181e-05,
      "loss": 0.6132,
      "step": 1299300
    },
    {
      "epoch": 20.398744113029828,
      "grad_norm": 3.735290050506592,
      "learning_rate": 3.725078492935636e-05,
      "loss": 0.6317,
      "step": 1299400
    },
    {
      "epoch": 20.400313971742545,
      "grad_norm": 2.5883100032806396,
      "learning_rate": 3.724980376766091e-05,
      "loss": 0.6433,
      "step": 1299500
    },
    {
      "epoch": 20.401883830455258,
      "grad_norm": 3.1579933166503906,
      "learning_rate": 3.724882260596547e-05,
      "loss": 0.5864,
      "step": 1299600
    },
    {
      "epoch": 20.403453689167975,
      "grad_norm": 2.1298773288726807,
      "learning_rate": 3.724784144427001e-05,
      "loss": 0.6294,
      "step": 1299700
    },
    {
      "epoch": 20.405023547880692,
      "grad_norm": 4.1311187744140625,
      "learning_rate": 3.724686028257457e-05,
      "loss": 0.5735,
      "step": 1299800
    },
    {
      "epoch": 20.406593406593405,
      "grad_norm": 2.927804470062256,
      "learning_rate": 3.724587912087912e-05,
      "loss": 0.5748,
      "step": 1299900
    },
    {
      "epoch": 20.408163265306122,
      "grad_norm": 4.004611968994141,
      "learning_rate": 3.724489795918368e-05,
      "loss": 0.6063,
      "step": 1300000
    },
    {
      "epoch": 20.40973312401884,
      "grad_norm": 3.859496831893921,
      "learning_rate": 3.724391679748823e-05,
      "loss": 0.6052,
      "step": 1300100
    },
    {
      "epoch": 20.411302982731556,
      "grad_norm": 2.821880340576172,
      "learning_rate": 3.724293563579278e-05,
      "loss": 0.6097,
      "step": 1300200
    },
    {
      "epoch": 20.41287284144427,
      "grad_norm": 5.051207542419434,
      "learning_rate": 3.724195447409733e-05,
      "loss": 0.5498,
      "step": 1300300
    },
    {
      "epoch": 20.414442700156986,
      "grad_norm": 3.5168049335479736,
      "learning_rate": 3.724097331240188e-05,
      "loss": 0.5959,
      "step": 1300400
    },
    {
      "epoch": 20.416012558869703,
      "grad_norm": 4.451422214508057,
      "learning_rate": 3.723999215070644e-05,
      "loss": 0.6022,
      "step": 1300500
    },
    {
      "epoch": 20.417582417582416,
      "grad_norm": 4.320207595825195,
      "learning_rate": 3.723901098901099e-05,
      "loss": 0.627,
      "step": 1300600
    },
    {
      "epoch": 20.419152276295133,
      "grad_norm": 3.7744686603546143,
      "learning_rate": 3.723802982731555e-05,
      "loss": 0.6116,
      "step": 1300700
    },
    {
      "epoch": 20.42072213500785,
      "grad_norm": 3.1987297534942627,
      "learning_rate": 3.7237048665620093e-05,
      "loss": 0.6151,
      "step": 1300800
    },
    {
      "epoch": 20.422291993720567,
      "grad_norm": 3.1123414039611816,
      "learning_rate": 3.723606750392465e-05,
      "loss": 0.5976,
      "step": 1300900
    },
    {
      "epoch": 20.42386185243328,
      "grad_norm": 2.351393699645996,
      "learning_rate": 3.72350863422292e-05,
      "loss": 0.5946,
      "step": 1301000
    },
    {
      "epoch": 20.425431711145997,
      "grad_norm": 3.040177345275879,
      "learning_rate": 3.723410518053375e-05,
      "loss": 0.5953,
      "step": 1301100
    },
    {
      "epoch": 20.427001569858714,
      "grad_norm": 5.578187942504883,
      "learning_rate": 3.7233124018838304e-05,
      "loss": 0.6421,
      "step": 1301200
    },
    {
      "epoch": 20.428571428571427,
      "grad_norm": 3.2861850261688232,
      "learning_rate": 3.723214285714286e-05,
      "loss": 0.5835,
      "step": 1301300
    },
    {
      "epoch": 20.430141287284144,
      "grad_norm": 4.627170562744141,
      "learning_rate": 3.723116169544741e-05,
      "loss": 0.5985,
      "step": 1301400
    },
    {
      "epoch": 20.43171114599686,
      "grad_norm": 4.601833343505859,
      "learning_rate": 3.7230180533751964e-05,
      "loss": 0.6236,
      "step": 1301500
    },
    {
      "epoch": 20.433281004709578,
      "grad_norm": 4.513233661651611,
      "learning_rate": 3.7229199372056515e-05,
      "loss": 0.6238,
      "step": 1301600
    },
    {
      "epoch": 20.43485086342229,
      "grad_norm": 4.286055564880371,
      "learning_rate": 3.722821821036107e-05,
      "loss": 0.6011,
      "step": 1301700
    },
    {
      "epoch": 20.436420722135008,
      "grad_norm": 3.486083745956421,
      "learning_rate": 3.722723704866562e-05,
      "loss": 0.6111,
      "step": 1301800
    },
    {
      "epoch": 20.437990580847725,
      "grad_norm": 2.729161500930786,
      "learning_rate": 3.7226255886970175e-05,
      "loss": 0.5704,
      "step": 1301900
    },
    {
      "epoch": 20.439560439560438,
      "grad_norm": 3.8266539573669434,
      "learning_rate": 3.7225274725274726e-05,
      "loss": 0.5911,
      "step": 1302000
    },
    {
      "epoch": 20.441130298273155,
      "grad_norm": 2.855867385864258,
      "learning_rate": 3.722429356357928e-05,
      "loss": 0.5896,
      "step": 1302100
    },
    {
      "epoch": 20.44270015698587,
      "grad_norm": 3.70068621635437,
      "learning_rate": 3.722331240188383e-05,
      "loss": 0.6262,
      "step": 1302200
    },
    {
      "epoch": 20.44427001569859,
      "grad_norm": 4.1349263191223145,
      "learning_rate": 3.7222331240188385e-05,
      "loss": 0.5947,
      "step": 1302300
    },
    {
      "epoch": 20.445839874411302,
      "grad_norm": 4.234058380126953,
      "learning_rate": 3.7221350078492936e-05,
      "loss": 0.6296,
      "step": 1302400
    },
    {
      "epoch": 20.44740973312402,
      "grad_norm": 3.6344051361083984,
      "learning_rate": 3.722036891679749e-05,
      "loss": 0.6239,
      "step": 1302500
    },
    {
      "epoch": 20.448979591836736,
      "grad_norm": 3.6423463821411133,
      "learning_rate": 3.7219387755102045e-05,
      "loss": 0.5877,
      "step": 1302600
    },
    {
      "epoch": 20.45054945054945,
      "grad_norm": 4.007368087768555,
      "learning_rate": 3.7218406593406596e-05,
      "loss": 0.6318,
      "step": 1302700
    },
    {
      "epoch": 20.452119309262166,
      "grad_norm": 3.4594337940216064,
      "learning_rate": 3.7217425431711154e-05,
      "loss": 0.5919,
      "step": 1302800
    },
    {
      "epoch": 20.453689167974883,
      "grad_norm": 3.6152055263519287,
      "learning_rate": 3.72164442700157e-05,
      "loss": 0.5958,
      "step": 1302900
    },
    {
      "epoch": 20.4552590266876,
      "grad_norm": 3.599860191345215,
      "learning_rate": 3.7215463108320256e-05,
      "loss": 0.6119,
      "step": 1303000
    },
    {
      "epoch": 20.456828885400313,
      "grad_norm": 3.667747735977173,
      "learning_rate": 3.721448194662481e-05,
      "loss": 0.5958,
      "step": 1303100
    },
    {
      "epoch": 20.45839874411303,
      "grad_norm": 3.41092848777771,
      "learning_rate": 3.721350078492936e-05,
      "loss": 0.5902,
      "step": 1303200
    },
    {
      "epoch": 20.459968602825747,
      "grad_norm": 3.2361485958099365,
      "learning_rate": 3.721251962323391e-05,
      "loss": 0.5816,
      "step": 1303300
    },
    {
      "epoch": 20.46153846153846,
      "grad_norm": 4.558144569396973,
      "learning_rate": 3.7211538461538466e-05,
      "loss": 0.6247,
      "step": 1303400
    },
    {
      "epoch": 20.463108320251177,
      "grad_norm": 4.3247857093811035,
      "learning_rate": 3.721055729984302e-05,
      "loss": 0.6272,
      "step": 1303500
    },
    {
      "epoch": 20.464678178963894,
      "grad_norm": 3.7447702884674072,
      "learning_rate": 3.720957613814757e-05,
      "loss": 0.6233,
      "step": 1303600
    },
    {
      "epoch": 20.46624803767661,
      "grad_norm": 3.8004302978515625,
      "learning_rate": 3.720859497645212e-05,
      "loss": 0.563,
      "step": 1303700
    },
    {
      "epoch": 20.467817896389324,
      "grad_norm": 3.4419138431549072,
      "learning_rate": 3.720761381475668e-05,
      "loss": 0.6002,
      "step": 1303800
    },
    {
      "epoch": 20.46938775510204,
      "grad_norm": 3.6118392944335938,
      "learning_rate": 3.720663265306122e-05,
      "loss": 0.6298,
      "step": 1303900
    },
    {
      "epoch": 20.470957613814758,
      "grad_norm": 4.130102634429932,
      "learning_rate": 3.720565149136578e-05,
      "loss": 0.6146,
      "step": 1304000
    },
    {
      "epoch": 20.47252747252747,
      "grad_norm": 4.010310173034668,
      "learning_rate": 3.720467032967033e-05,
      "loss": 0.6144,
      "step": 1304100
    },
    {
      "epoch": 20.474097331240188,
      "grad_norm": 3.646200656890869,
      "learning_rate": 3.720368916797489e-05,
      "loss": 0.5953,
      "step": 1304200
    },
    {
      "epoch": 20.475667189952905,
      "grad_norm": 3.45068097114563,
      "learning_rate": 3.720270800627943e-05,
      "loss": 0.6252,
      "step": 1304300
    },
    {
      "epoch": 20.47723704866562,
      "grad_norm": 4.176631927490234,
      "learning_rate": 3.720172684458399e-05,
      "loss": 0.5864,
      "step": 1304400
    },
    {
      "epoch": 20.478806907378335,
      "grad_norm": 4.5550537109375,
      "learning_rate": 3.720074568288854e-05,
      "loss": 0.656,
      "step": 1304500
    },
    {
      "epoch": 20.48037676609105,
      "grad_norm": 4.307987213134766,
      "learning_rate": 3.719976452119309e-05,
      "loss": 0.5694,
      "step": 1304600
    },
    {
      "epoch": 20.48194662480377,
      "grad_norm": 3.844461679458618,
      "learning_rate": 3.719878335949765e-05,
      "loss": 0.6719,
      "step": 1304700
    },
    {
      "epoch": 20.483516483516482,
      "grad_norm": 4.68532133102417,
      "learning_rate": 3.71978021978022e-05,
      "loss": 0.57,
      "step": 1304800
    },
    {
      "epoch": 20.4850863422292,
      "grad_norm": 3.9274940490722656,
      "learning_rate": 3.719682103610676e-05,
      "loss": 0.5488,
      "step": 1304900
    },
    {
      "epoch": 20.486656200941916,
      "grad_norm": 3.644120216369629,
      "learning_rate": 3.71958398744113e-05,
      "loss": 0.6368,
      "step": 1305000
    },
    {
      "epoch": 20.488226059654632,
      "grad_norm": 3.492115020751953,
      "learning_rate": 3.719485871271586e-05,
      "loss": 0.576,
      "step": 1305100
    },
    {
      "epoch": 20.489795918367346,
      "grad_norm": 3.5146713256835938,
      "learning_rate": 3.719387755102041e-05,
      "loss": 0.5752,
      "step": 1305200
    },
    {
      "epoch": 20.491365777080063,
      "grad_norm": 2.9117069244384766,
      "learning_rate": 3.719289638932496e-05,
      "loss": 0.6089,
      "step": 1305300
    },
    {
      "epoch": 20.49293563579278,
      "grad_norm": 2.277632713317871,
      "learning_rate": 3.719191522762951e-05,
      "loss": 0.5913,
      "step": 1305400
    },
    {
      "epoch": 20.494505494505496,
      "grad_norm": 3.5535430908203125,
      "learning_rate": 3.719093406593407e-05,
      "loss": 0.565,
      "step": 1305500
    },
    {
      "epoch": 20.49607535321821,
      "grad_norm": 4.162916660308838,
      "learning_rate": 3.718995290423862e-05,
      "loss": 0.607,
      "step": 1305600
    },
    {
      "epoch": 20.497645211930926,
      "grad_norm": 4.064634799957275,
      "learning_rate": 3.718897174254317e-05,
      "loss": 0.6038,
      "step": 1305700
    },
    {
      "epoch": 20.499215070643643,
      "grad_norm": 4.268766403198242,
      "learning_rate": 3.7187990580847724e-05,
      "loss": 0.6134,
      "step": 1305800
    },
    {
      "epoch": 20.500784929356357,
      "grad_norm": 3.702382802963257,
      "learning_rate": 3.718700941915228e-05,
      "loss": 0.6061,
      "step": 1305900
    },
    {
      "epoch": 20.502354788069074,
      "grad_norm": 3.9859962463378906,
      "learning_rate": 3.7186028257456826e-05,
      "loss": 0.5766,
      "step": 1306000
    },
    {
      "epoch": 20.50392464678179,
      "grad_norm": 3.981647491455078,
      "learning_rate": 3.7185047095761384e-05,
      "loss": 0.6239,
      "step": 1306100
    },
    {
      "epoch": 20.505494505494504,
      "grad_norm": 4.823580741882324,
      "learning_rate": 3.7184065934065934e-05,
      "loss": 0.6386,
      "step": 1306200
    },
    {
      "epoch": 20.50706436420722,
      "grad_norm": 3.4227335453033447,
      "learning_rate": 3.718308477237049e-05,
      "loss": 0.6256,
      "step": 1306300
    },
    {
      "epoch": 20.508634222919937,
      "grad_norm": 3.682300090789795,
      "learning_rate": 3.7182103610675036e-05,
      "loss": 0.5717,
      "step": 1306400
    },
    {
      "epoch": 20.510204081632654,
      "grad_norm": 3.1473629474639893,
      "learning_rate": 3.7181122448979594e-05,
      "loss": 0.595,
      "step": 1306500
    },
    {
      "epoch": 20.511773940345368,
      "grad_norm": 3.218315839767456,
      "learning_rate": 3.7180141287284145e-05,
      "loss": 0.6152,
      "step": 1306600
    },
    {
      "epoch": 20.513343799058084,
      "grad_norm": 5.086394786834717,
      "learning_rate": 3.7179160125588696e-05,
      "loss": 0.6158,
      "step": 1306700
    },
    {
      "epoch": 20.5149136577708,
      "grad_norm": 2.765285015106201,
      "learning_rate": 3.7178178963893254e-05,
      "loss": 0.5645,
      "step": 1306800
    },
    {
      "epoch": 20.516483516483518,
      "grad_norm": 3.034738063812256,
      "learning_rate": 3.7177197802197805e-05,
      "loss": 0.5947,
      "step": 1306900
    },
    {
      "epoch": 20.51805337519623,
      "grad_norm": 4.7790374755859375,
      "learning_rate": 3.717621664050236e-05,
      "loss": 0.591,
      "step": 1307000
    },
    {
      "epoch": 20.51962323390895,
      "grad_norm": 3.575969696044922,
      "learning_rate": 3.717523547880691e-05,
      "loss": 0.6354,
      "step": 1307100
    },
    {
      "epoch": 20.521193092621665,
      "grad_norm": 4.423664569854736,
      "learning_rate": 3.7174254317111465e-05,
      "loss": 0.6035,
      "step": 1307200
    },
    {
      "epoch": 20.52276295133438,
      "grad_norm": 3.195819139480591,
      "learning_rate": 3.7173273155416016e-05,
      "loss": 0.5749,
      "step": 1307300
    },
    {
      "epoch": 20.524332810047095,
      "grad_norm": 3.435478687286377,
      "learning_rate": 3.7172291993720567e-05,
      "loss": 0.6114,
      "step": 1307400
    },
    {
      "epoch": 20.525902668759812,
      "grad_norm": 3.318600654602051,
      "learning_rate": 3.717131083202512e-05,
      "loss": 0.573,
      "step": 1307500
    },
    {
      "epoch": 20.52747252747253,
      "grad_norm": 4.179750919342041,
      "learning_rate": 3.7170329670329675e-05,
      "loss": 0.5819,
      "step": 1307600
    },
    {
      "epoch": 20.529042386185242,
      "grad_norm": 3.796828508377075,
      "learning_rate": 3.7169348508634226e-05,
      "loss": 0.6044,
      "step": 1307700
    },
    {
      "epoch": 20.53061224489796,
      "grad_norm": 2.9366631507873535,
      "learning_rate": 3.716836734693878e-05,
      "loss": 0.5971,
      "step": 1307800
    },
    {
      "epoch": 20.532182103610676,
      "grad_norm": 2.2334790229797363,
      "learning_rate": 3.716738618524333e-05,
      "loss": 0.627,
      "step": 1307900
    },
    {
      "epoch": 20.53375196232339,
      "grad_norm": 3.8991942405700684,
      "learning_rate": 3.7166405023547886e-05,
      "loss": 0.6097,
      "step": 1308000
    },
    {
      "epoch": 20.535321821036106,
      "grad_norm": 4.137880802154541,
      "learning_rate": 3.716542386185243e-05,
      "loss": 0.6134,
      "step": 1308100
    },
    {
      "epoch": 20.536891679748823,
      "grad_norm": 4.269184589385986,
      "learning_rate": 3.716444270015699e-05,
      "loss": 0.6271,
      "step": 1308200
    },
    {
      "epoch": 20.53846153846154,
      "grad_norm": 3.2657225131988525,
      "learning_rate": 3.716346153846154e-05,
      "loss": 0.6195,
      "step": 1308300
    },
    {
      "epoch": 20.540031397174253,
      "grad_norm": 3.134624481201172,
      "learning_rate": 3.716248037676609e-05,
      "loss": 0.5958,
      "step": 1308400
    },
    {
      "epoch": 20.54160125588697,
      "grad_norm": 2.5665085315704346,
      "learning_rate": 3.716149921507064e-05,
      "loss": 0.5835,
      "step": 1308500
    },
    {
      "epoch": 20.543171114599687,
      "grad_norm": 3.695495843887329,
      "learning_rate": 3.71605180533752e-05,
      "loss": 0.6124,
      "step": 1308600
    },
    {
      "epoch": 20.5447409733124,
      "grad_norm": 4.141228675842285,
      "learning_rate": 3.715953689167975e-05,
      "loss": 0.6201,
      "step": 1308700
    },
    {
      "epoch": 20.546310832025117,
      "grad_norm": 4.592745304107666,
      "learning_rate": 3.71585557299843e-05,
      "loss": 0.6234,
      "step": 1308800
    },
    {
      "epoch": 20.547880690737834,
      "grad_norm": 2.976799964904785,
      "learning_rate": 3.715757456828886e-05,
      "loss": 0.6234,
      "step": 1308900
    },
    {
      "epoch": 20.54945054945055,
      "grad_norm": 4.094064712524414,
      "learning_rate": 3.715659340659341e-05,
      "loss": 0.6163,
      "step": 1309000
    },
    {
      "epoch": 20.551020408163264,
      "grad_norm": 3.4291837215423584,
      "learning_rate": 3.715561224489796e-05,
      "loss": 0.5859,
      "step": 1309100
    },
    {
      "epoch": 20.55259026687598,
      "grad_norm": 4.608650207519531,
      "learning_rate": 3.715463108320251e-05,
      "loss": 0.6011,
      "step": 1309200
    },
    {
      "epoch": 20.554160125588698,
      "grad_norm": 3.8372762203216553,
      "learning_rate": 3.715364992150707e-05,
      "loss": 0.6299,
      "step": 1309300
    },
    {
      "epoch": 20.55572998430141,
      "grad_norm": 3.112779378890991,
      "learning_rate": 3.715266875981162e-05,
      "loss": 0.6129,
      "step": 1309400
    },
    {
      "epoch": 20.55729984301413,
      "grad_norm": 3.2946231365203857,
      "learning_rate": 3.715168759811617e-05,
      "loss": 0.583,
      "step": 1309500
    },
    {
      "epoch": 20.558869701726845,
      "grad_norm": 4.4097514152526855,
      "learning_rate": 3.715070643642072e-05,
      "loss": 0.6138,
      "step": 1309600
    },
    {
      "epoch": 20.560439560439562,
      "grad_norm": 2.8878910541534424,
      "learning_rate": 3.714972527472528e-05,
      "loss": 0.6002,
      "step": 1309700
    },
    {
      "epoch": 20.562009419152275,
      "grad_norm": 2.5098304748535156,
      "learning_rate": 3.7148744113029824e-05,
      "loss": 0.6123,
      "step": 1309800
    },
    {
      "epoch": 20.563579277864992,
      "grad_norm": 3.6769988536834717,
      "learning_rate": 3.714776295133438e-05,
      "loss": 0.5946,
      "step": 1309900
    },
    {
      "epoch": 20.56514913657771,
      "grad_norm": 3.6266894340515137,
      "learning_rate": 3.714678178963893e-05,
      "loss": 0.6147,
      "step": 1310000
    },
    {
      "epoch": 20.566718995290422,
      "grad_norm": 1.9824540615081787,
      "learning_rate": 3.714580062794349e-05,
      "loss": 0.6169,
      "step": 1310100
    },
    {
      "epoch": 20.56828885400314,
      "grad_norm": 3.150857925415039,
      "learning_rate": 3.7144819466248035e-05,
      "loss": 0.5962,
      "step": 1310200
    },
    {
      "epoch": 20.569858712715856,
      "grad_norm": 4.011586666107178,
      "learning_rate": 3.714383830455259e-05,
      "loss": 0.626,
      "step": 1310300
    },
    {
      "epoch": 20.571428571428573,
      "grad_norm": 2.836862087249756,
      "learning_rate": 3.7142857142857143e-05,
      "loss": 0.6306,
      "step": 1310400
    },
    {
      "epoch": 20.572998430141286,
      "grad_norm": 4.364623069763184,
      "learning_rate": 3.7141875981161694e-05,
      "loss": 0.6364,
      "step": 1310500
    },
    {
      "epoch": 20.574568288854003,
      "grad_norm": 3.7781341075897217,
      "learning_rate": 3.7140894819466245e-05,
      "loss": 0.5949,
      "step": 1310600
    },
    {
      "epoch": 20.57613814756672,
      "grad_norm": 4.0833892822265625,
      "learning_rate": 3.71399136577708e-05,
      "loss": 0.5833,
      "step": 1310700
    },
    {
      "epoch": 20.577708006279433,
      "grad_norm": 4.764669418334961,
      "learning_rate": 3.7138932496075354e-05,
      "loss": 0.5851,
      "step": 1310800
    },
    {
      "epoch": 20.57927786499215,
      "grad_norm": 4.176788330078125,
      "learning_rate": 3.7137951334379905e-05,
      "loss": 0.6168,
      "step": 1310900
    },
    {
      "epoch": 20.580847723704867,
      "grad_norm": 3.978808641433716,
      "learning_rate": 3.713697017268446e-05,
      "loss": 0.636,
      "step": 1311000
    },
    {
      "epoch": 20.582417582417584,
      "grad_norm": 4.902353763580322,
      "learning_rate": 3.7135989010989014e-05,
      "loss": 0.6037,
      "step": 1311100
    },
    {
      "epoch": 20.583987441130297,
      "grad_norm": 3.1260783672332764,
      "learning_rate": 3.7135007849293565e-05,
      "loss": 0.5966,
      "step": 1311200
    },
    {
      "epoch": 20.585557299843014,
      "grad_norm": 3.949895143508911,
      "learning_rate": 3.7134026687598116e-05,
      "loss": 0.5958,
      "step": 1311300
    },
    {
      "epoch": 20.58712715855573,
      "grad_norm": 3.850196599960327,
      "learning_rate": 3.7133045525902674e-05,
      "loss": 0.5696,
      "step": 1311400
    },
    {
      "epoch": 20.588697017268444,
      "grad_norm": 3.1710522174835205,
      "learning_rate": 3.7132064364207225e-05,
      "loss": 0.6168,
      "step": 1311500
    },
    {
      "epoch": 20.59026687598116,
      "grad_norm": 4.373574256896973,
      "learning_rate": 3.7131083202511776e-05,
      "loss": 0.6143,
      "step": 1311600
    },
    {
      "epoch": 20.591836734693878,
      "grad_norm": 3.604597330093384,
      "learning_rate": 3.7130102040816327e-05,
      "loss": 0.5854,
      "step": 1311700
    },
    {
      "epoch": 20.593406593406595,
      "grad_norm": 3.5632522106170654,
      "learning_rate": 3.7129120879120884e-05,
      "loss": 0.5914,
      "step": 1311800
    },
    {
      "epoch": 20.594976452119308,
      "grad_norm": 4.065102577209473,
      "learning_rate": 3.712813971742543e-05,
      "loss": 0.6295,
      "step": 1311900
    },
    {
      "epoch": 20.596546310832025,
      "grad_norm": 2.6543846130371094,
      "learning_rate": 3.7127158555729986e-05,
      "loss": 0.6099,
      "step": 1312000
    },
    {
      "epoch": 20.598116169544742,
      "grad_norm": 4.861347675323486,
      "learning_rate": 3.712617739403454e-05,
      "loss": 0.5988,
      "step": 1312100
    },
    {
      "epoch": 20.599686028257455,
      "grad_norm": 3.6698365211486816,
      "learning_rate": 3.7125196232339095e-05,
      "loss": 0.6142,
      "step": 1312200
    },
    {
      "epoch": 20.601255886970172,
      "grad_norm": 4.362133979797363,
      "learning_rate": 3.712421507064364e-05,
      "loss": 0.6489,
      "step": 1312300
    },
    {
      "epoch": 20.60282574568289,
      "grad_norm": 4.220186233520508,
      "learning_rate": 3.71232339089482e-05,
      "loss": 0.5944,
      "step": 1312400
    },
    {
      "epoch": 20.604395604395606,
      "grad_norm": 3.5739240646362305,
      "learning_rate": 3.712225274725275e-05,
      "loss": 0.5828,
      "step": 1312500
    },
    {
      "epoch": 20.60596546310832,
      "grad_norm": 3.2043683528900146,
      "learning_rate": 3.71212715855573e-05,
      "loss": 0.6577,
      "step": 1312600
    },
    {
      "epoch": 20.607535321821036,
      "grad_norm": 4.329518795013428,
      "learning_rate": 3.712029042386185e-05,
      "loss": 0.6112,
      "step": 1312700
    },
    {
      "epoch": 20.609105180533753,
      "grad_norm": 4.211597442626953,
      "learning_rate": 3.711930926216641e-05,
      "loss": 0.6059,
      "step": 1312800
    },
    {
      "epoch": 20.610675039246466,
      "grad_norm": 4.889952182769775,
      "learning_rate": 3.711832810047096e-05,
      "loss": 0.589,
      "step": 1312900
    },
    {
      "epoch": 20.612244897959183,
      "grad_norm": 3.7877166271209717,
      "learning_rate": 3.711734693877551e-05,
      "loss": 0.5934,
      "step": 1313000
    },
    {
      "epoch": 20.6138147566719,
      "grad_norm": 3.768686294555664,
      "learning_rate": 3.711636577708007e-05,
      "loss": 0.6003,
      "step": 1313100
    },
    {
      "epoch": 20.615384615384617,
      "grad_norm": 3.539156436920166,
      "learning_rate": 3.711538461538462e-05,
      "loss": 0.5907,
      "step": 1313200
    },
    {
      "epoch": 20.61695447409733,
      "grad_norm": 4.254587173461914,
      "learning_rate": 3.711440345368917e-05,
      "loss": 0.5863,
      "step": 1313300
    },
    {
      "epoch": 20.618524332810047,
      "grad_norm": 4.440286636352539,
      "learning_rate": 3.711342229199372e-05,
      "loss": 0.5673,
      "step": 1313400
    },
    {
      "epoch": 20.620094191522764,
      "grad_norm": 4.22780704498291,
      "learning_rate": 3.711244113029828e-05,
      "loss": 0.609,
      "step": 1313500
    },
    {
      "epoch": 20.621664050235477,
      "grad_norm": 5.5200629234313965,
      "learning_rate": 3.711145996860283e-05,
      "loss": 0.6269,
      "step": 1313600
    },
    {
      "epoch": 20.623233908948194,
      "grad_norm": 4.434651851654053,
      "learning_rate": 3.711047880690738e-05,
      "loss": 0.6264,
      "step": 1313700
    },
    {
      "epoch": 20.62480376766091,
      "grad_norm": 2.9698245525360107,
      "learning_rate": 3.710949764521193e-05,
      "loss": 0.6091,
      "step": 1313800
    },
    {
      "epoch": 20.626373626373628,
      "grad_norm": 5.3097453117370605,
      "learning_rate": 3.710851648351649e-05,
      "loss": 0.6228,
      "step": 1313900
    },
    {
      "epoch": 20.62794348508634,
      "grad_norm": 3.620797872543335,
      "learning_rate": 3.710753532182103e-05,
      "loss": 0.5995,
      "step": 1314000
    },
    {
      "epoch": 20.629513343799058,
      "grad_norm": 2.8887686729431152,
      "learning_rate": 3.710655416012559e-05,
      "loss": 0.5871,
      "step": 1314100
    },
    {
      "epoch": 20.631083202511775,
      "grad_norm": 3.7088921070098877,
      "learning_rate": 3.710557299843014e-05,
      "loss": 0.5851,
      "step": 1314200
    },
    {
      "epoch": 20.632653061224488,
      "grad_norm": 3.7430951595306396,
      "learning_rate": 3.71045918367347e-05,
      "loss": 0.572,
      "step": 1314300
    },
    {
      "epoch": 20.634222919937205,
      "grad_norm": 3.969412326812744,
      "learning_rate": 3.7103610675039244e-05,
      "loss": 0.6393,
      "step": 1314400
    },
    {
      "epoch": 20.635792778649922,
      "grad_norm": 3.8636868000030518,
      "learning_rate": 3.71026295133438e-05,
      "loss": 0.5816,
      "step": 1314500
    },
    {
      "epoch": 20.63736263736264,
      "grad_norm": 3.8178727626800537,
      "learning_rate": 3.710164835164835e-05,
      "loss": 0.6225,
      "step": 1314600
    },
    {
      "epoch": 20.638932496075352,
      "grad_norm": 3.4897077083587646,
      "learning_rate": 3.7100667189952903e-05,
      "loss": 0.6065,
      "step": 1314700
    },
    {
      "epoch": 20.64050235478807,
      "grad_norm": 4.1396613121032715,
      "learning_rate": 3.7099686028257454e-05,
      "loss": 0.5934,
      "step": 1314800
    },
    {
      "epoch": 20.642072213500786,
      "grad_norm": 3.9617762565612793,
      "learning_rate": 3.709870486656201e-05,
      "loss": 0.5958,
      "step": 1314900
    },
    {
      "epoch": 20.643642072213503,
      "grad_norm": 4.643797874450684,
      "learning_rate": 3.709772370486656e-05,
      "loss": 0.6601,
      "step": 1315000
    },
    {
      "epoch": 20.645211930926216,
      "grad_norm": 3.4601986408233643,
      "learning_rate": 3.7096742543171114e-05,
      "loss": 0.6399,
      "step": 1315100
    },
    {
      "epoch": 20.646781789638933,
      "grad_norm": 3.428386688232422,
      "learning_rate": 3.709576138147567e-05,
      "loss": 0.5904,
      "step": 1315200
    },
    {
      "epoch": 20.64835164835165,
      "grad_norm": 3.2520363330841064,
      "learning_rate": 3.709478021978022e-05,
      "loss": 0.6068,
      "step": 1315300
    },
    {
      "epoch": 20.649921507064363,
      "grad_norm": 4.137323379516602,
      "learning_rate": 3.7093799058084774e-05,
      "loss": 0.6106,
      "step": 1315400
    },
    {
      "epoch": 20.65149136577708,
      "grad_norm": 5.102063179016113,
      "learning_rate": 3.7092817896389325e-05,
      "loss": 0.5999,
      "step": 1315500
    },
    {
      "epoch": 20.653061224489797,
      "grad_norm": 3.753147602081299,
      "learning_rate": 3.709183673469388e-05,
      "loss": 0.5804,
      "step": 1315600
    },
    {
      "epoch": 20.65463108320251,
      "grad_norm": 3.449183464050293,
      "learning_rate": 3.7090855572998434e-05,
      "loss": 0.6064,
      "step": 1315700
    },
    {
      "epoch": 20.656200941915227,
      "grad_norm": 3.9559125900268555,
      "learning_rate": 3.7089874411302985e-05,
      "loss": 0.6183,
      "step": 1315800
    },
    {
      "epoch": 20.657770800627944,
      "grad_norm": 3.5028796195983887,
      "learning_rate": 3.7088893249607536e-05,
      "loss": 0.5803,
      "step": 1315900
    },
    {
      "epoch": 20.65934065934066,
      "grad_norm": 3.767895460128784,
      "learning_rate": 3.708791208791209e-05,
      "loss": 0.6026,
      "step": 1316000
    },
    {
      "epoch": 20.660910518053374,
      "grad_norm": 3.823373794555664,
      "learning_rate": 3.708693092621664e-05,
      "loss": 0.6484,
      "step": 1316100
    },
    {
      "epoch": 20.66248037676609,
      "grad_norm": 4.045233726501465,
      "learning_rate": 3.7085949764521195e-05,
      "loss": 0.5792,
      "step": 1316200
    },
    {
      "epoch": 20.664050235478808,
      "grad_norm": 4.563967227935791,
      "learning_rate": 3.7084968602825746e-05,
      "loss": 0.6017,
      "step": 1316300
    },
    {
      "epoch": 20.665620094191524,
      "grad_norm": 4.243650436401367,
      "learning_rate": 3.7083987441130304e-05,
      "loss": 0.59,
      "step": 1316400
    },
    {
      "epoch": 20.667189952904238,
      "grad_norm": 4.348287105560303,
      "learning_rate": 3.708300627943485e-05,
      "loss": 0.6194,
      "step": 1316500
    },
    {
      "epoch": 20.668759811616955,
      "grad_norm": 3.9106247425079346,
      "learning_rate": 3.7082025117739406e-05,
      "loss": 0.5868,
      "step": 1316600
    },
    {
      "epoch": 20.67032967032967,
      "grad_norm": 4.494052886962891,
      "learning_rate": 3.708104395604396e-05,
      "loss": 0.5727,
      "step": 1316700
    },
    {
      "epoch": 20.671899529042385,
      "grad_norm": 3.5707039833068848,
      "learning_rate": 3.708006279434851e-05,
      "loss": 0.6244,
      "step": 1316800
    },
    {
      "epoch": 20.6734693877551,
      "grad_norm": 3.8036856651306152,
      "learning_rate": 3.707908163265306e-05,
      "loss": 0.5835,
      "step": 1316900
    },
    {
      "epoch": 20.67503924646782,
      "grad_norm": 4.131434440612793,
      "learning_rate": 3.707810047095762e-05,
      "loss": 0.5856,
      "step": 1317000
    },
    {
      "epoch": 20.676609105180535,
      "grad_norm": 3.9150607585906982,
      "learning_rate": 3.707711930926217e-05,
      "loss": 0.611,
      "step": 1317100
    },
    {
      "epoch": 20.67817896389325,
      "grad_norm": 3.9153213500976562,
      "learning_rate": 3.707613814756672e-05,
      "loss": 0.5866,
      "step": 1317200
    },
    {
      "epoch": 20.679748822605966,
      "grad_norm": 3.60629940032959,
      "learning_rate": 3.7075156985871276e-05,
      "loss": 0.6159,
      "step": 1317300
    },
    {
      "epoch": 20.681318681318682,
      "grad_norm": 3.7662696838378906,
      "learning_rate": 3.707417582417583e-05,
      "loss": 0.5974,
      "step": 1317400
    },
    {
      "epoch": 20.682888540031396,
      "grad_norm": 4.035317897796631,
      "learning_rate": 3.707319466248038e-05,
      "loss": 0.6194,
      "step": 1317500
    },
    {
      "epoch": 20.684458398744113,
      "grad_norm": 5.42805814743042,
      "learning_rate": 3.707221350078493e-05,
      "loss": 0.6266,
      "step": 1317600
    },
    {
      "epoch": 20.68602825745683,
      "grad_norm": 3.3233888149261475,
      "learning_rate": 3.707123233908949e-05,
      "loss": 0.599,
      "step": 1317700
    },
    {
      "epoch": 20.687598116169546,
      "grad_norm": 3.7252471446990967,
      "learning_rate": 3.707025117739404e-05,
      "loss": 0.6142,
      "step": 1317800
    },
    {
      "epoch": 20.68916797488226,
      "grad_norm": 4.013424873352051,
      "learning_rate": 3.706927001569859e-05,
      "loss": 0.6315,
      "step": 1317900
    },
    {
      "epoch": 20.690737833594977,
      "grad_norm": 3.977269172668457,
      "learning_rate": 3.706828885400314e-05,
      "loss": 0.5924,
      "step": 1318000
    },
    {
      "epoch": 20.692307692307693,
      "grad_norm": 4.290365219116211,
      "learning_rate": 3.70673076923077e-05,
      "loss": 0.6067,
      "step": 1318100
    },
    {
      "epoch": 20.693877551020407,
      "grad_norm": 4.4408135414123535,
      "learning_rate": 3.706632653061224e-05,
      "loss": 0.5909,
      "step": 1318200
    },
    {
      "epoch": 20.695447409733124,
      "grad_norm": 3.391244888305664,
      "learning_rate": 3.70653453689168e-05,
      "loss": 0.6215,
      "step": 1318300
    },
    {
      "epoch": 20.69701726844584,
      "grad_norm": 2.8103606700897217,
      "learning_rate": 3.706436420722135e-05,
      "loss": 0.6295,
      "step": 1318400
    },
    {
      "epoch": 20.698587127158557,
      "grad_norm": 3.713170051574707,
      "learning_rate": 3.706338304552591e-05,
      "loss": 0.5917,
      "step": 1318500
    },
    {
      "epoch": 20.70015698587127,
      "grad_norm": 3.207050085067749,
      "learning_rate": 3.706240188383045e-05,
      "loss": 0.6136,
      "step": 1318600
    },
    {
      "epoch": 20.701726844583987,
      "grad_norm": 2.9453299045562744,
      "learning_rate": 3.706142072213501e-05,
      "loss": 0.6096,
      "step": 1318700
    },
    {
      "epoch": 20.703296703296704,
      "grad_norm": 4.16970682144165,
      "learning_rate": 3.706043956043956e-05,
      "loss": 0.6323,
      "step": 1318800
    },
    {
      "epoch": 20.704866562009418,
      "grad_norm": 3.6075704097747803,
      "learning_rate": 3.705945839874411e-05,
      "loss": 0.5849,
      "step": 1318900
    },
    {
      "epoch": 20.706436420722135,
      "grad_norm": 4.034801959991455,
      "learning_rate": 3.7058477237048663e-05,
      "loss": 0.5781,
      "step": 1319000
    },
    {
      "epoch": 20.70800627943485,
      "grad_norm": 3.698395013809204,
      "learning_rate": 3.705749607535322e-05,
      "loss": 0.6499,
      "step": 1319100
    },
    {
      "epoch": 20.70957613814757,
      "grad_norm": 4.480855464935303,
      "learning_rate": 3.705651491365777e-05,
      "loss": 0.6131,
      "step": 1319200
    },
    {
      "epoch": 20.71114599686028,
      "grad_norm": 5.521316051483154,
      "learning_rate": 3.705553375196232e-05,
      "loss": 0.6485,
      "step": 1319300
    },
    {
      "epoch": 20.712715855573,
      "grad_norm": 3.624650239944458,
      "learning_rate": 3.705455259026688e-05,
      "loss": 0.6359,
      "step": 1319400
    },
    {
      "epoch": 20.714285714285715,
      "grad_norm": 2.514633893966675,
      "learning_rate": 3.705357142857143e-05,
      "loss": 0.623,
      "step": 1319500
    },
    {
      "epoch": 20.71585557299843,
      "grad_norm": 3.3273448944091797,
      "learning_rate": 3.705259026687598e-05,
      "loss": 0.5754,
      "step": 1319600
    },
    {
      "epoch": 20.717425431711145,
      "grad_norm": 3.8486440181732178,
      "learning_rate": 3.7051609105180534e-05,
      "loss": 0.6222,
      "step": 1319700
    },
    {
      "epoch": 20.718995290423862,
      "grad_norm": 4.218324184417725,
      "learning_rate": 3.705062794348509e-05,
      "loss": 0.6078,
      "step": 1319800
    },
    {
      "epoch": 20.72056514913658,
      "grad_norm": 4.547643184661865,
      "learning_rate": 3.704964678178964e-05,
      "loss": 0.5969,
      "step": 1319900
    },
    {
      "epoch": 20.722135007849293,
      "grad_norm": 3.6145384311676025,
      "learning_rate": 3.7048665620094194e-05,
      "loss": 0.5844,
      "step": 1320000
    },
    {
      "epoch": 20.72370486656201,
      "grad_norm": 4.441208839416504,
      "learning_rate": 3.7047684458398745e-05,
      "loss": 0.6168,
      "step": 1320100
    },
    {
      "epoch": 20.725274725274726,
      "grad_norm": 4.271193981170654,
      "learning_rate": 3.70467032967033e-05,
      "loss": 0.6126,
      "step": 1320200
    },
    {
      "epoch": 20.72684458398744,
      "grad_norm": 3.0969972610473633,
      "learning_rate": 3.7045722135007846e-05,
      "loss": 0.571,
      "step": 1320300
    },
    {
      "epoch": 20.728414442700156,
      "grad_norm": 3.5571391582489014,
      "learning_rate": 3.7044740973312404e-05,
      "loss": 0.5941,
      "step": 1320400
    },
    {
      "epoch": 20.729984301412873,
      "grad_norm": 4.3140668869018555,
      "learning_rate": 3.7043759811616955e-05,
      "loss": 0.6425,
      "step": 1320500
    },
    {
      "epoch": 20.73155416012559,
      "grad_norm": 4.639063358306885,
      "learning_rate": 3.704277864992151e-05,
      "loss": 0.6327,
      "step": 1320600
    },
    {
      "epoch": 20.733124018838303,
      "grad_norm": 3.8594489097595215,
      "learning_rate": 3.704179748822606e-05,
      "loss": 0.5998,
      "step": 1320700
    },
    {
      "epoch": 20.73469387755102,
      "grad_norm": 2.564481258392334,
      "learning_rate": 3.7040816326530615e-05,
      "loss": 0.6019,
      "step": 1320800
    },
    {
      "epoch": 20.736263736263737,
      "grad_norm": 2.2380428314208984,
      "learning_rate": 3.7039835164835166e-05,
      "loss": 0.6091,
      "step": 1320900
    },
    {
      "epoch": 20.73783359497645,
      "grad_norm": 4.825990200042725,
      "learning_rate": 3.703885400313972e-05,
      "loss": 0.5664,
      "step": 1321000
    },
    {
      "epoch": 20.739403453689167,
      "grad_norm": 3.8352980613708496,
      "learning_rate": 3.703787284144427e-05,
      "loss": 0.5963,
      "step": 1321100
    },
    {
      "epoch": 20.740973312401884,
      "grad_norm": 3.6536951065063477,
      "learning_rate": 3.7036891679748826e-05,
      "loss": 0.6172,
      "step": 1321200
    },
    {
      "epoch": 20.7425431711146,
      "grad_norm": 3.584937334060669,
      "learning_rate": 3.703591051805338e-05,
      "loss": 0.5912,
      "step": 1321300
    },
    {
      "epoch": 20.744113029827314,
      "grad_norm": 4.261327743530273,
      "learning_rate": 3.703492935635793e-05,
      "loss": 0.5864,
      "step": 1321400
    },
    {
      "epoch": 20.74568288854003,
      "grad_norm": 3.0202956199645996,
      "learning_rate": 3.7033948194662485e-05,
      "loss": 0.5892,
      "step": 1321500
    },
    {
      "epoch": 20.747252747252748,
      "grad_norm": 4.385462760925293,
      "learning_rate": 3.7032967032967036e-05,
      "loss": 0.6073,
      "step": 1321600
    },
    {
      "epoch": 20.74882260596546,
      "grad_norm": 4.050488471984863,
      "learning_rate": 3.703198587127159e-05,
      "loss": 0.6071,
      "step": 1321700
    },
    {
      "epoch": 20.75039246467818,
      "grad_norm": 4.841207981109619,
      "learning_rate": 3.703100470957614e-05,
      "loss": 0.6022,
      "step": 1321800
    },
    {
      "epoch": 20.751962323390895,
      "grad_norm": 4.697842597961426,
      "learning_rate": 3.7030023547880696e-05,
      "loss": 0.6179,
      "step": 1321900
    },
    {
      "epoch": 20.753532182103612,
      "grad_norm": 2.2209720611572266,
      "learning_rate": 3.702904238618525e-05,
      "loss": 0.5994,
      "step": 1322000
    },
    {
      "epoch": 20.755102040816325,
      "grad_norm": 4.0992865562438965,
      "learning_rate": 3.70280612244898e-05,
      "loss": 0.5588,
      "step": 1322100
    },
    {
      "epoch": 20.756671899529042,
      "grad_norm": 3.335301637649536,
      "learning_rate": 3.702708006279435e-05,
      "loss": 0.5998,
      "step": 1322200
    },
    {
      "epoch": 20.75824175824176,
      "grad_norm": 3.4724340438842773,
      "learning_rate": 3.702609890109891e-05,
      "loss": 0.5968,
      "step": 1322300
    },
    {
      "epoch": 20.759811616954472,
      "grad_norm": 2.4894704818725586,
      "learning_rate": 3.702511773940345e-05,
      "loss": 0.5584,
      "step": 1322400
    },
    {
      "epoch": 20.76138147566719,
      "grad_norm": 4.393025875091553,
      "learning_rate": 3.702413657770801e-05,
      "loss": 0.6222,
      "step": 1322500
    },
    {
      "epoch": 20.762951334379906,
      "grad_norm": 3.9663190841674805,
      "learning_rate": 3.702315541601256e-05,
      "loss": 0.6605,
      "step": 1322600
    },
    {
      "epoch": 20.764521193092623,
      "grad_norm": 3.398738384246826,
      "learning_rate": 3.702217425431712e-05,
      "loss": 0.6239,
      "step": 1322700
    },
    {
      "epoch": 20.766091051805336,
      "grad_norm": 3.757807493209839,
      "learning_rate": 3.702119309262166e-05,
      "loss": 0.609,
      "step": 1322800
    },
    {
      "epoch": 20.767660910518053,
      "grad_norm": 3.3565635681152344,
      "learning_rate": 3.702021193092622e-05,
      "loss": 0.6641,
      "step": 1322900
    },
    {
      "epoch": 20.76923076923077,
      "grad_norm": 4.092781066894531,
      "learning_rate": 3.701923076923077e-05,
      "loss": 0.6046,
      "step": 1323000
    },
    {
      "epoch": 20.770800627943487,
      "grad_norm": 4.103079319000244,
      "learning_rate": 3.701824960753532e-05,
      "loss": 0.6024,
      "step": 1323100
    },
    {
      "epoch": 20.7723704866562,
      "grad_norm": 7.301079750061035,
      "learning_rate": 3.701726844583987e-05,
      "loss": 0.6102,
      "step": 1323200
    },
    {
      "epoch": 20.773940345368917,
      "grad_norm": 4.082769393920898,
      "learning_rate": 3.701628728414443e-05,
      "loss": 0.5817,
      "step": 1323300
    },
    {
      "epoch": 20.775510204081634,
      "grad_norm": 3.3882462978363037,
      "learning_rate": 3.701530612244898e-05,
      "loss": 0.6377,
      "step": 1323400
    },
    {
      "epoch": 20.777080062794347,
      "grad_norm": 3.765099048614502,
      "learning_rate": 3.701432496075353e-05,
      "loss": 0.6336,
      "step": 1323500
    },
    {
      "epoch": 20.778649921507064,
      "grad_norm": 4.529642105102539,
      "learning_rate": 3.701334379905809e-05,
      "loss": 0.6128,
      "step": 1323600
    },
    {
      "epoch": 20.78021978021978,
      "grad_norm": 2.8906500339508057,
      "learning_rate": 3.701236263736264e-05,
      "loss": 0.5935,
      "step": 1323700
    },
    {
      "epoch": 20.781789638932494,
      "grad_norm": 3.6028664112091064,
      "learning_rate": 3.701138147566719e-05,
      "loss": 0.6133,
      "step": 1323800
    },
    {
      "epoch": 20.78335949764521,
      "grad_norm": 4.148712635040283,
      "learning_rate": 3.701040031397174e-05,
      "loss": 0.6083,
      "step": 1323900
    },
    {
      "epoch": 20.784929356357928,
      "grad_norm": 3.0636825561523438,
      "learning_rate": 3.70094191522763e-05,
      "loss": 0.5903,
      "step": 1324000
    },
    {
      "epoch": 20.786499215070645,
      "grad_norm": 2.5995843410491943,
      "learning_rate": 3.700843799058085e-05,
      "loss": 0.5943,
      "step": 1324100
    },
    {
      "epoch": 20.788069073783358,
      "grad_norm": 4.2170634269714355,
      "learning_rate": 3.70074568288854e-05,
      "loss": 0.5639,
      "step": 1324200
    },
    {
      "epoch": 20.789638932496075,
      "grad_norm": 4.114895820617676,
      "learning_rate": 3.7006475667189954e-05,
      "loss": 0.6327,
      "step": 1324300
    },
    {
      "epoch": 20.791208791208792,
      "grad_norm": 3.245769500732422,
      "learning_rate": 3.700549450549451e-05,
      "loss": 0.5962,
      "step": 1324400
    },
    {
      "epoch": 20.79277864992151,
      "grad_norm": 2.470649003982544,
      "learning_rate": 3.7004513343799055e-05,
      "loss": 0.5857,
      "step": 1324500
    },
    {
      "epoch": 20.794348508634222,
      "grad_norm": 3.793280839920044,
      "learning_rate": 3.700353218210361e-05,
      "loss": 0.6324,
      "step": 1324600
    },
    {
      "epoch": 20.79591836734694,
      "grad_norm": 4.39627742767334,
      "learning_rate": 3.7002551020408164e-05,
      "loss": 0.5883,
      "step": 1324700
    },
    {
      "epoch": 20.797488226059656,
      "grad_norm": 4.499181270599365,
      "learning_rate": 3.700156985871272e-05,
      "loss": 0.562,
      "step": 1324800
    },
    {
      "epoch": 20.79905808477237,
      "grad_norm": 3.041501760482788,
      "learning_rate": 3.7000588697017266e-05,
      "loss": 0.6127,
      "step": 1324900
    },
    {
      "epoch": 20.800627943485086,
      "grad_norm": 3.5716233253479004,
      "learning_rate": 3.6999607535321824e-05,
      "loss": 0.6598,
      "step": 1325000
    },
    {
      "epoch": 20.802197802197803,
      "grad_norm": 3.304676055908203,
      "learning_rate": 3.6998626373626375e-05,
      "loss": 0.6189,
      "step": 1325100
    },
    {
      "epoch": 20.80376766091052,
      "grad_norm": 3.646613359451294,
      "learning_rate": 3.6997645211930926e-05,
      "loss": 0.6053,
      "step": 1325200
    },
    {
      "epoch": 20.805337519623233,
      "grad_norm": 3.6913490295410156,
      "learning_rate": 3.699666405023548e-05,
      "loss": 0.5835,
      "step": 1325300
    },
    {
      "epoch": 20.80690737833595,
      "grad_norm": 2.9456496238708496,
      "learning_rate": 3.6995682888540035e-05,
      "loss": 0.6012,
      "step": 1325400
    },
    {
      "epoch": 20.808477237048667,
      "grad_norm": 4.238808631896973,
      "learning_rate": 3.6994701726844586e-05,
      "loss": 0.6041,
      "step": 1325500
    },
    {
      "epoch": 20.81004709576138,
      "grad_norm": 2.4786806106567383,
      "learning_rate": 3.6993720565149137e-05,
      "loss": 0.6249,
      "step": 1325600
    },
    {
      "epoch": 20.811616954474097,
      "grad_norm": 3.5158300399780273,
      "learning_rate": 3.6992739403453694e-05,
      "loss": 0.6122,
      "step": 1325700
    },
    {
      "epoch": 20.813186813186814,
      "grad_norm": 3.9974465370178223,
      "learning_rate": 3.6991758241758245e-05,
      "loss": 0.6315,
      "step": 1325800
    },
    {
      "epoch": 20.81475667189953,
      "grad_norm": 4.078076362609863,
      "learning_rate": 3.6990777080062796e-05,
      "loss": 0.5946,
      "step": 1325900
    },
    {
      "epoch": 20.816326530612244,
      "grad_norm": 3.2803690433502197,
      "learning_rate": 3.698979591836735e-05,
      "loss": 0.6102,
      "step": 1326000
    },
    {
      "epoch": 20.81789638932496,
      "grad_norm": 4.587934970855713,
      "learning_rate": 3.6988814756671905e-05,
      "loss": 0.5872,
      "step": 1326100
    },
    {
      "epoch": 20.819466248037678,
      "grad_norm": 4.301013946533203,
      "learning_rate": 3.6987833594976456e-05,
      "loss": 0.6049,
      "step": 1326200
    },
    {
      "epoch": 20.82103610675039,
      "grad_norm": 4.078636169433594,
      "learning_rate": 3.698685243328101e-05,
      "loss": 0.6087,
      "step": 1326300
    },
    {
      "epoch": 20.822605965463108,
      "grad_norm": 4.348320960998535,
      "learning_rate": 3.698587127158556e-05,
      "loss": 0.6362,
      "step": 1326400
    },
    {
      "epoch": 20.824175824175825,
      "grad_norm": 3.7087864875793457,
      "learning_rate": 3.6984890109890116e-05,
      "loss": 0.6122,
      "step": 1326500
    },
    {
      "epoch": 20.82574568288854,
      "grad_norm": 2.8927969932556152,
      "learning_rate": 3.698390894819466e-05,
      "loss": 0.59,
      "step": 1326600
    },
    {
      "epoch": 20.827315541601255,
      "grad_norm": 3.4120359420776367,
      "learning_rate": 3.698292778649922e-05,
      "loss": 0.6021,
      "step": 1326700
    },
    {
      "epoch": 20.828885400313972,
      "grad_norm": 3.821150779724121,
      "learning_rate": 3.698194662480377e-05,
      "loss": 0.6282,
      "step": 1326800
    },
    {
      "epoch": 20.83045525902669,
      "grad_norm": 3.198338031768799,
      "learning_rate": 3.6980965463108326e-05,
      "loss": 0.5968,
      "step": 1326900
    },
    {
      "epoch": 20.832025117739402,
      "grad_norm": 4.802027702331543,
      "learning_rate": 3.697998430141287e-05,
      "loss": 0.5992,
      "step": 1327000
    },
    {
      "epoch": 20.83359497645212,
      "grad_norm": 3.7404227256774902,
      "learning_rate": 3.697900313971743e-05,
      "loss": 0.632,
      "step": 1327100
    },
    {
      "epoch": 20.835164835164836,
      "grad_norm": 3.737469434738159,
      "learning_rate": 3.697802197802198e-05,
      "loss": 0.6422,
      "step": 1327200
    },
    {
      "epoch": 20.836734693877553,
      "grad_norm": 3.7507686614990234,
      "learning_rate": 3.697704081632653e-05,
      "loss": 0.6019,
      "step": 1327300
    },
    {
      "epoch": 20.838304552590266,
      "grad_norm": 3.288843870162964,
      "learning_rate": 3.697605965463108e-05,
      "loss": 0.6208,
      "step": 1327400
    },
    {
      "epoch": 20.839874411302983,
      "grad_norm": 3.973883628845215,
      "learning_rate": 3.697507849293564e-05,
      "loss": 0.5982,
      "step": 1327500
    },
    {
      "epoch": 20.8414442700157,
      "grad_norm": 4.110309600830078,
      "learning_rate": 3.697409733124019e-05,
      "loss": 0.6063,
      "step": 1327600
    },
    {
      "epoch": 20.843014128728413,
      "grad_norm": 3.451974868774414,
      "learning_rate": 3.697311616954474e-05,
      "loss": 0.6109,
      "step": 1327700
    },
    {
      "epoch": 20.84458398744113,
      "grad_norm": 4.818766117095947,
      "learning_rate": 3.69721350078493e-05,
      "loss": 0.6547,
      "step": 1327800
    },
    {
      "epoch": 20.846153846153847,
      "grad_norm": 3.0907928943634033,
      "learning_rate": 3.697115384615385e-05,
      "loss": 0.5903,
      "step": 1327900
    },
    {
      "epoch": 20.847723704866564,
      "grad_norm": 3.509953260421753,
      "learning_rate": 3.69701726844584e-05,
      "loss": 0.6298,
      "step": 1328000
    },
    {
      "epoch": 20.849293563579277,
      "grad_norm": 3.5168871879577637,
      "learning_rate": 3.696919152276295e-05,
      "loss": 0.5809,
      "step": 1328100
    },
    {
      "epoch": 20.850863422291994,
      "grad_norm": 3.691420793533325,
      "learning_rate": 3.696821036106751e-05,
      "loss": 0.6096,
      "step": 1328200
    },
    {
      "epoch": 20.85243328100471,
      "grad_norm": 4.480794429779053,
      "learning_rate": 3.696722919937206e-05,
      "loss": 0.5845,
      "step": 1328300
    },
    {
      "epoch": 20.854003139717424,
      "grad_norm": 2.482173442840576,
      "learning_rate": 3.696624803767661e-05,
      "loss": 0.5774,
      "step": 1328400
    },
    {
      "epoch": 20.85557299843014,
      "grad_norm": 2.736142635345459,
      "learning_rate": 3.696526687598116e-05,
      "loss": 0.5896,
      "step": 1328500
    },
    {
      "epoch": 20.857142857142858,
      "grad_norm": 3.6264846324920654,
      "learning_rate": 3.696428571428572e-05,
      "loss": 0.5948,
      "step": 1328600
    },
    {
      "epoch": 20.858712715855575,
      "grad_norm": 4.107805252075195,
      "learning_rate": 3.6963304552590264e-05,
      "loss": 0.5836,
      "step": 1328700
    },
    {
      "epoch": 20.860282574568288,
      "grad_norm": 3.727832317352295,
      "learning_rate": 3.696232339089482e-05,
      "loss": 0.6151,
      "step": 1328800
    },
    {
      "epoch": 20.861852433281005,
      "grad_norm": 2.767914295196533,
      "learning_rate": 3.696134222919937e-05,
      "loss": 0.6357,
      "step": 1328900
    },
    {
      "epoch": 20.86342229199372,
      "grad_norm": 4.267704486846924,
      "learning_rate": 3.696036106750393e-05,
      "loss": 0.6232,
      "step": 1329000
    },
    {
      "epoch": 20.864992150706435,
      "grad_norm": 3.3971660137176514,
      "learning_rate": 3.6959379905808475e-05,
      "loss": 0.6149,
      "step": 1329100
    },
    {
      "epoch": 20.86656200941915,
      "grad_norm": 3.8682751655578613,
      "learning_rate": 3.695839874411303e-05,
      "loss": 0.6338,
      "step": 1329200
    },
    {
      "epoch": 20.86813186813187,
      "grad_norm": 3.337388515472412,
      "learning_rate": 3.6957417582417584e-05,
      "loss": 0.5955,
      "step": 1329300
    },
    {
      "epoch": 20.869701726844585,
      "grad_norm": 3.696990489959717,
      "learning_rate": 3.6956436420722135e-05,
      "loss": 0.615,
      "step": 1329400
    },
    {
      "epoch": 20.8712715855573,
      "grad_norm": 4.199710369110107,
      "learning_rate": 3.6955455259026686e-05,
      "loss": 0.6343,
      "step": 1329500
    },
    {
      "epoch": 20.872841444270016,
      "grad_norm": 3.9567296504974365,
      "learning_rate": 3.6954474097331244e-05,
      "loss": 0.657,
      "step": 1329600
    },
    {
      "epoch": 20.874411302982733,
      "grad_norm": 3.679658889770508,
      "learning_rate": 3.6953492935635795e-05,
      "loss": 0.6389,
      "step": 1329700
    },
    {
      "epoch": 20.875981161695446,
      "grad_norm": 4.2724738121032715,
      "learning_rate": 3.6952511773940346e-05,
      "loss": 0.5982,
      "step": 1329800
    },
    {
      "epoch": 20.877551020408163,
      "grad_norm": 2.4603981971740723,
      "learning_rate": 3.69515306122449e-05,
      "loss": 0.6159,
      "step": 1329900
    },
    {
      "epoch": 20.87912087912088,
      "grad_norm": 4.3614349365234375,
      "learning_rate": 3.6950549450549454e-05,
      "loss": 0.5746,
      "step": 1330000
    },
    {
      "epoch": 20.880690737833596,
      "grad_norm": 3.985427141189575,
      "learning_rate": 3.6949568288854005e-05,
      "loss": 0.6166,
      "step": 1330100
    },
    {
      "epoch": 20.88226059654631,
      "grad_norm": 3.9985787868499756,
      "learning_rate": 3.6948587127158556e-05,
      "loss": 0.5995,
      "step": 1330200
    },
    {
      "epoch": 20.883830455259027,
      "grad_norm": 4.272475242614746,
      "learning_rate": 3.6947605965463114e-05,
      "loss": 0.6071,
      "step": 1330300
    },
    {
      "epoch": 20.885400313971743,
      "grad_norm": 3.6288764476776123,
      "learning_rate": 3.6946624803767665e-05,
      "loss": 0.6399,
      "step": 1330400
    },
    {
      "epoch": 20.886970172684457,
      "grad_norm": 4.285538673400879,
      "learning_rate": 3.6945643642072216e-05,
      "loss": 0.626,
      "step": 1330500
    },
    {
      "epoch": 20.888540031397174,
      "grad_norm": 4.455911636352539,
      "learning_rate": 3.694466248037677e-05,
      "loss": 0.6306,
      "step": 1330600
    },
    {
      "epoch": 20.89010989010989,
      "grad_norm": 3.1628835201263428,
      "learning_rate": 3.6943681318681325e-05,
      "loss": 0.6316,
      "step": 1330700
    },
    {
      "epoch": 20.891679748822607,
      "grad_norm": 4.572815895080566,
      "learning_rate": 3.694270015698587e-05,
      "loss": 0.5867,
      "step": 1330800
    },
    {
      "epoch": 20.89324960753532,
      "grad_norm": 2.808715581893921,
      "learning_rate": 3.694171899529043e-05,
      "loss": 0.6188,
      "step": 1330900
    },
    {
      "epoch": 20.894819466248038,
      "grad_norm": 4.13047456741333,
      "learning_rate": 3.694073783359498e-05,
      "loss": 0.6156,
      "step": 1331000
    },
    {
      "epoch": 20.896389324960754,
      "grad_norm": 4.255822658538818,
      "learning_rate": 3.693975667189953e-05,
      "loss": 0.6058,
      "step": 1331100
    },
    {
      "epoch": 20.897959183673468,
      "grad_norm": 4.807093143463135,
      "learning_rate": 3.693877551020408e-05,
      "loss": 0.5956,
      "step": 1331200
    },
    {
      "epoch": 20.899529042386185,
      "grad_norm": 3.991642713546753,
      "learning_rate": 3.693779434850864e-05,
      "loss": 0.6057,
      "step": 1331300
    },
    {
      "epoch": 20.9010989010989,
      "grad_norm": 3.2500922679901123,
      "learning_rate": 3.693681318681319e-05,
      "loss": 0.6003,
      "step": 1331400
    },
    {
      "epoch": 20.90266875981162,
      "grad_norm": 3.631650686264038,
      "learning_rate": 3.693583202511774e-05,
      "loss": 0.5922,
      "step": 1331500
    },
    {
      "epoch": 20.90423861852433,
      "grad_norm": 3.6770832538604736,
      "learning_rate": 3.693485086342229e-05,
      "loss": 0.5975,
      "step": 1331600
    },
    {
      "epoch": 20.90580847723705,
      "grad_norm": 3.0348916053771973,
      "learning_rate": 3.693386970172685e-05,
      "loss": 0.5794,
      "step": 1331700
    },
    {
      "epoch": 20.907378335949765,
      "grad_norm": 2.857525587081909,
      "learning_rate": 3.69328885400314e-05,
      "loss": 0.5733,
      "step": 1331800
    },
    {
      "epoch": 20.90894819466248,
      "grad_norm": 2.700183629989624,
      "learning_rate": 3.693190737833595e-05,
      "loss": 0.6048,
      "step": 1331900
    },
    {
      "epoch": 20.910518053375196,
      "grad_norm": 3.755722999572754,
      "learning_rate": 3.693092621664051e-05,
      "loss": 0.5619,
      "step": 1332000
    },
    {
      "epoch": 20.912087912087912,
      "grad_norm": 3.555943012237549,
      "learning_rate": 3.692994505494506e-05,
      "loss": 0.6224,
      "step": 1332100
    },
    {
      "epoch": 20.91365777080063,
      "grad_norm": 3.9931929111480713,
      "learning_rate": 3.692896389324961e-05,
      "loss": 0.6419,
      "step": 1332200
    },
    {
      "epoch": 20.915227629513343,
      "grad_norm": 4.651151657104492,
      "learning_rate": 3.692798273155416e-05,
      "loss": 0.6392,
      "step": 1332300
    },
    {
      "epoch": 20.91679748822606,
      "grad_norm": 3.663980007171631,
      "learning_rate": 3.692700156985872e-05,
      "loss": 0.5984,
      "step": 1332400
    },
    {
      "epoch": 20.918367346938776,
      "grad_norm": 4.130058288574219,
      "learning_rate": 3.692602040816326e-05,
      "loss": 0.6087,
      "step": 1332500
    },
    {
      "epoch": 20.919937205651493,
      "grad_norm": 3.593484401702881,
      "learning_rate": 3.692503924646782e-05,
      "loss": 0.6226,
      "step": 1332600
    },
    {
      "epoch": 20.921507064364206,
      "grad_norm": 4.63340425491333,
      "learning_rate": 3.692405808477237e-05,
      "loss": 0.6125,
      "step": 1332700
    },
    {
      "epoch": 20.923076923076923,
      "grad_norm": 4.555907726287842,
      "learning_rate": 3.692307692307693e-05,
      "loss": 0.6016,
      "step": 1332800
    },
    {
      "epoch": 20.92464678178964,
      "grad_norm": 5.251066207885742,
      "learning_rate": 3.6922095761381473e-05,
      "loss": 0.607,
      "step": 1332900
    },
    {
      "epoch": 20.926216640502354,
      "grad_norm": 4.640294551849365,
      "learning_rate": 3.692111459968603e-05,
      "loss": 0.55,
      "step": 1333000
    },
    {
      "epoch": 20.92778649921507,
      "grad_norm": 3.6077396869659424,
      "learning_rate": 3.692013343799058e-05,
      "loss": 0.6132,
      "step": 1333100
    },
    {
      "epoch": 20.929356357927787,
      "grad_norm": 3.283299446105957,
      "learning_rate": 3.691915227629513e-05,
      "loss": 0.5795,
      "step": 1333200
    },
    {
      "epoch": 20.9309262166405,
      "grad_norm": 4.178593158721924,
      "learning_rate": 3.6918171114599684e-05,
      "loss": 0.6003,
      "step": 1333300
    },
    {
      "epoch": 20.932496075353217,
      "grad_norm": 3.4779629707336426,
      "learning_rate": 3.691718995290424e-05,
      "loss": 0.6114,
      "step": 1333400
    },
    {
      "epoch": 20.934065934065934,
      "grad_norm": 3.662252426147461,
      "learning_rate": 3.691620879120879e-05,
      "loss": 0.6329,
      "step": 1333500
    },
    {
      "epoch": 20.93563579277865,
      "grad_norm": 3.7921299934387207,
      "learning_rate": 3.6915227629513344e-05,
      "loss": 0.5974,
      "step": 1333600
    },
    {
      "epoch": 20.937205651491364,
      "grad_norm": 4.3684844970703125,
      "learning_rate": 3.6914246467817895e-05,
      "loss": 0.6274,
      "step": 1333700
    },
    {
      "epoch": 20.93877551020408,
      "grad_norm": 3.0175840854644775,
      "learning_rate": 3.691326530612245e-05,
      "loss": 0.6018,
      "step": 1333800
    },
    {
      "epoch": 20.940345368916798,
      "grad_norm": 4.112744331359863,
      "learning_rate": 3.6912284144427004e-05,
      "loss": 0.6197,
      "step": 1333900
    },
    {
      "epoch": 20.941915227629515,
      "grad_norm": 3.8266489505767822,
      "learning_rate": 3.6911302982731555e-05,
      "loss": 0.6142,
      "step": 1334000
    },
    {
      "epoch": 20.94348508634223,
      "grad_norm": 3.3111977577209473,
      "learning_rate": 3.691032182103611e-05,
      "loss": 0.6232,
      "step": 1334100
    },
    {
      "epoch": 20.945054945054945,
      "grad_norm": 4.1752095222473145,
      "learning_rate": 3.690934065934066e-05,
      "loss": 0.6373,
      "step": 1334200
    },
    {
      "epoch": 20.946624803767662,
      "grad_norm": 5.21837854385376,
      "learning_rate": 3.6908359497645214e-05,
      "loss": 0.6096,
      "step": 1334300
    },
    {
      "epoch": 20.948194662480375,
      "grad_norm": 3.890549659729004,
      "learning_rate": 3.6907378335949765e-05,
      "loss": 0.6025,
      "step": 1334400
    },
    {
      "epoch": 20.949764521193092,
      "grad_norm": 4.104059219360352,
      "learning_rate": 3.690639717425432e-05,
      "loss": 0.6066,
      "step": 1334500
    },
    {
      "epoch": 20.95133437990581,
      "grad_norm": 3.768746852874756,
      "learning_rate": 3.690541601255887e-05,
      "loss": 0.6258,
      "step": 1334600
    },
    {
      "epoch": 20.952904238618526,
      "grad_norm": 2.3095703125,
      "learning_rate": 3.6904434850863425e-05,
      "loss": 0.5951,
      "step": 1334700
    },
    {
      "epoch": 20.95447409733124,
      "grad_norm": 3.679469108581543,
      "learning_rate": 3.6903453689167976e-05,
      "loss": 0.6138,
      "step": 1334800
    },
    {
      "epoch": 20.956043956043956,
      "grad_norm": 2.736232280731201,
      "learning_rate": 3.6902472527472534e-05,
      "loss": 0.6048,
      "step": 1334900
    },
    {
      "epoch": 20.957613814756673,
      "grad_norm": 4.293885231018066,
      "learning_rate": 3.690149136577708e-05,
      "loss": 0.5991,
      "step": 1335000
    },
    {
      "epoch": 20.959183673469386,
      "grad_norm": 4.145871162414551,
      "learning_rate": 3.6900510204081636e-05,
      "loss": 0.6126,
      "step": 1335100
    },
    {
      "epoch": 20.960753532182103,
      "grad_norm": 4.226126670837402,
      "learning_rate": 3.689952904238619e-05,
      "loss": 0.6208,
      "step": 1335200
    },
    {
      "epoch": 20.96232339089482,
      "grad_norm": 4.154458999633789,
      "learning_rate": 3.689854788069074e-05,
      "loss": 0.588,
      "step": 1335300
    },
    {
      "epoch": 20.963893249607537,
      "grad_norm": 3.8826282024383545,
      "learning_rate": 3.689756671899529e-05,
      "loss": 0.6112,
      "step": 1335400
    },
    {
      "epoch": 20.96546310832025,
      "grad_norm": 2.0767083168029785,
      "learning_rate": 3.6896585557299846e-05,
      "loss": 0.5733,
      "step": 1335500
    },
    {
      "epoch": 20.967032967032967,
      "grad_norm": 3.7499170303344727,
      "learning_rate": 3.68956043956044e-05,
      "loss": 0.6234,
      "step": 1335600
    },
    {
      "epoch": 20.968602825745684,
      "grad_norm": 5.039628505706787,
      "learning_rate": 3.689462323390895e-05,
      "loss": 0.6369,
      "step": 1335700
    },
    {
      "epoch": 20.970172684458397,
      "grad_norm": 3.8868391513824463,
      "learning_rate": 3.68936420722135e-05,
      "loss": 0.6101,
      "step": 1335800
    },
    {
      "epoch": 20.971742543171114,
      "grad_norm": 5.063313961029053,
      "learning_rate": 3.689266091051806e-05,
      "loss": 0.6301,
      "step": 1335900
    },
    {
      "epoch": 20.97331240188383,
      "grad_norm": 5.183345317840576,
      "learning_rate": 3.689167974882261e-05,
      "loss": 0.6225,
      "step": 1336000
    },
    {
      "epoch": 20.974882260596548,
      "grad_norm": 3.9296226501464844,
      "learning_rate": 3.689069858712716e-05,
      "loss": 0.6215,
      "step": 1336100
    },
    {
      "epoch": 20.97645211930926,
      "grad_norm": 4.110641002655029,
      "learning_rate": 3.688971742543172e-05,
      "loss": 0.6205,
      "step": 1336200
    },
    {
      "epoch": 20.978021978021978,
      "grad_norm": 3.438624858856201,
      "learning_rate": 3.688873626373627e-05,
      "loss": 0.6081,
      "step": 1336300
    },
    {
      "epoch": 20.979591836734695,
      "grad_norm": 4.251312255859375,
      "learning_rate": 3.688775510204082e-05,
      "loss": 0.598,
      "step": 1336400
    },
    {
      "epoch": 20.98116169544741,
      "grad_norm": 3.594464063644409,
      "learning_rate": 3.688677394034537e-05,
      "loss": 0.6109,
      "step": 1336500
    },
    {
      "epoch": 20.982731554160125,
      "grad_norm": 3.957390308380127,
      "learning_rate": 3.688579277864993e-05,
      "loss": 0.6026,
      "step": 1336600
    },
    {
      "epoch": 20.984301412872842,
      "grad_norm": 4.664790153503418,
      "learning_rate": 3.688481161695447e-05,
      "loss": 0.6491,
      "step": 1336700
    },
    {
      "epoch": 20.98587127158556,
      "grad_norm": 4.25600528717041,
      "learning_rate": 3.688383045525903e-05,
      "loss": 0.6299,
      "step": 1336800
    },
    {
      "epoch": 20.987441130298272,
      "grad_norm": 4.44882345199585,
      "learning_rate": 3.688284929356358e-05,
      "loss": 0.6041,
      "step": 1336900
    },
    {
      "epoch": 20.98901098901099,
      "grad_norm": 3.5207431316375732,
      "learning_rate": 3.688186813186814e-05,
      "loss": 0.5728,
      "step": 1337000
    },
    {
      "epoch": 20.990580847723706,
      "grad_norm": 4.224768161773682,
      "learning_rate": 3.688088697017268e-05,
      "loss": 0.6157,
      "step": 1337100
    },
    {
      "epoch": 20.99215070643642,
      "grad_norm": 3.7693445682525635,
      "learning_rate": 3.687990580847724e-05,
      "loss": 0.6202,
      "step": 1337200
    },
    {
      "epoch": 20.993720565149136,
      "grad_norm": 4.462636947631836,
      "learning_rate": 3.687892464678179e-05,
      "loss": 0.5823,
      "step": 1337300
    },
    {
      "epoch": 20.995290423861853,
      "grad_norm": 4.361663818359375,
      "learning_rate": 3.687794348508634e-05,
      "loss": 0.6078,
      "step": 1337400
    },
    {
      "epoch": 20.99686028257457,
      "grad_norm": 3.707716941833496,
      "learning_rate": 3.687696232339089e-05,
      "loss": 0.5757,
      "step": 1337500
    },
    {
      "epoch": 20.998430141287283,
      "grad_norm": 4.240847587585449,
      "learning_rate": 3.687598116169545e-05,
      "loss": 0.6118,
      "step": 1337600
    },
    {
      "epoch": 21.0,
      "grad_norm": 4.072724342346191,
      "learning_rate": 3.6875e-05,
      "loss": 0.6256,
      "step": 1337700
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.0280859470367432,
      "eval_runtime": 14.7725,
      "eval_samples_per_second": 226.976,
      "eval_steps_per_second": 226.976,
      "step": 1337700
    },
    {
      "epoch": 21.0,
      "eval_loss": 0.46851980686187744,
      "eval_runtime": 284.4863,
      "eval_samples_per_second": 223.912,
      "eval_steps_per_second": 223.912,
      "step": 1337700
    },
    {
      "epoch": 21.001569858712717,
      "grad_norm": 4.392258644104004,
      "learning_rate": 3.687401883830455e-05,
      "loss": 0.5814,
      "step": 1337800
    },
    {
      "epoch": 21.00313971742543,
      "grad_norm": 3.6104164123535156,
      "learning_rate": 3.6873037676609104e-05,
      "loss": 0.6099,
      "step": 1337900
    },
    {
      "epoch": 21.004709576138147,
      "grad_norm": 3.829066038131714,
      "learning_rate": 3.687205651491366e-05,
      "loss": 0.5905,
      "step": 1338000
    },
    {
      "epoch": 21.006279434850864,
      "grad_norm": 3.9264142513275146,
      "learning_rate": 3.687107535321821e-05,
      "loss": 0.5854,
      "step": 1338100
    },
    {
      "epoch": 21.00784929356358,
      "grad_norm": 3.9792590141296387,
      "learning_rate": 3.6870094191522764e-05,
      "loss": 0.6054,
      "step": 1338200
    },
    {
      "epoch": 21.009419152276294,
      "grad_norm": 3.4475831985473633,
      "learning_rate": 3.686911302982732e-05,
      "loss": 0.5873,
      "step": 1338300
    },
    {
      "epoch": 21.01098901098901,
      "grad_norm": 4.023141384124756,
      "learning_rate": 3.686813186813187e-05,
      "loss": 0.6264,
      "step": 1338400
    },
    {
      "epoch": 21.012558869701728,
      "grad_norm": 3.9892561435699463,
      "learning_rate": 3.686715070643642e-05,
      "loss": 0.5842,
      "step": 1338500
    },
    {
      "epoch": 21.01412872841444,
      "grad_norm": 3.919421911239624,
      "learning_rate": 3.6866169544740974e-05,
      "loss": 0.5956,
      "step": 1338600
    },
    {
      "epoch": 21.015698587127158,
      "grad_norm": 4.479656219482422,
      "learning_rate": 3.686518838304553e-05,
      "loss": 0.5932,
      "step": 1338700
    },
    {
      "epoch": 21.017268445839875,
      "grad_norm": 3.0199222564697266,
      "learning_rate": 3.6864207221350076e-05,
      "loss": 0.5865,
      "step": 1338800
    },
    {
      "epoch": 21.01883830455259,
      "grad_norm": 3.6702232360839844,
      "learning_rate": 3.6863226059654634e-05,
      "loss": 0.5818,
      "step": 1338900
    },
    {
      "epoch": 21.020408163265305,
      "grad_norm": 3.05228328704834,
      "learning_rate": 3.6862244897959185e-05,
      "loss": 0.6232,
      "step": 1339000
    },
    {
      "epoch": 21.021978021978022,
      "grad_norm": 4.663589954376221,
      "learning_rate": 3.686126373626374e-05,
      "loss": 0.5814,
      "step": 1339100
    },
    {
      "epoch": 21.02354788069074,
      "grad_norm": 2.9025635719299316,
      "learning_rate": 3.686028257456829e-05,
      "loss": 0.5993,
      "step": 1339200
    },
    {
      "epoch": 21.025117739403452,
      "grad_norm": 4.494048595428467,
      "learning_rate": 3.6859301412872845e-05,
      "loss": 0.5621,
      "step": 1339300
    },
    {
      "epoch": 21.02668759811617,
      "grad_norm": 3.692321538925171,
      "learning_rate": 3.6858320251177396e-05,
      "loss": 0.5913,
      "step": 1339400
    },
    {
      "epoch": 21.028257456828886,
      "grad_norm": 3.9202463626861572,
      "learning_rate": 3.685733908948195e-05,
      "loss": 0.5999,
      "step": 1339500
    },
    {
      "epoch": 21.029827315541603,
      "grad_norm": 4.1329264640808105,
      "learning_rate": 3.68563579277865e-05,
      "loss": 0.6007,
      "step": 1339600
    },
    {
      "epoch": 21.031397174254316,
      "grad_norm": 3.6630818843841553,
      "learning_rate": 3.6855376766091055e-05,
      "loss": 0.5938,
      "step": 1339700
    },
    {
      "epoch": 21.032967032967033,
      "grad_norm": 3.3954334259033203,
      "learning_rate": 3.6854395604395606e-05,
      "loss": 0.5999,
      "step": 1339800
    },
    {
      "epoch": 21.03453689167975,
      "grad_norm": 3.0026350021362305,
      "learning_rate": 3.685341444270016e-05,
      "loss": 0.596,
      "step": 1339900
    },
    {
      "epoch": 21.036106750392463,
      "grad_norm": 3.3052706718444824,
      "learning_rate": 3.685243328100471e-05,
      "loss": 0.572,
      "step": 1340000
    },
    {
      "epoch": 21.03767660910518,
      "grad_norm": 4.2955193519592285,
      "learning_rate": 3.6851452119309266e-05,
      "loss": 0.5746,
      "step": 1340100
    },
    {
      "epoch": 21.039246467817897,
      "grad_norm": 4.6812286376953125,
      "learning_rate": 3.685047095761382e-05,
      "loss": 0.6113,
      "step": 1340200
    },
    {
      "epoch": 21.040816326530614,
      "grad_norm": 2.5722341537475586,
      "learning_rate": 3.684948979591837e-05,
      "loss": 0.6042,
      "step": 1340300
    },
    {
      "epoch": 21.042386185243327,
      "grad_norm": 3.1977176666259766,
      "learning_rate": 3.6848508634222926e-05,
      "loss": 0.5812,
      "step": 1340400
    },
    {
      "epoch": 21.043956043956044,
      "grad_norm": 3.0755136013031006,
      "learning_rate": 3.684752747252748e-05,
      "loss": 0.6089,
      "step": 1340500
    },
    {
      "epoch": 21.04552590266876,
      "grad_norm": 4.6110992431640625,
      "learning_rate": 3.684654631083203e-05,
      "loss": 0.6079,
      "step": 1340600
    },
    {
      "epoch": 21.047095761381474,
      "grad_norm": 3.4445462226867676,
      "learning_rate": 3.684556514913658e-05,
      "loss": 0.5962,
      "step": 1340700
    },
    {
      "epoch": 21.04866562009419,
      "grad_norm": 4.440483093261719,
      "learning_rate": 3.6844583987441136e-05,
      "loss": 0.6119,
      "step": 1340800
    },
    {
      "epoch": 21.050235478806908,
      "grad_norm": 3.7768232822418213,
      "learning_rate": 3.684360282574568e-05,
      "loss": 0.5814,
      "step": 1340900
    },
    {
      "epoch": 21.051805337519625,
      "grad_norm": 4.1300530433654785,
      "learning_rate": 3.684262166405024e-05,
      "loss": 0.5966,
      "step": 1341000
    },
    {
      "epoch": 21.053375196232338,
      "grad_norm": 3.4623231887817383,
      "learning_rate": 3.684164050235479e-05,
      "loss": 0.5747,
      "step": 1341100
    },
    {
      "epoch": 21.054945054945055,
      "grad_norm": 3.758148431777954,
      "learning_rate": 3.684065934065935e-05,
      "loss": 0.6087,
      "step": 1341200
    },
    {
      "epoch": 21.05651491365777,
      "grad_norm": 2.756788730621338,
      "learning_rate": 3.683967817896389e-05,
      "loss": 0.621,
      "step": 1341300
    },
    {
      "epoch": 21.058084772370485,
      "grad_norm": 4.043806552886963,
      "learning_rate": 3.683869701726845e-05,
      "loss": 0.5936,
      "step": 1341400
    },
    {
      "epoch": 21.059654631083202,
      "grad_norm": 3.861480951309204,
      "learning_rate": 3.6837715855573e-05,
      "loss": 0.5868,
      "step": 1341500
    },
    {
      "epoch": 21.06122448979592,
      "grad_norm": 4.133249282836914,
      "learning_rate": 3.683673469387755e-05,
      "loss": 0.6264,
      "step": 1341600
    },
    {
      "epoch": 21.062794348508636,
      "grad_norm": 4.34005069732666,
      "learning_rate": 3.68357535321821e-05,
      "loss": 0.6043,
      "step": 1341700
    },
    {
      "epoch": 21.06436420722135,
      "grad_norm": 3.335075855255127,
      "learning_rate": 3.683477237048666e-05,
      "loss": 0.6007,
      "step": 1341800
    },
    {
      "epoch": 21.065934065934066,
      "grad_norm": 3.4642975330352783,
      "learning_rate": 3.683379120879121e-05,
      "loss": 0.5913,
      "step": 1341900
    },
    {
      "epoch": 21.067503924646783,
      "grad_norm": 4.145870685577393,
      "learning_rate": 3.683281004709576e-05,
      "loss": 0.6122,
      "step": 1342000
    },
    {
      "epoch": 21.069073783359496,
      "grad_norm": 3.851052761077881,
      "learning_rate": 3.683182888540031e-05,
      "loss": 0.5922,
      "step": 1342100
    },
    {
      "epoch": 21.070643642072213,
      "grad_norm": 5.480164527893066,
      "learning_rate": 3.683084772370487e-05,
      "loss": 0.6313,
      "step": 1342200
    },
    {
      "epoch": 21.07221350078493,
      "grad_norm": 4.431918144226074,
      "learning_rate": 3.682986656200942e-05,
      "loss": 0.6264,
      "step": 1342300
    },
    {
      "epoch": 21.073783359497646,
      "grad_norm": 4.256930828094482,
      "learning_rate": 3.682888540031397e-05,
      "loss": 0.5916,
      "step": 1342400
    },
    {
      "epoch": 21.07535321821036,
      "grad_norm": 3.942992687225342,
      "learning_rate": 3.682790423861853e-05,
      "loss": 0.6106,
      "step": 1342500
    },
    {
      "epoch": 21.076923076923077,
      "grad_norm": 3.5250906944274902,
      "learning_rate": 3.682692307692308e-05,
      "loss": 0.5761,
      "step": 1342600
    },
    {
      "epoch": 21.078492935635794,
      "grad_norm": 5.014654636383057,
      "learning_rate": 3.682594191522763e-05,
      "loss": 0.5371,
      "step": 1342700
    },
    {
      "epoch": 21.08006279434851,
      "grad_norm": 4.870841979980469,
      "learning_rate": 3.682496075353218e-05,
      "loss": 0.5816,
      "step": 1342800
    },
    {
      "epoch": 21.081632653061224,
      "grad_norm": 3.1939327716827393,
      "learning_rate": 3.682397959183674e-05,
      "loss": 0.6028,
      "step": 1342900
    },
    {
      "epoch": 21.08320251177394,
      "grad_norm": 4.294623374938965,
      "learning_rate": 3.6822998430141285e-05,
      "loss": 0.585,
      "step": 1343000
    },
    {
      "epoch": 21.084772370486657,
      "grad_norm": 4.31331205368042,
      "learning_rate": 3.682201726844584e-05,
      "loss": 0.6316,
      "step": 1343100
    },
    {
      "epoch": 21.08634222919937,
      "grad_norm": 4.164341926574707,
      "learning_rate": 3.6821036106750394e-05,
      "loss": 0.5852,
      "step": 1343200
    },
    {
      "epoch": 21.087912087912088,
      "grad_norm": 4.055959701538086,
      "learning_rate": 3.682005494505495e-05,
      "loss": 0.6031,
      "step": 1343300
    },
    {
      "epoch": 21.089481946624804,
      "grad_norm": 4.352728366851807,
      "learning_rate": 3.6819073783359496e-05,
      "loss": 0.5752,
      "step": 1343400
    },
    {
      "epoch": 21.09105180533752,
      "grad_norm": 3.9803106784820557,
      "learning_rate": 3.6818092621664054e-05,
      "loss": 0.5873,
      "step": 1343500
    },
    {
      "epoch": 21.092621664050235,
      "grad_norm": 4.514247894287109,
      "learning_rate": 3.6817111459968605e-05,
      "loss": 0.6205,
      "step": 1343600
    },
    {
      "epoch": 21.09419152276295,
      "grad_norm": 4.064187049865723,
      "learning_rate": 3.6816130298273156e-05,
      "loss": 0.6186,
      "step": 1343700
    },
    {
      "epoch": 21.09576138147567,
      "grad_norm": 3.541863203048706,
      "learning_rate": 3.6815149136577707e-05,
      "loss": 0.6237,
      "step": 1343800
    },
    {
      "epoch": 21.09733124018838,
      "grad_norm": 4.544143199920654,
      "learning_rate": 3.6814167974882264e-05,
      "loss": 0.6446,
      "step": 1343900
    },
    {
      "epoch": 21.0989010989011,
      "grad_norm": 4.222254276275635,
      "learning_rate": 3.6813186813186815e-05,
      "loss": 0.5808,
      "step": 1344000
    },
    {
      "epoch": 21.100470957613815,
      "grad_norm": 3.9860482215881348,
      "learning_rate": 3.6812205651491366e-05,
      "loss": 0.5918,
      "step": 1344100
    },
    {
      "epoch": 21.102040816326532,
      "grad_norm": 3.7884817123413086,
      "learning_rate": 3.681122448979592e-05,
      "loss": 0.5842,
      "step": 1344200
    },
    {
      "epoch": 21.103610675039246,
      "grad_norm": 3.8430428504943848,
      "learning_rate": 3.6810243328100475e-05,
      "loss": 0.6183,
      "step": 1344300
    },
    {
      "epoch": 21.105180533751962,
      "grad_norm": 4.370066165924072,
      "learning_rate": 3.6809262166405026e-05,
      "loss": 0.5896,
      "step": 1344400
    },
    {
      "epoch": 21.10675039246468,
      "grad_norm": 4.435179710388184,
      "learning_rate": 3.680828100470958e-05,
      "loss": 0.5921,
      "step": 1344500
    },
    {
      "epoch": 21.108320251177393,
      "grad_norm": 4.022490501403809,
      "learning_rate": 3.680729984301413e-05,
      "loss": 0.6297,
      "step": 1344600
    },
    {
      "epoch": 21.10989010989011,
      "grad_norm": 3.670210361480713,
      "learning_rate": 3.6806318681318686e-05,
      "loss": 0.5733,
      "step": 1344700
    },
    {
      "epoch": 21.111459968602826,
      "grad_norm": 3.75957989692688,
      "learning_rate": 3.680533751962324e-05,
      "loss": 0.5883,
      "step": 1344800
    },
    {
      "epoch": 21.113029827315543,
      "grad_norm": 3.4988956451416016,
      "learning_rate": 3.680435635792779e-05,
      "loss": 0.5742,
      "step": 1344900
    },
    {
      "epoch": 21.114599686028257,
      "grad_norm": 4.530427932739258,
      "learning_rate": 3.6803375196232345e-05,
      "loss": 0.6033,
      "step": 1345000
    },
    {
      "epoch": 21.116169544740973,
      "grad_norm": 2.794325113296509,
      "learning_rate": 3.680239403453689e-05,
      "loss": 0.6416,
      "step": 1345100
    },
    {
      "epoch": 21.11773940345369,
      "grad_norm": 3.785543441772461,
      "learning_rate": 3.680141287284145e-05,
      "loss": 0.5921,
      "step": 1345200
    },
    {
      "epoch": 21.119309262166404,
      "grad_norm": 3.851339101791382,
      "learning_rate": 3.6800431711146e-05,
      "loss": 0.5988,
      "step": 1345300
    },
    {
      "epoch": 21.12087912087912,
      "grad_norm": 3.9690825939178467,
      "learning_rate": 3.6799450549450556e-05,
      "loss": 0.6132,
      "step": 1345400
    },
    {
      "epoch": 21.122448979591837,
      "grad_norm": 3.2277615070343018,
      "learning_rate": 3.67984693877551e-05,
      "loss": 0.5877,
      "step": 1345500
    },
    {
      "epoch": 21.124018838304554,
      "grad_norm": 3.5301079750061035,
      "learning_rate": 3.679748822605966e-05,
      "loss": 0.6063,
      "step": 1345600
    },
    {
      "epoch": 21.125588697017267,
      "grad_norm": 4.045032978057861,
      "learning_rate": 3.679650706436421e-05,
      "loss": 0.5617,
      "step": 1345700
    },
    {
      "epoch": 21.127158555729984,
      "grad_norm": 3.564110279083252,
      "learning_rate": 3.679552590266876e-05,
      "loss": 0.5906,
      "step": 1345800
    },
    {
      "epoch": 21.1287284144427,
      "grad_norm": 2.8987739086151123,
      "learning_rate": 3.679454474097331e-05,
      "loss": 0.5957,
      "step": 1345900
    },
    {
      "epoch": 21.130298273155415,
      "grad_norm": 4.2231597900390625,
      "learning_rate": 3.679356357927787e-05,
      "loss": 0.5694,
      "step": 1346000
    },
    {
      "epoch": 21.13186813186813,
      "grad_norm": 3.4601426124572754,
      "learning_rate": 3.679258241758242e-05,
      "loss": 0.625,
      "step": 1346100
    },
    {
      "epoch": 21.13343799058085,
      "grad_norm": 4.455708026885986,
      "learning_rate": 3.679160125588697e-05,
      "loss": 0.5741,
      "step": 1346200
    },
    {
      "epoch": 21.135007849293565,
      "grad_norm": 3.0176453590393066,
      "learning_rate": 3.679062009419152e-05,
      "loss": 0.6271,
      "step": 1346300
    },
    {
      "epoch": 21.13657770800628,
      "grad_norm": 3.6026086807250977,
      "learning_rate": 3.678963893249608e-05,
      "loss": 0.5912,
      "step": 1346400
    },
    {
      "epoch": 21.138147566718995,
      "grad_norm": 4.479974746704102,
      "learning_rate": 3.678865777080063e-05,
      "loss": 0.5776,
      "step": 1346500
    },
    {
      "epoch": 21.139717425431712,
      "grad_norm": 3.960669755935669,
      "learning_rate": 3.678767660910518e-05,
      "loss": 0.6237,
      "step": 1346600
    },
    {
      "epoch": 21.141287284144425,
      "grad_norm": 3.455995559692383,
      "learning_rate": 3.678669544740973e-05,
      "loss": 0.5982,
      "step": 1346700
    },
    {
      "epoch": 21.142857142857142,
      "grad_norm": 3.386899471282959,
      "learning_rate": 3.678571428571429e-05,
      "loss": 0.5839,
      "step": 1346800
    },
    {
      "epoch": 21.14442700156986,
      "grad_norm": 3.2942724227905273,
      "learning_rate": 3.678473312401884e-05,
      "loss": 0.5964,
      "step": 1346900
    },
    {
      "epoch": 21.145996860282576,
      "grad_norm": 4.2673659324646,
      "learning_rate": 3.678375196232339e-05,
      "loss": 0.5773,
      "step": 1347000
    },
    {
      "epoch": 21.14756671899529,
      "grad_norm": 2.395937442779541,
      "learning_rate": 3.678277080062795e-05,
      "loss": 0.5872,
      "step": 1347100
    },
    {
      "epoch": 21.149136577708006,
      "grad_norm": 4.238954544067383,
      "learning_rate": 3.6781789638932494e-05,
      "loss": 0.5952,
      "step": 1347200
    },
    {
      "epoch": 21.150706436420723,
      "grad_norm": 4.947434902191162,
      "learning_rate": 3.678080847723705e-05,
      "loss": 0.5777,
      "step": 1347300
    },
    {
      "epoch": 21.152276295133436,
      "grad_norm": 3.3875064849853516,
      "learning_rate": 3.67798273155416e-05,
      "loss": 0.6209,
      "step": 1347400
    },
    {
      "epoch": 21.153846153846153,
      "grad_norm": 3.9732205867767334,
      "learning_rate": 3.677884615384616e-05,
      "loss": 0.5886,
      "step": 1347500
    },
    {
      "epoch": 21.15541601255887,
      "grad_norm": 3.2747976779937744,
      "learning_rate": 3.6777864992150705e-05,
      "loss": 0.5907,
      "step": 1347600
    },
    {
      "epoch": 21.156985871271587,
      "grad_norm": 4.117037773132324,
      "learning_rate": 3.677688383045526e-05,
      "loss": 0.59,
      "step": 1347700
    },
    {
      "epoch": 21.1585557299843,
      "grad_norm": 3.6487443447113037,
      "learning_rate": 3.6775902668759814e-05,
      "loss": 0.601,
      "step": 1347800
    },
    {
      "epoch": 21.160125588697017,
      "grad_norm": 4.2233500480651855,
      "learning_rate": 3.6774921507064365e-05,
      "loss": 0.5968,
      "step": 1347900
    },
    {
      "epoch": 21.161695447409734,
      "grad_norm": 2.9739673137664795,
      "learning_rate": 3.6773940345368916e-05,
      "loss": 0.6139,
      "step": 1348000
    },
    {
      "epoch": 21.163265306122447,
      "grad_norm": 4.3536601066589355,
      "learning_rate": 3.677295918367347e-05,
      "loss": 0.5771,
      "step": 1348100
    },
    {
      "epoch": 21.164835164835164,
      "grad_norm": 3.8063738346099854,
      "learning_rate": 3.6771978021978024e-05,
      "loss": 0.5801,
      "step": 1348200
    },
    {
      "epoch": 21.16640502354788,
      "grad_norm": 4.590837001800537,
      "learning_rate": 3.6770996860282575e-05,
      "loss": 0.5911,
      "step": 1348300
    },
    {
      "epoch": 21.167974882260598,
      "grad_norm": 3.250169515609741,
      "learning_rate": 3.6770015698587126e-05,
      "loss": 0.5696,
      "step": 1348400
    },
    {
      "epoch": 21.16954474097331,
      "grad_norm": 3.122512102127075,
      "learning_rate": 3.6769034536891684e-05,
      "loss": 0.5997,
      "step": 1348500
    },
    {
      "epoch": 21.171114599686028,
      "grad_norm": 4.696746349334717,
      "learning_rate": 3.6768053375196235e-05,
      "loss": 0.6375,
      "step": 1348600
    },
    {
      "epoch": 21.172684458398745,
      "grad_norm": 3.1371724605560303,
      "learning_rate": 3.6767072213500786e-05,
      "loss": 0.5981,
      "step": 1348700
    },
    {
      "epoch": 21.17425431711146,
      "grad_norm": 3.87748384475708,
      "learning_rate": 3.676609105180534e-05,
      "loss": 0.5716,
      "step": 1348800
    },
    {
      "epoch": 21.175824175824175,
      "grad_norm": 3.7011635303497314,
      "learning_rate": 3.6765109890109895e-05,
      "loss": 0.5743,
      "step": 1348900
    },
    {
      "epoch": 21.177394034536892,
      "grad_norm": 3.6106669902801514,
      "learning_rate": 3.6764128728414446e-05,
      "loss": 0.6241,
      "step": 1349000
    },
    {
      "epoch": 21.17896389324961,
      "grad_norm": 2.997453451156616,
      "learning_rate": 3.6763147566719e-05,
      "loss": 0.5938,
      "step": 1349100
    },
    {
      "epoch": 21.180533751962322,
      "grad_norm": 3.6285176277160645,
      "learning_rate": 3.6762166405023554e-05,
      "loss": 0.6003,
      "step": 1349200
    },
    {
      "epoch": 21.18210361067504,
      "grad_norm": 3.7613253593444824,
      "learning_rate": 3.67611852433281e-05,
      "loss": 0.5923,
      "step": 1349300
    },
    {
      "epoch": 21.183673469387756,
      "grad_norm": 3.656525135040283,
      "learning_rate": 3.6760204081632656e-05,
      "loss": 0.5452,
      "step": 1349400
    },
    {
      "epoch": 21.18524332810047,
      "grad_norm": 2.4802255630493164,
      "learning_rate": 3.675922291993721e-05,
      "loss": 0.6046,
      "step": 1349500
    },
    {
      "epoch": 21.186813186813186,
      "grad_norm": 2.559129476547241,
      "learning_rate": 3.6758241758241765e-05,
      "loss": 0.5871,
      "step": 1349600
    },
    {
      "epoch": 21.188383045525903,
      "grad_norm": 2.9951624870300293,
      "learning_rate": 3.675726059654631e-05,
      "loss": 0.5819,
      "step": 1349700
    },
    {
      "epoch": 21.18995290423862,
      "grad_norm": 3.178088903427124,
      "learning_rate": 3.675627943485087e-05,
      "loss": 0.6041,
      "step": 1349800
    },
    {
      "epoch": 21.191522762951333,
      "grad_norm": 3.600039482116699,
      "learning_rate": 3.675529827315542e-05,
      "loss": 0.588,
      "step": 1349900
    },
    {
      "epoch": 21.19309262166405,
      "grad_norm": 3.8038883209228516,
      "learning_rate": 3.675431711145997e-05,
      "loss": 0.6032,
      "step": 1350000
    },
    {
      "epoch": 21.194662480376767,
      "grad_norm": 5.092477321624756,
      "learning_rate": 3.675333594976452e-05,
      "loss": 0.5811,
      "step": 1350100
    },
    {
      "epoch": 21.19623233908948,
      "grad_norm": 2.979173183441162,
      "learning_rate": 3.675235478806908e-05,
      "loss": 0.6251,
      "step": 1350200
    },
    {
      "epoch": 21.197802197802197,
      "grad_norm": 5.606533527374268,
      "learning_rate": 3.675137362637363e-05,
      "loss": 0.5532,
      "step": 1350300
    },
    {
      "epoch": 21.199372056514914,
      "grad_norm": 4.332458019256592,
      "learning_rate": 3.675039246467818e-05,
      "loss": 0.6357,
      "step": 1350400
    },
    {
      "epoch": 21.20094191522763,
      "grad_norm": 4.38649845123291,
      "learning_rate": 3.674941130298273e-05,
      "loss": 0.5813,
      "step": 1350500
    },
    {
      "epoch": 21.202511773940344,
      "grad_norm": 3.049398422241211,
      "learning_rate": 3.674843014128729e-05,
      "loss": 0.6381,
      "step": 1350600
    },
    {
      "epoch": 21.20408163265306,
      "grad_norm": 5.776185035705566,
      "learning_rate": 3.674744897959184e-05,
      "loss": 0.6048,
      "step": 1350700
    },
    {
      "epoch": 21.205651491365778,
      "grad_norm": 4.2594523429870605,
      "learning_rate": 3.674646781789639e-05,
      "loss": 0.5865,
      "step": 1350800
    },
    {
      "epoch": 21.20722135007849,
      "grad_norm": 3.3128442764282227,
      "learning_rate": 3.674548665620094e-05,
      "loss": 0.6011,
      "step": 1350900
    },
    {
      "epoch": 21.208791208791208,
      "grad_norm": 4.074321746826172,
      "learning_rate": 3.67445054945055e-05,
      "loss": 0.5977,
      "step": 1351000
    },
    {
      "epoch": 21.210361067503925,
      "grad_norm": 4.806250095367432,
      "learning_rate": 3.674352433281005e-05,
      "loss": 0.5907,
      "step": 1351100
    },
    {
      "epoch": 21.211930926216642,
      "grad_norm": 3.8796985149383545,
      "learning_rate": 3.67425431711146e-05,
      "loss": 0.6182,
      "step": 1351200
    },
    {
      "epoch": 21.213500784929355,
      "grad_norm": 5.7088518142700195,
      "learning_rate": 3.674156200941916e-05,
      "loss": 0.6213,
      "step": 1351300
    },
    {
      "epoch": 21.215070643642072,
      "grad_norm": 5.036240100860596,
      "learning_rate": 3.67405808477237e-05,
      "loss": 0.5971,
      "step": 1351400
    },
    {
      "epoch": 21.21664050235479,
      "grad_norm": 3.2970950603485107,
      "learning_rate": 3.673959968602826e-05,
      "loss": 0.5871,
      "step": 1351500
    },
    {
      "epoch": 21.218210361067506,
      "grad_norm": 3.4903714656829834,
      "learning_rate": 3.673861852433281e-05,
      "loss": 0.6085,
      "step": 1351600
    },
    {
      "epoch": 21.21978021978022,
      "grad_norm": 4.610553741455078,
      "learning_rate": 3.673763736263737e-05,
      "loss": 0.6009,
      "step": 1351700
    },
    {
      "epoch": 21.221350078492936,
      "grad_norm": 3.4392666816711426,
      "learning_rate": 3.6736656200941914e-05,
      "loss": 0.5938,
      "step": 1351800
    },
    {
      "epoch": 21.222919937205653,
      "grad_norm": 4.054658889770508,
      "learning_rate": 3.673567503924647e-05,
      "loss": 0.6449,
      "step": 1351900
    },
    {
      "epoch": 21.224489795918366,
      "grad_norm": 3.8610446453094482,
      "learning_rate": 3.673469387755102e-05,
      "loss": 0.5849,
      "step": 1352000
    },
    {
      "epoch": 21.226059654631083,
      "grad_norm": 4.590151309967041,
      "learning_rate": 3.6733712715855574e-05,
      "loss": 0.644,
      "step": 1352100
    },
    {
      "epoch": 21.2276295133438,
      "grad_norm": 4.926809787750244,
      "learning_rate": 3.6732731554160125e-05,
      "loss": 0.5822,
      "step": 1352200
    },
    {
      "epoch": 21.229199372056517,
      "grad_norm": 3.733257532119751,
      "learning_rate": 3.673175039246468e-05,
      "loss": 0.5986,
      "step": 1352300
    },
    {
      "epoch": 21.23076923076923,
      "grad_norm": 5.406045913696289,
      "learning_rate": 3.673076923076923e-05,
      "loss": 0.6086,
      "step": 1352400
    },
    {
      "epoch": 21.232339089481947,
      "grad_norm": 3.6825366020202637,
      "learning_rate": 3.6729788069073784e-05,
      "loss": 0.6351,
      "step": 1352500
    },
    {
      "epoch": 21.233908948194664,
      "grad_norm": 3.8466718196868896,
      "learning_rate": 3.6728806907378335e-05,
      "loss": 0.5956,
      "step": 1352600
    },
    {
      "epoch": 21.235478806907377,
      "grad_norm": 3.556140422821045,
      "learning_rate": 3.672782574568289e-05,
      "loss": 0.576,
      "step": 1352700
    },
    {
      "epoch": 21.237048665620094,
      "grad_norm": 3.6335785388946533,
      "learning_rate": 3.6726844583987444e-05,
      "loss": 0.5676,
      "step": 1352800
    },
    {
      "epoch": 21.23861852433281,
      "grad_norm": 2.2701222896575928,
      "learning_rate": 3.6725863422291995e-05,
      "loss": 0.6255,
      "step": 1352900
    },
    {
      "epoch": 21.240188383045528,
      "grad_norm": 3.7384800910949707,
      "learning_rate": 3.6724882260596546e-05,
      "loss": 0.6053,
      "step": 1353000
    },
    {
      "epoch": 21.24175824175824,
      "grad_norm": 4.663439750671387,
      "learning_rate": 3.6723901098901104e-05,
      "loss": 0.638,
      "step": 1353100
    },
    {
      "epoch": 21.243328100470958,
      "grad_norm": 4.164417743682861,
      "learning_rate": 3.6722919937205655e-05,
      "loss": 0.5814,
      "step": 1353200
    },
    {
      "epoch": 21.244897959183675,
      "grad_norm": 4.258523941040039,
      "learning_rate": 3.6721938775510206e-05,
      "loss": 0.625,
      "step": 1353300
    },
    {
      "epoch": 21.246467817896388,
      "grad_norm": 3.843132972717285,
      "learning_rate": 3.6720957613814763e-05,
      "loss": 0.5902,
      "step": 1353400
    },
    {
      "epoch": 21.248037676609105,
      "grad_norm": 3.985513925552368,
      "learning_rate": 3.671997645211931e-05,
      "loss": 0.6016,
      "step": 1353500
    },
    {
      "epoch": 21.24960753532182,
      "grad_norm": 2.466585874557495,
      "learning_rate": 3.6718995290423865e-05,
      "loss": 0.5553,
      "step": 1353600
    },
    {
      "epoch": 21.25117739403454,
      "grad_norm": 4.099727630615234,
      "learning_rate": 3.6718014128728416e-05,
      "loss": 0.6338,
      "step": 1353700
    },
    {
      "epoch": 21.252747252747252,
      "grad_norm": 4.550411701202393,
      "learning_rate": 3.671703296703297e-05,
      "loss": 0.5972,
      "step": 1353800
    },
    {
      "epoch": 21.25431711145997,
      "grad_norm": 3.8634798526763916,
      "learning_rate": 3.671605180533752e-05,
      "loss": 0.5705,
      "step": 1353900
    },
    {
      "epoch": 21.255886970172686,
      "grad_norm": 4.6094183921813965,
      "learning_rate": 3.6715070643642076e-05,
      "loss": 0.6189,
      "step": 1354000
    },
    {
      "epoch": 21.2574568288854,
      "grad_norm": 2.8932604789733887,
      "learning_rate": 3.671408948194663e-05,
      "loss": 0.574,
      "step": 1354100
    },
    {
      "epoch": 21.259026687598116,
      "grad_norm": 3.4307169914245605,
      "learning_rate": 3.671310832025118e-05,
      "loss": 0.5864,
      "step": 1354200
    },
    {
      "epoch": 21.260596546310833,
      "grad_norm": 3.944978952407837,
      "learning_rate": 3.671212715855573e-05,
      "loss": 0.5842,
      "step": 1354300
    },
    {
      "epoch": 21.26216640502355,
      "grad_norm": 3.8162286281585693,
      "learning_rate": 3.671114599686029e-05,
      "loss": 0.5955,
      "step": 1354400
    },
    {
      "epoch": 21.263736263736263,
      "grad_norm": 2.9801688194274902,
      "learning_rate": 3.671016483516483e-05,
      "loss": 0.5812,
      "step": 1354500
    },
    {
      "epoch": 21.26530612244898,
      "grad_norm": 3.05625319480896,
      "learning_rate": 3.670918367346939e-05,
      "loss": 0.5856,
      "step": 1354600
    },
    {
      "epoch": 21.266875981161697,
      "grad_norm": 2.296199083328247,
      "learning_rate": 3.670820251177394e-05,
      "loss": 0.5987,
      "step": 1354700
    },
    {
      "epoch": 21.26844583987441,
      "grad_norm": 3.4480772018432617,
      "learning_rate": 3.67072213500785e-05,
      "loss": 0.6194,
      "step": 1354800
    },
    {
      "epoch": 21.270015698587127,
      "grad_norm": 3.266078472137451,
      "learning_rate": 3.670624018838305e-05,
      "loss": 0.611,
      "step": 1354900
    },
    {
      "epoch": 21.271585557299844,
      "grad_norm": 4.13835334777832,
      "learning_rate": 3.67052590266876e-05,
      "loss": 0.6106,
      "step": 1355000
    },
    {
      "epoch": 21.27315541601256,
      "grad_norm": 3.743072032928467,
      "learning_rate": 3.670427786499215e-05,
      "loss": 0.6279,
      "step": 1355100
    },
    {
      "epoch": 21.274725274725274,
      "grad_norm": 3.679475784301758,
      "learning_rate": 3.67032967032967e-05,
      "loss": 0.6549,
      "step": 1355200
    },
    {
      "epoch": 21.27629513343799,
      "grad_norm": 3.1708309650421143,
      "learning_rate": 3.670231554160126e-05,
      "loss": 0.5835,
      "step": 1355300
    },
    {
      "epoch": 21.277864992150707,
      "grad_norm": 3.2949843406677246,
      "learning_rate": 3.670133437990581e-05,
      "loss": 0.584,
      "step": 1355400
    },
    {
      "epoch": 21.27943485086342,
      "grad_norm": 4.088054180145264,
      "learning_rate": 3.670035321821037e-05,
      "loss": 0.6253,
      "step": 1355500
    },
    {
      "epoch": 21.281004709576138,
      "grad_norm": 4.926584720611572,
      "learning_rate": 3.669937205651491e-05,
      "loss": 0.6213,
      "step": 1355600
    },
    {
      "epoch": 21.282574568288855,
      "grad_norm": 3.8650286197662354,
      "learning_rate": 3.669839089481947e-05,
      "loss": 0.5928,
      "step": 1355700
    },
    {
      "epoch": 21.28414442700157,
      "grad_norm": 2.7549006938934326,
      "learning_rate": 3.669740973312402e-05,
      "loss": 0.5833,
      "step": 1355800
    },
    {
      "epoch": 21.285714285714285,
      "grad_norm": 4.4833083152771,
      "learning_rate": 3.669642857142857e-05,
      "loss": 0.6313,
      "step": 1355900
    },
    {
      "epoch": 21.287284144427,
      "grad_norm": 3.090958833694458,
      "learning_rate": 3.669544740973312e-05,
      "loss": 0.5859,
      "step": 1356000
    },
    {
      "epoch": 21.28885400313972,
      "grad_norm": 3.6641218662261963,
      "learning_rate": 3.669446624803768e-05,
      "loss": 0.6308,
      "step": 1356100
    },
    {
      "epoch": 21.29042386185243,
      "grad_norm": 3.5580592155456543,
      "learning_rate": 3.669348508634223e-05,
      "loss": 0.572,
      "step": 1356200
    },
    {
      "epoch": 21.29199372056515,
      "grad_norm": 3.6105661392211914,
      "learning_rate": 3.669250392464678e-05,
      "loss": 0.606,
      "step": 1356300
    },
    {
      "epoch": 21.293563579277865,
      "grad_norm": 3.691847085952759,
      "learning_rate": 3.6691522762951334e-05,
      "loss": 0.6203,
      "step": 1356400
    },
    {
      "epoch": 21.295133437990582,
      "grad_norm": 3.5200579166412354,
      "learning_rate": 3.669054160125589e-05,
      "loss": 0.5965,
      "step": 1356500
    },
    {
      "epoch": 21.296703296703296,
      "grad_norm": 4.4803009033203125,
      "learning_rate": 3.6689560439560435e-05,
      "loss": 0.5978,
      "step": 1356600
    },
    {
      "epoch": 21.298273155416013,
      "grad_norm": 4.020823955535889,
      "learning_rate": 3.668857927786499e-05,
      "loss": 0.628,
      "step": 1356700
    },
    {
      "epoch": 21.29984301412873,
      "grad_norm": 4.57909631729126,
      "learning_rate": 3.6687598116169544e-05,
      "loss": 0.5791,
      "step": 1356800
    },
    {
      "epoch": 21.301412872841443,
      "grad_norm": 2.9666216373443604,
      "learning_rate": 3.66866169544741e-05,
      "loss": 0.5589,
      "step": 1356900
    },
    {
      "epoch": 21.30298273155416,
      "grad_norm": 2.7480459213256836,
      "learning_rate": 3.668563579277865e-05,
      "loss": 0.6184,
      "step": 1357000
    },
    {
      "epoch": 21.304552590266876,
      "grad_norm": 3.847006320953369,
      "learning_rate": 3.6684654631083204e-05,
      "loss": 0.5664,
      "step": 1357100
    },
    {
      "epoch": 21.306122448979593,
      "grad_norm": 3.6181511878967285,
      "learning_rate": 3.6683673469387755e-05,
      "loss": 0.6221,
      "step": 1357200
    },
    {
      "epoch": 21.307692307692307,
      "grad_norm": 4.15518856048584,
      "learning_rate": 3.6682692307692306e-05,
      "loss": 0.613,
      "step": 1357300
    },
    {
      "epoch": 21.309262166405023,
      "grad_norm": 4.082788467407227,
      "learning_rate": 3.6681711145996864e-05,
      "loss": 0.5996,
      "step": 1357400
    },
    {
      "epoch": 21.31083202511774,
      "grad_norm": 3.1372578144073486,
      "learning_rate": 3.6680729984301415e-05,
      "loss": 0.6024,
      "step": 1357500
    },
    {
      "epoch": 21.312401883830454,
      "grad_norm": 3.285574436187744,
      "learning_rate": 3.667974882260597e-05,
      "loss": 0.5938,
      "step": 1357600
    },
    {
      "epoch": 21.31397174254317,
      "grad_norm": 2.9938204288482666,
      "learning_rate": 3.667876766091052e-05,
      "loss": 0.6342,
      "step": 1357700
    },
    {
      "epoch": 21.315541601255887,
      "grad_norm": 3.8940324783325195,
      "learning_rate": 3.6677786499215074e-05,
      "loss": 0.5973,
      "step": 1357800
    },
    {
      "epoch": 21.317111459968604,
      "grad_norm": 3.846097469329834,
      "learning_rate": 3.6676805337519625e-05,
      "loss": 0.635,
      "step": 1357900
    },
    {
      "epoch": 21.318681318681318,
      "grad_norm": 3.7509984970092773,
      "learning_rate": 3.6675824175824176e-05,
      "loss": 0.5939,
      "step": 1358000
    },
    {
      "epoch": 21.320251177394034,
      "grad_norm": 3.7087063789367676,
      "learning_rate": 3.667484301412873e-05,
      "loss": 0.6273,
      "step": 1358100
    },
    {
      "epoch": 21.32182103610675,
      "grad_norm": 3.4641125202178955,
      "learning_rate": 3.6673861852433285e-05,
      "loss": 0.5933,
      "step": 1358200
    },
    {
      "epoch": 21.323390894819465,
      "grad_norm": 3.5512936115264893,
      "learning_rate": 3.6672880690737836e-05,
      "loss": 0.6301,
      "step": 1358300
    },
    {
      "epoch": 21.32496075353218,
      "grad_norm": 4.141427993774414,
      "learning_rate": 3.667189952904239e-05,
      "loss": 0.5968,
      "step": 1358400
    },
    {
      "epoch": 21.3265306122449,
      "grad_norm": 3.2503182888031006,
      "learning_rate": 3.667091836734694e-05,
      "loss": 0.6097,
      "step": 1358500
    },
    {
      "epoch": 21.328100470957615,
      "grad_norm": 2.504254102706909,
      "learning_rate": 3.6669937205651496e-05,
      "loss": 0.6009,
      "step": 1358600
    },
    {
      "epoch": 21.32967032967033,
      "grad_norm": 3.474416494369507,
      "learning_rate": 3.666895604395604e-05,
      "loss": 0.632,
      "step": 1358700
    },
    {
      "epoch": 21.331240188383045,
      "grad_norm": 3.6124472618103027,
      "learning_rate": 3.66679748822606e-05,
      "loss": 0.5934,
      "step": 1358800
    },
    {
      "epoch": 21.332810047095762,
      "grad_norm": 4.047684669494629,
      "learning_rate": 3.666699372056515e-05,
      "loss": 0.6103,
      "step": 1358900
    },
    {
      "epoch": 21.334379905808476,
      "grad_norm": 2.8834497928619385,
      "learning_rate": 3.6666012558869706e-05,
      "loss": 0.5753,
      "step": 1359000
    },
    {
      "epoch": 21.335949764521192,
      "grad_norm": 4.05163049697876,
      "learning_rate": 3.666503139717426e-05,
      "loss": 0.6026,
      "step": 1359100
    },
    {
      "epoch": 21.33751962323391,
      "grad_norm": 3.6922340393066406,
      "learning_rate": 3.666405023547881e-05,
      "loss": 0.6063,
      "step": 1359200
    },
    {
      "epoch": 21.339089481946626,
      "grad_norm": 3.8864409923553467,
      "learning_rate": 3.666306907378336e-05,
      "loss": 0.6057,
      "step": 1359300
    },
    {
      "epoch": 21.34065934065934,
      "grad_norm": 2.5895581245422363,
      "learning_rate": 3.666208791208791e-05,
      "loss": 0.5799,
      "step": 1359400
    },
    {
      "epoch": 21.342229199372056,
      "grad_norm": 3.1513850688934326,
      "learning_rate": 3.666110675039247e-05,
      "loss": 0.6044,
      "step": 1359500
    },
    {
      "epoch": 21.343799058084773,
      "grad_norm": 3.8768417835235596,
      "learning_rate": 3.666012558869702e-05,
      "loss": 0.57,
      "step": 1359600
    },
    {
      "epoch": 21.345368916797486,
      "grad_norm": 3.665149450302124,
      "learning_rate": 3.665914442700158e-05,
      "loss": 0.5998,
      "step": 1359700
    },
    {
      "epoch": 21.346938775510203,
      "grad_norm": 4.1005473136901855,
      "learning_rate": 3.665816326530612e-05,
      "loss": 0.5646,
      "step": 1359800
    },
    {
      "epoch": 21.34850863422292,
      "grad_norm": 3.9834063053131104,
      "learning_rate": 3.665718210361068e-05,
      "loss": 0.5524,
      "step": 1359900
    },
    {
      "epoch": 21.350078492935637,
      "grad_norm": 3.7899680137634277,
      "learning_rate": 3.665620094191523e-05,
      "loss": 0.6021,
      "step": 1360000
    },
    {
      "epoch": 21.35164835164835,
      "grad_norm": 2.5265655517578125,
      "learning_rate": 3.665521978021978e-05,
      "loss": 0.6019,
      "step": 1360100
    },
    {
      "epoch": 21.353218210361067,
      "grad_norm": 3.8044164180755615,
      "learning_rate": 3.665423861852433e-05,
      "loss": 0.5785,
      "step": 1360200
    },
    {
      "epoch": 21.354788069073784,
      "grad_norm": 4.191407203674316,
      "learning_rate": 3.665325745682889e-05,
      "loss": 0.5976,
      "step": 1360300
    },
    {
      "epoch": 21.356357927786497,
      "grad_norm": 3.9945313930511475,
      "learning_rate": 3.665227629513344e-05,
      "loss": 0.6329,
      "step": 1360400
    },
    {
      "epoch": 21.357927786499214,
      "grad_norm": 4.103731155395508,
      "learning_rate": 3.665129513343799e-05,
      "loss": 0.596,
      "step": 1360500
    },
    {
      "epoch": 21.35949764521193,
      "grad_norm": 1.9543108940124512,
      "learning_rate": 3.665031397174254e-05,
      "loss": 0.59,
      "step": 1360600
    },
    {
      "epoch": 21.361067503924648,
      "grad_norm": 4.662428855895996,
      "learning_rate": 3.66493328100471e-05,
      "loss": 0.6186,
      "step": 1360700
    },
    {
      "epoch": 21.36263736263736,
      "grad_norm": 3.616736650466919,
      "learning_rate": 3.6648351648351644e-05,
      "loss": 0.6092,
      "step": 1360800
    },
    {
      "epoch": 21.364207221350078,
      "grad_norm": 1.996598243713379,
      "learning_rate": 3.66473704866562e-05,
      "loss": 0.5808,
      "step": 1360900
    },
    {
      "epoch": 21.365777080062795,
      "grad_norm": 3.0862669944763184,
      "learning_rate": 3.664638932496075e-05,
      "loss": 0.6342,
      "step": 1361000
    },
    {
      "epoch": 21.367346938775512,
      "grad_norm": 3.762716054916382,
      "learning_rate": 3.664540816326531e-05,
      "loss": 0.5996,
      "step": 1361100
    },
    {
      "epoch": 21.368916797488225,
      "grad_norm": 2.9577219486236572,
      "learning_rate": 3.664442700156986e-05,
      "loss": 0.6278,
      "step": 1361200
    },
    {
      "epoch": 21.370486656200942,
      "grad_norm": 4.4279327392578125,
      "learning_rate": 3.664344583987441e-05,
      "loss": 0.607,
      "step": 1361300
    },
    {
      "epoch": 21.37205651491366,
      "grad_norm": 4.203034400939941,
      "learning_rate": 3.6642464678178964e-05,
      "loss": 0.5886,
      "step": 1361400
    },
    {
      "epoch": 21.373626373626372,
      "grad_norm": 4.893975257873535,
      "learning_rate": 3.6641483516483515e-05,
      "loss": 0.6054,
      "step": 1361500
    },
    {
      "epoch": 21.37519623233909,
      "grad_norm": 4.077204704284668,
      "learning_rate": 3.664050235478807e-05,
      "loss": 0.6195,
      "step": 1361600
    },
    {
      "epoch": 21.376766091051806,
      "grad_norm": 3.552314043045044,
      "learning_rate": 3.6639521193092624e-05,
      "loss": 0.6289,
      "step": 1361700
    },
    {
      "epoch": 21.378335949764523,
      "grad_norm": 4.805381774902344,
      "learning_rate": 3.663854003139718e-05,
      "loss": 0.6223,
      "step": 1361800
    },
    {
      "epoch": 21.379905808477236,
      "grad_norm": 4.615394115447998,
      "learning_rate": 3.6637558869701726e-05,
      "loss": 0.6249,
      "step": 1361900
    },
    {
      "epoch": 21.381475667189953,
      "grad_norm": 3.5712966918945312,
      "learning_rate": 3.663657770800628e-05,
      "loss": 0.6102,
      "step": 1362000
    },
    {
      "epoch": 21.38304552590267,
      "grad_norm": 4.352736949920654,
      "learning_rate": 3.6635596546310834e-05,
      "loss": 0.6465,
      "step": 1362100
    },
    {
      "epoch": 21.384615384615383,
      "grad_norm": 3.380007028579712,
      "learning_rate": 3.6634615384615385e-05,
      "loss": 0.568,
      "step": 1362200
    },
    {
      "epoch": 21.3861852433281,
      "grad_norm": 2.5656068325042725,
      "learning_rate": 3.6633634222919936e-05,
      "loss": 0.6165,
      "step": 1362300
    },
    {
      "epoch": 21.387755102040817,
      "grad_norm": 4.28638219833374,
      "learning_rate": 3.6632653061224494e-05,
      "loss": 0.6182,
      "step": 1362400
    },
    {
      "epoch": 21.389324960753534,
      "grad_norm": 4.32451868057251,
      "learning_rate": 3.6631671899529045e-05,
      "loss": 0.5763,
      "step": 1362500
    },
    {
      "epoch": 21.390894819466247,
      "grad_norm": 4.1272292137146,
      "learning_rate": 3.6630690737833596e-05,
      "loss": 0.6293,
      "step": 1362600
    },
    {
      "epoch": 21.392464678178964,
      "grad_norm": 3.484279155731201,
      "learning_rate": 3.662970957613815e-05,
      "loss": 0.6048,
      "step": 1362700
    },
    {
      "epoch": 21.39403453689168,
      "grad_norm": 3.6969950199127197,
      "learning_rate": 3.6628728414442705e-05,
      "loss": 0.6389,
      "step": 1362800
    },
    {
      "epoch": 21.395604395604394,
      "grad_norm": 4.627934455871582,
      "learning_rate": 3.662774725274725e-05,
      "loss": 0.5913,
      "step": 1362900
    },
    {
      "epoch": 21.39717425431711,
      "grad_norm": 3.1302597522735596,
      "learning_rate": 3.662676609105181e-05,
      "loss": 0.622,
      "step": 1363000
    },
    {
      "epoch": 21.398744113029828,
      "grad_norm": 4.391976833343506,
      "learning_rate": 3.662578492935636e-05,
      "loss": 0.5744,
      "step": 1363100
    },
    {
      "epoch": 21.400313971742545,
      "grad_norm": 3.7700228691101074,
      "learning_rate": 3.6624803767660915e-05,
      "loss": 0.6274,
      "step": 1363200
    },
    {
      "epoch": 21.401883830455258,
      "grad_norm": 4.071144104003906,
      "learning_rate": 3.6623822605965466e-05,
      "loss": 0.5995,
      "step": 1363300
    },
    {
      "epoch": 21.403453689167975,
      "grad_norm": 4.375800609588623,
      "learning_rate": 3.662284144427002e-05,
      "loss": 0.6377,
      "step": 1363400
    },
    {
      "epoch": 21.405023547880692,
      "grad_norm": 2.7241525650024414,
      "learning_rate": 3.662186028257457e-05,
      "loss": 0.5882,
      "step": 1363500
    },
    {
      "epoch": 21.406593406593405,
      "grad_norm": 3.894104242324829,
      "learning_rate": 3.662087912087912e-05,
      "loss": 0.6675,
      "step": 1363600
    },
    {
      "epoch": 21.408163265306122,
      "grad_norm": 4.059089183807373,
      "learning_rate": 3.661989795918368e-05,
      "loss": 0.5782,
      "step": 1363700
    },
    {
      "epoch": 21.40973312401884,
      "grad_norm": 4.3429436683654785,
      "learning_rate": 3.661891679748823e-05,
      "loss": 0.6107,
      "step": 1363800
    },
    {
      "epoch": 21.411302982731556,
      "grad_norm": 3.2262823581695557,
      "learning_rate": 3.6617935635792786e-05,
      "loss": 0.624,
      "step": 1363900
    },
    {
      "epoch": 21.41287284144427,
      "grad_norm": 3.5421271324157715,
      "learning_rate": 3.661695447409733e-05,
      "loss": 0.596,
      "step": 1364000
    },
    {
      "epoch": 21.414442700156986,
      "grad_norm": 4.272432804107666,
      "learning_rate": 3.661597331240189e-05,
      "loss": 0.6506,
      "step": 1364100
    },
    {
      "epoch": 21.416012558869703,
      "grad_norm": 3.534583806991577,
      "learning_rate": 3.661499215070644e-05,
      "loss": 0.5837,
      "step": 1364200
    },
    {
      "epoch": 21.417582417582416,
      "grad_norm": 3.296497106552124,
      "learning_rate": 3.661401098901099e-05,
      "loss": 0.643,
      "step": 1364300
    },
    {
      "epoch": 21.419152276295133,
      "grad_norm": 4.010897636413574,
      "learning_rate": 3.661302982731554e-05,
      "loss": 0.6492,
      "step": 1364400
    },
    {
      "epoch": 21.42072213500785,
      "grad_norm": 4.295329570770264,
      "learning_rate": 3.66120486656201e-05,
      "loss": 0.6324,
      "step": 1364500
    },
    {
      "epoch": 21.422291993720567,
      "grad_norm": 1.7120168209075928,
      "learning_rate": 3.661106750392465e-05,
      "loss": 0.5819,
      "step": 1364600
    },
    {
      "epoch": 21.42386185243328,
      "grad_norm": 4.505382061004639,
      "learning_rate": 3.66100863422292e-05,
      "loss": 0.6442,
      "step": 1364700
    },
    {
      "epoch": 21.425431711145997,
      "grad_norm": 2.307793617248535,
      "learning_rate": 3.660910518053375e-05,
      "loss": 0.5706,
      "step": 1364800
    },
    {
      "epoch": 21.427001569858714,
      "grad_norm": 4.141864776611328,
      "learning_rate": 3.660812401883831e-05,
      "loss": 0.6317,
      "step": 1364900
    },
    {
      "epoch": 21.428571428571427,
      "grad_norm": 3.9586715698242188,
      "learning_rate": 3.6607142857142853e-05,
      "loss": 0.5983,
      "step": 1365000
    },
    {
      "epoch": 21.430141287284144,
      "grad_norm": 4.145744323730469,
      "learning_rate": 3.660616169544741e-05,
      "loss": 0.62,
      "step": 1365100
    },
    {
      "epoch": 21.43171114599686,
      "grad_norm": 3.399094820022583,
      "learning_rate": 3.660518053375196e-05,
      "loss": 0.624,
      "step": 1365200
    },
    {
      "epoch": 21.433281004709578,
      "grad_norm": 4.769970893859863,
      "learning_rate": 3.660419937205652e-05,
      "loss": 0.5941,
      "step": 1365300
    },
    {
      "epoch": 21.43485086342229,
      "grad_norm": 4.440638542175293,
      "learning_rate": 3.660321821036107e-05,
      "loss": 0.5889,
      "step": 1365400
    },
    {
      "epoch": 21.436420722135008,
      "grad_norm": 3.316920757293701,
      "learning_rate": 3.660223704866562e-05,
      "loss": 0.5988,
      "step": 1365500
    },
    {
      "epoch": 21.437990580847725,
      "grad_norm": 4.941013813018799,
      "learning_rate": 3.660125588697017e-05,
      "loss": 0.6471,
      "step": 1365600
    },
    {
      "epoch": 21.439560439560438,
      "grad_norm": 4.420786380767822,
      "learning_rate": 3.6600274725274724e-05,
      "loss": 0.6117,
      "step": 1365700
    },
    {
      "epoch": 21.441130298273155,
      "grad_norm": 5.276794910430908,
      "learning_rate": 3.659929356357928e-05,
      "loss": 0.6121,
      "step": 1365800
    },
    {
      "epoch": 21.44270015698587,
      "grad_norm": 3.486814022064209,
      "learning_rate": 3.659831240188383e-05,
      "loss": 0.6065,
      "step": 1365900
    },
    {
      "epoch": 21.44427001569859,
      "grad_norm": 4.167566299438477,
      "learning_rate": 3.659733124018839e-05,
      "loss": 0.6167,
      "step": 1366000
    },
    {
      "epoch": 21.445839874411302,
      "grad_norm": 3.870098829269409,
      "learning_rate": 3.6596350078492935e-05,
      "loss": 0.563,
      "step": 1366100
    },
    {
      "epoch": 21.44740973312402,
      "grad_norm": 3.3865959644317627,
      "learning_rate": 3.659536891679749e-05,
      "loss": 0.6056,
      "step": 1366200
    },
    {
      "epoch": 21.448979591836736,
      "grad_norm": 3.581188201904297,
      "learning_rate": 3.659438775510204e-05,
      "loss": 0.599,
      "step": 1366300
    },
    {
      "epoch": 21.45054945054945,
      "grad_norm": 4.813455104827881,
      "learning_rate": 3.6593406593406594e-05,
      "loss": 0.5991,
      "step": 1366400
    },
    {
      "epoch": 21.452119309262166,
      "grad_norm": 4.582020282745361,
      "learning_rate": 3.6592425431711145e-05,
      "loss": 0.6322,
      "step": 1366500
    },
    {
      "epoch": 21.453689167974883,
      "grad_norm": 4.225686550140381,
      "learning_rate": 3.65914442700157e-05,
      "loss": 0.5917,
      "step": 1366600
    },
    {
      "epoch": 21.4552590266876,
      "grad_norm": 3.628079891204834,
      "learning_rate": 3.6590463108320254e-05,
      "loss": 0.6171,
      "step": 1366700
    },
    {
      "epoch": 21.456828885400313,
      "grad_norm": 3.854264974594116,
      "learning_rate": 3.6589481946624805e-05,
      "loss": 0.5998,
      "step": 1366800
    },
    {
      "epoch": 21.45839874411303,
      "grad_norm": 3.2469124794006348,
      "learning_rate": 3.6588500784929356e-05,
      "loss": 0.5828,
      "step": 1366900
    },
    {
      "epoch": 21.459968602825747,
      "grad_norm": 4.568091869354248,
      "learning_rate": 3.6587519623233914e-05,
      "loss": 0.5905,
      "step": 1367000
    },
    {
      "epoch": 21.46153846153846,
      "grad_norm": 3.5220603942871094,
      "learning_rate": 3.658653846153846e-05,
      "loss": 0.6049,
      "step": 1367100
    },
    {
      "epoch": 21.463108320251177,
      "grad_norm": 3.5718934535980225,
      "learning_rate": 3.6585557299843016e-05,
      "loss": 0.6674,
      "step": 1367200
    },
    {
      "epoch": 21.464678178963894,
      "grad_norm": 3.864102363586426,
      "learning_rate": 3.658457613814757e-05,
      "loss": 0.555,
      "step": 1367300
    },
    {
      "epoch": 21.46624803767661,
      "grad_norm": 3.628650188446045,
      "learning_rate": 3.6583594976452124e-05,
      "loss": 0.6057,
      "step": 1367400
    },
    {
      "epoch": 21.467817896389324,
      "grad_norm": 4.219428062438965,
      "learning_rate": 3.6582613814756675e-05,
      "loss": 0.5919,
      "step": 1367500
    },
    {
      "epoch": 21.46938775510204,
      "grad_norm": 3.6156632900238037,
      "learning_rate": 3.6581632653061226e-05,
      "loss": 0.6426,
      "step": 1367600
    },
    {
      "epoch": 21.470957613814758,
      "grad_norm": 3.4477033615112305,
      "learning_rate": 3.658065149136578e-05,
      "loss": 0.6102,
      "step": 1367700
    },
    {
      "epoch": 21.47252747252747,
      "grad_norm": 3.986522912979126,
      "learning_rate": 3.657967032967033e-05,
      "loss": 0.6218,
      "step": 1367800
    },
    {
      "epoch": 21.474097331240188,
      "grad_norm": 4.527287483215332,
      "learning_rate": 3.6578689167974886e-05,
      "loss": 0.5583,
      "step": 1367900
    },
    {
      "epoch": 21.475667189952905,
      "grad_norm": 3.243511438369751,
      "learning_rate": 3.657770800627944e-05,
      "loss": 0.5738,
      "step": 1368000
    },
    {
      "epoch": 21.47723704866562,
      "grad_norm": 3.8833351135253906,
      "learning_rate": 3.6576726844583995e-05,
      "loss": 0.5835,
      "step": 1368100
    },
    {
      "epoch": 21.478806907378335,
      "grad_norm": 4.1119465827941895,
      "learning_rate": 3.657574568288854e-05,
      "loss": 0.6032,
      "step": 1368200
    },
    {
      "epoch": 21.48037676609105,
      "grad_norm": 4.150993347167969,
      "learning_rate": 3.65747645211931e-05,
      "loss": 0.6156,
      "step": 1368300
    },
    {
      "epoch": 21.48194662480377,
      "grad_norm": 3.2844018936157227,
      "learning_rate": 3.657378335949765e-05,
      "loss": 0.5694,
      "step": 1368400
    },
    {
      "epoch": 21.483516483516482,
      "grad_norm": 4.2042036056518555,
      "learning_rate": 3.65728021978022e-05,
      "loss": 0.6205,
      "step": 1368500
    },
    {
      "epoch": 21.4850863422292,
      "grad_norm": 3.8905410766601562,
      "learning_rate": 3.657182103610675e-05,
      "loss": 0.5928,
      "step": 1368600
    },
    {
      "epoch": 21.486656200941916,
      "grad_norm": 4.039712429046631,
      "learning_rate": 3.657083987441131e-05,
      "loss": 0.604,
      "step": 1368700
    },
    {
      "epoch": 21.488226059654632,
      "grad_norm": 4.148369312286377,
      "learning_rate": 3.656985871271586e-05,
      "loss": 0.6342,
      "step": 1368800
    },
    {
      "epoch": 21.489795918367346,
      "grad_norm": 4.460221290588379,
      "learning_rate": 3.656887755102041e-05,
      "loss": 0.6413,
      "step": 1368900
    },
    {
      "epoch": 21.491365777080063,
      "grad_norm": 3.408878803253174,
      "learning_rate": 3.656789638932496e-05,
      "loss": 0.597,
      "step": 1369000
    },
    {
      "epoch": 21.49293563579278,
      "grad_norm": 4.963842868804932,
      "learning_rate": 3.656691522762952e-05,
      "loss": 0.6329,
      "step": 1369100
    },
    {
      "epoch": 21.494505494505496,
      "grad_norm": 3.377678632736206,
      "learning_rate": 3.656593406593406e-05,
      "loss": 0.6341,
      "step": 1369200
    },
    {
      "epoch": 21.49607535321821,
      "grad_norm": 3.6984832286834717,
      "learning_rate": 3.656495290423862e-05,
      "loss": 0.6095,
      "step": 1369300
    },
    {
      "epoch": 21.497645211930926,
      "grad_norm": 4.653759479522705,
      "learning_rate": 3.656397174254317e-05,
      "loss": 0.6259,
      "step": 1369400
    },
    {
      "epoch": 21.499215070643643,
      "grad_norm": 3.5763232707977295,
      "learning_rate": 3.656299058084773e-05,
      "loss": 0.6187,
      "step": 1369500
    },
    {
      "epoch": 21.500784929356357,
      "grad_norm": 3.7745625972747803,
      "learning_rate": 3.656200941915228e-05,
      "loss": 0.6093,
      "step": 1369600
    },
    {
      "epoch": 21.502354788069074,
      "grad_norm": 3.782442808151245,
      "learning_rate": 3.656102825745683e-05,
      "loss": 0.5999,
      "step": 1369700
    },
    {
      "epoch": 21.50392464678179,
      "grad_norm": 3.273418664932251,
      "learning_rate": 3.656004709576138e-05,
      "loss": 0.5732,
      "step": 1369800
    },
    {
      "epoch": 21.505494505494504,
      "grad_norm": 4.282756328582764,
      "learning_rate": 3.655906593406593e-05,
      "loss": 0.5769,
      "step": 1369900
    },
    {
      "epoch": 21.50706436420722,
      "grad_norm": 3.4563097953796387,
      "learning_rate": 3.655808477237049e-05,
      "loss": 0.5611,
      "step": 1370000
    },
    {
      "epoch": 21.508634222919937,
      "grad_norm": 4.8558831214904785,
      "learning_rate": 3.655710361067504e-05,
      "loss": 0.5816,
      "step": 1370100
    },
    {
      "epoch": 21.510204081632654,
      "grad_norm": 3.6447317600250244,
      "learning_rate": 3.65561224489796e-05,
      "loss": 0.6419,
      "step": 1370200
    },
    {
      "epoch": 21.511773940345368,
      "grad_norm": 4.025426387786865,
      "learning_rate": 3.6555141287284144e-05,
      "loss": 0.5722,
      "step": 1370300
    },
    {
      "epoch": 21.513343799058084,
      "grad_norm": 5.465605735778809,
      "learning_rate": 3.65541601255887e-05,
      "loss": 0.5782,
      "step": 1370400
    },
    {
      "epoch": 21.5149136577708,
      "grad_norm": 2.5395689010620117,
      "learning_rate": 3.655317896389325e-05,
      "loss": 0.5871,
      "step": 1370500
    },
    {
      "epoch": 21.516483516483518,
      "grad_norm": 3.4942212104797363,
      "learning_rate": 3.65521978021978e-05,
      "loss": 0.6094,
      "step": 1370600
    },
    {
      "epoch": 21.51805337519623,
      "grad_norm": 4.014894485473633,
      "learning_rate": 3.6551216640502354e-05,
      "loss": 0.583,
      "step": 1370700
    },
    {
      "epoch": 21.51962323390895,
      "grad_norm": 4.348936557769775,
      "learning_rate": 3.655023547880691e-05,
      "loss": 0.5889,
      "step": 1370800
    },
    {
      "epoch": 21.521193092621665,
      "grad_norm": 3.530665159225464,
      "learning_rate": 3.654925431711146e-05,
      "loss": 0.5889,
      "step": 1370900
    },
    {
      "epoch": 21.52276295133438,
      "grad_norm": 5.00381326675415,
      "learning_rate": 3.6548273155416014e-05,
      "loss": 0.6129,
      "step": 1371000
    },
    {
      "epoch": 21.524332810047095,
      "grad_norm": 4.141069412231445,
      "learning_rate": 3.6547291993720565e-05,
      "loss": 0.5704,
      "step": 1371100
    },
    {
      "epoch": 21.525902668759812,
      "grad_norm": 2.4303195476531982,
      "learning_rate": 3.654631083202512e-05,
      "loss": 0.6059,
      "step": 1371200
    },
    {
      "epoch": 21.52747252747253,
      "grad_norm": 3.227339267730713,
      "learning_rate": 3.654532967032967e-05,
      "loss": 0.6013,
      "step": 1371300
    },
    {
      "epoch": 21.529042386185242,
      "grad_norm": 4.046861171722412,
      "learning_rate": 3.6544348508634225e-05,
      "loss": 0.6331,
      "step": 1371400
    },
    {
      "epoch": 21.53061224489796,
      "grad_norm": 4.095394611358643,
      "learning_rate": 3.6543367346938776e-05,
      "loss": 0.6132,
      "step": 1371500
    },
    {
      "epoch": 21.532182103610676,
      "grad_norm": 3.830181360244751,
      "learning_rate": 3.6542386185243333e-05,
      "loss": 0.6114,
      "step": 1371600
    },
    {
      "epoch": 21.53375196232339,
      "grad_norm": 4.416003704071045,
      "learning_rate": 3.6541405023547884e-05,
      "loss": 0.5993,
      "step": 1371700
    },
    {
      "epoch": 21.535321821036106,
      "grad_norm": 2.782550811767578,
      "learning_rate": 3.6540423861852435e-05,
      "loss": 0.632,
      "step": 1371800
    },
    {
      "epoch": 21.536891679748823,
      "grad_norm": 3.923846960067749,
      "learning_rate": 3.6539442700156986e-05,
      "loss": 0.6159,
      "step": 1371900
    },
    {
      "epoch": 21.53846153846154,
      "grad_norm": 3.948456287384033,
      "learning_rate": 3.653846153846154e-05,
      "loss": 0.5848,
      "step": 1372000
    },
    {
      "epoch": 21.540031397174253,
      "grad_norm": 4.406022548675537,
      "learning_rate": 3.6537480376766095e-05,
      "loss": 0.5777,
      "step": 1372100
    },
    {
      "epoch": 21.54160125588697,
      "grad_norm": 3.140805721282959,
      "learning_rate": 3.6536499215070646e-05,
      "loss": 0.5929,
      "step": 1372200
    },
    {
      "epoch": 21.543171114599687,
      "grad_norm": 4.124387264251709,
      "learning_rate": 3.6535518053375204e-05,
      "loss": 0.6409,
      "step": 1372300
    },
    {
      "epoch": 21.5447409733124,
      "grad_norm": 3.174858331680298,
      "learning_rate": 3.653453689167975e-05,
      "loss": 0.6148,
      "step": 1372400
    },
    {
      "epoch": 21.546310832025117,
      "grad_norm": 2.3891940116882324,
      "learning_rate": 3.6533555729984306e-05,
      "loss": 0.5781,
      "step": 1372500
    },
    {
      "epoch": 21.547880690737834,
      "grad_norm": 3.473748207092285,
      "learning_rate": 3.653257456828886e-05,
      "loss": 0.6133,
      "step": 1372600
    },
    {
      "epoch": 21.54945054945055,
      "grad_norm": 4.138552188873291,
      "learning_rate": 3.653159340659341e-05,
      "loss": 0.6239,
      "step": 1372700
    },
    {
      "epoch": 21.551020408163264,
      "grad_norm": 3.2715582847595215,
      "learning_rate": 3.653061224489796e-05,
      "loss": 0.6044,
      "step": 1372800
    },
    {
      "epoch": 21.55259026687598,
      "grad_norm": 3.3559186458587646,
      "learning_rate": 3.6529631083202517e-05,
      "loss": 0.6339,
      "step": 1372900
    },
    {
      "epoch": 21.554160125588698,
      "grad_norm": 4.188608169555664,
      "learning_rate": 3.652864992150707e-05,
      "loss": 0.6214,
      "step": 1373000
    },
    {
      "epoch": 21.55572998430141,
      "grad_norm": 3.576901912689209,
      "learning_rate": 3.652766875981162e-05,
      "loss": 0.6016,
      "step": 1373100
    },
    {
      "epoch": 21.55729984301413,
      "grad_norm": 4.222551345825195,
      "learning_rate": 3.652668759811617e-05,
      "loss": 0.6085,
      "step": 1373200
    },
    {
      "epoch": 21.558869701726845,
      "grad_norm": 4.524730205535889,
      "learning_rate": 3.652570643642073e-05,
      "loss": 0.614,
      "step": 1373300
    },
    {
      "epoch": 21.560439560439562,
      "grad_norm": 2.5503413677215576,
      "learning_rate": 3.652472527472527e-05,
      "loss": 0.6229,
      "step": 1373400
    },
    {
      "epoch": 21.562009419152275,
      "grad_norm": 2.75500750541687,
      "learning_rate": 3.652374411302983e-05,
      "loss": 0.5542,
      "step": 1373500
    },
    {
      "epoch": 21.563579277864992,
      "grad_norm": 4.766430377960205,
      "learning_rate": 3.652276295133438e-05,
      "loss": 0.5882,
      "step": 1373600
    },
    {
      "epoch": 21.56514913657771,
      "grad_norm": 3.884671688079834,
      "learning_rate": 3.652178178963894e-05,
      "loss": 0.6301,
      "step": 1373700
    },
    {
      "epoch": 21.566718995290422,
      "grad_norm": 2.828207015991211,
      "learning_rate": 3.652080062794349e-05,
      "loss": 0.6139,
      "step": 1373800
    },
    {
      "epoch": 21.56828885400314,
      "grad_norm": 4.572966575622559,
      "learning_rate": 3.651981946624804e-05,
      "loss": 0.6012,
      "step": 1373900
    },
    {
      "epoch": 21.569858712715856,
      "grad_norm": 4.185916900634766,
      "learning_rate": 3.651883830455259e-05,
      "loss": 0.5948,
      "step": 1374000
    },
    {
      "epoch": 21.571428571428573,
      "grad_norm": 2.529736280441284,
      "learning_rate": 3.651785714285714e-05,
      "loss": 0.6149,
      "step": 1374100
    },
    {
      "epoch": 21.572998430141286,
      "grad_norm": 3.2955918312072754,
      "learning_rate": 3.65168759811617e-05,
      "loss": 0.5887,
      "step": 1374200
    },
    {
      "epoch": 21.574568288854003,
      "grad_norm": 3.2186684608459473,
      "learning_rate": 3.651589481946625e-05,
      "loss": 0.5841,
      "step": 1374300
    },
    {
      "epoch": 21.57613814756672,
      "grad_norm": 3.167187452316284,
      "learning_rate": 3.651491365777081e-05,
      "loss": 0.5953,
      "step": 1374400
    },
    {
      "epoch": 21.577708006279433,
      "grad_norm": 3.7669663429260254,
      "learning_rate": 3.651393249607535e-05,
      "loss": 0.607,
      "step": 1374500
    },
    {
      "epoch": 21.57927786499215,
      "grad_norm": 4.353275775909424,
      "learning_rate": 3.651295133437991e-05,
      "loss": 0.6299,
      "step": 1374600
    },
    {
      "epoch": 21.580847723704867,
      "grad_norm": 4.987461566925049,
      "learning_rate": 3.651197017268446e-05,
      "loss": 0.6117,
      "step": 1374700
    },
    {
      "epoch": 21.582417582417584,
      "grad_norm": 4.235017776489258,
      "learning_rate": 3.651098901098901e-05,
      "loss": 0.5887,
      "step": 1374800
    },
    {
      "epoch": 21.583987441130297,
      "grad_norm": 2.9011478424072266,
      "learning_rate": 3.651000784929356e-05,
      "loss": 0.6051,
      "step": 1374900
    },
    {
      "epoch": 21.585557299843014,
      "grad_norm": 3.8395607471466064,
      "learning_rate": 3.650902668759812e-05,
      "loss": 0.6189,
      "step": 1375000
    },
    {
      "epoch": 21.58712715855573,
      "grad_norm": 3.642338514328003,
      "learning_rate": 3.650804552590267e-05,
      "loss": 0.6146,
      "step": 1375100
    },
    {
      "epoch": 21.588697017268444,
      "grad_norm": 5.745870590209961,
      "learning_rate": 3.650706436420722e-05,
      "loss": 0.6272,
      "step": 1375200
    },
    {
      "epoch": 21.59026687598116,
      "grad_norm": 4.798277378082275,
      "learning_rate": 3.6506083202511774e-05,
      "loss": 0.6368,
      "step": 1375300
    },
    {
      "epoch": 21.591836734693878,
      "grad_norm": 4.551138877868652,
      "learning_rate": 3.650510204081633e-05,
      "loss": 0.6247,
      "step": 1375400
    },
    {
      "epoch": 21.593406593406595,
      "grad_norm": 4.499887466430664,
      "learning_rate": 3.6504120879120876e-05,
      "loss": 0.6306,
      "step": 1375500
    },
    {
      "epoch": 21.594976452119308,
      "grad_norm": 4.448768615722656,
      "learning_rate": 3.6503139717425434e-05,
      "loss": 0.5577,
      "step": 1375600
    },
    {
      "epoch": 21.596546310832025,
      "grad_norm": 3.8358466625213623,
      "learning_rate": 3.6502158555729985e-05,
      "loss": 0.6082,
      "step": 1375700
    },
    {
      "epoch": 21.598116169544742,
      "grad_norm": 3.473161458969116,
      "learning_rate": 3.650117739403454e-05,
      "loss": 0.6106,
      "step": 1375800
    },
    {
      "epoch": 21.599686028257455,
      "grad_norm": 4.049084186553955,
      "learning_rate": 3.650019623233909e-05,
      "loss": 0.6099,
      "step": 1375900
    },
    {
      "epoch": 21.601255886970172,
      "grad_norm": 3.2605581283569336,
      "learning_rate": 3.6499215070643644e-05,
      "loss": 0.6322,
      "step": 1376000
    },
    {
      "epoch": 21.60282574568289,
      "grad_norm": 4.438331604003906,
      "learning_rate": 3.6498233908948195e-05,
      "loss": 0.627,
      "step": 1376100
    },
    {
      "epoch": 21.604395604395606,
      "grad_norm": 3.7957379817962646,
      "learning_rate": 3.6497252747252746e-05,
      "loss": 0.6016,
      "step": 1376200
    },
    {
      "epoch": 21.60596546310832,
      "grad_norm": 3.823902130126953,
      "learning_rate": 3.6496271585557304e-05,
      "loss": 0.6126,
      "step": 1376300
    },
    {
      "epoch": 21.607535321821036,
      "grad_norm": 3.5835177898406982,
      "learning_rate": 3.6495290423861855e-05,
      "loss": 0.6174,
      "step": 1376400
    },
    {
      "epoch": 21.609105180533753,
      "grad_norm": 2.2728195190429688,
      "learning_rate": 3.6494309262166406e-05,
      "loss": 0.6103,
      "step": 1376500
    },
    {
      "epoch": 21.610675039246466,
      "grad_norm": 3.9076366424560547,
      "learning_rate": 3.649332810047096e-05,
      "loss": 0.5727,
      "step": 1376600
    },
    {
      "epoch": 21.612244897959183,
      "grad_norm": 3.783846139907837,
      "learning_rate": 3.6492346938775515e-05,
      "loss": 0.5719,
      "step": 1376700
    },
    {
      "epoch": 21.6138147566719,
      "grad_norm": 3.7852065563201904,
      "learning_rate": 3.6491365777080066e-05,
      "loss": 0.5723,
      "step": 1376800
    },
    {
      "epoch": 21.615384615384617,
      "grad_norm": 3.517057180404663,
      "learning_rate": 3.649038461538462e-05,
      "loss": 0.5918,
      "step": 1376900
    },
    {
      "epoch": 21.61695447409733,
      "grad_norm": 3.2632429599761963,
      "learning_rate": 3.648940345368917e-05,
      "loss": 0.5918,
      "step": 1377000
    },
    {
      "epoch": 21.618524332810047,
      "grad_norm": 3.468524932861328,
      "learning_rate": 3.6488422291993725e-05,
      "loss": 0.6039,
      "step": 1377100
    },
    {
      "epoch": 21.620094191522764,
      "grad_norm": 4.003661155700684,
      "learning_rate": 3.648744113029827e-05,
      "loss": 0.6218,
      "step": 1377200
    },
    {
      "epoch": 21.621664050235477,
      "grad_norm": 4.119612216949463,
      "learning_rate": 3.648645996860283e-05,
      "loss": 0.6105,
      "step": 1377300
    },
    {
      "epoch": 21.623233908948194,
      "grad_norm": 3.3841865062713623,
      "learning_rate": 3.648547880690738e-05,
      "loss": 0.5247,
      "step": 1377400
    },
    {
      "epoch": 21.62480376766091,
      "grad_norm": 1.7747647762298584,
      "learning_rate": 3.6484497645211936e-05,
      "loss": 0.5977,
      "step": 1377500
    },
    {
      "epoch": 21.626373626373628,
      "grad_norm": 3.033592700958252,
      "learning_rate": 3.648351648351648e-05,
      "loss": 0.5972,
      "step": 1377600
    },
    {
      "epoch": 21.62794348508634,
      "grad_norm": 3.9575905799865723,
      "learning_rate": 3.648253532182104e-05,
      "loss": 0.5529,
      "step": 1377700
    },
    {
      "epoch": 21.629513343799058,
      "grad_norm": 3.1895968914031982,
      "learning_rate": 3.648155416012559e-05,
      "loss": 0.5996,
      "step": 1377800
    },
    {
      "epoch": 21.631083202511775,
      "grad_norm": 3.703897476196289,
      "learning_rate": 3.648057299843014e-05,
      "loss": 0.6219,
      "step": 1377900
    },
    {
      "epoch": 21.632653061224488,
      "grad_norm": 3.4655845165252686,
      "learning_rate": 3.64795918367347e-05,
      "loss": 0.5833,
      "step": 1378000
    },
    {
      "epoch": 21.634222919937205,
      "grad_norm": 3.1493258476257324,
      "learning_rate": 3.647861067503925e-05,
      "loss": 0.5981,
      "step": 1378100
    },
    {
      "epoch": 21.635792778649922,
      "grad_norm": 4.265926361083984,
      "learning_rate": 3.64776295133438e-05,
      "loss": 0.6242,
      "step": 1378200
    },
    {
      "epoch": 21.63736263736264,
      "grad_norm": 2.2541515827178955,
      "learning_rate": 3.647664835164835e-05,
      "loss": 0.6373,
      "step": 1378300
    },
    {
      "epoch": 21.638932496075352,
      "grad_norm": 4.35601806640625,
      "learning_rate": 3.647566718995291e-05,
      "loss": 0.5827,
      "step": 1378400
    },
    {
      "epoch": 21.64050235478807,
      "grad_norm": 4.720536708831787,
      "learning_rate": 3.647468602825746e-05,
      "loss": 0.6292,
      "step": 1378500
    },
    {
      "epoch": 21.642072213500786,
      "grad_norm": 3.9170377254486084,
      "learning_rate": 3.647370486656201e-05,
      "loss": 0.637,
      "step": 1378600
    },
    {
      "epoch": 21.643642072213503,
      "grad_norm": 5.060369491577148,
      "learning_rate": 3.647272370486656e-05,
      "loss": 0.6279,
      "step": 1378700
    },
    {
      "epoch": 21.645211930926216,
      "grad_norm": 4.219995021820068,
      "learning_rate": 3.647174254317112e-05,
      "loss": 0.6433,
      "step": 1378800
    },
    {
      "epoch": 21.646781789638933,
      "grad_norm": 3.433800458908081,
      "learning_rate": 3.647076138147567e-05,
      "loss": 0.6207,
      "step": 1378900
    },
    {
      "epoch": 21.64835164835165,
      "grad_norm": 4.326803207397461,
      "learning_rate": 3.646978021978022e-05,
      "loss": 0.5707,
      "step": 1379000
    },
    {
      "epoch": 21.649921507064363,
      "grad_norm": 2.815325975418091,
      "learning_rate": 3.646879905808477e-05,
      "loss": 0.6205,
      "step": 1379100
    },
    {
      "epoch": 21.65149136577708,
      "grad_norm": 3.71968412399292,
      "learning_rate": 3.646781789638933e-05,
      "loss": 0.5855,
      "step": 1379200
    },
    {
      "epoch": 21.653061224489797,
      "grad_norm": 4.750795841217041,
      "learning_rate": 3.6466836734693874e-05,
      "loss": 0.594,
      "step": 1379300
    },
    {
      "epoch": 21.65463108320251,
      "grad_norm": 3.620633602142334,
      "learning_rate": 3.646585557299843e-05,
      "loss": 0.6627,
      "step": 1379400
    },
    {
      "epoch": 21.656200941915227,
      "grad_norm": 3.9063000679016113,
      "learning_rate": 3.646487441130298e-05,
      "loss": 0.6087,
      "step": 1379500
    },
    {
      "epoch": 21.657770800627944,
      "grad_norm": 2.0413761138916016,
      "learning_rate": 3.646389324960754e-05,
      "loss": 0.591,
      "step": 1379600
    },
    {
      "epoch": 21.65934065934066,
      "grad_norm": 4.426599025726318,
      "learning_rate": 3.6462912087912085e-05,
      "loss": 0.6375,
      "step": 1379700
    },
    {
      "epoch": 21.660910518053374,
      "grad_norm": 5.192397594451904,
      "learning_rate": 3.646193092621664e-05,
      "loss": 0.6404,
      "step": 1379800
    },
    {
      "epoch": 21.66248037676609,
      "grad_norm": 3.939962387084961,
      "learning_rate": 3.6460949764521194e-05,
      "loss": 0.6067,
      "step": 1379900
    },
    {
      "epoch": 21.664050235478808,
      "grad_norm": 2.538296699523926,
      "learning_rate": 3.6459968602825745e-05,
      "loss": 0.623,
      "step": 1380000
    },
    {
      "epoch": 21.665620094191524,
      "grad_norm": 4.361063003540039,
      "learning_rate": 3.6458987441130296e-05,
      "loss": 0.6167,
      "step": 1380100
    },
    {
      "epoch": 21.667189952904238,
      "grad_norm": 3.7083740234375,
      "learning_rate": 3.645800627943485e-05,
      "loss": 0.6336,
      "step": 1380200
    },
    {
      "epoch": 21.668759811616955,
      "grad_norm": 4.046097755432129,
      "learning_rate": 3.6457025117739404e-05,
      "loss": 0.6025,
      "step": 1380300
    },
    {
      "epoch": 21.67032967032967,
      "grad_norm": 3.666694164276123,
      "learning_rate": 3.6456043956043955e-05,
      "loss": 0.6211,
      "step": 1380400
    },
    {
      "epoch": 21.671899529042385,
      "grad_norm": 2.89719820022583,
      "learning_rate": 3.645506279434851e-05,
      "loss": 0.6343,
      "step": 1380500
    },
    {
      "epoch": 21.6734693877551,
      "grad_norm": 4.335228443145752,
      "learning_rate": 3.6454081632653064e-05,
      "loss": 0.6217,
      "step": 1380600
    },
    {
      "epoch": 21.67503924646782,
      "grad_norm": 3.1194348335266113,
      "learning_rate": 3.6453100470957615e-05,
      "loss": 0.602,
      "step": 1380700
    },
    {
      "epoch": 21.676609105180535,
      "grad_norm": 3.7638983726501465,
      "learning_rate": 3.6452119309262166e-05,
      "loss": 0.5901,
      "step": 1380800
    },
    {
      "epoch": 21.67817896389325,
      "grad_norm": 1.7041417360305786,
      "learning_rate": 3.6451138147566724e-05,
      "loss": 0.6271,
      "step": 1380900
    },
    {
      "epoch": 21.679748822605966,
      "grad_norm": 3.8567748069763184,
      "learning_rate": 3.6450156985871275e-05,
      "loss": 0.6272,
      "step": 1381000
    },
    {
      "epoch": 21.681318681318682,
      "grad_norm": 3.7479004859924316,
      "learning_rate": 3.6449175824175826e-05,
      "loss": 0.5778,
      "step": 1381100
    },
    {
      "epoch": 21.682888540031396,
      "grad_norm": 3.7742769718170166,
      "learning_rate": 3.644819466248038e-05,
      "loss": 0.6248,
      "step": 1381200
    },
    {
      "epoch": 21.684458398744113,
      "grad_norm": 4.005265712738037,
      "learning_rate": 3.6447213500784934e-05,
      "loss": 0.6019,
      "step": 1381300
    },
    {
      "epoch": 21.68602825745683,
      "grad_norm": 3.921658515930176,
      "learning_rate": 3.644623233908948e-05,
      "loss": 0.5885,
      "step": 1381400
    },
    {
      "epoch": 21.687598116169546,
      "grad_norm": 4.774213790893555,
      "learning_rate": 3.6445251177394036e-05,
      "loss": 0.6228,
      "step": 1381500
    },
    {
      "epoch": 21.68916797488226,
      "grad_norm": 4.281524658203125,
      "learning_rate": 3.644427001569859e-05,
      "loss": 0.5507,
      "step": 1381600
    },
    {
      "epoch": 21.690737833594977,
      "grad_norm": 2.9655959606170654,
      "learning_rate": 3.6443288854003145e-05,
      "loss": 0.6335,
      "step": 1381700
    },
    {
      "epoch": 21.692307692307693,
      "grad_norm": 4.229341983795166,
      "learning_rate": 3.644230769230769e-05,
      "loss": 0.6081,
      "step": 1381800
    },
    {
      "epoch": 21.693877551020407,
      "grad_norm": 3.6701810359954834,
      "learning_rate": 3.644132653061225e-05,
      "loss": 0.5915,
      "step": 1381900
    },
    {
      "epoch": 21.695447409733124,
      "grad_norm": 3.115464687347412,
      "learning_rate": 3.64403453689168e-05,
      "loss": 0.6073,
      "step": 1382000
    },
    {
      "epoch": 21.69701726844584,
      "grad_norm": 2.1094894409179688,
      "learning_rate": 3.643936420722135e-05,
      "loss": 0.5788,
      "step": 1382100
    },
    {
      "epoch": 21.698587127158557,
      "grad_norm": 4.16753625869751,
      "learning_rate": 3.64383830455259e-05,
      "loss": 0.5692,
      "step": 1382200
    },
    {
      "epoch": 21.70015698587127,
      "grad_norm": 3.949084997177124,
      "learning_rate": 3.643740188383046e-05,
      "loss": 0.5929,
      "step": 1382300
    },
    {
      "epoch": 21.701726844583987,
      "grad_norm": 3.459804058074951,
      "learning_rate": 3.643642072213501e-05,
      "loss": 0.6326,
      "step": 1382400
    },
    {
      "epoch": 21.703296703296704,
      "grad_norm": 3.8582324981689453,
      "learning_rate": 3.643543956043956e-05,
      "loss": 0.5858,
      "step": 1382500
    },
    {
      "epoch": 21.704866562009418,
      "grad_norm": 3.9230878353118896,
      "learning_rate": 3.643445839874412e-05,
      "loss": 0.5689,
      "step": 1382600
    },
    {
      "epoch": 21.706436420722135,
      "grad_norm": 3.9932992458343506,
      "learning_rate": 3.643347723704867e-05,
      "loss": 0.6213,
      "step": 1382700
    },
    {
      "epoch": 21.70800627943485,
      "grad_norm": 3.92673921585083,
      "learning_rate": 3.643249607535322e-05,
      "loss": 0.6407,
      "step": 1382800
    },
    {
      "epoch": 21.70957613814757,
      "grad_norm": 3.3706982135772705,
      "learning_rate": 3.643151491365777e-05,
      "loss": 0.6126,
      "step": 1382900
    },
    {
      "epoch": 21.71114599686028,
      "grad_norm": 2.5963008403778076,
      "learning_rate": 3.643053375196233e-05,
      "loss": 0.5727,
      "step": 1383000
    },
    {
      "epoch": 21.712715855573,
      "grad_norm": 3.1789772510528564,
      "learning_rate": 3.642955259026688e-05,
      "loss": 0.5929,
      "step": 1383100
    },
    {
      "epoch": 21.714285714285715,
      "grad_norm": 4.022958278656006,
      "learning_rate": 3.642857142857143e-05,
      "loss": 0.5861,
      "step": 1383200
    },
    {
      "epoch": 21.71585557299843,
      "grad_norm": 3.5931499004364014,
      "learning_rate": 3.642759026687598e-05,
      "loss": 0.5985,
      "step": 1383300
    },
    {
      "epoch": 21.717425431711145,
      "grad_norm": 3.90818190574646,
      "learning_rate": 3.642660910518054e-05,
      "loss": 0.5941,
      "step": 1383400
    },
    {
      "epoch": 21.718995290423862,
      "grad_norm": 4.048319339752197,
      "learning_rate": 3.642562794348508e-05,
      "loss": 0.5879,
      "step": 1383500
    },
    {
      "epoch": 21.72056514913658,
      "grad_norm": 3.9114463329315186,
      "learning_rate": 3.642464678178964e-05,
      "loss": 0.621,
      "step": 1383600
    },
    {
      "epoch": 21.722135007849293,
      "grad_norm": 4.017016887664795,
      "learning_rate": 3.642366562009419e-05,
      "loss": 0.6146,
      "step": 1383700
    },
    {
      "epoch": 21.72370486656201,
      "grad_norm": 2.815398693084717,
      "learning_rate": 3.642268445839875e-05,
      "loss": 0.5775,
      "step": 1383800
    },
    {
      "epoch": 21.725274725274726,
      "grad_norm": 4.0533127784729,
      "learning_rate": 3.6421703296703294e-05,
      "loss": 0.6431,
      "step": 1383900
    },
    {
      "epoch": 21.72684458398744,
      "grad_norm": 3.3317761421203613,
      "learning_rate": 3.642072213500785e-05,
      "loss": 0.5951,
      "step": 1384000
    },
    {
      "epoch": 21.728414442700156,
      "grad_norm": 3.883948802947998,
      "learning_rate": 3.64197409733124e-05,
      "loss": 0.6384,
      "step": 1384100
    },
    {
      "epoch": 21.729984301412873,
      "grad_norm": 3.7005155086517334,
      "learning_rate": 3.6418759811616954e-05,
      "loss": 0.6051,
      "step": 1384200
    },
    {
      "epoch": 21.73155416012559,
      "grad_norm": 3.3322646617889404,
      "learning_rate": 3.6417778649921505e-05,
      "loss": 0.5936,
      "step": 1384300
    },
    {
      "epoch": 21.733124018838303,
      "grad_norm": 3.867342472076416,
      "learning_rate": 3.641679748822606e-05,
      "loss": 0.5949,
      "step": 1384400
    },
    {
      "epoch": 21.73469387755102,
      "grad_norm": 3.323354482650757,
      "learning_rate": 3.641581632653061e-05,
      "loss": 0.6245,
      "step": 1384500
    },
    {
      "epoch": 21.736263736263737,
      "grad_norm": 4.959872722625732,
      "learning_rate": 3.6414835164835164e-05,
      "loss": 0.6147,
      "step": 1384600
    },
    {
      "epoch": 21.73783359497645,
      "grad_norm": 2.5488741397857666,
      "learning_rate": 3.641385400313972e-05,
      "loss": 0.6124,
      "step": 1384700
    },
    {
      "epoch": 21.739403453689167,
      "grad_norm": 2.740158796310425,
      "learning_rate": 3.641287284144427e-05,
      "loss": 0.5908,
      "step": 1384800
    },
    {
      "epoch": 21.740973312401884,
      "grad_norm": 4.26071310043335,
      "learning_rate": 3.6411891679748824e-05,
      "loss": 0.6103,
      "step": 1384900
    },
    {
      "epoch": 21.7425431711146,
      "grad_norm": 4.358144283294678,
      "learning_rate": 3.6410910518053375e-05,
      "loss": 0.6213,
      "step": 1385000
    },
    {
      "epoch": 21.744113029827314,
      "grad_norm": 4.390620231628418,
      "learning_rate": 3.640992935635793e-05,
      "loss": 0.6237,
      "step": 1385100
    },
    {
      "epoch": 21.74568288854003,
      "grad_norm": 3.4467108249664307,
      "learning_rate": 3.6408948194662484e-05,
      "loss": 0.5893,
      "step": 1385200
    },
    {
      "epoch": 21.747252747252748,
      "grad_norm": 3.3709380626678467,
      "learning_rate": 3.6407967032967035e-05,
      "loss": 0.6199,
      "step": 1385300
    },
    {
      "epoch": 21.74882260596546,
      "grad_norm": 2.979492425918579,
      "learning_rate": 3.6406985871271586e-05,
      "loss": 0.5801,
      "step": 1385400
    },
    {
      "epoch": 21.75039246467818,
      "grad_norm": 2.254718065261841,
      "learning_rate": 3.6406004709576143e-05,
      "loss": 0.5858,
      "step": 1385500
    },
    {
      "epoch": 21.751962323390895,
      "grad_norm": 3.953153371810913,
      "learning_rate": 3.640502354788069e-05,
      "loss": 0.5764,
      "step": 1385600
    },
    {
      "epoch": 21.753532182103612,
      "grad_norm": 4.29767370223999,
      "learning_rate": 3.6404042386185245e-05,
      "loss": 0.6107,
      "step": 1385700
    },
    {
      "epoch": 21.755102040816325,
      "grad_norm": 3.911159038543701,
      "learning_rate": 3.6403061224489796e-05,
      "loss": 0.6201,
      "step": 1385800
    },
    {
      "epoch": 21.756671899529042,
      "grad_norm": 3.7358016967773438,
      "learning_rate": 3.6402080062794354e-05,
      "loss": 0.5971,
      "step": 1385900
    },
    {
      "epoch": 21.75824175824176,
      "grad_norm": 3.5338377952575684,
      "learning_rate": 3.64010989010989e-05,
      "loss": 0.5933,
      "step": 1386000
    },
    {
      "epoch": 21.759811616954472,
      "grad_norm": 5.1511359214782715,
      "learning_rate": 3.6400117739403456e-05,
      "loss": 0.5846,
      "step": 1386100
    },
    {
      "epoch": 21.76138147566719,
      "grad_norm": 4.552029609680176,
      "learning_rate": 3.639913657770801e-05,
      "loss": 0.5888,
      "step": 1386200
    },
    {
      "epoch": 21.762951334379906,
      "grad_norm": 5.2005510330200195,
      "learning_rate": 3.639815541601256e-05,
      "loss": 0.559,
      "step": 1386300
    },
    {
      "epoch": 21.764521193092623,
      "grad_norm": 3.5900371074676514,
      "learning_rate": 3.639717425431711e-05,
      "loss": 0.5875,
      "step": 1386400
    },
    {
      "epoch": 21.766091051805336,
      "grad_norm": 4.7400803565979,
      "learning_rate": 3.639619309262167e-05,
      "loss": 0.6012,
      "step": 1386500
    },
    {
      "epoch": 21.767660910518053,
      "grad_norm": 3.5855112075805664,
      "learning_rate": 3.639521193092622e-05,
      "loss": 0.6193,
      "step": 1386600
    },
    {
      "epoch": 21.76923076923077,
      "grad_norm": 3.772005081176758,
      "learning_rate": 3.639423076923077e-05,
      "loss": 0.6247,
      "step": 1386700
    },
    {
      "epoch": 21.770800627943487,
      "grad_norm": 3.8653948307037354,
      "learning_rate": 3.6393249607535327e-05,
      "loss": 0.5973,
      "step": 1386800
    },
    {
      "epoch": 21.7723704866562,
      "grad_norm": 3.7856574058532715,
      "learning_rate": 3.639226844583988e-05,
      "loss": 0.6249,
      "step": 1386900
    },
    {
      "epoch": 21.773940345368917,
      "grad_norm": 4.374238014221191,
      "learning_rate": 3.639128728414443e-05,
      "loss": 0.5886,
      "step": 1387000
    },
    {
      "epoch": 21.775510204081634,
      "grad_norm": 4.006422996520996,
      "learning_rate": 3.639030612244898e-05,
      "loss": 0.5744,
      "step": 1387100
    },
    {
      "epoch": 21.777080062794347,
      "grad_norm": 3.9141042232513428,
      "learning_rate": 3.638932496075354e-05,
      "loss": 0.6141,
      "step": 1387200
    },
    {
      "epoch": 21.778649921507064,
      "grad_norm": 3.7538950443267822,
      "learning_rate": 3.638834379905809e-05,
      "loss": 0.6013,
      "step": 1387300
    },
    {
      "epoch": 21.78021978021978,
      "grad_norm": 3.3277292251586914,
      "learning_rate": 3.638736263736264e-05,
      "loss": 0.5923,
      "step": 1387400
    },
    {
      "epoch": 21.781789638932494,
      "grad_norm": 5.211674690246582,
      "learning_rate": 3.638638147566719e-05,
      "loss": 0.6367,
      "step": 1387500
    },
    {
      "epoch": 21.78335949764521,
      "grad_norm": 3.828960418701172,
      "learning_rate": 3.638540031397175e-05,
      "loss": 0.6193,
      "step": 1387600
    },
    {
      "epoch": 21.784929356357928,
      "grad_norm": 3.7797462940216064,
      "learning_rate": 3.638441915227629e-05,
      "loss": 0.5988,
      "step": 1387700
    },
    {
      "epoch": 21.786499215070645,
      "grad_norm": 3.9243712425231934,
      "learning_rate": 3.638343799058085e-05,
      "loss": 0.575,
      "step": 1387800
    },
    {
      "epoch": 21.788069073783358,
      "grad_norm": 4.431875705718994,
      "learning_rate": 3.63824568288854e-05,
      "loss": 0.6173,
      "step": 1387900
    },
    {
      "epoch": 21.789638932496075,
      "grad_norm": 4.159072399139404,
      "learning_rate": 3.638147566718996e-05,
      "loss": 0.6146,
      "step": 1388000
    },
    {
      "epoch": 21.791208791208792,
      "grad_norm": 4.587090492248535,
      "learning_rate": 3.63804945054945e-05,
      "loss": 0.5996,
      "step": 1388100
    },
    {
      "epoch": 21.79277864992151,
      "grad_norm": 3.9402823448181152,
      "learning_rate": 3.637951334379906e-05,
      "loss": 0.6037,
      "step": 1388200
    },
    {
      "epoch": 21.794348508634222,
      "grad_norm": 3.883242607116699,
      "learning_rate": 3.637853218210361e-05,
      "loss": 0.6355,
      "step": 1388300
    },
    {
      "epoch": 21.79591836734694,
      "grad_norm": 3.307368278503418,
      "learning_rate": 3.637755102040816e-05,
      "loss": 0.5864,
      "step": 1388400
    },
    {
      "epoch": 21.797488226059656,
      "grad_norm": 4.0937089920043945,
      "learning_rate": 3.6376569858712714e-05,
      "loss": 0.6221,
      "step": 1388500
    },
    {
      "epoch": 21.79905808477237,
      "grad_norm": 3.8140389919281006,
      "learning_rate": 3.637558869701727e-05,
      "loss": 0.6032,
      "step": 1388600
    },
    {
      "epoch": 21.800627943485086,
      "grad_norm": 3.2732536792755127,
      "learning_rate": 3.637460753532182e-05,
      "loss": 0.6148,
      "step": 1388700
    },
    {
      "epoch": 21.802197802197803,
      "grad_norm": 4.676023483276367,
      "learning_rate": 3.637362637362637e-05,
      "loss": 0.578,
      "step": 1388800
    },
    {
      "epoch": 21.80376766091052,
      "grad_norm": 3.207145929336548,
      "learning_rate": 3.637264521193093e-05,
      "loss": 0.6224,
      "step": 1388900
    },
    {
      "epoch": 21.805337519623233,
      "grad_norm": 3.7210094928741455,
      "learning_rate": 3.637166405023548e-05,
      "loss": 0.575,
      "step": 1389000
    },
    {
      "epoch": 21.80690737833595,
      "grad_norm": 4.312845706939697,
      "learning_rate": 3.637068288854003e-05,
      "loss": 0.6254,
      "step": 1389100
    },
    {
      "epoch": 21.808477237048667,
      "grad_norm": 1.9523409605026245,
      "learning_rate": 3.6369701726844584e-05,
      "loss": 0.5905,
      "step": 1389200
    },
    {
      "epoch": 21.81004709576138,
      "grad_norm": 3.9689674377441406,
      "learning_rate": 3.636872056514914e-05,
      "loss": 0.6377,
      "step": 1389300
    },
    {
      "epoch": 21.811616954474097,
      "grad_norm": 4.103492259979248,
      "learning_rate": 3.636773940345369e-05,
      "loss": 0.6163,
      "step": 1389400
    },
    {
      "epoch": 21.813186813186814,
      "grad_norm": 3.068333864212036,
      "learning_rate": 3.6366758241758244e-05,
      "loss": 0.6296,
      "step": 1389500
    },
    {
      "epoch": 21.81475667189953,
      "grad_norm": 3.868187189102173,
      "learning_rate": 3.6365777080062795e-05,
      "loss": 0.5912,
      "step": 1389600
    },
    {
      "epoch": 21.816326530612244,
      "grad_norm": 3.5718472003936768,
      "learning_rate": 3.636479591836735e-05,
      "loss": 0.5922,
      "step": 1389700
    },
    {
      "epoch": 21.81789638932496,
      "grad_norm": 4.652326583862305,
      "learning_rate": 3.63638147566719e-05,
      "loss": 0.6342,
      "step": 1389800
    },
    {
      "epoch": 21.819466248037678,
      "grad_norm": 3.3697621822357178,
      "learning_rate": 3.6362833594976454e-05,
      "loss": 0.5993,
      "step": 1389900
    },
    {
      "epoch": 21.82103610675039,
      "grad_norm": 3.6220619678497314,
      "learning_rate": 3.6361852433281005e-05,
      "loss": 0.6057,
      "step": 1390000
    },
    {
      "epoch": 21.822605965463108,
      "grad_norm": 4.408295631408691,
      "learning_rate": 3.636087127158556e-05,
      "loss": 0.5878,
      "step": 1390100
    },
    {
      "epoch": 21.824175824175825,
      "grad_norm": 2.3837194442749023,
      "learning_rate": 3.635989010989011e-05,
      "loss": 0.6057,
      "step": 1390200
    },
    {
      "epoch": 21.82574568288854,
      "grad_norm": 3.8439464569091797,
      "learning_rate": 3.6358908948194665e-05,
      "loss": 0.5931,
      "step": 1390300
    },
    {
      "epoch": 21.827315541601255,
      "grad_norm": 2.4383747577667236,
      "learning_rate": 3.6357927786499216e-05,
      "loss": 0.5596,
      "step": 1390400
    },
    {
      "epoch": 21.828885400313972,
      "grad_norm": 3.479536294937134,
      "learning_rate": 3.635694662480377e-05,
      "loss": 0.6154,
      "step": 1390500
    },
    {
      "epoch": 21.83045525902669,
      "grad_norm": 3.895981550216675,
      "learning_rate": 3.635596546310832e-05,
      "loss": 0.5922,
      "step": 1390600
    },
    {
      "epoch": 21.832025117739402,
      "grad_norm": 4.252613067626953,
      "learning_rate": 3.6354984301412876e-05,
      "loss": 0.6081,
      "step": 1390700
    },
    {
      "epoch": 21.83359497645212,
      "grad_norm": 4.437611103057861,
      "learning_rate": 3.635400313971743e-05,
      "loss": 0.5992,
      "step": 1390800
    },
    {
      "epoch": 21.835164835164836,
      "grad_norm": 3.3982841968536377,
      "learning_rate": 3.635302197802198e-05,
      "loss": 0.6271,
      "step": 1390900
    },
    {
      "epoch": 21.836734693877553,
      "grad_norm": 3.983760118484497,
      "learning_rate": 3.6352040816326536e-05,
      "loss": 0.5846,
      "step": 1391000
    },
    {
      "epoch": 21.838304552590266,
      "grad_norm": 4.130186557769775,
      "learning_rate": 3.6351059654631086e-05,
      "loss": 0.6011,
      "step": 1391100
    },
    {
      "epoch": 21.839874411302983,
      "grad_norm": 2.63494873046875,
      "learning_rate": 3.635007849293564e-05,
      "loss": 0.6645,
      "step": 1391200
    },
    {
      "epoch": 21.8414442700157,
      "grad_norm": 3.328275680541992,
      "learning_rate": 3.634909733124019e-05,
      "loss": 0.616,
      "step": 1391300
    },
    {
      "epoch": 21.843014128728413,
      "grad_norm": 2.5314040184020996,
      "learning_rate": 3.6348116169544746e-05,
      "loss": 0.5592,
      "step": 1391400
    },
    {
      "epoch": 21.84458398744113,
      "grad_norm": 3.4467575550079346,
      "learning_rate": 3.63471350078493e-05,
      "loss": 0.5991,
      "step": 1391500
    },
    {
      "epoch": 21.846153846153847,
      "grad_norm": 5.208155155181885,
      "learning_rate": 3.634615384615385e-05,
      "loss": 0.6018,
      "step": 1391600
    },
    {
      "epoch": 21.847723704866564,
      "grad_norm": 4.716089725494385,
      "learning_rate": 3.63451726844584e-05,
      "loss": 0.598,
      "step": 1391700
    },
    {
      "epoch": 21.849293563579277,
      "grad_norm": 4.568299293518066,
      "learning_rate": 3.634419152276296e-05,
      "loss": 0.6084,
      "step": 1391800
    },
    {
      "epoch": 21.850863422291994,
      "grad_norm": 2.3517191410064697,
      "learning_rate": 3.63432103610675e-05,
      "loss": 0.6342,
      "step": 1391900
    },
    {
      "epoch": 21.85243328100471,
      "grad_norm": 4.741611480712891,
      "learning_rate": 3.634222919937206e-05,
      "loss": 0.5925,
      "step": 1392000
    },
    {
      "epoch": 21.854003139717424,
      "grad_norm": 3.296541929244995,
      "learning_rate": 3.634124803767661e-05,
      "loss": 0.6198,
      "step": 1392100
    },
    {
      "epoch": 21.85557299843014,
      "grad_norm": 4.03100061416626,
      "learning_rate": 3.634026687598117e-05,
      "loss": 0.6453,
      "step": 1392200
    },
    {
      "epoch": 21.857142857142858,
      "grad_norm": 2.0039961338043213,
      "learning_rate": 3.633928571428571e-05,
      "loss": 0.5785,
      "step": 1392300
    },
    {
      "epoch": 21.858712715855575,
      "grad_norm": 3.844611406326294,
      "learning_rate": 3.633830455259027e-05,
      "loss": 0.5928,
      "step": 1392400
    },
    {
      "epoch": 21.860282574568288,
      "grad_norm": 2.897822856903076,
      "learning_rate": 3.633732339089482e-05,
      "loss": 0.5864,
      "step": 1392500
    },
    {
      "epoch": 21.861852433281005,
      "grad_norm": 3.8437724113464355,
      "learning_rate": 3.633634222919937e-05,
      "loss": 0.57,
      "step": 1392600
    },
    {
      "epoch": 21.86342229199372,
      "grad_norm": 3.6359052658081055,
      "learning_rate": 3.633536106750392e-05,
      "loss": 0.6058,
      "step": 1392700
    },
    {
      "epoch": 21.864992150706435,
      "grad_norm": 3.2114810943603516,
      "learning_rate": 3.633437990580848e-05,
      "loss": 0.6034,
      "step": 1392800
    },
    {
      "epoch": 21.86656200941915,
      "grad_norm": 2.947247266769409,
      "learning_rate": 3.633339874411303e-05,
      "loss": 0.6062,
      "step": 1392900
    },
    {
      "epoch": 21.86813186813187,
      "grad_norm": 2.7661972045898438,
      "learning_rate": 3.633241758241758e-05,
      "loss": 0.5904,
      "step": 1393000
    },
    {
      "epoch": 21.869701726844585,
      "grad_norm": 4.239750862121582,
      "learning_rate": 3.633143642072214e-05,
      "loss": 0.5962,
      "step": 1393100
    },
    {
      "epoch": 21.8712715855573,
      "grad_norm": 4.250373840332031,
      "learning_rate": 3.633045525902669e-05,
      "loss": 0.6261,
      "step": 1393200
    },
    {
      "epoch": 21.872841444270016,
      "grad_norm": 4.802877902984619,
      "learning_rate": 3.632947409733124e-05,
      "loss": 0.6138,
      "step": 1393300
    },
    {
      "epoch": 21.874411302982733,
      "grad_norm": 3.582200765609741,
      "learning_rate": 3.632849293563579e-05,
      "loss": 0.5943,
      "step": 1393400
    },
    {
      "epoch": 21.875981161695446,
      "grad_norm": 3.145965099334717,
      "learning_rate": 3.632751177394035e-05,
      "loss": 0.5848,
      "step": 1393500
    },
    {
      "epoch": 21.877551020408163,
      "grad_norm": 4.700042724609375,
      "learning_rate": 3.63265306122449e-05,
      "loss": 0.5721,
      "step": 1393600
    },
    {
      "epoch": 21.87912087912088,
      "grad_norm": 3.016847610473633,
      "learning_rate": 3.632554945054945e-05,
      "loss": 0.6251,
      "step": 1393700
    },
    {
      "epoch": 21.880690737833596,
      "grad_norm": 3.241225242614746,
      "learning_rate": 3.6324568288854004e-05,
      "loss": 0.5836,
      "step": 1393800
    },
    {
      "epoch": 21.88226059654631,
      "grad_norm": 2.703526496887207,
      "learning_rate": 3.632358712715856e-05,
      "loss": 0.5911,
      "step": 1393900
    },
    {
      "epoch": 21.883830455259027,
      "grad_norm": 4.3264546394348145,
      "learning_rate": 3.6322605965463106e-05,
      "loss": 0.6209,
      "step": 1394000
    },
    {
      "epoch": 21.885400313971743,
      "grad_norm": 4.2553486824035645,
      "learning_rate": 3.632162480376766e-05,
      "loss": 0.6481,
      "step": 1394100
    },
    {
      "epoch": 21.886970172684457,
      "grad_norm": 2.5741169452667236,
      "learning_rate": 3.6320643642072214e-05,
      "loss": 0.6027,
      "step": 1394200
    },
    {
      "epoch": 21.888540031397174,
      "grad_norm": 4.208929538726807,
      "learning_rate": 3.631966248037677e-05,
      "loss": 0.6118,
      "step": 1394300
    },
    {
      "epoch": 21.89010989010989,
      "grad_norm": 3.7633707523345947,
      "learning_rate": 3.6318681318681316e-05,
      "loss": 0.6194,
      "step": 1394400
    },
    {
      "epoch": 21.891679748822607,
      "grad_norm": 2.843498945236206,
      "learning_rate": 3.6317700156985874e-05,
      "loss": 0.6254,
      "step": 1394500
    },
    {
      "epoch": 21.89324960753532,
      "grad_norm": 4.496248245239258,
      "learning_rate": 3.6316718995290425e-05,
      "loss": 0.6169,
      "step": 1394600
    },
    {
      "epoch": 21.894819466248038,
      "grad_norm": 4.7350640296936035,
      "learning_rate": 3.6315737833594976e-05,
      "loss": 0.6225,
      "step": 1394700
    },
    {
      "epoch": 21.896389324960754,
      "grad_norm": 3.632951021194458,
      "learning_rate": 3.631475667189953e-05,
      "loss": 0.6448,
      "step": 1394800
    },
    {
      "epoch": 21.897959183673468,
      "grad_norm": 4.391133785247803,
      "learning_rate": 3.6313775510204085e-05,
      "loss": 0.5664,
      "step": 1394900
    },
    {
      "epoch": 21.899529042386185,
      "grad_norm": 4.4945831298828125,
      "learning_rate": 3.6312794348508636e-05,
      "loss": 0.6139,
      "step": 1395000
    },
    {
      "epoch": 21.9010989010989,
      "grad_norm": 3.208070755004883,
      "learning_rate": 3.631181318681319e-05,
      "loss": 0.598,
      "step": 1395100
    },
    {
      "epoch": 21.90266875981162,
      "grad_norm": 3.8232812881469727,
      "learning_rate": 3.6310832025117744e-05,
      "loss": 0.6304,
      "step": 1395200
    },
    {
      "epoch": 21.90423861852433,
      "grad_norm": 4.241525650024414,
      "learning_rate": 3.6309850863422295e-05,
      "loss": 0.609,
      "step": 1395300
    },
    {
      "epoch": 21.90580847723705,
      "grad_norm": 4.7607855796813965,
      "learning_rate": 3.6308869701726846e-05,
      "loss": 0.6237,
      "step": 1395400
    },
    {
      "epoch": 21.907378335949765,
      "grad_norm": 2.369882583618164,
      "learning_rate": 3.63078885400314e-05,
      "loss": 0.6358,
      "step": 1395500
    },
    {
      "epoch": 21.90894819466248,
      "grad_norm": 4.488943099975586,
      "learning_rate": 3.6306907378335955e-05,
      "loss": 0.6269,
      "step": 1395600
    },
    {
      "epoch": 21.910518053375196,
      "grad_norm": 3.738568067550659,
      "learning_rate": 3.6305926216640506e-05,
      "loss": 0.6256,
      "step": 1395700
    },
    {
      "epoch": 21.912087912087912,
      "grad_norm": 3.1874425411224365,
      "learning_rate": 3.630494505494506e-05,
      "loss": 0.5909,
      "step": 1395800
    },
    {
      "epoch": 21.91365777080063,
      "grad_norm": 3.216994285583496,
      "learning_rate": 3.630396389324961e-05,
      "loss": 0.6043,
      "step": 1395900
    },
    {
      "epoch": 21.915227629513343,
      "grad_norm": 4.646515846252441,
      "learning_rate": 3.6302982731554166e-05,
      "loss": 0.618,
      "step": 1396000
    },
    {
      "epoch": 21.91679748822606,
      "grad_norm": 2.3262457847595215,
      "learning_rate": 3.630200156985871e-05,
      "loss": 0.6158,
      "step": 1396100
    },
    {
      "epoch": 21.918367346938776,
      "grad_norm": 4.684047222137451,
      "learning_rate": 3.630102040816327e-05,
      "loss": 0.5738,
      "step": 1396200
    },
    {
      "epoch": 21.919937205651493,
      "grad_norm": 4.097138404846191,
      "learning_rate": 3.630003924646782e-05,
      "loss": 0.5954,
      "step": 1396300
    },
    {
      "epoch": 21.921507064364206,
      "grad_norm": 3.5384533405303955,
      "learning_rate": 3.6299058084772377e-05,
      "loss": 0.5659,
      "step": 1396400
    },
    {
      "epoch": 21.923076923076923,
      "grad_norm": 4.514888286590576,
      "learning_rate": 3.629807692307692e-05,
      "loss": 0.6046,
      "step": 1396500
    },
    {
      "epoch": 21.92464678178964,
      "grad_norm": 3.3217482566833496,
      "learning_rate": 3.629709576138148e-05,
      "loss": 0.6244,
      "step": 1396600
    },
    {
      "epoch": 21.926216640502354,
      "grad_norm": 3.115706205368042,
      "learning_rate": 3.629611459968603e-05,
      "loss": 0.5948,
      "step": 1396700
    },
    {
      "epoch": 21.92778649921507,
      "grad_norm": 3.7805871963500977,
      "learning_rate": 3.629513343799058e-05,
      "loss": 0.6495,
      "step": 1396800
    },
    {
      "epoch": 21.929356357927787,
      "grad_norm": 3.698744535446167,
      "learning_rate": 3.629415227629513e-05,
      "loss": 0.6095,
      "step": 1396900
    },
    {
      "epoch": 21.9309262166405,
      "grad_norm": 3.825347661972046,
      "learning_rate": 3.629317111459969e-05,
      "loss": 0.5766,
      "step": 1397000
    },
    {
      "epoch": 21.932496075353217,
      "grad_norm": 3.761270046234131,
      "learning_rate": 3.629218995290424e-05,
      "loss": 0.563,
      "step": 1397100
    },
    {
      "epoch": 21.934065934065934,
      "grad_norm": 3.975186824798584,
      "learning_rate": 3.629120879120879e-05,
      "loss": 0.5893,
      "step": 1397200
    },
    {
      "epoch": 21.93563579277865,
      "grad_norm": 3.433596134185791,
      "learning_rate": 3.629022762951335e-05,
      "loss": 0.5827,
      "step": 1397300
    },
    {
      "epoch": 21.937205651491364,
      "grad_norm": 3.8518741130828857,
      "learning_rate": 3.62892464678179e-05,
      "loss": 0.5924,
      "step": 1397400
    },
    {
      "epoch": 21.93877551020408,
      "grad_norm": 3.502535104751587,
      "learning_rate": 3.628826530612245e-05,
      "loss": 0.6748,
      "step": 1397500
    },
    {
      "epoch": 21.940345368916798,
      "grad_norm": 2.9014716148376465,
      "learning_rate": 3.6287284144427e-05,
      "loss": 0.6259,
      "step": 1397600
    },
    {
      "epoch": 21.941915227629515,
      "grad_norm": 3.9226605892181396,
      "learning_rate": 3.628630298273156e-05,
      "loss": 0.5896,
      "step": 1397700
    },
    {
      "epoch": 21.94348508634223,
      "grad_norm": 4.508181571960449,
      "learning_rate": 3.628532182103611e-05,
      "loss": 0.5684,
      "step": 1397800
    },
    {
      "epoch": 21.945054945054945,
      "grad_norm": 3.3025949001312256,
      "learning_rate": 3.628434065934066e-05,
      "loss": 0.6189,
      "step": 1397900
    },
    {
      "epoch": 21.946624803767662,
      "grad_norm": 4.138285160064697,
      "learning_rate": 3.628335949764521e-05,
      "loss": 0.6073,
      "step": 1398000
    },
    {
      "epoch": 21.948194662480375,
      "grad_norm": 3.5313222408294678,
      "learning_rate": 3.628237833594977e-05,
      "loss": 0.5624,
      "step": 1398100
    },
    {
      "epoch": 21.949764521193092,
      "grad_norm": 2.979595422744751,
      "learning_rate": 3.6281397174254315e-05,
      "loss": 0.5846,
      "step": 1398200
    },
    {
      "epoch": 21.95133437990581,
      "grad_norm": 4.056680679321289,
      "learning_rate": 3.628041601255887e-05,
      "loss": 0.6025,
      "step": 1398300
    },
    {
      "epoch": 21.952904238618526,
      "grad_norm": 4.262609481811523,
      "learning_rate": 3.627943485086342e-05,
      "loss": 0.5879,
      "step": 1398400
    },
    {
      "epoch": 21.95447409733124,
      "grad_norm": 3.9430315494537354,
      "learning_rate": 3.627845368916798e-05,
      "loss": 0.6367,
      "step": 1398500
    },
    {
      "epoch": 21.956043956043956,
      "grad_norm": 3.8755862712860107,
      "learning_rate": 3.6277472527472525e-05,
      "loss": 0.6169,
      "step": 1398600
    },
    {
      "epoch": 21.957613814756673,
      "grad_norm": 3.925348997116089,
      "learning_rate": 3.627649136577708e-05,
      "loss": 0.591,
      "step": 1398700
    },
    {
      "epoch": 21.959183673469386,
      "grad_norm": 3.9143455028533936,
      "learning_rate": 3.6275510204081634e-05,
      "loss": 0.6179,
      "step": 1398800
    },
    {
      "epoch": 21.960753532182103,
      "grad_norm": 4.332235336303711,
      "learning_rate": 3.6274529042386185e-05,
      "loss": 0.6508,
      "step": 1398900
    },
    {
      "epoch": 21.96232339089482,
      "grad_norm": 3.4996650218963623,
      "learning_rate": 3.6273547880690736e-05,
      "loss": 0.596,
      "step": 1399000
    },
    {
      "epoch": 21.963893249607537,
      "grad_norm": 5.578746795654297,
      "learning_rate": 3.6272566718995294e-05,
      "loss": 0.5971,
      "step": 1399100
    },
    {
      "epoch": 21.96546310832025,
      "grad_norm": 4.505214691162109,
      "learning_rate": 3.6271585557299845e-05,
      "loss": 0.5844,
      "step": 1399200
    },
    {
      "epoch": 21.967032967032967,
      "grad_norm": 3.223693609237671,
      "learning_rate": 3.6270604395604396e-05,
      "loss": 0.5847,
      "step": 1399300
    },
    {
      "epoch": 21.968602825745684,
      "grad_norm": 4.466167449951172,
      "learning_rate": 3.6269623233908953e-05,
      "loss": 0.6624,
      "step": 1399400
    },
    {
      "epoch": 21.970172684458397,
      "grad_norm": 3.9445855617523193,
      "learning_rate": 3.6268642072213504e-05,
      "loss": 0.6246,
      "step": 1399500
    },
    {
      "epoch": 21.971742543171114,
      "grad_norm": 3.9570846557617188,
      "learning_rate": 3.6267660910518055e-05,
      "loss": 0.6329,
      "step": 1399600
    },
    {
      "epoch": 21.97331240188383,
      "grad_norm": 3.864408016204834,
      "learning_rate": 3.6266679748822606e-05,
      "loss": 0.5779,
      "step": 1399700
    },
    {
      "epoch": 21.974882260596548,
      "grad_norm": 3.6714224815368652,
      "learning_rate": 3.6265698587127164e-05,
      "loss": 0.6183,
      "step": 1399800
    },
    {
      "epoch": 21.97645211930926,
      "grad_norm": 3.2398080825805664,
      "learning_rate": 3.626471742543171e-05,
      "loss": 0.5929,
      "step": 1399900
    },
    {
      "epoch": 21.978021978021978,
      "grad_norm": 3.5526764392852783,
      "learning_rate": 3.6263736263736266e-05,
      "loss": 0.5899,
      "step": 1400000
    },
    {
      "epoch": 21.979591836734695,
      "grad_norm": 3.994009494781494,
      "learning_rate": 3.626275510204082e-05,
      "loss": 0.5964,
      "step": 1400100
    },
    {
      "epoch": 21.98116169544741,
      "grad_norm": 4.543193340301514,
      "learning_rate": 3.6261773940345375e-05,
      "loss": 0.621,
      "step": 1400200
    },
    {
      "epoch": 21.982731554160125,
      "grad_norm": 4.266977787017822,
      "learning_rate": 3.626079277864992e-05,
      "loss": 0.6079,
      "step": 1400300
    },
    {
      "epoch": 21.984301412872842,
      "grad_norm": 4.192319393157959,
      "learning_rate": 3.625981161695448e-05,
      "loss": 0.6341,
      "step": 1400400
    },
    {
      "epoch": 21.98587127158556,
      "grad_norm": 4.555765151977539,
      "learning_rate": 3.625883045525903e-05,
      "loss": 0.594,
      "step": 1400500
    },
    {
      "epoch": 21.987441130298272,
      "grad_norm": 4.445291042327881,
      "learning_rate": 3.625784929356358e-05,
      "loss": 0.5964,
      "step": 1400600
    },
    {
      "epoch": 21.98901098901099,
      "grad_norm": 3.787315845489502,
      "learning_rate": 3.625686813186813e-05,
      "loss": 0.5577,
      "step": 1400700
    },
    {
      "epoch": 21.990580847723706,
      "grad_norm": 5.856096267700195,
      "learning_rate": 3.625588697017269e-05,
      "loss": 0.5666,
      "step": 1400800
    },
    {
      "epoch": 21.99215070643642,
      "grad_norm": 2.925323486328125,
      "learning_rate": 3.625490580847724e-05,
      "loss": 0.6136,
      "step": 1400900
    },
    {
      "epoch": 21.993720565149136,
      "grad_norm": 3.765662431716919,
      "learning_rate": 3.625392464678179e-05,
      "loss": 0.5766,
      "step": 1401000
    },
    {
      "epoch": 21.995290423861853,
      "grad_norm": 4.118640422821045,
      "learning_rate": 3.625294348508634e-05,
      "loss": 0.6134,
      "step": 1401100
    },
    {
      "epoch": 21.99686028257457,
      "grad_norm": 3.660252332687378,
      "learning_rate": 3.62519623233909e-05,
      "loss": 0.6468,
      "step": 1401200
    },
    {
      "epoch": 21.998430141287283,
      "grad_norm": 3.0395617485046387,
      "learning_rate": 3.625098116169545e-05,
      "loss": 0.5776,
      "step": 1401300
    },
    {
      "epoch": 22.0,
      "grad_norm": 3.3769896030426025,
      "learning_rate": 3.625e-05,
      "loss": 0.5688,
      "step": 1401400
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.0353668928146362,
      "eval_runtime": 14.8442,
      "eval_samples_per_second": 225.879,
      "eval_steps_per_second": 225.879,
      "step": 1401400
    },
    {
      "epoch": 22.0,
      "eval_loss": 0.4671611785888672,
      "eval_runtime": 297.7025,
      "eval_samples_per_second": 213.972,
      "eval_steps_per_second": 213.972,
      "step": 1401400
    },
    {
      "epoch": 22.001569858712717,
      "grad_norm": 3.2236368656158447,
      "learning_rate": 3.624901883830456e-05,
      "loss": 0.5904,
      "step": 1401500
    },
    {
      "epoch": 22.00313971742543,
      "grad_norm": 3.588557243347168,
      "learning_rate": 3.624803767660911e-05,
      "loss": 0.6282,
      "step": 1401600
    },
    {
      "epoch": 22.004709576138147,
      "grad_norm": 3.589139223098755,
      "learning_rate": 3.624705651491366e-05,
      "loss": 0.6419,
      "step": 1401700
    },
    {
      "epoch": 22.006279434850864,
      "grad_norm": 3.849829912185669,
      "learning_rate": 3.624607535321821e-05,
      "loss": 0.6239,
      "step": 1401800
    },
    {
      "epoch": 22.00784929356358,
      "grad_norm": 3.052927017211914,
      "learning_rate": 3.624509419152277e-05,
      "loss": 0.617,
      "step": 1401900
    },
    {
      "epoch": 22.009419152276294,
      "grad_norm": 3.7526488304138184,
      "learning_rate": 3.624411302982731e-05,
      "loss": 0.6108,
      "step": 1402000
    },
    {
      "epoch": 22.01098901098901,
      "grad_norm": 4.477334022521973,
      "learning_rate": 3.624313186813187e-05,
      "loss": 0.5953,
      "step": 1402100
    },
    {
      "epoch": 22.012558869701728,
      "grad_norm": 4.075374126434326,
      "learning_rate": 3.624215070643642e-05,
      "loss": 0.5667,
      "step": 1402200
    },
    {
      "epoch": 22.01412872841444,
      "grad_norm": 4.789662837982178,
      "learning_rate": 3.624116954474098e-05,
      "loss": 0.6007,
      "step": 1402300
    },
    {
      "epoch": 22.015698587127158,
      "grad_norm": 3.2626593112945557,
      "learning_rate": 3.6240188383045524e-05,
      "loss": 0.6195,
      "step": 1402400
    },
    {
      "epoch": 22.017268445839875,
      "grad_norm": 4.471670627593994,
      "learning_rate": 3.623920722135008e-05,
      "loss": 0.5743,
      "step": 1402500
    },
    {
      "epoch": 22.01883830455259,
      "grad_norm": 4.5422492027282715,
      "learning_rate": 3.623822605965463e-05,
      "loss": 0.6178,
      "step": 1402600
    },
    {
      "epoch": 22.020408163265305,
      "grad_norm": 4.163614273071289,
      "learning_rate": 3.623724489795918e-05,
      "loss": 0.611,
      "step": 1402700
    },
    {
      "epoch": 22.021978021978022,
      "grad_norm": 3.743671417236328,
      "learning_rate": 3.6236263736263734e-05,
      "loss": 0.5943,
      "step": 1402800
    },
    {
      "epoch": 22.02354788069074,
      "grad_norm": 4.536067485809326,
      "learning_rate": 3.623528257456829e-05,
      "loss": 0.6112,
      "step": 1402900
    },
    {
      "epoch": 22.025117739403452,
      "grad_norm": 4.621490001678467,
      "learning_rate": 3.623430141287284e-05,
      "loss": 0.6396,
      "step": 1403000
    },
    {
      "epoch": 22.02668759811617,
      "grad_norm": 4.240236282348633,
      "learning_rate": 3.6233320251177394e-05,
      "loss": 0.6063,
      "step": 1403100
    },
    {
      "epoch": 22.028257456828886,
      "grad_norm": 4.148473262786865,
      "learning_rate": 3.6232339089481945e-05,
      "loss": 0.5814,
      "step": 1403200
    },
    {
      "epoch": 22.029827315541603,
      "grad_norm": 4.193689823150635,
      "learning_rate": 3.62313579277865e-05,
      "loss": 0.5735,
      "step": 1403300
    },
    {
      "epoch": 22.031397174254316,
      "grad_norm": 4.229245662689209,
      "learning_rate": 3.6230376766091054e-05,
      "loss": 0.6292,
      "step": 1403400
    },
    {
      "epoch": 22.032967032967033,
      "grad_norm": 4.2122626304626465,
      "learning_rate": 3.6229395604395605e-05,
      "loss": 0.6094,
      "step": 1403500
    },
    {
      "epoch": 22.03453689167975,
      "grad_norm": 4.856922149658203,
      "learning_rate": 3.622841444270016e-05,
      "loss": 0.5859,
      "step": 1403600
    },
    {
      "epoch": 22.036106750392463,
      "grad_norm": 4.0572686195373535,
      "learning_rate": 3.6227433281004713e-05,
      "loss": 0.6095,
      "step": 1403700
    },
    {
      "epoch": 22.03767660910518,
      "grad_norm": 4.16928243637085,
      "learning_rate": 3.6226452119309264e-05,
      "loss": 0.5909,
      "step": 1403800
    },
    {
      "epoch": 22.039246467817897,
      "grad_norm": 3.2896816730499268,
      "learning_rate": 3.6225470957613815e-05,
      "loss": 0.5714,
      "step": 1403900
    },
    {
      "epoch": 22.040816326530614,
      "grad_norm": 3.6974217891693115,
      "learning_rate": 3.622448979591837e-05,
      "loss": 0.6058,
      "step": 1404000
    },
    {
      "epoch": 22.042386185243327,
      "grad_norm": 4.006863594055176,
      "learning_rate": 3.622350863422292e-05,
      "loss": 0.5895,
      "step": 1404100
    },
    {
      "epoch": 22.043956043956044,
      "grad_norm": 3.7942304611206055,
      "learning_rate": 3.6222527472527475e-05,
      "loss": 0.5373,
      "step": 1404200
    },
    {
      "epoch": 22.04552590266876,
      "grad_norm": 3.4317803382873535,
      "learning_rate": 3.6221546310832026e-05,
      "loss": 0.5847,
      "step": 1404300
    },
    {
      "epoch": 22.047095761381474,
      "grad_norm": 3.8154098987579346,
      "learning_rate": 3.6220565149136584e-05,
      "loss": 0.6101,
      "step": 1404400
    },
    {
      "epoch": 22.04866562009419,
      "grad_norm": 4.298559188842773,
      "learning_rate": 3.621958398744113e-05,
      "loss": 0.6046,
      "step": 1404500
    },
    {
      "epoch": 22.050235478806908,
      "grad_norm": 3.8774101734161377,
      "learning_rate": 3.6218602825745686e-05,
      "loss": 0.6328,
      "step": 1404600
    },
    {
      "epoch": 22.051805337519625,
      "grad_norm": 4.040590763092041,
      "learning_rate": 3.621762166405024e-05,
      "loss": 0.5945,
      "step": 1404700
    },
    {
      "epoch": 22.053375196232338,
      "grad_norm": 2.7089672088623047,
      "learning_rate": 3.621664050235479e-05,
      "loss": 0.6233,
      "step": 1404800
    },
    {
      "epoch": 22.054945054945055,
      "grad_norm": 2.6225671768188477,
      "learning_rate": 3.621565934065934e-05,
      "loss": 0.6052,
      "step": 1404900
    },
    {
      "epoch": 22.05651491365777,
      "grad_norm": 4.2449188232421875,
      "learning_rate": 3.6214678178963897e-05,
      "loss": 0.5573,
      "step": 1405000
    },
    {
      "epoch": 22.058084772370485,
      "grad_norm": 2.6288177967071533,
      "learning_rate": 3.621369701726845e-05,
      "loss": 0.5805,
      "step": 1405100
    },
    {
      "epoch": 22.059654631083202,
      "grad_norm": 3.288977861404419,
      "learning_rate": 3.6212715855573e-05,
      "loss": 0.5723,
      "step": 1405200
    },
    {
      "epoch": 22.06122448979592,
      "grad_norm": 4.690011501312256,
      "learning_rate": 3.621173469387755e-05,
      "loss": 0.586,
      "step": 1405300
    },
    {
      "epoch": 22.062794348508636,
      "grad_norm": 2.7272932529449463,
      "learning_rate": 3.621075353218211e-05,
      "loss": 0.5825,
      "step": 1405400
    },
    {
      "epoch": 22.06436420722135,
      "grad_norm": 3.8357608318328857,
      "learning_rate": 3.620977237048666e-05,
      "loss": 0.5894,
      "step": 1405500
    },
    {
      "epoch": 22.065934065934066,
      "grad_norm": 4.310985088348389,
      "learning_rate": 3.620879120879121e-05,
      "loss": 0.6037,
      "step": 1405600
    },
    {
      "epoch": 22.067503924646783,
      "grad_norm": 3.7800774574279785,
      "learning_rate": 3.620781004709577e-05,
      "loss": 0.5705,
      "step": 1405700
    },
    {
      "epoch": 22.069073783359496,
      "grad_norm": 4.065859317779541,
      "learning_rate": 3.620682888540032e-05,
      "loss": 0.5774,
      "step": 1405800
    },
    {
      "epoch": 22.070643642072213,
      "grad_norm": 3.6928999423980713,
      "learning_rate": 3.620584772370487e-05,
      "loss": 0.5888,
      "step": 1405900
    },
    {
      "epoch": 22.07221350078493,
      "grad_norm": 4.063069820404053,
      "learning_rate": 3.620486656200942e-05,
      "loss": 0.6125,
      "step": 1406000
    },
    {
      "epoch": 22.073783359497646,
      "grad_norm": 3.4741618633270264,
      "learning_rate": 3.620388540031398e-05,
      "loss": 0.5773,
      "step": 1406100
    },
    {
      "epoch": 22.07535321821036,
      "grad_norm": 5.213173866271973,
      "learning_rate": 3.620290423861852e-05,
      "loss": 0.5788,
      "step": 1406200
    },
    {
      "epoch": 22.076923076923077,
      "grad_norm": 3.7032482624053955,
      "learning_rate": 3.620192307692308e-05,
      "loss": 0.5756,
      "step": 1406300
    },
    {
      "epoch": 22.078492935635794,
      "grad_norm": 3.203277111053467,
      "learning_rate": 3.620094191522763e-05,
      "loss": 0.629,
      "step": 1406400
    },
    {
      "epoch": 22.08006279434851,
      "grad_norm": 5.167716026306152,
      "learning_rate": 3.619996075353219e-05,
      "loss": 0.64,
      "step": 1406500
    },
    {
      "epoch": 22.081632653061224,
      "grad_norm": 3.9558987617492676,
      "learning_rate": 3.619897959183673e-05,
      "loss": 0.6197,
      "step": 1406600
    },
    {
      "epoch": 22.08320251177394,
      "grad_norm": 4.084933757781982,
      "learning_rate": 3.619799843014129e-05,
      "loss": 0.5898,
      "step": 1406700
    },
    {
      "epoch": 22.084772370486657,
      "grad_norm": 3.8821699619293213,
      "learning_rate": 3.619701726844584e-05,
      "loss": 0.5877,
      "step": 1406800
    },
    {
      "epoch": 22.08634222919937,
      "grad_norm": 4.361866474151611,
      "learning_rate": 3.619603610675039e-05,
      "loss": 0.6184,
      "step": 1406900
    },
    {
      "epoch": 22.087912087912088,
      "grad_norm": 3.9987573623657227,
      "learning_rate": 3.619505494505494e-05,
      "loss": 0.6014,
      "step": 1407000
    },
    {
      "epoch": 22.089481946624804,
      "grad_norm": 2.802072048187256,
      "learning_rate": 3.61940737833595e-05,
      "loss": 0.6435,
      "step": 1407100
    },
    {
      "epoch": 22.09105180533752,
      "grad_norm": 5.143001556396484,
      "learning_rate": 3.619309262166405e-05,
      "loss": 0.5967,
      "step": 1407200
    },
    {
      "epoch": 22.092621664050235,
      "grad_norm": 4.444273471832275,
      "learning_rate": 3.61921114599686e-05,
      "loss": 0.6163,
      "step": 1407300
    },
    {
      "epoch": 22.09419152276295,
      "grad_norm": 2.141709804534912,
      "learning_rate": 3.6191130298273154e-05,
      "loss": 0.5793,
      "step": 1407400
    },
    {
      "epoch": 22.09576138147567,
      "grad_norm": 4.009089469909668,
      "learning_rate": 3.619014913657771e-05,
      "loss": 0.5972,
      "step": 1407500
    },
    {
      "epoch": 22.09733124018838,
      "grad_norm": 4.369388580322266,
      "learning_rate": 3.618916797488226e-05,
      "loss": 0.5987,
      "step": 1407600
    },
    {
      "epoch": 22.0989010989011,
      "grad_norm": 3.5331432819366455,
      "learning_rate": 3.6188186813186814e-05,
      "loss": 0.5941,
      "step": 1407700
    },
    {
      "epoch": 22.100470957613815,
      "grad_norm": 2.503828287124634,
      "learning_rate": 3.618720565149137e-05,
      "loss": 0.5757,
      "step": 1407800
    },
    {
      "epoch": 22.102040816326532,
      "grad_norm": 3.094930410385132,
      "learning_rate": 3.618622448979592e-05,
      "loss": 0.6149,
      "step": 1407900
    },
    {
      "epoch": 22.103610675039246,
      "grad_norm": 3.451143980026245,
      "learning_rate": 3.6185243328100473e-05,
      "loss": 0.6108,
      "step": 1408000
    },
    {
      "epoch": 22.105180533751962,
      "grad_norm": 3.0934455394744873,
      "learning_rate": 3.6184262166405024e-05,
      "loss": 0.6089,
      "step": 1408100
    },
    {
      "epoch": 22.10675039246468,
      "grad_norm": 4.7262725830078125,
      "learning_rate": 3.618328100470958e-05,
      "loss": 0.605,
      "step": 1408200
    },
    {
      "epoch": 22.108320251177393,
      "grad_norm": 3.842510461807251,
      "learning_rate": 3.6182299843014126e-05,
      "loss": 0.5652,
      "step": 1408300
    },
    {
      "epoch": 22.10989010989011,
      "grad_norm": 3.131824016571045,
      "learning_rate": 3.6181318681318684e-05,
      "loss": 0.6014,
      "step": 1408400
    },
    {
      "epoch": 22.111459968602826,
      "grad_norm": 4.038754940032959,
      "learning_rate": 3.6180337519623235e-05,
      "loss": 0.5819,
      "step": 1408500
    },
    {
      "epoch": 22.113029827315543,
      "grad_norm": 3.8911852836608887,
      "learning_rate": 3.617935635792779e-05,
      "loss": 0.5958,
      "step": 1408600
    },
    {
      "epoch": 22.114599686028257,
      "grad_norm": 3.6953160762786865,
      "learning_rate": 3.617837519623234e-05,
      "loss": 0.5909,
      "step": 1408700
    },
    {
      "epoch": 22.116169544740973,
      "grad_norm": 2.5951273441314697,
      "learning_rate": 3.6177394034536895e-05,
      "loss": 0.5833,
      "step": 1408800
    },
    {
      "epoch": 22.11773940345369,
      "grad_norm": 4.512247562408447,
      "learning_rate": 3.6176412872841446e-05,
      "loss": 0.6196,
      "step": 1408900
    },
    {
      "epoch": 22.119309262166404,
      "grad_norm": 4.068941593170166,
      "learning_rate": 3.6175431711146e-05,
      "loss": 0.6207,
      "step": 1409000
    },
    {
      "epoch": 22.12087912087912,
      "grad_norm": 4.521698951721191,
      "learning_rate": 3.617445054945055e-05,
      "loss": 0.5938,
      "step": 1409100
    },
    {
      "epoch": 22.122448979591837,
      "grad_norm": 4.245397567749023,
      "learning_rate": 3.6173469387755106e-05,
      "loss": 0.6054,
      "step": 1409200
    },
    {
      "epoch": 22.124018838304554,
      "grad_norm": 4.159144401550293,
      "learning_rate": 3.6172488226059656e-05,
      "loss": 0.5919,
      "step": 1409300
    },
    {
      "epoch": 22.125588697017267,
      "grad_norm": 4.249189853668213,
      "learning_rate": 3.617150706436421e-05,
      "loss": 0.6186,
      "step": 1409400
    },
    {
      "epoch": 22.127158555729984,
      "grad_norm": 4.599828720092773,
      "learning_rate": 3.617052590266876e-05,
      "loss": 0.5915,
      "step": 1409500
    },
    {
      "epoch": 22.1287284144427,
      "grad_norm": 3.350644826889038,
      "learning_rate": 3.6169544740973316e-05,
      "loss": 0.5548,
      "step": 1409600
    },
    {
      "epoch": 22.130298273155415,
      "grad_norm": 4.537264823913574,
      "learning_rate": 3.616856357927787e-05,
      "loss": 0.6073,
      "step": 1409700
    },
    {
      "epoch": 22.13186813186813,
      "grad_norm": 4.186446666717529,
      "learning_rate": 3.616758241758242e-05,
      "loss": 0.5836,
      "step": 1409800
    },
    {
      "epoch": 22.13343799058085,
      "grad_norm": 5.0841474533081055,
      "learning_rate": 3.6166601255886976e-05,
      "loss": 0.5924,
      "step": 1409900
    },
    {
      "epoch": 22.135007849293565,
      "grad_norm": 3.8203036785125732,
      "learning_rate": 3.616562009419153e-05,
      "loss": 0.562,
      "step": 1410000
    },
    {
      "epoch": 22.13657770800628,
      "grad_norm": 3.286219835281372,
      "learning_rate": 3.616463893249608e-05,
      "loss": 0.582,
      "step": 1410100
    },
    {
      "epoch": 22.138147566718995,
      "grad_norm": 3.1908271312713623,
      "learning_rate": 3.616365777080063e-05,
      "loss": 0.6099,
      "step": 1410200
    },
    {
      "epoch": 22.139717425431712,
      "grad_norm": 4.699625015258789,
      "learning_rate": 3.616267660910519e-05,
      "loss": 0.6425,
      "step": 1410300
    },
    {
      "epoch": 22.141287284144425,
      "grad_norm": 4.09807825088501,
      "learning_rate": 3.616169544740973e-05,
      "loss": 0.586,
      "step": 1410400
    },
    {
      "epoch": 22.142857142857142,
      "grad_norm": 4.800072193145752,
      "learning_rate": 3.616071428571429e-05,
      "loss": 0.6258,
      "step": 1410500
    },
    {
      "epoch": 22.14442700156986,
      "grad_norm": 4.336878776550293,
      "learning_rate": 3.615973312401884e-05,
      "loss": 0.5794,
      "step": 1410600
    },
    {
      "epoch": 22.145996860282576,
      "grad_norm": 2.293177604675293,
      "learning_rate": 3.61587519623234e-05,
      "loss": 0.6311,
      "step": 1410700
    },
    {
      "epoch": 22.14756671899529,
      "grad_norm": 3.0548970699310303,
      "learning_rate": 3.615777080062794e-05,
      "loss": 0.5978,
      "step": 1410800
    },
    {
      "epoch": 22.149136577708006,
      "grad_norm": 2.4634103775024414,
      "learning_rate": 3.61567896389325e-05,
      "loss": 0.633,
      "step": 1410900
    },
    {
      "epoch": 22.150706436420723,
      "grad_norm": 4.0642781257629395,
      "learning_rate": 3.615580847723705e-05,
      "loss": 0.6269,
      "step": 1411000
    },
    {
      "epoch": 22.152276295133436,
      "grad_norm": 3.885821580886841,
      "learning_rate": 3.61548273155416e-05,
      "loss": 0.6298,
      "step": 1411100
    },
    {
      "epoch": 22.153846153846153,
      "grad_norm": 4.239480972290039,
      "learning_rate": 3.615384615384615e-05,
      "loss": 0.592,
      "step": 1411200
    },
    {
      "epoch": 22.15541601255887,
      "grad_norm": 2.9606661796569824,
      "learning_rate": 3.615286499215071e-05,
      "loss": 0.5521,
      "step": 1411300
    },
    {
      "epoch": 22.156985871271587,
      "grad_norm": 4.127005577087402,
      "learning_rate": 3.615188383045526e-05,
      "loss": 0.6305,
      "step": 1411400
    },
    {
      "epoch": 22.1585557299843,
      "grad_norm": 4.083991527557373,
      "learning_rate": 3.615090266875981e-05,
      "loss": 0.5897,
      "step": 1411500
    },
    {
      "epoch": 22.160125588697017,
      "grad_norm": 3.1275198459625244,
      "learning_rate": 3.614992150706436e-05,
      "loss": 0.5849,
      "step": 1411600
    },
    {
      "epoch": 22.161695447409734,
      "grad_norm": 4.584899425506592,
      "learning_rate": 3.614894034536892e-05,
      "loss": 0.6162,
      "step": 1411700
    },
    {
      "epoch": 22.163265306122447,
      "grad_norm": 4.603682041168213,
      "learning_rate": 3.614795918367347e-05,
      "loss": 0.6156,
      "step": 1411800
    },
    {
      "epoch": 22.164835164835164,
      "grad_norm": 3.930140972137451,
      "learning_rate": 3.614697802197802e-05,
      "loss": 0.5603,
      "step": 1411900
    },
    {
      "epoch": 22.16640502354788,
      "grad_norm": 3.2140889167785645,
      "learning_rate": 3.614599686028258e-05,
      "loss": 0.5984,
      "step": 1412000
    },
    {
      "epoch": 22.167974882260598,
      "grad_norm": 3.292537212371826,
      "learning_rate": 3.614501569858713e-05,
      "loss": 0.6151,
      "step": 1412100
    },
    {
      "epoch": 22.16954474097331,
      "grad_norm": 4.363612651824951,
      "learning_rate": 3.614403453689168e-05,
      "loss": 0.5946,
      "step": 1412200
    },
    {
      "epoch": 22.171114599686028,
      "grad_norm": 3.4128963947296143,
      "learning_rate": 3.614305337519623e-05,
      "loss": 0.627,
      "step": 1412300
    },
    {
      "epoch": 22.172684458398745,
      "grad_norm": 3.867868185043335,
      "learning_rate": 3.614207221350079e-05,
      "loss": 0.5915,
      "step": 1412400
    },
    {
      "epoch": 22.17425431711146,
      "grad_norm": 2.274165630340576,
      "learning_rate": 3.6141091051805335e-05,
      "loss": 0.5918,
      "step": 1412500
    },
    {
      "epoch": 22.175824175824175,
      "grad_norm": 3.7354848384857178,
      "learning_rate": 3.614010989010989e-05,
      "loss": 0.6042,
      "step": 1412600
    },
    {
      "epoch": 22.177394034536892,
      "grad_norm": 2.420732259750366,
      "learning_rate": 3.6139128728414444e-05,
      "loss": 0.5814,
      "step": 1412700
    },
    {
      "epoch": 22.17896389324961,
      "grad_norm": 3.7752411365509033,
      "learning_rate": 3.6138147566719e-05,
      "loss": 0.5767,
      "step": 1412800
    },
    {
      "epoch": 22.180533751962322,
      "grad_norm": 4.322625160217285,
      "learning_rate": 3.6137166405023546e-05,
      "loss": 0.5859,
      "step": 1412900
    },
    {
      "epoch": 22.18210361067504,
      "grad_norm": 2.956700563430786,
      "learning_rate": 3.6136185243328104e-05,
      "loss": 0.6247,
      "step": 1413000
    },
    {
      "epoch": 22.183673469387756,
      "grad_norm": 3.294606924057007,
      "learning_rate": 3.6135204081632655e-05,
      "loss": 0.5949,
      "step": 1413100
    },
    {
      "epoch": 22.18524332810047,
      "grad_norm": 4.28504753112793,
      "learning_rate": 3.6134222919937206e-05,
      "loss": 0.6123,
      "step": 1413200
    },
    {
      "epoch": 22.186813186813186,
      "grad_norm": 5.067091941833496,
      "learning_rate": 3.613324175824176e-05,
      "loss": 0.5919,
      "step": 1413300
    },
    {
      "epoch": 22.188383045525903,
      "grad_norm": 4.070371150970459,
      "learning_rate": 3.6132260596546314e-05,
      "loss": 0.5987,
      "step": 1413400
    },
    {
      "epoch": 22.18995290423862,
      "grad_norm": 4.167627811431885,
      "learning_rate": 3.6131279434850865e-05,
      "loss": 0.6077,
      "step": 1413500
    },
    {
      "epoch": 22.191522762951333,
      "grad_norm": 2.256826877593994,
      "learning_rate": 3.6130298273155416e-05,
      "loss": 0.5969,
      "step": 1413600
    },
    {
      "epoch": 22.19309262166405,
      "grad_norm": 5.868027687072754,
      "learning_rate": 3.612931711145997e-05,
      "loss": 0.6372,
      "step": 1413700
    },
    {
      "epoch": 22.194662480376767,
      "grad_norm": 3.265625238418579,
      "learning_rate": 3.6128335949764525e-05,
      "loss": 0.5775,
      "step": 1413800
    },
    {
      "epoch": 22.19623233908948,
      "grad_norm": 3.9023914337158203,
      "learning_rate": 3.6127354788069076e-05,
      "loss": 0.5934,
      "step": 1413900
    },
    {
      "epoch": 22.197802197802197,
      "grad_norm": 4.889155387878418,
      "learning_rate": 3.612637362637363e-05,
      "loss": 0.5911,
      "step": 1414000
    },
    {
      "epoch": 22.199372056514914,
      "grad_norm": 3.132359266281128,
      "learning_rate": 3.6125392464678185e-05,
      "loss": 0.6414,
      "step": 1414100
    },
    {
      "epoch": 22.20094191522763,
      "grad_norm": 3.043281316757202,
      "learning_rate": 3.6124411302982736e-05,
      "loss": 0.6312,
      "step": 1414200
    },
    {
      "epoch": 22.202511773940344,
      "grad_norm": 3.9568231105804443,
      "learning_rate": 3.612343014128729e-05,
      "loss": 0.5751,
      "step": 1414300
    },
    {
      "epoch": 22.20408163265306,
      "grad_norm": 3.507659673690796,
      "learning_rate": 3.612244897959184e-05,
      "loss": 0.5504,
      "step": 1414400
    },
    {
      "epoch": 22.205651491365778,
      "grad_norm": 4.67863130569458,
      "learning_rate": 3.6121467817896396e-05,
      "loss": 0.5923,
      "step": 1414500
    },
    {
      "epoch": 22.20722135007849,
      "grad_norm": 3.0234627723693848,
      "learning_rate": 3.612048665620094e-05,
      "loss": 0.6149,
      "step": 1414600
    },
    {
      "epoch": 22.208791208791208,
      "grad_norm": 2.63576602935791,
      "learning_rate": 3.61195054945055e-05,
      "loss": 0.6252,
      "step": 1414700
    },
    {
      "epoch": 22.210361067503925,
      "grad_norm": 2.285628080368042,
      "learning_rate": 3.611852433281005e-05,
      "loss": 0.5528,
      "step": 1414800
    },
    {
      "epoch": 22.211930926216642,
      "grad_norm": 2.8240387439727783,
      "learning_rate": 3.6117543171114606e-05,
      "loss": 0.5806,
      "step": 1414900
    },
    {
      "epoch": 22.213500784929355,
      "grad_norm": 3.9488816261291504,
      "learning_rate": 3.611656200941915e-05,
      "loss": 0.6082,
      "step": 1415000
    },
    {
      "epoch": 22.215070643642072,
      "grad_norm": 4.580999851226807,
      "learning_rate": 3.611558084772371e-05,
      "loss": 0.5927,
      "step": 1415100
    },
    {
      "epoch": 22.21664050235479,
      "grad_norm": 4.009020805358887,
      "learning_rate": 3.611459968602826e-05,
      "loss": 0.5764,
      "step": 1415200
    },
    {
      "epoch": 22.218210361067506,
      "grad_norm": 4.865563869476318,
      "learning_rate": 3.611361852433281e-05,
      "loss": 0.6037,
      "step": 1415300
    },
    {
      "epoch": 22.21978021978022,
      "grad_norm": 3.946510076522827,
      "learning_rate": 3.611263736263736e-05,
      "loss": 0.6214,
      "step": 1415400
    },
    {
      "epoch": 22.221350078492936,
      "grad_norm": 3.3339016437530518,
      "learning_rate": 3.611165620094192e-05,
      "loss": 0.5849,
      "step": 1415500
    },
    {
      "epoch": 22.222919937205653,
      "grad_norm": 3.9080777168273926,
      "learning_rate": 3.611067503924647e-05,
      "loss": 0.6205,
      "step": 1415600
    },
    {
      "epoch": 22.224489795918366,
      "grad_norm": 4.964885234832764,
      "learning_rate": 3.610969387755102e-05,
      "loss": 0.6228,
      "step": 1415700
    },
    {
      "epoch": 22.226059654631083,
      "grad_norm": 5.111438274383545,
      "learning_rate": 3.610871271585557e-05,
      "loss": 0.601,
      "step": 1415800
    },
    {
      "epoch": 22.2276295133438,
      "grad_norm": 3.6908769607543945,
      "learning_rate": 3.610773155416013e-05,
      "loss": 0.5964,
      "step": 1415900
    },
    {
      "epoch": 22.229199372056517,
      "grad_norm": 3.252664804458618,
      "learning_rate": 3.610675039246468e-05,
      "loss": 0.6193,
      "step": 1416000
    },
    {
      "epoch": 22.23076923076923,
      "grad_norm": 4.574094295501709,
      "learning_rate": 3.610576923076923e-05,
      "loss": 0.6019,
      "step": 1416100
    },
    {
      "epoch": 22.232339089481947,
      "grad_norm": 4.547747611999512,
      "learning_rate": 3.610478806907379e-05,
      "loss": 0.5974,
      "step": 1416200
    },
    {
      "epoch": 22.233908948194664,
      "grad_norm": 3.008263349533081,
      "learning_rate": 3.610380690737834e-05,
      "loss": 0.6276,
      "step": 1416300
    },
    {
      "epoch": 22.235478806907377,
      "grad_norm": 3.46989369392395,
      "learning_rate": 3.610282574568289e-05,
      "loss": 0.581,
      "step": 1416400
    },
    {
      "epoch": 22.237048665620094,
      "grad_norm": 3.028141498565674,
      "learning_rate": 3.610184458398744e-05,
      "loss": 0.624,
      "step": 1416500
    },
    {
      "epoch": 22.23861852433281,
      "grad_norm": 3.3903822898864746,
      "learning_rate": 3.6100863422292e-05,
      "loss": 0.6158,
      "step": 1416600
    },
    {
      "epoch": 22.240188383045528,
      "grad_norm": 3.7021241188049316,
      "learning_rate": 3.6099882260596544e-05,
      "loss": 0.577,
      "step": 1416700
    },
    {
      "epoch": 22.24175824175824,
      "grad_norm": 3.930199384689331,
      "learning_rate": 3.60989010989011e-05,
      "loss": 0.5867,
      "step": 1416800
    },
    {
      "epoch": 22.243328100470958,
      "grad_norm": 3.4389188289642334,
      "learning_rate": 3.609791993720565e-05,
      "loss": 0.6107,
      "step": 1416900
    },
    {
      "epoch": 22.244897959183675,
      "grad_norm": 3.8124256134033203,
      "learning_rate": 3.609693877551021e-05,
      "loss": 0.6291,
      "step": 1417000
    },
    {
      "epoch": 22.246467817896388,
      "grad_norm": 3.1200692653656006,
      "learning_rate": 3.6095957613814755e-05,
      "loss": 0.6015,
      "step": 1417100
    },
    {
      "epoch": 22.248037676609105,
      "grad_norm": 3.587639808654785,
      "learning_rate": 3.609497645211931e-05,
      "loss": 0.6299,
      "step": 1417200
    },
    {
      "epoch": 22.24960753532182,
      "grad_norm": 3.720501184463501,
      "learning_rate": 3.6093995290423864e-05,
      "loss": 0.6306,
      "step": 1417300
    },
    {
      "epoch": 22.25117739403454,
      "grad_norm": 3.533364772796631,
      "learning_rate": 3.6093014128728415e-05,
      "loss": 0.6724,
      "step": 1417400
    },
    {
      "epoch": 22.252747252747252,
      "grad_norm": 3.787151575088501,
      "learning_rate": 3.6092032967032966e-05,
      "loss": 0.6247,
      "step": 1417500
    },
    {
      "epoch": 22.25431711145997,
      "grad_norm": 4.075816631317139,
      "learning_rate": 3.6091051805337523e-05,
      "loss": 0.5665,
      "step": 1417600
    },
    {
      "epoch": 22.255886970172686,
      "grad_norm": 4.130476951599121,
      "learning_rate": 3.6090070643642074e-05,
      "loss": 0.6247,
      "step": 1417700
    },
    {
      "epoch": 22.2574568288854,
      "grad_norm": 3.58542799949646,
      "learning_rate": 3.6089089481946625e-05,
      "loss": 0.613,
      "step": 1417800
    },
    {
      "epoch": 22.259026687598116,
      "grad_norm": 3.028852701187134,
      "learning_rate": 3.6088108320251176e-05,
      "loss": 0.5728,
      "step": 1417900
    },
    {
      "epoch": 22.260596546310833,
      "grad_norm": 4.176759243011475,
      "learning_rate": 3.6087127158555734e-05,
      "loss": 0.6063,
      "step": 1418000
    },
    {
      "epoch": 22.26216640502355,
      "grad_norm": 4.15891170501709,
      "learning_rate": 3.6086145996860285e-05,
      "loss": 0.5857,
      "step": 1418100
    },
    {
      "epoch": 22.263736263736263,
      "grad_norm": 3.7354369163513184,
      "learning_rate": 3.6085164835164836e-05,
      "loss": 0.5627,
      "step": 1418200
    },
    {
      "epoch": 22.26530612244898,
      "grad_norm": 3.802546262741089,
      "learning_rate": 3.6084183673469394e-05,
      "loss": 0.5979,
      "step": 1418300
    },
    {
      "epoch": 22.266875981161697,
      "grad_norm": 3.5666072368621826,
      "learning_rate": 3.6083202511773945e-05,
      "loss": 0.5656,
      "step": 1418400
    },
    {
      "epoch": 22.26844583987441,
      "grad_norm": 3.5253474712371826,
      "learning_rate": 3.6082221350078496e-05,
      "loss": 0.613,
      "step": 1418500
    },
    {
      "epoch": 22.270015698587127,
      "grad_norm": 3.646904706954956,
      "learning_rate": 3.608124018838305e-05,
      "loss": 0.5871,
      "step": 1418600
    },
    {
      "epoch": 22.271585557299844,
      "grad_norm": 3.342581033706665,
      "learning_rate": 3.6080259026687605e-05,
      "loss": 0.5842,
      "step": 1418700
    },
    {
      "epoch": 22.27315541601256,
      "grad_norm": 3.1690547466278076,
      "learning_rate": 3.607927786499215e-05,
      "loss": 0.5806,
      "step": 1418800
    },
    {
      "epoch": 22.274725274725274,
      "grad_norm": 4.201406002044678,
      "learning_rate": 3.6078296703296707e-05,
      "loss": 0.591,
      "step": 1418900
    },
    {
      "epoch": 22.27629513343799,
      "grad_norm": 3.867171049118042,
      "learning_rate": 3.607731554160126e-05,
      "loss": 0.5586,
      "step": 1419000
    },
    {
      "epoch": 22.277864992150707,
      "grad_norm": 4.119516849517822,
      "learning_rate": 3.6076334379905815e-05,
      "loss": 0.6013,
      "step": 1419100
    },
    {
      "epoch": 22.27943485086342,
      "grad_norm": 3.7778797149658203,
      "learning_rate": 3.607535321821036e-05,
      "loss": 0.5774,
      "step": 1419200
    },
    {
      "epoch": 22.281004709576138,
      "grad_norm": 4.382348537445068,
      "learning_rate": 3.607437205651492e-05,
      "loss": 0.5936,
      "step": 1419300
    },
    {
      "epoch": 22.282574568288855,
      "grad_norm": 2.8189315795898438,
      "learning_rate": 3.607339089481947e-05,
      "loss": 0.6284,
      "step": 1419400
    },
    {
      "epoch": 22.28414442700157,
      "grad_norm": 4.333746433258057,
      "learning_rate": 3.607240973312402e-05,
      "loss": 0.6366,
      "step": 1419500
    },
    {
      "epoch": 22.285714285714285,
      "grad_norm": 3.0841548442840576,
      "learning_rate": 3.607142857142857e-05,
      "loss": 0.6412,
      "step": 1419600
    },
    {
      "epoch": 22.287284144427,
      "grad_norm": 3.4709842205047607,
      "learning_rate": 3.607044740973313e-05,
      "loss": 0.6378,
      "step": 1419700
    },
    {
      "epoch": 22.28885400313972,
      "grad_norm": 4.26614236831665,
      "learning_rate": 3.606946624803768e-05,
      "loss": 0.5612,
      "step": 1419800
    },
    {
      "epoch": 22.29042386185243,
      "grad_norm": 4.7774457931518555,
      "learning_rate": 3.606848508634223e-05,
      "loss": 0.5981,
      "step": 1419900
    },
    {
      "epoch": 22.29199372056515,
      "grad_norm": 4.060633659362793,
      "learning_rate": 3.606750392464678e-05,
      "loss": 0.6295,
      "step": 1420000
    },
    {
      "epoch": 22.293563579277865,
      "grad_norm": 3.678278923034668,
      "learning_rate": 3.606652276295134e-05,
      "loss": 0.5341,
      "step": 1420100
    },
    {
      "epoch": 22.295133437990582,
      "grad_norm": 4.1069111824035645,
      "learning_rate": 3.606554160125589e-05,
      "loss": 0.6008,
      "step": 1420200
    },
    {
      "epoch": 22.296703296703296,
      "grad_norm": 3.0745837688446045,
      "learning_rate": 3.606456043956044e-05,
      "loss": 0.6053,
      "step": 1420300
    },
    {
      "epoch": 22.298273155416013,
      "grad_norm": 3.2098915576934814,
      "learning_rate": 3.6063579277865e-05,
      "loss": 0.6303,
      "step": 1420400
    },
    {
      "epoch": 22.29984301412873,
      "grad_norm": 4.131343364715576,
      "learning_rate": 3.606259811616955e-05,
      "loss": 0.615,
      "step": 1420500
    },
    {
      "epoch": 22.301412872841443,
      "grad_norm": 4.170322895050049,
      "learning_rate": 3.60616169544741e-05,
      "loss": 0.6009,
      "step": 1420600
    },
    {
      "epoch": 22.30298273155416,
      "grad_norm": 3.7123918533325195,
      "learning_rate": 3.606063579277865e-05,
      "loss": 0.6136,
      "step": 1420700
    },
    {
      "epoch": 22.304552590266876,
      "grad_norm": 3.7090048789978027,
      "learning_rate": 3.605965463108321e-05,
      "loss": 0.6102,
      "step": 1420800
    },
    {
      "epoch": 22.306122448979593,
      "grad_norm": 3.7663590908050537,
      "learning_rate": 3.605867346938775e-05,
      "loss": 0.5949,
      "step": 1420900
    },
    {
      "epoch": 22.307692307692307,
      "grad_norm": 1.9632854461669922,
      "learning_rate": 3.605769230769231e-05,
      "loss": 0.6096,
      "step": 1421000
    },
    {
      "epoch": 22.309262166405023,
      "grad_norm": 4.400132656097412,
      "learning_rate": 3.605671114599686e-05,
      "loss": 0.6088,
      "step": 1421100
    },
    {
      "epoch": 22.31083202511774,
      "grad_norm": 4.1981201171875,
      "learning_rate": 3.605572998430142e-05,
      "loss": 0.6012,
      "step": 1421200
    },
    {
      "epoch": 22.312401883830454,
      "grad_norm": 4.1082353591918945,
      "learning_rate": 3.6054748822605964e-05,
      "loss": 0.5814,
      "step": 1421300
    },
    {
      "epoch": 22.31397174254317,
      "grad_norm": 3.9802513122558594,
      "learning_rate": 3.605376766091052e-05,
      "loss": 0.6148,
      "step": 1421400
    },
    {
      "epoch": 22.315541601255887,
      "grad_norm": 3.5625717639923096,
      "learning_rate": 3.605278649921507e-05,
      "loss": 0.6124,
      "step": 1421500
    },
    {
      "epoch": 22.317111459968604,
      "grad_norm": 3.9628188610076904,
      "learning_rate": 3.6051805337519624e-05,
      "loss": 0.5962,
      "step": 1421600
    },
    {
      "epoch": 22.318681318681318,
      "grad_norm": 4.417172431945801,
      "learning_rate": 3.6050824175824175e-05,
      "loss": 0.5832,
      "step": 1421700
    },
    {
      "epoch": 22.320251177394034,
      "grad_norm": 3.749800682067871,
      "learning_rate": 3.604984301412873e-05,
      "loss": 0.5549,
      "step": 1421800
    },
    {
      "epoch": 22.32182103610675,
      "grad_norm": 3.6718952655792236,
      "learning_rate": 3.604886185243328e-05,
      "loss": 0.6321,
      "step": 1421900
    },
    {
      "epoch": 22.323390894819465,
      "grad_norm": 4.6457390785217285,
      "learning_rate": 3.6047880690737834e-05,
      "loss": 0.5767,
      "step": 1422000
    },
    {
      "epoch": 22.32496075353218,
      "grad_norm": 4.002288341522217,
      "learning_rate": 3.6046899529042385e-05,
      "loss": 0.5963,
      "step": 1422100
    },
    {
      "epoch": 22.3265306122449,
      "grad_norm": 3.568603277206421,
      "learning_rate": 3.604591836734694e-05,
      "loss": 0.5663,
      "step": 1422200
    },
    {
      "epoch": 22.328100470957615,
      "grad_norm": 3.9698736667633057,
      "learning_rate": 3.6044937205651494e-05,
      "loss": 0.6066,
      "step": 1422300
    },
    {
      "epoch": 22.32967032967033,
      "grad_norm": 3.9203367233276367,
      "learning_rate": 3.6043956043956045e-05,
      "loss": 0.589,
      "step": 1422400
    },
    {
      "epoch": 22.331240188383045,
      "grad_norm": 3.2057905197143555,
      "learning_rate": 3.6042974882260596e-05,
      "loss": 0.5684,
      "step": 1422500
    },
    {
      "epoch": 22.332810047095762,
      "grad_norm": 2.1744625568389893,
      "learning_rate": 3.604199372056515e-05,
      "loss": 0.6388,
      "step": 1422600
    },
    {
      "epoch": 22.334379905808476,
      "grad_norm": 4.571945667266846,
      "learning_rate": 3.6041012558869705e-05,
      "loss": 0.5918,
      "step": 1422700
    },
    {
      "epoch": 22.335949764521192,
      "grad_norm": 4.365999698638916,
      "learning_rate": 3.6040031397174256e-05,
      "loss": 0.6418,
      "step": 1422800
    },
    {
      "epoch": 22.33751962323391,
      "grad_norm": 3.966524839401245,
      "learning_rate": 3.6039050235478814e-05,
      "loss": 0.5865,
      "step": 1422900
    },
    {
      "epoch": 22.339089481946626,
      "grad_norm": 4.00433874130249,
      "learning_rate": 3.603806907378336e-05,
      "loss": 0.5983,
      "step": 1423000
    },
    {
      "epoch": 22.34065934065934,
      "grad_norm": 3.57511043548584,
      "learning_rate": 3.6037087912087916e-05,
      "loss": 0.6036,
      "step": 1423100
    },
    {
      "epoch": 22.342229199372056,
      "grad_norm": 4.097342491149902,
      "learning_rate": 3.6036106750392467e-05,
      "loss": 0.5593,
      "step": 1423200
    },
    {
      "epoch": 22.343799058084773,
      "grad_norm": 2.9864110946655273,
      "learning_rate": 3.603512558869702e-05,
      "loss": 0.6115,
      "step": 1423300
    },
    {
      "epoch": 22.345368916797486,
      "grad_norm": 3.836972713470459,
      "learning_rate": 3.603414442700157e-05,
      "loss": 0.575,
      "step": 1423400
    },
    {
      "epoch": 22.346938775510203,
      "grad_norm": 3.431974172592163,
      "learning_rate": 3.6033163265306126e-05,
      "loss": 0.5877,
      "step": 1423500
    },
    {
      "epoch": 22.34850863422292,
      "grad_norm": 3.3256709575653076,
      "learning_rate": 3.603218210361068e-05,
      "loss": 0.5498,
      "step": 1423600
    },
    {
      "epoch": 22.350078492935637,
      "grad_norm": 3.9299604892730713,
      "learning_rate": 3.603120094191523e-05,
      "loss": 0.532,
      "step": 1423700
    },
    {
      "epoch": 22.35164835164835,
      "grad_norm": 4.994016647338867,
      "learning_rate": 3.603021978021978e-05,
      "loss": 0.6037,
      "step": 1423800
    },
    {
      "epoch": 22.353218210361067,
      "grad_norm": 3.74967098236084,
      "learning_rate": 3.602923861852434e-05,
      "loss": 0.5769,
      "step": 1423900
    },
    {
      "epoch": 22.354788069073784,
      "grad_norm": 4.331031799316406,
      "learning_rate": 3.602825745682888e-05,
      "loss": 0.594,
      "step": 1424000
    },
    {
      "epoch": 22.356357927786497,
      "grad_norm": 4.023274898529053,
      "learning_rate": 3.602727629513344e-05,
      "loss": 0.59,
      "step": 1424100
    },
    {
      "epoch": 22.357927786499214,
      "grad_norm": 2.9388434886932373,
      "learning_rate": 3.602629513343799e-05,
      "loss": 0.563,
      "step": 1424200
    },
    {
      "epoch": 22.35949764521193,
      "grad_norm": 4.158663749694824,
      "learning_rate": 3.602531397174255e-05,
      "loss": 0.5942,
      "step": 1424300
    },
    {
      "epoch": 22.361067503924648,
      "grad_norm": 3.7097251415252686,
      "learning_rate": 3.60243328100471e-05,
      "loss": 0.6011,
      "step": 1424400
    },
    {
      "epoch": 22.36263736263736,
      "grad_norm": 3.609894275665283,
      "learning_rate": 3.602335164835165e-05,
      "loss": 0.6004,
      "step": 1424500
    },
    {
      "epoch": 22.364207221350078,
      "grad_norm": 4.140631198883057,
      "learning_rate": 3.60223704866562e-05,
      "loss": 0.6083,
      "step": 1424600
    },
    {
      "epoch": 22.365777080062795,
      "grad_norm": 4.474793434143066,
      "learning_rate": 3.602138932496075e-05,
      "loss": 0.569,
      "step": 1424700
    },
    {
      "epoch": 22.367346938775512,
      "grad_norm": 3.0019705295562744,
      "learning_rate": 3.602040816326531e-05,
      "loss": 0.5876,
      "step": 1424800
    },
    {
      "epoch": 22.368916797488225,
      "grad_norm": 3.959869861602783,
      "learning_rate": 3.601942700156986e-05,
      "loss": 0.6063,
      "step": 1424900
    },
    {
      "epoch": 22.370486656200942,
      "grad_norm": 2.295010805130005,
      "learning_rate": 3.601844583987442e-05,
      "loss": 0.5897,
      "step": 1425000
    },
    {
      "epoch": 22.37205651491366,
      "grad_norm": 3.127675771713257,
      "learning_rate": 3.601746467817896e-05,
      "loss": 0.5917,
      "step": 1425100
    },
    {
      "epoch": 22.373626373626372,
      "grad_norm": 4.41741418838501,
      "learning_rate": 3.601648351648352e-05,
      "loss": 0.5955,
      "step": 1425200
    },
    {
      "epoch": 22.37519623233909,
      "grad_norm": 3.879793167114258,
      "learning_rate": 3.601550235478807e-05,
      "loss": 0.5731,
      "step": 1425300
    },
    {
      "epoch": 22.376766091051806,
      "grad_norm": 4.09039831161499,
      "learning_rate": 3.601452119309262e-05,
      "loss": 0.62,
      "step": 1425400
    },
    {
      "epoch": 22.378335949764523,
      "grad_norm": 3.754396915435791,
      "learning_rate": 3.601354003139717e-05,
      "loss": 0.597,
      "step": 1425500
    },
    {
      "epoch": 22.379905808477236,
      "grad_norm": 3.904099225997925,
      "learning_rate": 3.601255886970173e-05,
      "loss": 0.617,
      "step": 1425600
    },
    {
      "epoch": 22.381475667189953,
      "grad_norm": 4.361974239349365,
      "learning_rate": 3.601157770800628e-05,
      "loss": 0.5852,
      "step": 1425700
    },
    {
      "epoch": 22.38304552590267,
      "grad_norm": 3.4813079833984375,
      "learning_rate": 3.601059654631083e-05,
      "loss": 0.6185,
      "step": 1425800
    },
    {
      "epoch": 22.384615384615383,
      "grad_norm": 2.6913983821868896,
      "learning_rate": 3.6009615384615384e-05,
      "loss": 0.5854,
      "step": 1425900
    },
    {
      "epoch": 22.3861852433281,
      "grad_norm": 4.011472225189209,
      "learning_rate": 3.600863422291994e-05,
      "loss": 0.6219,
      "step": 1426000
    },
    {
      "epoch": 22.387755102040817,
      "grad_norm": 4.202413558959961,
      "learning_rate": 3.6007653061224486e-05,
      "loss": 0.6141,
      "step": 1426100
    },
    {
      "epoch": 22.389324960753534,
      "grad_norm": 3.0000603199005127,
      "learning_rate": 3.6006671899529043e-05,
      "loss": 0.594,
      "step": 1426200
    },
    {
      "epoch": 22.390894819466247,
      "grad_norm": 4.412604808807373,
      "learning_rate": 3.6005690737833594e-05,
      "loss": 0.5787,
      "step": 1426300
    },
    {
      "epoch": 22.392464678178964,
      "grad_norm": 3.7262086868286133,
      "learning_rate": 3.600470957613815e-05,
      "loss": 0.5826,
      "step": 1426400
    },
    {
      "epoch": 22.39403453689168,
      "grad_norm": 4.628640651702881,
      "learning_rate": 3.60037284144427e-05,
      "loss": 0.6129,
      "step": 1426500
    },
    {
      "epoch": 22.395604395604394,
      "grad_norm": 3.273209810256958,
      "learning_rate": 3.6002747252747254e-05,
      "loss": 0.6147,
      "step": 1426600
    },
    {
      "epoch": 22.39717425431711,
      "grad_norm": 3.3018887042999268,
      "learning_rate": 3.6001766091051805e-05,
      "loss": 0.5941,
      "step": 1426700
    },
    {
      "epoch": 22.398744113029828,
      "grad_norm": 4.880702495574951,
      "learning_rate": 3.6000784929356356e-05,
      "loss": 0.5972,
      "step": 1426800
    },
    {
      "epoch": 22.400313971742545,
      "grad_norm": 3.0151336193084717,
      "learning_rate": 3.5999803767660914e-05,
      "loss": 0.5995,
      "step": 1426900
    },
    {
      "epoch": 22.401883830455258,
      "grad_norm": 4.584394454956055,
      "learning_rate": 3.5998822605965465e-05,
      "loss": 0.584,
      "step": 1427000
    },
    {
      "epoch": 22.403453689167975,
      "grad_norm": 2.9446637630462646,
      "learning_rate": 3.599784144427002e-05,
      "loss": 0.5713,
      "step": 1427100
    },
    {
      "epoch": 22.405023547880692,
      "grad_norm": 4.421199798583984,
      "learning_rate": 3.599686028257457e-05,
      "loss": 0.5785,
      "step": 1427200
    },
    {
      "epoch": 22.406593406593405,
      "grad_norm": 3.966233491897583,
      "learning_rate": 3.5995879120879125e-05,
      "loss": 0.5574,
      "step": 1427300
    },
    {
      "epoch": 22.408163265306122,
      "grad_norm": 3.413464069366455,
      "learning_rate": 3.5994897959183676e-05,
      "loss": 0.5869,
      "step": 1427400
    },
    {
      "epoch": 22.40973312401884,
      "grad_norm": 3.1390209197998047,
      "learning_rate": 3.5993916797488226e-05,
      "loss": 0.5786,
      "step": 1427500
    },
    {
      "epoch": 22.411302982731556,
      "grad_norm": 4.560723304748535,
      "learning_rate": 3.599293563579278e-05,
      "loss": 0.6539,
      "step": 1427600
    },
    {
      "epoch": 22.41287284144427,
      "grad_norm": 3.370143175125122,
      "learning_rate": 3.5991954474097335e-05,
      "loss": 0.5831,
      "step": 1427700
    },
    {
      "epoch": 22.414442700156986,
      "grad_norm": 4.453356742858887,
      "learning_rate": 3.5990973312401886e-05,
      "loss": 0.5962,
      "step": 1427800
    },
    {
      "epoch": 22.416012558869703,
      "grad_norm": 4.128498554229736,
      "learning_rate": 3.598999215070644e-05,
      "loss": 0.5691,
      "step": 1427900
    },
    {
      "epoch": 22.417582417582416,
      "grad_norm": 3.9324662685394287,
      "learning_rate": 3.598901098901099e-05,
      "loss": 0.5924,
      "step": 1428000
    },
    {
      "epoch": 22.419152276295133,
      "grad_norm": 3.7181763648986816,
      "learning_rate": 3.5988029827315546e-05,
      "loss": 0.6401,
      "step": 1428100
    },
    {
      "epoch": 22.42072213500785,
      "grad_norm": 3.7927026748657227,
      "learning_rate": 3.598704866562009e-05,
      "loss": 0.6218,
      "step": 1428200
    },
    {
      "epoch": 22.422291993720567,
      "grad_norm": 4.299880504608154,
      "learning_rate": 3.598606750392465e-05,
      "loss": 0.5971,
      "step": 1428300
    },
    {
      "epoch": 22.42386185243328,
      "grad_norm": 3.3885021209716797,
      "learning_rate": 3.59850863422292e-05,
      "loss": 0.5724,
      "step": 1428400
    },
    {
      "epoch": 22.425431711145997,
      "grad_norm": 5.025173187255859,
      "learning_rate": 3.598410518053376e-05,
      "loss": 0.6181,
      "step": 1428500
    },
    {
      "epoch": 22.427001569858714,
      "grad_norm": 3.7659695148468018,
      "learning_rate": 3.598312401883831e-05,
      "loss": 0.6348,
      "step": 1428600
    },
    {
      "epoch": 22.428571428571427,
      "grad_norm": 4.23173713684082,
      "learning_rate": 3.598214285714286e-05,
      "loss": 0.604,
      "step": 1428700
    },
    {
      "epoch": 22.430141287284144,
      "grad_norm": 2.281315565109253,
      "learning_rate": 3.598116169544741e-05,
      "loss": 0.571,
      "step": 1428800
    },
    {
      "epoch": 22.43171114599686,
      "grad_norm": 4.071268081665039,
      "learning_rate": 3.598018053375196e-05,
      "loss": 0.569,
      "step": 1428900
    },
    {
      "epoch": 22.433281004709578,
      "grad_norm": 4.130321979522705,
      "learning_rate": 3.597919937205652e-05,
      "loss": 0.58,
      "step": 1429000
    },
    {
      "epoch": 22.43485086342229,
      "grad_norm": 3.465017080307007,
      "learning_rate": 3.597821821036107e-05,
      "loss": 0.6039,
      "step": 1429100
    },
    {
      "epoch": 22.436420722135008,
      "grad_norm": 4.323594093322754,
      "learning_rate": 3.597723704866563e-05,
      "loss": 0.6137,
      "step": 1429200
    },
    {
      "epoch": 22.437990580847725,
      "grad_norm": 4.330095291137695,
      "learning_rate": 3.597625588697017e-05,
      "loss": 0.6132,
      "step": 1429300
    },
    {
      "epoch": 22.439560439560438,
      "grad_norm": 4.193085670471191,
      "learning_rate": 3.597527472527473e-05,
      "loss": 0.6047,
      "step": 1429400
    },
    {
      "epoch": 22.441130298273155,
      "grad_norm": 4.621711254119873,
      "learning_rate": 3.597429356357928e-05,
      "loss": 0.5859,
      "step": 1429500
    },
    {
      "epoch": 22.44270015698587,
      "grad_norm": 4.102947235107422,
      "learning_rate": 3.597331240188383e-05,
      "loss": 0.5899,
      "step": 1429600
    },
    {
      "epoch": 22.44427001569859,
      "grad_norm": 3.380460739135742,
      "learning_rate": 3.597233124018838e-05,
      "loss": 0.5802,
      "step": 1429700
    },
    {
      "epoch": 22.445839874411302,
      "grad_norm": 3.88171124458313,
      "learning_rate": 3.597135007849294e-05,
      "loss": 0.6103,
      "step": 1429800
    },
    {
      "epoch": 22.44740973312402,
      "grad_norm": 3.8965344429016113,
      "learning_rate": 3.597036891679749e-05,
      "loss": 0.6291,
      "step": 1429900
    },
    {
      "epoch": 22.448979591836736,
      "grad_norm": 3.673233985900879,
      "learning_rate": 3.596938775510204e-05,
      "loss": 0.6041,
      "step": 1430000
    },
    {
      "epoch": 22.45054945054945,
      "grad_norm": 3.917778730392456,
      "learning_rate": 3.596840659340659e-05,
      "loss": 0.596,
      "step": 1430100
    },
    {
      "epoch": 22.452119309262166,
      "grad_norm": 5.379960536956787,
      "learning_rate": 3.596742543171115e-05,
      "loss": 0.5639,
      "step": 1430200
    },
    {
      "epoch": 22.453689167974883,
      "grad_norm": 3.1025192737579346,
      "learning_rate": 3.5966444270015695e-05,
      "loss": 0.5967,
      "step": 1430300
    },
    {
      "epoch": 22.4552590266876,
      "grad_norm": 4.732589244842529,
      "learning_rate": 3.596546310832025e-05,
      "loss": 0.6168,
      "step": 1430400
    },
    {
      "epoch": 22.456828885400313,
      "grad_norm": 3.8058249950408936,
      "learning_rate": 3.59644819466248e-05,
      "loss": 0.5844,
      "step": 1430500
    },
    {
      "epoch": 22.45839874411303,
      "grad_norm": 3.746335506439209,
      "learning_rate": 3.596350078492936e-05,
      "loss": 0.5937,
      "step": 1430600
    },
    {
      "epoch": 22.459968602825747,
      "grad_norm": 4.436462879180908,
      "learning_rate": 3.596251962323391e-05,
      "loss": 0.5986,
      "step": 1430700
    },
    {
      "epoch": 22.46153846153846,
      "grad_norm": 3.4626333713531494,
      "learning_rate": 3.596153846153846e-05,
      "loss": 0.5846,
      "step": 1430800
    },
    {
      "epoch": 22.463108320251177,
      "grad_norm": 3.9875173568725586,
      "learning_rate": 3.5960557299843014e-05,
      "loss": 0.5906,
      "step": 1430900
    },
    {
      "epoch": 22.464678178963894,
      "grad_norm": 3.93038010597229,
      "learning_rate": 3.5959576138147565e-05,
      "loss": 0.6227,
      "step": 1431000
    },
    {
      "epoch": 22.46624803767661,
      "grad_norm": 3.2964887619018555,
      "learning_rate": 3.595859497645212e-05,
      "loss": 0.6379,
      "step": 1431100
    },
    {
      "epoch": 22.467817896389324,
      "grad_norm": 4.149914741516113,
      "learning_rate": 3.5957613814756674e-05,
      "loss": 0.6395,
      "step": 1431200
    },
    {
      "epoch": 22.46938775510204,
      "grad_norm": 2.0993692874908447,
      "learning_rate": 3.595663265306123e-05,
      "loss": 0.6261,
      "step": 1431300
    },
    {
      "epoch": 22.470957613814758,
      "grad_norm": 2.9064433574676514,
      "learning_rate": 3.5955651491365776e-05,
      "loss": 0.5907,
      "step": 1431400
    },
    {
      "epoch": 22.47252747252747,
      "grad_norm": 4.277837753295898,
      "learning_rate": 3.5954670329670334e-05,
      "loss": 0.5842,
      "step": 1431500
    },
    {
      "epoch": 22.474097331240188,
      "grad_norm": 4.720608234405518,
      "learning_rate": 3.5953689167974884e-05,
      "loss": 0.6614,
      "step": 1431600
    },
    {
      "epoch": 22.475667189952905,
      "grad_norm": 3.327993869781494,
      "learning_rate": 3.5952708006279435e-05,
      "loss": 0.5879,
      "step": 1431700
    },
    {
      "epoch": 22.47723704866562,
      "grad_norm": 3.249830722808838,
      "learning_rate": 3.5951726844583986e-05,
      "loss": 0.5908,
      "step": 1431800
    },
    {
      "epoch": 22.478806907378335,
      "grad_norm": 5.044301509857178,
      "learning_rate": 3.5950745682888544e-05,
      "loss": 0.5939,
      "step": 1431900
    },
    {
      "epoch": 22.48037676609105,
      "grad_norm": 3.7204930782318115,
      "learning_rate": 3.5949764521193095e-05,
      "loss": 0.5887,
      "step": 1432000
    },
    {
      "epoch": 22.48194662480377,
      "grad_norm": 3.2102696895599365,
      "learning_rate": 3.5948783359497646e-05,
      "loss": 0.6286,
      "step": 1432100
    },
    {
      "epoch": 22.483516483516482,
      "grad_norm": 3.3090274333953857,
      "learning_rate": 3.59478021978022e-05,
      "loss": 0.645,
      "step": 1432200
    },
    {
      "epoch": 22.4850863422292,
      "grad_norm": 3.6089932918548584,
      "learning_rate": 3.5946821036106755e-05,
      "loss": 0.5777,
      "step": 1432300
    },
    {
      "epoch": 22.486656200941916,
      "grad_norm": 3.837303876876831,
      "learning_rate": 3.59458398744113e-05,
      "loss": 0.6124,
      "step": 1432400
    },
    {
      "epoch": 22.488226059654632,
      "grad_norm": 4.107878684997559,
      "learning_rate": 3.594485871271586e-05,
      "loss": 0.5897,
      "step": 1432500
    },
    {
      "epoch": 22.489795918367346,
      "grad_norm": 3.366828441619873,
      "learning_rate": 3.594387755102041e-05,
      "loss": 0.6092,
      "step": 1432600
    },
    {
      "epoch": 22.491365777080063,
      "grad_norm": 2.824540853500366,
      "learning_rate": 3.5942896389324966e-05,
      "loss": 0.5779,
      "step": 1432700
    },
    {
      "epoch": 22.49293563579278,
      "grad_norm": 3.8671352863311768,
      "learning_rate": 3.5941915227629517e-05,
      "loss": 0.57,
      "step": 1432800
    },
    {
      "epoch": 22.494505494505496,
      "grad_norm": 4.1621222496032715,
      "learning_rate": 3.594093406593407e-05,
      "loss": 0.6125,
      "step": 1432900
    },
    {
      "epoch": 22.49607535321821,
      "grad_norm": 4.645153522491455,
      "learning_rate": 3.593995290423862e-05,
      "loss": 0.5936,
      "step": 1433000
    },
    {
      "epoch": 22.497645211930926,
      "grad_norm": 4.143319606781006,
      "learning_rate": 3.593897174254317e-05,
      "loss": 0.593,
      "step": 1433100
    },
    {
      "epoch": 22.499215070643643,
      "grad_norm": 4.964723110198975,
      "learning_rate": 3.593799058084773e-05,
      "loss": 0.5751,
      "step": 1433200
    },
    {
      "epoch": 22.500784929356357,
      "grad_norm": 3.7488152980804443,
      "learning_rate": 3.593700941915228e-05,
      "loss": 0.5868,
      "step": 1433300
    },
    {
      "epoch": 22.502354788069074,
      "grad_norm": 2.0045053958892822,
      "learning_rate": 3.5936028257456836e-05,
      "loss": 0.5838,
      "step": 1433400
    },
    {
      "epoch": 22.50392464678179,
      "grad_norm": 3.7242465019226074,
      "learning_rate": 3.593504709576138e-05,
      "loss": 0.637,
      "step": 1433500
    },
    {
      "epoch": 22.505494505494504,
      "grad_norm": 3.956840991973877,
      "learning_rate": 3.593406593406594e-05,
      "loss": 0.6163,
      "step": 1433600
    },
    {
      "epoch": 22.50706436420722,
      "grad_norm": 3.6330466270446777,
      "learning_rate": 3.593308477237049e-05,
      "loss": 0.6258,
      "step": 1433700
    },
    {
      "epoch": 22.508634222919937,
      "grad_norm": 3.7217466831207275,
      "learning_rate": 3.593210361067504e-05,
      "loss": 0.6106,
      "step": 1433800
    },
    {
      "epoch": 22.510204081632654,
      "grad_norm": 3.282698154449463,
      "learning_rate": 3.593112244897959e-05,
      "loss": 0.6186,
      "step": 1433900
    },
    {
      "epoch": 22.511773940345368,
      "grad_norm": 3.7546286582946777,
      "learning_rate": 3.593014128728415e-05,
      "loss": 0.5661,
      "step": 1434000
    },
    {
      "epoch": 22.513343799058084,
      "grad_norm": 5.67857027053833,
      "learning_rate": 3.59291601255887e-05,
      "loss": 0.582,
      "step": 1434100
    },
    {
      "epoch": 22.5149136577708,
      "grad_norm": 3.0201151371002197,
      "learning_rate": 3.592817896389325e-05,
      "loss": 0.6351,
      "step": 1434200
    },
    {
      "epoch": 22.516483516483518,
      "grad_norm": 4.1362128257751465,
      "learning_rate": 3.59271978021978e-05,
      "loss": 0.5926,
      "step": 1434300
    },
    {
      "epoch": 22.51805337519623,
      "grad_norm": 4.557837009429932,
      "learning_rate": 3.592621664050236e-05,
      "loss": 0.6229,
      "step": 1434400
    },
    {
      "epoch": 22.51962323390895,
      "grad_norm": 5.206784725189209,
      "learning_rate": 3.5925235478806904e-05,
      "loss": 0.6315,
      "step": 1434500
    },
    {
      "epoch": 22.521193092621665,
      "grad_norm": 2.8504621982574463,
      "learning_rate": 3.592425431711146e-05,
      "loss": 0.5535,
      "step": 1434600
    },
    {
      "epoch": 22.52276295133438,
      "grad_norm": 3.66963529586792,
      "learning_rate": 3.592327315541601e-05,
      "loss": 0.5825,
      "step": 1434700
    },
    {
      "epoch": 22.524332810047095,
      "grad_norm": 3.882632255554199,
      "learning_rate": 3.592229199372057e-05,
      "loss": 0.6006,
      "step": 1434800
    },
    {
      "epoch": 22.525902668759812,
      "grad_norm": 3.7615678310394287,
      "learning_rate": 3.592131083202512e-05,
      "loss": 0.5967,
      "step": 1434900
    },
    {
      "epoch": 22.52747252747253,
      "grad_norm": 3.9645254611968994,
      "learning_rate": 3.592032967032967e-05,
      "loss": 0.6022,
      "step": 1435000
    },
    {
      "epoch": 22.529042386185242,
      "grad_norm": 3.697169542312622,
      "learning_rate": 3.591934850863422e-05,
      "loss": 0.5434,
      "step": 1435100
    },
    {
      "epoch": 22.53061224489796,
      "grad_norm": 2.4395053386688232,
      "learning_rate": 3.5918367346938774e-05,
      "loss": 0.6504,
      "step": 1435200
    },
    {
      "epoch": 22.532182103610676,
      "grad_norm": 3.3373918533325195,
      "learning_rate": 3.591738618524333e-05,
      "loss": 0.5812,
      "step": 1435300
    },
    {
      "epoch": 22.53375196232339,
      "grad_norm": 2.736480236053467,
      "learning_rate": 3.591640502354788e-05,
      "loss": 0.5822,
      "step": 1435400
    },
    {
      "epoch": 22.535321821036106,
      "grad_norm": 4.0196356773376465,
      "learning_rate": 3.591542386185244e-05,
      "loss": 0.6395,
      "step": 1435500
    },
    {
      "epoch": 22.536891679748823,
      "grad_norm": 2.761472463607788,
      "learning_rate": 3.5914442700156985e-05,
      "loss": 0.5999,
      "step": 1435600
    },
    {
      "epoch": 22.53846153846154,
      "grad_norm": 3.7020366191864014,
      "learning_rate": 3.591346153846154e-05,
      "loss": 0.6141,
      "step": 1435700
    },
    {
      "epoch": 22.540031397174253,
      "grad_norm": 2.8208963871002197,
      "learning_rate": 3.5912480376766093e-05,
      "loss": 0.6135,
      "step": 1435800
    },
    {
      "epoch": 22.54160125588697,
      "grad_norm": 3.3810765743255615,
      "learning_rate": 3.5911499215070644e-05,
      "loss": 0.5939,
      "step": 1435900
    },
    {
      "epoch": 22.543171114599687,
      "grad_norm": 4.575100898742676,
      "learning_rate": 3.5910518053375195e-05,
      "loss": 0.6233,
      "step": 1436000
    },
    {
      "epoch": 22.5447409733124,
      "grad_norm": 3.4404256343841553,
      "learning_rate": 3.590953689167975e-05,
      "loss": 0.6028,
      "step": 1436100
    },
    {
      "epoch": 22.546310832025117,
      "grad_norm": 2.591435432434082,
      "learning_rate": 3.5908555729984304e-05,
      "loss": 0.5876,
      "step": 1436200
    },
    {
      "epoch": 22.547880690737834,
      "grad_norm": 2.839709758758545,
      "learning_rate": 3.5907574568288855e-05,
      "loss": 0.6302,
      "step": 1436300
    },
    {
      "epoch": 22.54945054945055,
      "grad_norm": 2.601332902908325,
      "learning_rate": 3.5906593406593406e-05,
      "loss": 0.6047,
      "step": 1436400
    },
    {
      "epoch": 22.551020408163264,
      "grad_norm": 2.8258488178253174,
      "learning_rate": 3.5905612244897964e-05,
      "loss": 0.6247,
      "step": 1436500
    },
    {
      "epoch": 22.55259026687598,
      "grad_norm": 5.021007537841797,
      "learning_rate": 3.590463108320251e-05,
      "loss": 0.5956,
      "step": 1436600
    },
    {
      "epoch": 22.554160125588698,
      "grad_norm": 3.800560712814331,
      "learning_rate": 3.5903649921507066e-05,
      "loss": 0.5739,
      "step": 1436700
    },
    {
      "epoch": 22.55572998430141,
      "grad_norm": 4.392967224121094,
      "learning_rate": 3.590266875981162e-05,
      "loss": 0.6176,
      "step": 1436800
    },
    {
      "epoch": 22.55729984301413,
      "grad_norm": 5.097110271453857,
      "learning_rate": 3.5901687598116175e-05,
      "loss": 0.635,
      "step": 1436900
    },
    {
      "epoch": 22.558869701726845,
      "grad_norm": 3.0787718296051025,
      "learning_rate": 3.5900706436420726e-05,
      "loss": 0.602,
      "step": 1437000
    },
    {
      "epoch": 22.560439560439562,
      "grad_norm": 5.359051704406738,
      "learning_rate": 3.5899725274725277e-05,
      "loss": 0.5702,
      "step": 1437100
    },
    {
      "epoch": 22.562009419152275,
      "grad_norm": 4.161962985992432,
      "learning_rate": 3.589874411302983e-05,
      "loss": 0.5877,
      "step": 1437200
    },
    {
      "epoch": 22.563579277864992,
      "grad_norm": 3.9153213500976562,
      "learning_rate": 3.589776295133438e-05,
      "loss": 0.585,
      "step": 1437300
    },
    {
      "epoch": 22.56514913657771,
      "grad_norm": 5.529809951782227,
      "learning_rate": 3.5896781789638936e-05,
      "loss": 0.6313,
      "step": 1437400
    },
    {
      "epoch": 22.566718995290422,
      "grad_norm": 4.140153408050537,
      "learning_rate": 3.589580062794349e-05,
      "loss": 0.6393,
      "step": 1437500
    },
    {
      "epoch": 22.56828885400314,
      "grad_norm": 3.670638084411621,
      "learning_rate": 3.5894819466248045e-05,
      "loss": 0.6122,
      "step": 1437600
    },
    {
      "epoch": 22.569858712715856,
      "grad_norm": 4.1831769943237305,
      "learning_rate": 3.589383830455259e-05,
      "loss": 0.5703,
      "step": 1437700
    },
    {
      "epoch": 22.571428571428573,
      "grad_norm": 3.9416093826293945,
      "learning_rate": 3.589285714285715e-05,
      "loss": 0.633,
      "step": 1437800
    },
    {
      "epoch": 22.572998430141286,
      "grad_norm": 3.3716423511505127,
      "learning_rate": 3.58918759811617e-05,
      "loss": 0.6235,
      "step": 1437900
    },
    {
      "epoch": 22.574568288854003,
      "grad_norm": 3.9027435779571533,
      "learning_rate": 3.589089481946625e-05,
      "loss": 0.592,
      "step": 1438000
    },
    {
      "epoch": 22.57613814756672,
      "grad_norm": 3.9724268913269043,
      "learning_rate": 3.58899136577708e-05,
      "loss": 0.5898,
      "step": 1438100
    },
    {
      "epoch": 22.577708006279433,
      "grad_norm": 3.5333542823791504,
      "learning_rate": 3.588893249607536e-05,
      "loss": 0.6119,
      "step": 1438200
    },
    {
      "epoch": 22.57927786499215,
      "grad_norm": 3.1400146484375,
      "learning_rate": 3.588795133437991e-05,
      "loss": 0.5999,
      "step": 1438300
    },
    {
      "epoch": 22.580847723704867,
      "grad_norm": 3.336239814758301,
      "learning_rate": 3.588697017268446e-05,
      "loss": 0.6115,
      "step": 1438400
    },
    {
      "epoch": 22.582417582417584,
      "grad_norm": 3.4644620418548584,
      "learning_rate": 3.588598901098901e-05,
      "loss": 0.604,
      "step": 1438500
    },
    {
      "epoch": 22.583987441130297,
      "grad_norm": 2.7648279666900635,
      "learning_rate": 3.588500784929357e-05,
      "loss": 0.6066,
      "step": 1438600
    },
    {
      "epoch": 22.585557299843014,
      "grad_norm": 3.447355270385742,
      "learning_rate": 3.588402668759811e-05,
      "loss": 0.5788,
      "step": 1438700
    },
    {
      "epoch": 22.58712715855573,
      "grad_norm": 3.9891929626464844,
      "learning_rate": 3.588304552590267e-05,
      "loss": 0.6175,
      "step": 1438800
    },
    {
      "epoch": 22.588697017268444,
      "grad_norm": 2.6856863498687744,
      "learning_rate": 3.588206436420722e-05,
      "loss": 0.6072,
      "step": 1438900
    },
    {
      "epoch": 22.59026687598116,
      "grad_norm": 3.3005142211914062,
      "learning_rate": 3.588108320251178e-05,
      "loss": 0.5878,
      "step": 1439000
    },
    {
      "epoch": 22.591836734693878,
      "grad_norm": 2.4372477531433105,
      "learning_rate": 3.588010204081633e-05,
      "loss": 0.551,
      "step": 1439100
    },
    {
      "epoch": 22.593406593406595,
      "grad_norm": 3.337824583053589,
      "learning_rate": 3.587912087912088e-05,
      "loss": 0.5983,
      "step": 1439200
    },
    {
      "epoch": 22.594976452119308,
      "grad_norm": 4.050719738006592,
      "learning_rate": 3.587813971742543e-05,
      "loss": 0.6179,
      "step": 1439300
    },
    {
      "epoch": 22.596546310832025,
      "grad_norm": 4.756496429443359,
      "learning_rate": 3.587715855572998e-05,
      "loss": 0.6027,
      "step": 1439400
    },
    {
      "epoch": 22.598116169544742,
      "grad_norm": 4.364186763763428,
      "learning_rate": 3.587617739403454e-05,
      "loss": 0.5992,
      "step": 1439500
    },
    {
      "epoch": 22.599686028257455,
      "grad_norm": 4.134413242340088,
      "learning_rate": 3.587519623233909e-05,
      "loss": 0.6009,
      "step": 1439600
    },
    {
      "epoch": 22.601255886970172,
      "grad_norm": 3.2463977336883545,
      "learning_rate": 3.587421507064365e-05,
      "loss": 0.6322,
      "step": 1439700
    },
    {
      "epoch": 22.60282574568289,
      "grad_norm": 3.8926360607147217,
      "learning_rate": 3.5873233908948194e-05,
      "loss": 0.6207,
      "step": 1439800
    },
    {
      "epoch": 22.604395604395606,
      "grad_norm": 4.15302848815918,
      "learning_rate": 3.587225274725275e-05,
      "loss": 0.57,
      "step": 1439900
    },
    {
      "epoch": 22.60596546310832,
      "grad_norm": 4.154567718505859,
      "learning_rate": 3.58712715855573e-05,
      "loss": 0.6116,
      "step": 1440000
    },
    {
      "epoch": 22.607535321821036,
      "grad_norm": 3.330636739730835,
      "learning_rate": 3.5870290423861853e-05,
      "loss": 0.598,
      "step": 1440100
    },
    {
      "epoch": 22.609105180533753,
      "grad_norm": 4.636809349060059,
      "learning_rate": 3.5869309262166404e-05,
      "loss": 0.5988,
      "step": 1440200
    },
    {
      "epoch": 22.610675039246466,
      "grad_norm": 2.9470624923706055,
      "learning_rate": 3.586832810047096e-05,
      "loss": 0.5518,
      "step": 1440300
    },
    {
      "epoch": 22.612244897959183,
      "grad_norm": 3.382524251937866,
      "learning_rate": 3.586734693877551e-05,
      "loss": 0.6044,
      "step": 1440400
    },
    {
      "epoch": 22.6138147566719,
      "grad_norm": 4.373133659362793,
      "learning_rate": 3.5866365777080064e-05,
      "loss": 0.5913,
      "step": 1440500
    },
    {
      "epoch": 22.615384615384617,
      "grad_norm": 3.7339601516723633,
      "learning_rate": 3.5865384615384615e-05,
      "loss": 0.6225,
      "step": 1440600
    },
    {
      "epoch": 22.61695447409733,
      "grad_norm": 4.037573337554932,
      "learning_rate": 3.586440345368917e-05,
      "loss": 0.6393,
      "step": 1440700
    },
    {
      "epoch": 22.618524332810047,
      "grad_norm": 4.080997467041016,
      "learning_rate": 3.586342229199372e-05,
      "loss": 0.6181,
      "step": 1440800
    },
    {
      "epoch": 22.620094191522764,
      "grad_norm": 3.4322943687438965,
      "learning_rate": 3.5862441130298275e-05,
      "loss": 0.5879,
      "step": 1440900
    },
    {
      "epoch": 22.621664050235477,
      "grad_norm": 2.9120218753814697,
      "learning_rate": 3.5861459968602826e-05,
      "loss": 0.6209,
      "step": 1441000
    },
    {
      "epoch": 22.623233908948194,
      "grad_norm": 3.42134690284729,
      "learning_rate": 3.5860478806907384e-05,
      "loss": 0.6244,
      "step": 1441100
    },
    {
      "epoch": 22.62480376766091,
      "grad_norm": 4.168644905090332,
      "learning_rate": 3.5859497645211935e-05,
      "loss": 0.5907,
      "step": 1441200
    },
    {
      "epoch": 22.626373626373628,
      "grad_norm": 3.0860016345977783,
      "learning_rate": 3.5858516483516486e-05,
      "loss": 0.5952,
      "step": 1441300
    },
    {
      "epoch": 22.62794348508634,
      "grad_norm": 4.05230712890625,
      "learning_rate": 3.5857535321821037e-05,
      "loss": 0.5918,
      "step": 1441400
    },
    {
      "epoch": 22.629513343799058,
      "grad_norm": 3.3225672245025635,
      "learning_rate": 3.585655416012559e-05,
      "loss": 0.5993,
      "step": 1441500
    },
    {
      "epoch": 22.631083202511775,
      "grad_norm": 3.8972129821777344,
      "learning_rate": 3.5855572998430145e-05,
      "loss": 0.6273,
      "step": 1441600
    },
    {
      "epoch": 22.632653061224488,
      "grad_norm": 4.186933994293213,
      "learning_rate": 3.5854591836734696e-05,
      "loss": 0.5964,
      "step": 1441700
    },
    {
      "epoch": 22.634222919937205,
      "grad_norm": 3.636967897415161,
      "learning_rate": 3.5853610675039254e-05,
      "loss": 0.5822,
      "step": 1441800
    },
    {
      "epoch": 22.635792778649922,
      "grad_norm": 3.089505434036255,
      "learning_rate": 3.58526295133438e-05,
      "loss": 0.5733,
      "step": 1441900
    },
    {
      "epoch": 22.63736263736264,
      "grad_norm": 3.965930223464966,
      "learning_rate": 3.5851648351648356e-05,
      "loss": 0.5756,
      "step": 1442000
    },
    {
      "epoch": 22.638932496075352,
      "grad_norm": 3.883516550064087,
      "learning_rate": 3.585066718995291e-05,
      "loss": 0.5955,
      "step": 1442100
    },
    {
      "epoch": 22.64050235478807,
      "grad_norm": 4.517428874969482,
      "learning_rate": 3.584968602825746e-05,
      "loss": 0.5614,
      "step": 1442200
    },
    {
      "epoch": 22.642072213500786,
      "grad_norm": 4.171856880187988,
      "learning_rate": 3.584870486656201e-05,
      "loss": 0.5948,
      "step": 1442300
    },
    {
      "epoch": 22.643642072213503,
      "grad_norm": 3.9020183086395264,
      "learning_rate": 3.584772370486657e-05,
      "loss": 0.6145,
      "step": 1442400
    },
    {
      "epoch": 22.645211930926216,
      "grad_norm": 4.013748645782471,
      "learning_rate": 3.584674254317112e-05,
      "loss": 0.6152,
      "step": 1442500
    },
    {
      "epoch": 22.646781789638933,
      "grad_norm": 3.7254695892333984,
      "learning_rate": 3.584576138147567e-05,
      "loss": 0.593,
      "step": 1442600
    },
    {
      "epoch": 22.64835164835165,
      "grad_norm": 3.5252325534820557,
      "learning_rate": 3.584478021978022e-05,
      "loss": 0.5986,
      "step": 1442700
    },
    {
      "epoch": 22.649921507064363,
      "grad_norm": 4.73537540435791,
      "learning_rate": 3.584379905808478e-05,
      "loss": 0.6169,
      "step": 1442800
    },
    {
      "epoch": 22.65149136577708,
      "grad_norm": 2.923292398452759,
      "learning_rate": 3.584281789638932e-05,
      "loss": 0.6036,
      "step": 1442900
    },
    {
      "epoch": 22.653061224489797,
      "grad_norm": 3.91750431060791,
      "learning_rate": 3.584183673469388e-05,
      "loss": 0.6518,
      "step": 1443000
    },
    {
      "epoch": 22.65463108320251,
      "grad_norm": 2.7316720485687256,
      "learning_rate": 3.584085557299843e-05,
      "loss": 0.6469,
      "step": 1443100
    },
    {
      "epoch": 22.656200941915227,
      "grad_norm": 3.4263317584991455,
      "learning_rate": 3.583987441130299e-05,
      "loss": 0.6286,
      "step": 1443200
    },
    {
      "epoch": 22.657770800627944,
      "grad_norm": 2.4786295890808105,
      "learning_rate": 3.583889324960754e-05,
      "loss": 0.6386,
      "step": 1443300
    },
    {
      "epoch": 22.65934065934066,
      "grad_norm": 38.65776443481445,
      "learning_rate": 3.583791208791209e-05,
      "loss": 0.5671,
      "step": 1443400
    },
    {
      "epoch": 22.660910518053374,
      "grad_norm": 3.2472527027130127,
      "learning_rate": 3.583693092621664e-05,
      "loss": 0.6091,
      "step": 1443500
    },
    {
      "epoch": 22.66248037676609,
      "grad_norm": 3.0079476833343506,
      "learning_rate": 3.583594976452119e-05,
      "loss": 0.5851,
      "step": 1443600
    },
    {
      "epoch": 22.664050235478808,
      "grad_norm": 4.040999889373779,
      "learning_rate": 3.583496860282575e-05,
      "loss": 0.5763,
      "step": 1443700
    },
    {
      "epoch": 22.665620094191524,
      "grad_norm": 4.832161903381348,
      "learning_rate": 3.58339874411303e-05,
      "loss": 0.6253,
      "step": 1443800
    },
    {
      "epoch": 22.667189952904238,
      "grad_norm": 4.189826965332031,
      "learning_rate": 3.583300627943486e-05,
      "loss": 0.6301,
      "step": 1443900
    },
    {
      "epoch": 22.668759811616955,
      "grad_norm": 3.2842156887054443,
      "learning_rate": 3.58320251177394e-05,
      "loss": 0.5838,
      "step": 1444000
    },
    {
      "epoch": 22.67032967032967,
      "grad_norm": 4.109511852264404,
      "learning_rate": 3.583104395604396e-05,
      "loss": 0.63,
      "step": 1444100
    },
    {
      "epoch": 22.671899529042385,
      "grad_norm": 4.5880255699157715,
      "learning_rate": 3.583006279434851e-05,
      "loss": 0.6042,
      "step": 1444200
    },
    {
      "epoch": 22.6734693877551,
      "grad_norm": 4.298643112182617,
      "learning_rate": 3.582908163265306e-05,
      "loss": 0.593,
      "step": 1444300
    },
    {
      "epoch": 22.67503924646782,
      "grad_norm": 3.5755348205566406,
      "learning_rate": 3.5828100470957613e-05,
      "loss": 0.6034,
      "step": 1444400
    },
    {
      "epoch": 22.676609105180535,
      "grad_norm": 4.2271013259887695,
      "learning_rate": 3.582711930926217e-05,
      "loss": 0.5957,
      "step": 1444500
    },
    {
      "epoch": 22.67817896389325,
      "grad_norm": 3.749373435974121,
      "learning_rate": 3.582613814756672e-05,
      "loss": 0.5941,
      "step": 1444600
    },
    {
      "epoch": 22.679748822605966,
      "grad_norm": 4.213930130004883,
      "learning_rate": 3.582515698587127e-05,
      "loss": 0.5786,
      "step": 1444700
    },
    {
      "epoch": 22.681318681318682,
      "grad_norm": 3.4855141639709473,
      "learning_rate": 3.5824175824175824e-05,
      "loss": 0.6058,
      "step": 1444800
    },
    {
      "epoch": 22.682888540031396,
      "grad_norm": 3.472383499145508,
      "learning_rate": 3.582319466248038e-05,
      "loss": 0.5961,
      "step": 1444900
    },
    {
      "epoch": 22.684458398744113,
      "grad_norm": 4.442141532897949,
      "learning_rate": 3.5822213500784926e-05,
      "loss": 0.6046,
      "step": 1445000
    },
    {
      "epoch": 22.68602825745683,
      "grad_norm": 2.279639482498169,
      "learning_rate": 3.5821232339089484e-05,
      "loss": 0.6286,
      "step": 1445100
    },
    {
      "epoch": 22.687598116169546,
      "grad_norm": 3.8375420570373535,
      "learning_rate": 3.5820251177394035e-05,
      "loss": 0.5983,
      "step": 1445200
    },
    {
      "epoch": 22.68916797488226,
      "grad_norm": 2.9793307781219482,
      "learning_rate": 3.5819270015698586e-05,
      "loss": 0.5684,
      "step": 1445300
    },
    {
      "epoch": 22.690737833594977,
      "grad_norm": 3.9041056632995605,
      "learning_rate": 3.5818288854003144e-05,
      "loss": 0.6135,
      "step": 1445400
    },
    {
      "epoch": 22.692307692307693,
      "grad_norm": 4.2882399559021,
      "learning_rate": 3.5817307692307695e-05,
      "loss": 0.5533,
      "step": 1445500
    },
    {
      "epoch": 22.693877551020407,
      "grad_norm": 5.3017354011535645,
      "learning_rate": 3.5816326530612245e-05,
      "loss": 0.5878,
      "step": 1445600
    },
    {
      "epoch": 22.695447409733124,
      "grad_norm": 2.5529823303222656,
      "learning_rate": 3.5815345368916796e-05,
      "loss": 0.6275,
      "step": 1445700
    },
    {
      "epoch": 22.69701726844584,
      "grad_norm": 3.5233688354492188,
      "learning_rate": 3.5814364207221354e-05,
      "loss": 0.5785,
      "step": 1445800
    },
    {
      "epoch": 22.698587127158557,
      "grad_norm": 5.246692657470703,
      "learning_rate": 3.5813383045525905e-05,
      "loss": 0.615,
      "step": 1445900
    },
    {
      "epoch": 22.70015698587127,
      "grad_norm": 3.498487710952759,
      "learning_rate": 3.5812401883830456e-05,
      "loss": 0.6223,
      "step": 1446000
    },
    {
      "epoch": 22.701726844583987,
      "grad_norm": 4.010478496551514,
      "learning_rate": 3.581142072213501e-05,
      "loss": 0.6202,
      "step": 1446100
    },
    {
      "epoch": 22.703296703296704,
      "grad_norm": 3.392259120941162,
      "learning_rate": 3.5810439560439565e-05,
      "loss": 0.5914,
      "step": 1446200
    },
    {
      "epoch": 22.704866562009418,
      "grad_norm": 3.7558414936065674,
      "learning_rate": 3.5809458398744116e-05,
      "loss": 0.6304,
      "step": 1446300
    },
    {
      "epoch": 22.706436420722135,
      "grad_norm": 3.7560791969299316,
      "learning_rate": 3.580847723704867e-05,
      "loss": 0.5726,
      "step": 1446400
    },
    {
      "epoch": 22.70800627943485,
      "grad_norm": 4.5906081199646,
      "learning_rate": 3.580749607535322e-05,
      "loss": 0.6252,
      "step": 1446500
    },
    {
      "epoch": 22.70957613814757,
      "grad_norm": 3.2370357513427734,
      "learning_rate": 3.5806514913657776e-05,
      "loss": 0.6006,
      "step": 1446600
    },
    {
      "epoch": 22.71114599686028,
      "grad_norm": 3.048288583755493,
      "learning_rate": 3.580553375196232e-05,
      "loss": 0.5924,
      "step": 1446700
    },
    {
      "epoch": 22.712715855573,
      "grad_norm": 4.213072299957275,
      "learning_rate": 3.580455259026688e-05,
      "loss": 0.6074,
      "step": 1446800
    },
    {
      "epoch": 22.714285714285715,
      "grad_norm": 2.9385569095611572,
      "learning_rate": 3.580357142857143e-05,
      "loss": 0.5873,
      "step": 1446900
    },
    {
      "epoch": 22.71585557299843,
      "grad_norm": 5.086169242858887,
      "learning_rate": 3.5802590266875986e-05,
      "loss": 0.6356,
      "step": 1447000
    },
    {
      "epoch": 22.717425431711145,
      "grad_norm": 4.035457134246826,
      "learning_rate": 3.580160910518053e-05,
      "loss": 0.6432,
      "step": 1447100
    },
    {
      "epoch": 22.718995290423862,
      "grad_norm": 3.369290828704834,
      "learning_rate": 3.580062794348509e-05,
      "loss": 0.5991,
      "step": 1447200
    },
    {
      "epoch": 22.72056514913658,
      "grad_norm": 4.007020473480225,
      "learning_rate": 3.579964678178964e-05,
      "loss": 0.6398,
      "step": 1447300
    },
    {
      "epoch": 22.722135007849293,
      "grad_norm": 3.4803543090820312,
      "learning_rate": 3.579866562009419e-05,
      "loss": 0.5808,
      "step": 1447400
    },
    {
      "epoch": 22.72370486656201,
      "grad_norm": 3.803227663040161,
      "learning_rate": 3.579768445839875e-05,
      "loss": 0.624,
      "step": 1447500
    },
    {
      "epoch": 22.725274725274726,
      "grad_norm": 3.624802350997925,
      "learning_rate": 3.57967032967033e-05,
      "loss": 0.6021,
      "step": 1447600
    },
    {
      "epoch": 22.72684458398744,
      "grad_norm": 2.4606235027313232,
      "learning_rate": 3.579572213500785e-05,
      "loss": 0.5908,
      "step": 1447700
    },
    {
      "epoch": 22.728414442700156,
      "grad_norm": 3.6082093715667725,
      "learning_rate": 3.57947409733124e-05,
      "loss": 0.6527,
      "step": 1447800
    },
    {
      "epoch": 22.729984301412873,
      "grad_norm": 2.9909427165985107,
      "learning_rate": 3.579375981161696e-05,
      "loss": 0.6487,
      "step": 1447900
    },
    {
      "epoch": 22.73155416012559,
      "grad_norm": 2.274550676345825,
      "learning_rate": 3.579277864992151e-05,
      "loss": 0.5879,
      "step": 1448000
    },
    {
      "epoch": 22.733124018838303,
      "grad_norm": 3.362377643585205,
      "learning_rate": 3.579179748822606e-05,
      "loss": 0.6509,
      "step": 1448100
    },
    {
      "epoch": 22.73469387755102,
      "grad_norm": 1.7621737718582153,
      "learning_rate": 3.579081632653061e-05,
      "loss": 0.5716,
      "step": 1448200
    },
    {
      "epoch": 22.736263736263737,
      "grad_norm": 5.298645496368408,
      "learning_rate": 3.578983516483517e-05,
      "loss": 0.6088,
      "step": 1448300
    },
    {
      "epoch": 22.73783359497645,
      "grad_norm": 3.3989412784576416,
      "learning_rate": 3.578885400313972e-05,
      "loss": 0.5945,
      "step": 1448400
    },
    {
      "epoch": 22.739403453689167,
      "grad_norm": 3.2476744651794434,
      "learning_rate": 3.578787284144427e-05,
      "loss": 0.616,
      "step": 1448500
    },
    {
      "epoch": 22.740973312401884,
      "grad_norm": 4.017758846282959,
      "learning_rate": 3.578689167974882e-05,
      "loss": 0.6223,
      "step": 1448600
    },
    {
      "epoch": 22.7425431711146,
      "grad_norm": 3.7947096824645996,
      "learning_rate": 3.578591051805338e-05,
      "loss": 0.6254,
      "step": 1448700
    },
    {
      "epoch": 22.744113029827314,
      "grad_norm": 3.638751745223999,
      "learning_rate": 3.5784929356357924e-05,
      "loss": 0.612,
      "step": 1448800
    },
    {
      "epoch": 22.74568288854003,
      "grad_norm": 3.845444440841675,
      "learning_rate": 3.578394819466248e-05,
      "loss": 0.666,
      "step": 1448900
    },
    {
      "epoch": 22.747252747252748,
      "grad_norm": 3.760714530944824,
      "learning_rate": 3.578296703296703e-05,
      "loss": 0.6387,
      "step": 1449000
    },
    {
      "epoch": 22.74882260596546,
      "grad_norm": 3.5130527019500732,
      "learning_rate": 3.578198587127159e-05,
      "loss": 0.6056,
      "step": 1449100
    },
    {
      "epoch": 22.75039246467818,
      "grad_norm": 2.7477457523345947,
      "learning_rate": 3.5781004709576135e-05,
      "loss": 0.5819,
      "step": 1449200
    },
    {
      "epoch": 22.751962323390895,
      "grad_norm": 4.776356220245361,
      "learning_rate": 3.578002354788069e-05,
      "loss": 0.626,
      "step": 1449300
    },
    {
      "epoch": 22.753532182103612,
      "grad_norm": 3.589848279953003,
      "learning_rate": 3.5779042386185244e-05,
      "loss": 0.5946,
      "step": 1449400
    },
    {
      "epoch": 22.755102040816325,
      "grad_norm": 3.406724452972412,
      "learning_rate": 3.5778061224489795e-05,
      "loss": 0.5895,
      "step": 1449500
    },
    {
      "epoch": 22.756671899529042,
      "grad_norm": 4.481411457061768,
      "learning_rate": 3.577708006279435e-05,
      "loss": 0.5829,
      "step": 1449600
    },
    {
      "epoch": 22.75824175824176,
      "grad_norm": 4.372668266296387,
      "learning_rate": 3.5776098901098903e-05,
      "loss": 0.6795,
      "step": 1449700
    },
    {
      "epoch": 22.759811616954472,
      "grad_norm": 3.0768799781799316,
      "learning_rate": 3.5775117739403454e-05,
      "loss": 0.6203,
      "step": 1449800
    },
    {
      "epoch": 22.76138147566719,
      "grad_norm": 3.105558156967163,
      "learning_rate": 3.5774136577708005e-05,
      "loss": 0.6157,
      "step": 1449900
    },
    {
      "epoch": 22.762951334379906,
      "grad_norm": 4.033507347106934,
      "learning_rate": 3.577315541601256e-05,
      "loss": 0.6234,
      "step": 1450000
    },
    {
      "epoch": 22.764521193092623,
      "grad_norm": 4.086367130279541,
      "learning_rate": 3.5772174254317114e-05,
      "loss": 0.5882,
      "step": 1450100
    },
    {
      "epoch": 22.766091051805336,
      "grad_norm": 4.258218288421631,
      "learning_rate": 3.5771193092621665e-05,
      "loss": 0.5873,
      "step": 1450200
    },
    {
      "epoch": 22.767660910518053,
      "grad_norm": 3.846710205078125,
      "learning_rate": 3.5770211930926216e-05,
      "loss": 0.5755,
      "step": 1450300
    },
    {
      "epoch": 22.76923076923077,
      "grad_norm": 4.301424980163574,
      "learning_rate": 3.5769230769230774e-05,
      "loss": 0.5686,
      "step": 1450400
    },
    {
      "epoch": 22.770800627943487,
      "grad_norm": 4.243144989013672,
      "learning_rate": 3.5768249607535325e-05,
      "loss": 0.5963,
      "step": 1450500
    },
    {
      "epoch": 22.7723704866562,
      "grad_norm": 2.9982974529266357,
      "learning_rate": 3.5767268445839876e-05,
      "loss": 0.5863,
      "step": 1450600
    },
    {
      "epoch": 22.773940345368917,
      "grad_norm": 4.067108631134033,
      "learning_rate": 3.576628728414443e-05,
      "loss": 0.6287,
      "step": 1450700
    },
    {
      "epoch": 22.775510204081634,
      "grad_norm": 4.036685943603516,
      "learning_rate": 3.5765306122448985e-05,
      "loss": 0.555,
      "step": 1450800
    },
    {
      "epoch": 22.777080062794347,
      "grad_norm": 4.665701389312744,
      "learning_rate": 3.576432496075353e-05,
      "loss": 0.5603,
      "step": 1450900
    },
    {
      "epoch": 22.778649921507064,
      "grad_norm": 3.6385369300842285,
      "learning_rate": 3.5763343799058087e-05,
      "loss": 0.5861,
      "step": 1451000
    },
    {
      "epoch": 22.78021978021978,
      "grad_norm": 4.615034103393555,
      "learning_rate": 3.576236263736264e-05,
      "loss": 0.6146,
      "step": 1451100
    },
    {
      "epoch": 22.781789638932494,
      "grad_norm": 5.104202747344971,
      "learning_rate": 3.5761381475667195e-05,
      "loss": 0.6069,
      "step": 1451200
    },
    {
      "epoch": 22.78335949764521,
      "grad_norm": 4.288865089416504,
      "learning_rate": 3.576040031397174e-05,
      "loss": 0.5922,
      "step": 1451300
    },
    {
      "epoch": 22.784929356357928,
      "grad_norm": 2.4671895503997803,
      "learning_rate": 3.57594191522763e-05,
      "loss": 0.6134,
      "step": 1451400
    },
    {
      "epoch": 22.786499215070645,
      "grad_norm": 3.773141384124756,
      "learning_rate": 3.575843799058085e-05,
      "loss": 0.5845,
      "step": 1451500
    },
    {
      "epoch": 22.788069073783358,
      "grad_norm": 3.3533923625946045,
      "learning_rate": 3.57574568288854e-05,
      "loss": 0.5904,
      "step": 1451600
    },
    {
      "epoch": 22.789638932496075,
      "grad_norm": 4.214210033416748,
      "learning_rate": 3.575647566718996e-05,
      "loss": 0.5783,
      "step": 1451700
    },
    {
      "epoch": 22.791208791208792,
      "grad_norm": 4.047238349914551,
      "learning_rate": 3.575549450549451e-05,
      "loss": 0.6355,
      "step": 1451800
    },
    {
      "epoch": 22.79277864992151,
      "grad_norm": 4.85498571395874,
      "learning_rate": 3.575451334379906e-05,
      "loss": 0.5759,
      "step": 1451900
    },
    {
      "epoch": 22.794348508634222,
      "grad_norm": 3.596468448638916,
      "learning_rate": 3.575353218210361e-05,
      "loss": 0.5646,
      "step": 1452000
    },
    {
      "epoch": 22.79591836734694,
      "grad_norm": 2.8072826862335205,
      "learning_rate": 3.575255102040817e-05,
      "loss": 0.5995,
      "step": 1452100
    },
    {
      "epoch": 22.797488226059656,
      "grad_norm": 3.2395071983337402,
      "learning_rate": 3.575156985871272e-05,
      "loss": 0.5989,
      "step": 1452200
    },
    {
      "epoch": 22.79905808477237,
      "grad_norm": 2.6713337898254395,
      "learning_rate": 3.575058869701727e-05,
      "loss": 0.5734,
      "step": 1452300
    },
    {
      "epoch": 22.800627943485086,
      "grad_norm": 3.8911192417144775,
      "learning_rate": 3.574960753532182e-05,
      "loss": 0.6343,
      "step": 1452400
    },
    {
      "epoch": 22.802197802197803,
      "grad_norm": 3.629995822906494,
      "learning_rate": 3.574862637362638e-05,
      "loss": 0.6145,
      "step": 1452500
    },
    {
      "epoch": 22.80376766091052,
      "grad_norm": 4.2484941482543945,
      "learning_rate": 3.574764521193093e-05,
      "loss": 0.6475,
      "step": 1452600
    },
    {
      "epoch": 22.805337519623233,
      "grad_norm": 4.2362871170043945,
      "learning_rate": 3.574666405023548e-05,
      "loss": 0.5821,
      "step": 1452700
    },
    {
      "epoch": 22.80690737833595,
      "grad_norm": 3.3891541957855225,
      "learning_rate": 3.574568288854003e-05,
      "loss": 0.6249,
      "step": 1452800
    },
    {
      "epoch": 22.808477237048667,
      "grad_norm": 4.142102241516113,
      "learning_rate": 3.574470172684459e-05,
      "loss": 0.5969,
      "step": 1452900
    },
    {
      "epoch": 22.81004709576138,
      "grad_norm": 3.190263032913208,
      "learning_rate": 3.574372056514913e-05,
      "loss": 0.6304,
      "step": 1453000
    },
    {
      "epoch": 22.811616954474097,
      "grad_norm": 3.9639177322387695,
      "learning_rate": 3.574273940345369e-05,
      "loss": 0.6056,
      "step": 1453100
    },
    {
      "epoch": 22.813186813186814,
      "grad_norm": 3.849876880645752,
      "learning_rate": 3.574175824175824e-05,
      "loss": 0.5824,
      "step": 1453200
    },
    {
      "epoch": 22.81475667189953,
      "grad_norm": 3.294914484024048,
      "learning_rate": 3.57407770800628e-05,
      "loss": 0.6272,
      "step": 1453300
    },
    {
      "epoch": 22.816326530612244,
      "grad_norm": 3.4074928760528564,
      "learning_rate": 3.5739795918367344e-05,
      "loss": 0.5941,
      "step": 1453400
    },
    {
      "epoch": 22.81789638932496,
      "grad_norm": 5.3189697265625,
      "learning_rate": 3.57388147566719e-05,
      "loss": 0.6206,
      "step": 1453500
    },
    {
      "epoch": 22.819466248037678,
      "grad_norm": 4.41290807723999,
      "learning_rate": 3.573783359497645e-05,
      "loss": 0.6017,
      "step": 1453600
    },
    {
      "epoch": 22.82103610675039,
      "grad_norm": 4.005010604858398,
      "learning_rate": 3.5736852433281004e-05,
      "loss": 0.5829,
      "step": 1453700
    },
    {
      "epoch": 22.822605965463108,
      "grad_norm": 3.316871404647827,
      "learning_rate": 3.573587127158556e-05,
      "loss": 0.6072,
      "step": 1453800
    },
    {
      "epoch": 22.824175824175825,
      "grad_norm": 5.299222469329834,
      "learning_rate": 3.573489010989011e-05,
      "loss": 0.6295,
      "step": 1453900
    },
    {
      "epoch": 22.82574568288854,
      "grad_norm": 4.027973175048828,
      "learning_rate": 3.5733908948194663e-05,
      "loss": 0.5953,
      "step": 1454000
    },
    {
      "epoch": 22.827315541601255,
      "grad_norm": 3.4232301712036133,
      "learning_rate": 3.5732927786499214e-05,
      "loss": 0.6212,
      "step": 1454100
    },
    {
      "epoch": 22.828885400313972,
      "grad_norm": 4.34772253036499,
      "learning_rate": 3.573194662480377e-05,
      "loss": 0.6019,
      "step": 1454200
    },
    {
      "epoch": 22.83045525902669,
      "grad_norm": 3.273026704788208,
      "learning_rate": 3.573096546310832e-05,
      "loss": 0.5947,
      "step": 1454300
    },
    {
      "epoch": 22.832025117739402,
      "grad_norm": 3.0515217781066895,
      "learning_rate": 3.5729984301412874e-05,
      "loss": 0.6012,
      "step": 1454400
    },
    {
      "epoch": 22.83359497645212,
      "grad_norm": 2.871591329574585,
      "learning_rate": 3.5729003139717425e-05,
      "loss": 0.5858,
      "step": 1454500
    },
    {
      "epoch": 22.835164835164836,
      "grad_norm": 4.9098711013793945,
      "learning_rate": 3.572802197802198e-05,
      "loss": 0.6305,
      "step": 1454600
    },
    {
      "epoch": 22.836734693877553,
      "grad_norm": 4.360967636108398,
      "learning_rate": 3.5727040816326534e-05,
      "loss": 0.6017,
      "step": 1454700
    },
    {
      "epoch": 22.838304552590266,
      "grad_norm": 3.8941338062286377,
      "learning_rate": 3.5726059654631085e-05,
      "loss": 0.5611,
      "step": 1454800
    },
    {
      "epoch": 22.839874411302983,
      "grad_norm": 3.84556245803833,
      "learning_rate": 3.5725078492935636e-05,
      "loss": 0.6109,
      "step": 1454900
    },
    {
      "epoch": 22.8414442700157,
      "grad_norm": 3.5616366863250732,
      "learning_rate": 3.5724097331240194e-05,
      "loss": 0.5968,
      "step": 1455000
    },
    {
      "epoch": 22.843014128728413,
      "grad_norm": 3.8278262615203857,
      "learning_rate": 3.572311616954474e-05,
      "loss": 0.5995,
      "step": 1455100
    },
    {
      "epoch": 22.84458398744113,
      "grad_norm": 4.047077655792236,
      "learning_rate": 3.5722135007849296e-05,
      "loss": 0.6204,
      "step": 1455200
    },
    {
      "epoch": 22.846153846153847,
      "grad_norm": 4.392649173736572,
      "learning_rate": 3.5721153846153847e-05,
      "loss": 0.5834,
      "step": 1455300
    },
    {
      "epoch": 22.847723704866564,
      "grad_norm": 3.7775588035583496,
      "learning_rate": 3.5720172684458404e-05,
      "loss": 0.6149,
      "step": 1455400
    },
    {
      "epoch": 22.849293563579277,
      "grad_norm": 4.347153663635254,
      "learning_rate": 3.571919152276295e-05,
      "loss": 0.6296,
      "step": 1455500
    },
    {
      "epoch": 22.850863422291994,
      "grad_norm": 3.349334716796875,
      "learning_rate": 3.5718210361067506e-05,
      "loss": 0.6314,
      "step": 1455600
    },
    {
      "epoch": 22.85243328100471,
      "grad_norm": 4.3099751472473145,
      "learning_rate": 3.571722919937206e-05,
      "loss": 0.5892,
      "step": 1455700
    },
    {
      "epoch": 22.854003139717424,
      "grad_norm": 3.9317426681518555,
      "learning_rate": 3.571624803767661e-05,
      "loss": 0.635,
      "step": 1455800
    },
    {
      "epoch": 22.85557299843014,
      "grad_norm": 4.426955699920654,
      "learning_rate": 3.5715266875981166e-05,
      "loss": 0.6047,
      "step": 1455900
    },
    {
      "epoch": 22.857142857142858,
      "grad_norm": 3.702432870864868,
      "learning_rate": 3.571428571428572e-05,
      "loss": 0.594,
      "step": 1456000
    },
    {
      "epoch": 22.858712715855575,
      "grad_norm": 4.003711700439453,
      "learning_rate": 3.571330455259027e-05,
      "loss": 0.6077,
      "step": 1456100
    },
    {
      "epoch": 22.860282574568288,
      "grad_norm": 2.7101223468780518,
      "learning_rate": 3.571232339089482e-05,
      "loss": 0.5744,
      "step": 1456200
    },
    {
      "epoch": 22.861852433281005,
      "grad_norm": 4.194736480712891,
      "learning_rate": 3.571134222919938e-05,
      "loss": 0.6389,
      "step": 1456300
    },
    {
      "epoch": 22.86342229199372,
      "grad_norm": 4.385947227478027,
      "learning_rate": 3.571036106750393e-05,
      "loss": 0.5991,
      "step": 1456400
    },
    {
      "epoch": 22.864992150706435,
      "grad_norm": 4.236241817474365,
      "learning_rate": 3.570937990580848e-05,
      "loss": 0.5989,
      "step": 1456500
    },
    {
      "epoch": 22.86656200941915,
      "grad_norm": 2.6960318088531494,
      "learning_rate": 3.570839874411303e-05,
      "loss": 0.6,
      "step": 1456600
    },
    {
      "epoch": 22.86813186813187,
      "grad_norm": 2.992499589920044,
      "learning_rate": 3.570741758241759e-05,
      "loss": 0.6306,
      "step": 1456700
    },
    {
      "epoch": 22.869701726844585,
      "grad_norm": 3.565258741378784,
      "learning_rate": 3.570643642072214e-05,
      "loss": 0.6226,
      "step": 1456800
    },
    {
      "epoch": 22.8712715855573,
      "grad_norm": 3.4111063480377197,
      "learning_rate": 3.570545525902669e-05,
      "loss": 0.6019,
      "step": 1456900
    },
    {
      "epoch": 22.872841444270016,
      "grad_norm": 4.308111190795898,
      "learning_rate": 3.570447409733124e-05,
      "loss": 0.6166,
      "step": 1457000
    },
    {
      "epoch": 22.874411302982733,
      "grad_norm": 3.5887250900268555,
      "learning_rate": 3.57034929356358e-05,
      "loss": 0.6064,
      "step": 1457100
    },
    {
      "epoch": 22.875981161695446,
      "grad_norm": 3.7668330669403076,
      "learning_rate": 3.570251177394034e-05,
      "loss": 0.6151,
      "step": 1457200
    },
    {
      "epoch": 22.877551020408163,
      "grad_norm": 3.8195974826812744,
      "learning_rate": 3.57015306122449e-05,
      "loss": 0.5911,
      "step": 1457300
    },
    {
      "epoch": 22.87912087912088,
      "grad_norm": 3.404684543609619,
      "learning_rate": 3.570054945054945e-05,
      "loss": 0.5928,
      "step": 1457400
    },
    {
      "epoch": 22.880690737833596,
      "grad_norm": 3.1080877780914307,
      "learning_rate": 3.569956828885401e-05,
      "loss": 0.6028,
      "step": 1457500
    },
    {
      "epoch": 22.88226059654631,
      "grad_norm": 3.8742377758026123,
      "learning_rate": 3.569858712715855e-05,
      "loss": 0.6457,
      "step": 1457600
    },
    {
      "epoch": 22.883830455259027,
      "grad_norm": 3.574730157852173,
      "learning_rate": 3.569760596546311e-05,
      "loss": 0.5728,
      "step": 1457700
    },
    {
      "epoch": 22.885400313971743,
      "grad_norm": 3.2179689407348633,
      "learning_rate": 3.569662480376766e-05,
      "loss": 0.6069,
      "step": 1457800
    },
    {
      "epoch": 22.886970172684457,
      "grad_norm": 4.178041458129883,
      "learning_rate": 3.569564364207221e-05,
      "loss": 0.5758,
      "step": 1457900
    },
    {
      "epoch": 22.888540031397174,
      "grad_norm": 3.827815055847168,
      "learning_rate": 3.5694662480376764e-05,
      "loss": 0.5967,
      "step": 1458000
    },
    {
      "epoch": 22.89010989010989,
      "grad_norm": 4.357875823974609,
      "learning_rate": 3.569368131868132e-05,
      "loss": 0.6041,
      "step": 1458100
    },
    {
      "epoch": 22.891679748822607,
      "grad_norm": 3.4739835262298584,
      "learning_rate": 3.569270015698587e-05,
      "loss": 0.6386,
      "step": 1458200
    },
    {
      "epoch": 22.89324960753532,
      "grad_norm": 3.478325843811035,
      "learning_rate": 3.5691718995290423e-05,
      "loss": 0.6017,
      "step": 1458300
    },
    {
      "epoch": 22.894819466248038,
      "grad_norm": 3.341792345046997,
      "learning_rate": 3.569073783359498e-05,
      "loss": 0.612,
      "step": 1458400
    },
    {
      "epoch": 22.896389324960754,
      "grad_norm": 3.571284770965576,
      "learning_rate": 3.568975667189953e-05,
      "loss": 0.6088,
      "step": 1458500
    },
    {
      "epoch": 22.897959183673468,
      "grad_norm": 3.8211984634399414,
      "learning_rate": 3.568877551020408e-05,
      "loss": 0.6111,
      "step": 1458600
    },
    {
      "epoch": 22.899529042386185,
      "grad_norm": 3.7620561122894287,
      "learning_rate": 3.5687794348508634e-05,
      "loss": 0.646,
      "step": 1458700
    },
    {
      "epoch": 22.9010989010989,
      "grad_norm": 3.7703137397766113,
      "learning_rate": 3.568681318681319e-05,
      "loss": 0.6304,
      "step": 1458800
    },
    {
      "epoch": 22.90266875981162,
      "grad_norm": 4.936290740966797,
      "learning_rate": 3.568583202511774e-05,
      "loss": 0.5992,
      "step": 1458900
    },
    {
      "epoch": 22.90423861852433,
      "grad_norm": 3.7430384159088135,
      "learning_rate": 3.5684850863422294e-05,
      "loss": 0.6117,
      "step": 1459000
    },
    {
      "epoch": 22.90580847723705,
      "grad_norm": 3.8864071369171143,
      "learning_rate": 3.5683869701726845e-05,
      "loss": 0.6039,
      "step": 1459100
    },
    {
      "epoch": 22.907378335949765,
      "grad_norm": 3.661651134490967,
      "learning_rate": 3.56828885400314e-05,
      "loss": 0.6117,
      "step": 1459200
    },
    {
      "epoch": 22.90894819466248,
      "grad_norm": 3.5959970951080322,
      "learning_rate": 3.568190737833595e-05,
      "loss": 0.5809,
      "step": 1459300
    },
    {
      "epoch": 22.910518053375196,
      "grad_norm": 3.432647228240967,
      "learning_rate": 3.5680926216640505e-05,
      "loss": 0.6221,
      "step": 1459400
    },
    {
      "epoch": 22.912087912087912,
      "grad_norm": 4.341708183288574,
      "learning_rate": 3.5679945054945056e-05,
      "loss": 0.585,
      "step": 1459500
    },
    {
      "epoch": 22.91365777080063,
      "grad_norm": 3.694283962249756,
      "learning_rate": 3.567896389324961e-05,
      "loss": 0.6017,
      "step": 1459600
    },
    {
      "epoch": 22.915227629513343,
      "grad_norm": 3.6714577674865723,
      "learning_rate": 3.567798273155416e-05,
      "loss": 0.6433,
      "step": 1459700
    },
    {
      "epoch": 22.91679748822606,
      "grad_norm": 3.631136894226074,
      "learning_rate": 3.5677001569858715e-05,
      "loss": 0.5537,
      "step": 1459800
    },
    {
      "epoch": 22.918367346938776,
      "grad_norm": 2.014897346496582,
      "learning_rate": 3.5676020408163266e-05,
      "loss": 0.5924,
      "step": 1459900
    },
    {
      "epoch": 22.919937205651493,
      "grad_norm": 4.170656204223633,
      "learning_rate": 3.567503924646782e-05,
      "loss": 0.5917,
      "step": 1460000
    },
    {
      "epoch": 22.921507064364206,
      "grad_norm": 2.8772387504577637,
      "learning_rate": 3.567405808477237e-05,
      "loss": 0.5873,
      "step": 1460100
    },
    {
      "epoch": 22.923076923076923,
      "grad_norm": 3.2071433067321777,
      "learning_rate": 3.5673076923076926e-05,
      "loss": 0.6232,
      "step": 1460200
    },
    {
      "epoch": 22.92464678178964,
      "grad_norm": 4.118580341339111,
      "learning_rate": 3.567209576138148e-05,
      "loss": 0.5989,
      "step": 1460300
    },
    {
      "epoch": 22.926216640502354,
      "grad_norm": 3.5363941192626953,
      "learning_rate": 3.567111459968603e-05,
      "loss": 0.5827,
      "step": 1460400
    },
    {
      "epoch": 22.92778649921507,
      "grad_norm": 5.608015537261963,
      "learning_rate": 3.5670133437990586e-05,
      "loss": 0.6078,
      "step": 1460500
    },
    {
      "epoch": 22.929356357927787,
      "grad_norm": 3.534442663192749,
      "learning_rate": 3.566915227629514e-05,
      "loss": 0.5695,
      "step": 1460600
    },
    {
      "epoch": 22.9309262166405,
      "grad_norm": 3.7780654430389404,
      "learning_rate": 3.566817111459969e-05,
      "loss": 0.6303,
      "step": 1460700
    },
    {
      "epoch": 22.932496075353217,
      "grad_norm": 2.50750994682312,
      "learning_rate": 3.566718995290424e-05,
      "loss": 0.5833,
      "step": 1460800
    },
    {
      "epoch": 22.934065934065934,
      "grad_norm": 3.663370132446289,
      "learning_rate": 3.5666208791208796e-05,
      "loss": 0.6131,
      "step": 1460900
    },
    {
      "epoch": 22.93563579277865,
      "grad_norm": 2.854983329772949,
      "learning_rate": 3.566522762951335e-05,
      "loss": 0.6163,
      "step": 1461000
    },
    {
      "epoch": 22.937205651491364,
      "grad_norm": 3.6751575469970703,
      "learning_rate": 3.56642464678179e-05,
      "loss": 0.6184,
      "step": 1461100
    },
    {
      "epoch": 22.93877551020408,
      "grad_norm": 3.7764713764190674,
      "learning_rate": 3.566326530612245e-05,
      "loss": 0.6279,
      "step": 1461200
    },
    {
      "epoch": 22.940345368916798,
      "grad_norm": 3.0983448028564453,
      "learning_rate": 3.566228414442701e-05,
      "loss": 0.6486,
      "step": 1461300
    },
    {
      "epoch": 22.941915227629515,
      "grad_norm": 2.7944278717041016,
      "learning_rate": 3.566130298273155e-05,
      "loss": 0.596,
      "step": 1461400
    },
    {
      "epoch": 22.94348508634223,
      "grad_norm": 4.281424045562744,
      "learning_rate": 3.566032182103611e-05,
      "loss": 0.6231,
      "step": 1461500
    },
    {
      "epoch": 22.945054945054945,
      "grad_norm": 3.4231514930725098,
      "learning_rate": 3.565934065934066e-05,
      "loss": 0.6308,
      "step": 1461600
    },
    {
      "epoch": 22.946624803767662,
      "grad_norm": 3.394392251968384,
      "learning_rate": 3.565835949764522e-05,
      "loss": 0.6097,
      "step": 1461700
    },
    {
      "epoch": 22.948194662480375,
      "grad_norm": 3.9797823429107666,
      "learning_rate": 3.565737833594976e-05,
      "loss": 0.5926,
      "step": 1461800
    },
    {
      "epoch": 22.949764521193092,
      "grad_norm": 2.6189002990722656,
      "learning_rate": 3.565639717425432e-05,
      "loss": 0.5957,
      "step": 1461900
    },
    {
      "epoch": 22.95133437990581,
      "grad_norm": 3.665452480316162,
      "learning_rate": 3.565541601255887e-05,
      "loss": 0.614,
      "step": 1462000
    },
    {
      "epoch": 22.952904238618526,
      "grad_norm": 4.28027868270874,
      "learning_rate": 3.565443485086342e-05,
      "loss": 0.623,
      "step": 1462100
    },
    {
      "epoch": 22.95447409733124,
      "grad_norm": 2.995163679122925,
      "learning_rate": 3.565345368916797e-05,
      "loss": 0.5731,
      "step": 1462200
    },
    {
      "epoch": 22.956043956043956,
      "grad_norm": 4.23071813583374,
      "learning_rate": 3.565247252747253e-05,
      "loss": 0.5936,
      "step": 1462300
    },
    {
      "epoch": 22.957613814756673,
      "grad_norm": 3.74066424369812,
      "learning_rate": 3.565149136577708e-05,
      "loss": 0.5965,
      "step": 1462400
    },
    {
      "epoch": 22.959183673469386,
      "grad_norm": 3.229763984680176,
      "learning_rate": 3.565051020408163e-05,
      "loss": 0.6179,
      "step": 1462500
    },
    {
      "epoch": 22.960753532182103,
      "grad_norm": 3.8054916858673096,
      "learning_rate": 3.564952904238619e-05,
      "loss": 0.6004,
      "step": 1462600
    },
    {
      "epoch": 22.96232339089482,
      "grad_norm": 4.619832992553711,
      "learning_rate": 3.564854788069074e-05,
      "loss": 0.6032,
      "step": 1462700
    },
    {
      "epoch": 22.963893249607537,
      "grad_norm": 4.184856414794922,
      "learning_rate": 3.564756671899529e-05,
      "loss": 0.5755,
      "step": 1462800
    },
    {
      "epoch": 22.96546310832025,
      "grad_norm": 4.314945697784424,
      "learning_rate": 3.564658555729984e-05,
      "loss": 0.603,
      "step": 1462900
    },
    {
      "epoch": 22.967032967032967,
      "grad_norm": 3.3651211261749268,
      "learning_rate": 3.56456043956044e-05,
      "loss": 0.5857,
      "step": 1463000
    },
    {
      "epoch": 22.968602825745684,
      "grad_norm": 3.235710382461548,
      "learning_rate": 3.564462323390895e-05,
      "loss": 0.5774,
      "step": 1463100
    },
    {
      "epoch": 22.970172684458397,
      "grad_norm": 4.1697001457214355,
      "learning_rate": 3.56436420722135e-05,
      "loss": 0.6013,
      "step": 1463200
    },
    {
      "epoch": 22.971742543171114,
      "grad_norm": 4.520525932312012,
      "learning_rate": 3.5642660910518054e-05,
      "loss": 0.5911,
      "step": 1463300
    },
    {
      "epoch": 22.97331240188383,
      "grad_norm": 3.7045681476593018,
      "learning_rate": 3.564167974882261e-05,
      "loss": 0.6161,
      "step": 1463400
    },
    {
      "epoch": 22.974882260596548,
      "grad_norm": 3.938626766204834,
      "learning_rate": 3.5640698587127156e-05,
      "loss": 0.5678,
      "step": 1463500
    },
    {
      "epoch": 22.97645211930926,
      "grad_norm": 3.6486175060272217,
      "learning_rate": 3.5639717425431714e-05,
      "loss": 0.6051,
      "step": 1463600
    },
    {
      "epoch": 22.978021978021978,
      "grad_norm": 4.858661651611328,
      "learning_rate": 3.5638736263736265e-05,
      "loss": 0.5643,
      "step": 1463700
    },
    {
      "epoch": 22.979591836734695,
      "grad_norm": 3.9842278957366943,
      "learning_rate": 3.563775510204082e-05,
      "loss": 0.563,
      "step": 1463800
    },
    {
      "epoch": 22.98116169544741,
      "grad_norm": 4.815523624420166,
      "learning_rate": 3.5636773940345366e-05,
      "loss": 0.5911,
      "step": 1463900
    },
    {
      "epoch": 22.982731554160125,
      "grad_norm": 4.373671054840088,
      "learning_rate": 3.5635792778649924e-05,
      "loss": 0.5707,
      "step": 1464000
    },
    {
      "epoch": 22.984301412872842,
      "grad_norm": 3.7513480186462402,
      "learning_rate": 3.5634811616954475e-05,
      "loss": 0.5867,
      "step": 1464100
    },
    {
      "epoch": 22.98587127158556,
      "grad_norm": 4.207093238830566,
      "learning_rate": 3.5633830455259026e-05,
      "loss": 0.589,
      "step": 1464200
    },
    {
      "epoch": 22.987441130298272,
      "grad_norm": 3.74714732170105,
      "learning_rate": 3.563284929356358e-05,
      "loss": 0.6497,
      "step": 1464300
    },
    {
      "epoch": 22.98901098901099,
      "grad_norm": 6.259380340576172,
      "learning_rate": 3.5631868131868135e-05,
      "loss": 0.5864,
      "step": 1464400
    },
    {
      "epoch": 22.990580847723706,
      "grad_norm": 3.326111078262329,
      "learning_rate": 3.5630886970172686e-05,
      "loss": 0.5475,
      "step": 1464500
    },
    {
      "epoch": 22.99215070643642,
      "grad_norm": 2.9293534755706787,
      "learning_rate": 3.562990580847724e-05,
      "loss": 0.5824,
      "step": 1464600
    },
    {
      "epoch": 22.993720565149136,
      "grad_norm": 4.558704853057861,
      "learning_rate": 3.5628924646781795e-05,
      "loss": 0.6056,
      "step": 1464700
    },
    {
      "epoch": 22.995290423861853,
      "grad_norm": 3.358454465866089,
      "learning_rate": 3.5627943485086346e-05,
      "loss": 0.5733,
      "step": 1464800
    },
    {
      "epoch": 22.99686028257457,
      "grad_norm": 3.939741373062134,
      "learning_rate": 3.5626962323390897e-05,
      "loss": 0.567,
      "step": 1464900
    },
    {
      "epoch": 22.998430141287283,
      "grad_norm": 3.960294723510742,
      "learning_rate": 3.562598116169545e-05,
      "loss": 0.5891,
      "step": 1465000
    },
    {
      "epoch": 23.0,
      "grad_norm": 3.043346643447876,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 0.6267,
      "step": 1465100
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.028606653213501,
      "eval_runtime": 14.6811,
      "eval_samples_per_second": 228.389,
      "eval_steps_per_second": 228.389,
      "step": 1465100
    },
    {
      "epoch": 23.0,
      "eval_loss": 0.46384990215301514,
      "eval_runtime": 280.2824,
      "eval_samples_per_second": 227.271,
      "eval_steps_per_second": 227.271,
      "step": 1465100
    },
    {
      "epoch": 23.001569858712717,
      "grad_norm": 4.059050559997559,
      "learning_rate": 3.5624018838304556e-05,
      "loss": 0.5566,
      "step": 1465200
    },
    {
      "epoch": 23.00313971742543,
      "grad_norm": 3.640219211578369,
      "learning_rate": 3.562303767660911e-05,
      "loss": 0.6116,
      "step": 1465300
    },
    {
      "epoch": 23.004709576138147,
      "grad_norm": 3.835517406463623,
      "learning_rate": 3.562205651491366e-05,
      "loss": 0.6248,
      "step": 1465400
    },
    {
      "epoch": 23.006279434850864,
      "grad_norm": 1.9663194417953491,
      "learning_rate": 3.5621075353218216e-05,
      "loss": 0.5439,
      "step": 1465500
    },
    {
      "epoch": 23.00784929356358,
      "grad_norm": 4.3677802085876465,
      "learning_rate": 3.562009419152276e-05,
      "loss": 0.5626,
      "step": 1465600
    },
    {
      "epoch": 23.009419152276294,
      "grad_norm": 4.452783584594727,
      "learning_rate": 3.561911302982732e-05,
      "loss": 0.5978,
      "step": 1465700
    },
    {
      "epoch": 23.01098901098901,
      "grad_norm": 3.5123367309570312,
      "learning_rate": 3.561813186813187e-05,
      "loss": 0.5984,
      "step": 1465800
    },
    {
      "epoch": 23.012558869701728,
      "grad_norm": 3.7972235679626465,
      "learning_rate": 3.561715070643643e-05,
      "loss": 0.5959,
      "step": 1465900
    },
    {
      "epoch": 23.01412872841444,
      "grad_norm": 4.3339996337890625,
      "learning_rate": 3.561616954474097e-05,
      "loss": 0.6425,
      "step": 1466000
    },
    {
      "epoch": 23.015698587127158,
      "grad_norm": 3.809767961502075,
      "learning_rate": 3.561518838304553e-05,
      "loss": 0.5987,
      "step": 1466100
    },
    {
      "epoch": 23.017268445839875,
      "grad_norm": 3.677692413330078,
      "learning_rate": 3.561420722135008e-05,
      "loss": 0.6222,
      "step": 1466200
    },
    {
      "epoch": 23.01883830455259,
      "grad_norm": 4.353548049926758,
      "learning_rate": 3.561322605965463e-05,
      "loss": 0.6003,
      "step": 1466300
    },
    {
      "epoch": 23.020408163265305,
      "grad_norm": 3.8689801692962646,
      "learning_rate": 3.561224489795918e-05,
      "loss": 0.6041,
      "step": 1466400
    },
    {
      "epoch": 23.021978021978022,
      "grad_norm": 4.190208911895752,
      "learning_rate": 3.561126373626374e-05,
      "loss": 0.5838,
      "step": 1466500
    },
    {
      "epoch": 23.02354788069074,
      "grad_norm": 3.925036668777466,
      "learning_rate": 3.561028257456829e-05,
      "loss": 0.5906,
      "step": 1466600
    },
    {
      "epoch": 23.025117739403452,
      "grad_norm": 4.205423355102539,
      "learning_rate": 3.560930141287284e-05,
      "loss": 0.6065,
      "step": 1466700
    },
    {
      "epoch": 23.02668759811617,
      "grad_norm": 3.90291428565979,
      "learning_rate": 3.56083202511774e-05,
      "loss": 0.6018,
      "step": 1466800
    },
    {
      "epoch": 23.028257456828886,
      "grad_norm": 4.227380752563477,
      "learning_rate": 3.560733908948195e-05,
      "loss": 0.5481,
      "step": 1466900
    },
    {
      "epoch": 23.029827315541603,
      "grad_norm": 4.177367210388184,
      "learning_rate": 3.56063579277865e-05,
      "loss": 0.595,
      "step": 1467000
    },
    {
      "epoch": 23.031397174254316,
      "grad_norm": 3.5624608993530273,
      "learning_rate": 3.560537676609105e-05,
      "loss": 0.5845,
      "step": 1467100
    },
    {
      "epoch": 23.032967032967033,
      "grad_norm": 3.376455545425415,
      "learning_rate": 3.560439560439561e-05,
      "loss": 0.6099,
      "step": 1467200
    },
    {
      "epoch": 23.03453689167975,
      "grad_norm": 2.6688668727874756,
      "learning_rate": 3.560341444270016e-05,
      "loss": 0.597,
      "step": 1467300
    },
    {
      "epoch": 23.036106750392463,
      "grad_norm": 3.8652963638305664,
      "learning_rate": 3.560243328100471e-05,
      "loss": 0.6059,
      "step": 1467400
    },
    {
      "epoch": 23.03767660910518,
      "grad_norm": 3.6788535118103027,
      "learning_rate": 3.560145211930926e-05,
      "loss": 0.5913,
      "step": 1467500
    },
    {
      "epoch": 23.039246467817897,
      "grad_norm": 4.774184226989746,
      "learning_rate": 3.560047095761382e-05,
      "loss": 0.5822,
      "step": 1467600
    },
    {
      "epoch": 23.040816326530614,
      "grad_norm": 4.606465816497803,
      "learning_rate": 3.5599489795918365e-05,
      "loss": 0.5648,
      "step": 1467700
    },
    {
      "epoch": 23.042386185243327,
      "grad_norm": 3.8655598163604736,
      "learning_rate": 3.559850863422292e-05,
      "loss": 0.6305,
      "step": 1467800
    },
    {
      "epoch": 23.043956043956044,
      "grad_norm": 4.258331298828125,
      "learning_rate": 3.5597527472527473e-05,
      "loss": 0.6263,
      "step": 1467900
    },
    {
      "epoch": 23.04552590266876,
      "grad_norm": 4.240036964416504,
      "learning_rate": 3.5596546310832024e-05,
      "loss": 0.5902,
      "step": 1468000
    },
    {
      "epoch": 23.047095761381474,
      "grad_norm": 3.1315319538116455,
      "learning_rate": 3.5595565149136575e-05,
      "loss": 0.6335,
      "step": 1468100
    },
    {
      "epoch": 23.04866562009419,
      "grad_norm": 4.251150608062744,
      "learning_rate": 3.559458398744113e-05,
      "loss": 0.6002,
      "step": 1468200
    },
    {
      "epoch": 23.050235478806908,
      "grad_norm": 3.615487813949585,
      "learning_rate": 3.5593602825745684e-05,
      "loss": 0.5706,
      "step": 1468300
    },
    {
      "epoch": 23.051805337519625,
      "grad_norm": 4.306823253631592,
      "learning_rate": 3.5592621664050235e-05,
      "loss": 0.6243,
      "step": 1468400
    },
    {
      "epoch": 23.053375196232338,
      "grad_norm": 3.3551793098449707,
      "learning_rate": 3.5591640502354786e-05,
      "loss": 0.5965,
      "step": 1468500
    },
    {
      "epoch": 23.054945054945055,
      "grad_norm": 3.7783925533294678,
      "learning_rate": 3.5590659340659344e-05,
      "loss": 0.5971,
      "step": 1468600
    },
    {
      "epoch": 23.05651491365777,
      "grad_norm": 2.106266498565674,
      "learning_rate": 3.5589678178963895e-05,
      "loss": 0.5954,
      "step": 1468700
    },
    {
      "epoch": 23.058084772370485,
      "grad_norm": 3.7065656185150146,
      "learning_rate": 3.5588697017268446e-05,
      "loss": 0.5602,
      "step": 1468800
    },
    {
      "epoch": 23.059654631083202,
      "grad_norm": 3.664849281311035,
      "learning_rate": 3.5587715855573004e-05,
      "loss": 0.6143,
      "step": 1468900
    },
    {
      "epoch": 23.06122448979592,
      "grad_norm": 4.139069080352783,
      "learning_rate": 3.5586734693877555e-05,
      "loss": 0.6015,
      "step": 1469000
    },
    {
      "epoch": 23.062794348508636,
      "grad_norm": 2.131362199783325,
      "learning_rate": 3.5585753532182106e-05,
      "loss": 0.5894,
      "step": 1469100
    },
    {
      "epoch": 23.06436420722135,
      "grad_norm": 3.564601182937622,
      "learning_rate": 3.5584772370486657e-05,
      "loss": 0.5659,
      "step": 1469200
    },
    {
      "epoch": 23.065934065934066,
      "grad_norm": 3.579331874847412,
      "learning_rate": 3.5583791208791214e-05,
      "loss": 0.6076,
      "step": 1469300
    },
    {
      "epoch": 23.067503924646783,
      "grad_norm": 4.756244659423828,
      "learning_rate": 3.558281004709576e-05,
      "loss": 0.621,
      "step": 1469400
    },
    {
      "epoch": 23.069073783359496,
      "grad_norm": 5.178867816925049,
      "learning_rate": 3.5581828885400316e-05,
      "loss": 0.5808,
      "step": 1469500
    },
    {
      "epoch": 23.070643642072213,
      "grad_norm": 3.519211769104004,
      "learning_rate": 3.558084772370487e-05,
      "loss": 0.6057,
      "step": 1469600
    },
    {
      "epoch": 23.07221350078493,
      "grad_norm": 3.6708476543426514,
      "learning_rate": 3.5579866562009425e-05,
      "loss": 0.5819,
      "step": 1469700
    },
    {
      "epoch": 23.073783359497646,
      "grad_norm": 3.455557346343994,
      "learning_rate": 3.557888540031397e-05,
      "loss": 0.5944,
      "step": 1469800
    },
    {
      "epoch": 23.07535321821036,
      "grad_norm": 4.013444423675537,
      "learning_rate": 3.557790423861853e-05,
      "loss": 0.5818,
      "step": 1469900
    },
    {
      "epoch": 23.076923076923077,
      "grad_norm": 3.7917418479919434,
      "learning_rate": 3.557692307692308e-05,
      "loss": 0.625,
      "step": 1470000
    },
    {
      "epoch": 23.078492935635794,
      "grad_norm": 2.9668893814086914,
      "learning_rate": 3.557594191522763e-05,
      "loss": 0.6241,
      "step": 1470100
    },
    {
      "epoch": 23.08006279434851,
      "grad_norm": 2.258918046951294,
      "learning_rate": 3.557496075353218e-05,
      "loss": 0.6236,
      "step": 1470200
    },
    {
      "epoch": 23.081632653061224,
      "grad_norm": 4.277212619781494,
      "learning_rate": 3.557397959183674e-05,
      "loss": 0.6051,
      "step": 1470300
    },
    {
      "epoch": 23.08320251177394,
      "grad_norm": 4.424725532531738,
      "learning_rate": 3.557299843014129e-05,
      "loss": 0.5666,
      "step": 1470400
    },
    {
      "epoch": 23.084772370486657,
      "grad_norm": 3.1774091720581055,
      "learning_rate": 3.557201726844584e-05,
      "loss": 0.6251,
      "step": 1470500
    },
    {
      "epoch": 23.08634222919937,
      "grad_norm": 3.9472804069519043,
      "learning_rate": 3.557103610675039e-05,
      "loss": 0.584,
      "step": 1470600
    },
    {
      "epoch": 23.087912087912088,
      "grad_norm": 3.8764607906341553,
      "learning_rate": 3.557005494505495e-05,
      "loss": 0.5874,
      "step": 1470700
    },
    {
      "epoch": 23.089481946624804,
      "grad_norm": 3.7461206912994385,
      "learning_rate": 3.55690737833595e-05,
      "loss": 0.5655,
      "step": 1470800
    },
    {
      "epoch": 23.09105180533752,
      "grad_norm": 4.394400119781494,
      "learning_rate": 3.556809262166405e-05,
      "loss": 0.5857,
      "step": 1470900
    },
    {
      "epoch": 23.092621664050235,
      "grad_norm": 3.59796404838562,
      "learning_rate": 3.556711145996861e-05,
      "loss": 0.6682,
      "step": 1471000
    },
    {
      "epoch": 23.09419152276295,
      "grad_norm": 3.626823663711548,
      "learning_rate": 3.556613029827316e-05,
      "loss": 0.602,
      "step": 1471100
    },
    {
      "epoch": 23.09576138147567,
      "grad_norm": 4.257149696350098,
      "learning_rate": 3.556514913657771e-05,
      "loss": 0.6054,
      "step": 1471200
    },
    {
      "epoch": 23.09733124018838,
      "grad_norm": 3.954432964324951,
      "learning_rate": 3.556416797488226e-05,
      "loss": 0.5729,
      "step": 1471300
    },
    {
      "epoch": 23.0989010989011,
      "grad_norm": 3.287195920944214,
      "learning_rate": 3.556318681318682e-05,
      "loss": 0.5996,
      "step": 1471400
    },
    {
      "epoch": 23.100470957613815,
      "grad_norm": 4.580582618713379,
      "learning_rate": 3.556220565149136e-05,
      "loss": 0.5567,
      "step": 1471500
    },
    {
      "epoch": 23.102040816326532,
      "grad_norm": 3.552560806274414,
      "learning_rate": 3.556122448979592e-05,
      "loss": 0.5758,
      "step": 1471600
    },
    {
      "epoch": 23.103610675039246,
      "grad_norm": 3.2009072303771973,
      "learning_rate": 3.556024332810047e-05,
      "loss": 0.6209,
      "step": 1471700
    },
    {
      "epoch": 23.105180533751962,
      "grad_norm": 4.809474468231201,
      "learning_rate": 3.555926216640503e-05,
      "loss": 0.5985,
      "step": 1471800
    },
    {
      "epoch": 23.10675039246468,
      "grad_norm": 3.797761917114258,
      "learning_rate": 3.5558281004709574e-05,
      "loss": 0.6177,
      "step": 1471900
    },
    {
      "epoch": 23.108320251177393,
      "grad_norm": 2.9433753490448,
      "learning_rate": 3.555729984301413e-05,
      "loss": 0.5886,
      "step": 1472000
    },
    {
      "epoch": 23.10989010989011,
      "grad_norm": 3.1020421981811523,
      "learning_rate": 3.555631868131868e-05,
      "loss": 0.6205,
      "step": 1472100
    },
    {
      "epoch": 23.111459968602826,
      "grad_norm": 4.498732566833496,
      "learning_rate": 3.5555337519623233e-05,
      "loss": 0.6155,
      "step": 1472200
    },
    {
      "epoch": 23.113029827315543,
      "grad_norm": 3.860032081604004,
      "learning_rate": 3.5554356357927784e-05,
      "loss": 0.568,
      "step": 1472300
    },
    {
      "epoch": 23.114599686028257,
      "grad_norm": 3.977947235107422,
      "learning_rate": 3.555337519623234e-05,
      "loss": 0.6123,
      "step": 1472400
    },
    {
      "epoch": 23.116169544740973,
      "grad_norm": 3.7003238201141357,
      "learning_rate": 3.555239403453689e-05,
      "loss": 0.5923,
      "step": 1472500
    },
    {
      "epoch": 23.11773940345369,
      "grad_norm": 4.089439392089844,
      "learning_rate": 3.5551412872841444e-05,
      "loss": 0.6436,
      "step": 1472600
    },
    {
      "epoch": 23.119309262166404,
      "grad_norm": 4.984036922454834,
      "learning_rate": 3.5550431711145995e-05,
      "loss": 0.6137,
      "step": 1472700
    },
    {
      "epoch": 23.12087912087912,
      "grad_norm": 3.5665388107299805,
      "learning_rate": 3.554945054945055e-05,
      "loss": 0.5806,
      "step": 1472800
    },
    {
      "epoch": 23.122448979591837,
      "grad_norm": 3.560971975326538,
      "learning_rate": 3.5548469387755104e-05,
      "loss": 0.6078,
      "step": 1472900
    },
    {
      "epoch": 23.124018838304554,
      "grad_norm": 3.2095234394073486,
      "learning_rate": 3.5547488226059655e-05,
      "loss": 0.5975,
      "step": 1473000
    },
    {
      "epoch": 23.125588697017267,
      "grad_norm": 3.515671968460083,
      "learning_rate": 3.554650706436421e-05,
      "loss": 0.5915,
      "step": 1473100
    },
    {
      "epoch": 23.127158555729984,
      "grad_norm": 4.846256256103516,
      "learning_rate": 3.5545525902668764e-05,
      "loss": 0.5568,
      "step": 1473200
    },
    {
      "epoch": 23.1287284144427,
      "grad_norm": 4.467901706695557,
      "learning_rate": 3.5544544740973315e-05,
      "loss": 0.5769,
      "step": 1473300
    },
    {
      "epoch": 23.130298273155415,
      "grad_norm": 3.245502471923828,
      "learning_rate": 3.5543563579277866e-05,
      "loss": 0.5576,
      "step": 1473400
    },
    {
      "epoch": 23.13186813186813,
      "grad_norm": 4.432887554168701,
      "learning_rate": 3.554258241758242e-05,
      "loss": 0.617,
      "step": 1473500
    },
    {
      "epoch": 23.13343799058085,
      "grad_norm": 3.8566277027130127,
      "learning_rate": 3.554160125588697e-05,
      "loss": 0.6109,
      "step": 1473600
    },
    {
      "epoch": 23.135007849293565,
      "grad_norm": 3.404447555541992,
      "learning_rate": 3.5540620094191525e-05,
      "loss": 0.5646,
      "step": 1473700
    },
    {
      "epoch": 23.13657770800628,
      "grad_norm": 4.368338108062744,
      "learning_rate": 3.5539638932496076e-05,
      "loss": 0.5739,
      "step": 1473800
    },
    {
      "epoch": 23.138147566718995,
      "grad_norm": 3.6762962341308594,
      "learning_rate": 3.5538657770800634e-05,
      "loss": 0.609,
      "step": 1473900
    },
    {
      "epoch": 23.139717425431712,
      "grad_norm": 3.468700647354126,
      "learning_rate": 3.553767660910518e-05,
      "loss": 0.6117,
      "step": 1474000
    },
    {
      "epoch": 23.141287284144425,
      "grad_norm": 3.385308027267456,
      "learning_rate": 3.5536695447409736e-05,
      "loss": 0.5668,
      "step": 1474100
    },
    {
      "epoch": 23.142857142857142,
      "grad_norm": 3.63873028755188,
      "learning_rate": 3.553571428571429e-05,
      "loss": 0.6136,
      "step": 1474200
    },
    {
      "epoch": 23.14442700156986,
      "grad_norm": 4.340702056884766,
      "learning_rate": 3.553473312401884e-05,
      "loss": 0.6191,
      "step": 1474300
    },
    {
      "epoch": 23.145996860282576,
      "grad_norm": 4.823784828186035,
      "learning_rate": 3.553375196232339e-05,
      "loss": 0.6496,
      "step": 1474400
    },
    {
      "epoch": 23.14756671899529,
      "grad_norm": 3.106106996536255,
      "learning_rate": 3.553277080062795e-05,
      "loss": 0.6367,
      "step": 1474500
    },
    {
      "epoch": 23.149136577708006,
      "grad_norm": 5.776456832885742,
      "learning_rate": 3.55317896389325e-05,
      "loss": 0.6174,
      "step": 1474600
    },
    {
      "epoch": 23.150706436420723,
      "grad_norm": 4.207027435302734,
      "learning_rate": 3.553080847723705e-05,
      "loss": 0.5735,
      "step": 1474700
    },
    {
      "epoch": 23.152276295133436,
      "grad_norm": 4.307800769805908,
      "learning_rate": 3.55298273155416e-05,
      "loss": 0.6036,
      "step": 1474800
    },
    {
      "epoch": 23.153846153846153,
      "grad_norm": 2.479952096939087,
      "learning_rate": 3.552884615384616e-05,
      "loss": 0.61,
      "step": 1474900
    },
    {
      "epoch": 23.15541601255887,
      "grad_norm": 3.2565178871154785,
      "learning_rate": 3.552786499215071e-05,
      "loss": 0.5933,
      "step": 1475000
    },
    {
      "epoch": 23.156985871271587,
      "grad_norm": 4.9420671463012695,
      "learning_rate": 3.552688383045526e-05,
      "loss": 0.6135,
      "step": 1475100
    },
    {
      "epoch": 23.1585557299843,
      "grad_norm": 4.842683792114258,
      "learning_rate": 3.552590266875982e-05,
      "loss": 0.598,
      "step": 1475200
    },
    {
      "epoch": 23.160125588697017,
      "grad_norm": 3.6756105422973633,
      "learning_rate": 3.552492150706437e-05,
      "loss": 0.5969,
      "step": 1475300
    },
    {
      "epoch": 23.161695447409734,
      "grad_norm": 4.022711277008057,
      "learning_rate": 3.552394034536892e-05,
      "loss": 0.6129,
      "step": 1475400
    },
    {
      "epoch": 23.163265306122447,
      "grad_norm": 4.2476606369018555,
      "learning_rate": 3.552295918367347e-05,
      "loss": 0.5819,
      "step": 1475500
    },
    {
      "epoch": 23.164835164835164,
      "grad_norm": 3.649872303009033,
      "learning_rate": 3.552197802197803e-05,
      "loss": 0.5883,
      "step": 1475600
    },
    {
      "epoch": 23.16640502354788,
      "grad_norm": 3.375171422958374,
      "learning_rate": 3.552099686028257e-05,
      "loss": 0.5892,
      "step": 1475700
    },
    {
      "epoch": 23.167974882260598,
      "grad_norm": 3.249793767929077,
      "learning_rate": 3.552001569858713e-05,
      "loss": 0.6029,
      "step": 1475800
    },
    {
      "epoch": 23.16954474097331,
      "grad_norm": 3.8114986419677734,
      "learning_rate": 3.551903453689168e-05,
      "loss": 0.6019,
      "step": 1475900
    },
    {
      "epoch": 23.171114599686028,
      "grad_norm": 3.954179048538208,
      "learning_rate": 3.551805337519624e-05,
      "loss": 0.594,
      "step": 1476000
    },
    {
      "epoch": 23.172684458398745,
      "grad_norm": 3.479578733444214,
      "learning_rate": 3.551707221350078e-05,
      "loss": 0.6119,
      "step": 1476100
    },
    {
      "epoch": 23.17425431711146,
      "grad_norm": 3.302947998046875,
      "learning_rate": 3.551609105180534e-05,
      "loss": 0.5895,
      "step": 1476200
    },
    {
      "epoch": 23.175824175824175,
      "grad_norm": 3.4841487407684326,
      "learning_rate": 3.551510989010989e-05,
      "loss": 0.5765,
      "step": 1476300
    },
    {
      "epoch": 23.177394034536892,
      "grad_norm": 3.428654432296753,
      "learning_rate": 3.551412872841444e-05,
      "loss": 0.5985,
      "step": 1476400
    },
    {
      "epoch": 23.17896389324961,
      "grad_norm": 4.855355739593506,
      "learning_rate": 3.5513147566718993e-05,
      "loss": 0.5958,
      "step": 1476500
    },
    {
      "epoch": 23.180533751962322,
      "grad_norm": 3.592510223388672,
      "learning_rate": 3.551216640502355e-05,
      "loss": 0.5914,
      "step": 1476600
    },
    {
      "epoch": 23.18210361067504,
      "grad_norm": 3.643571615219116,
      "learning_rate": 3.55111852433281e-05,
      "loss": 0.6103,
      "step": 1476700
    },
    {
      "epoch": 23.183673469387756,
      "grad_norm": 3.1144535541534424,
      "learning_rate": 3.551020408163265e-05,
      "loss": 0.6103,
      "step": 1476800
    },
    {
      "epoch": 23.18524332810047,
      "grad_norm": 2.708531618118286,
      "learning_rate": 3.5509222919937204e-05,
      "loss": 0.6022,
      "step": 1476900
    },
    {
      "epoch": 23.186813186813186,
      "grad_norm": 3.6641852855682373,
      "learning_rate": 3.550824175824176e-05,
      "loss": 0.6154,
      "step": 1477000
    },
    {
      "epoch": 23.188383045525903,
      "grad_norm": 3.4453933238983154,
      "learning_rate": 3.550726059654631e-05,
      "loss": 0.594,
      "step": 1477100
    },
    {
      "epoch": 23.18995290423862,
      "grad_norm": 4.38827657699585,
      "learning_rate": 3.5506279434850864e-05,
      "loss": 0.6354,
      "step": 1477200
    },
    {
      "epoch": 23.191522762951333,
      "grad_norm": 4.365200042724609,
      "learning_rate": 3.550529827315542e-05,
      "loss": 0.6042,
      "step": 1477300
    },
    {
      "epoch": 23.19309262166405,
      "grad_norm": 4.5416717529296875,
      "learning_rate": 3.550431711145997e-05,
      "loss": 0.5963,
      "step": 1477400
    },
    {
      "epoch": 23.194662480376767,
      "grad_norm": 4.089026927947998,
      "learning_rate": 3.5503335949764524e-05,
      "loss": 0.5881,
      "step": 1477500
    },
    {
      "epoch": 23.19623233908948,
      "grad_norm": 3.836747884750366,
      "learning_rate": 3.5502354788069075e-05,
      "loss": 0.6095,
      "step": 1477600
    },
    {
      "epoch": 23.197802197802197,
      "grad_norm": 2.905212640762329,
      "learning_rate": 3.550137362637363e-05,
      "loss": 0.6083,
      "step": 1477700
    },
    {
      "epoch": 23.199372056514914,
      "grad_norm": 4.509418487548828,
      "learning_rate": 3.5500392464678176e-05,
      "loss": 0.6154,
      "step": 1477800
    },
    {
      "epoch": 23.20094191522763,
      "grad_norm": 2.7859859466552734,
      "learning_rate": 3.5499411302982734e-05,
      "loss": 0.5763,
      "step": 1477900
    },
    {
      "epoch": 23.202511773940344,
      "grad_norm": 3.99983811378479,
      "learning_rate": 3.5498430141287285e-05,
      "loss": 0.5901,
      "step": 1478000
    },
    {
      "epoch": 23.20408163265306,
      "grad_norm": 3.7485766410827637,
      "learning_rate": 3.549744897959184e-05,
      "loss": 0.5697,
      "step": 1478100
    },
    {
      "epoch": 23.205651491365778,
      "grad_norm": 4.492529392242432,
      "learning_rate": 3.549646781789639e-05,
      "loss": 0.5841,
      "step": 1478200
    },
    {
      "epoch": 23.20722135007849,
      "grad_norm": 3.774024486541748,
      "learning_rate": 3.5495486656200945e-05,
      "loss": 0.5662,
      "step": 1478300
    },
    {
      "epoch": 23.208791208791208,
      "grad_norm": 3.58139705657959,
      "learning_rate": 3.5494505494505496e-05,
      "loss": 0.604,
      "step": 1478400
    },
    {
      "epoch": 23.210361067503925,
      "grad_norm": 3.455979108810425,
      "learning_rate": 3.549352433281005e-05,
      "loss": 0.6,
      "step": 1478500
    },
    {
      "epoch": 23.211930926216642,
      "grad_norm": 2.9182207584381104,
      "learning_rate": 3.54925431711146e-05,
      "loss": 0.5845,
      "step": 1478600
    },
    {
      "epoch": 23.213500784929355,
      "grad_norm": 1.7256048917770386,
      "learning_rate": 3.5491562009419156e-05,
      "loss": 0.6143,
      "step": 1478700
    },
    {
      "epoch": 23.215070643642072,
      "grad_norm": 3.854231119155884,
      "learning_rate": 3.549058084772371e-05,
      "loss": 0.6225,
      "step": 1478800
    },
    {
      "epoch": 23.21664050235479,
      "grad_norm": 4.315006732940674,
      "learning_rate": 3.548959968602826e-05,
      "loss": 0.5971,
      "step": 1478900
    },
    {
      "epoch": 23.218210361067506,
      "grad_norm": 4.607398509979248,
      "learning_rate": 3.548861852433281e-05,
      "loss": 0.6222,
      "step": 1479000
    },
    {
      "epoch": 23.21978021978022,
      "grad_norm": 3.4416134357452393,
      "learning_rate": 3.5487637362637366e-05,
      "loss": 0.5811,
      "step": 1479100
    },
    {
      "epoch": 23.221350078492936,
      "grad_norm": 5.4525065422058105,
      "learning_rate": 3.548665620094192e-05,
      "loss": 0.6269,
      "step": 1479200
    },
    {
      "epoch": 23.222919937205653,
      "grad_norm": 3.6116232872009277,
      "learning_rate": 3.548567503924647e-05,
      "loss": 0.6418,
      "step": 1479300
    },
    {
      "epoch": 23.224489795918366,
      "grad_norm": 3.7600975036621094,
      "learning_rate": 3.5484693877551026e-05,
      "loss": 0.5929,
      "step": 1479400
    },
    {
      "epoch": 23.226059654631083,
      "grad_norm": 3.1881890296936035,
      "learning_rate": 3.548371271585558e-05,
      "loss": 0.5945,
      "step": 1479500
    },
    {
      "epoch": 23.2276295133438,
      "grad_norm": 4.6566267013549805,
      "learning_rate": 3.548273155416013e-05,
      "loss": 0.6249,
      "step": 1479600
    },
    {
      "epoch": 23.229199372056517,
      "grad_norm": 4.504976272583008,
      "learning_rate": 3.548175039246468e-05,
      "loss": 0.5608,
      "step": 1479700
    },
    {
      "epoch": 23.23076923076923,
      "grad_norm": 4.551820755004883,
      "learning_rate": 3.548076923076924e-05,
      "loss": 0.5888,
      "step": 1479800
    },
    {
      "epoch": 23.232339089481947,
      "grad_norm": 4.60300350189209,
      "learning_rate": 3.547978806907378e-05,
      "loss": 0.608,
      "step": 1479900
    },
    {
      "epoch": 23.233908948194664,
      "grad_norm": 3.9689829349517822,
      "learning_rate": 3.547880690737834e-05,
      "loss": 0.6255,
      "step": 1480000
    },
    {
      "epoch": 23.235478806907377,
      "grad_norm": 3.030466079711914,
      "learning_rate": 3.547782574568289e-05,
      "loss": 0.5802,
      "step": 1480100
    },
    {
      "epoch": 23.237048665620094,
      "grad_norm": 3.998872756958008,
      "learning_rate": 3.547684458398745e-05,
      "loss": 0.6019,
      "step": 1480200
    },
    {
      "epoch": 23.23861852433281,
      "grad_norm": 3.190622091293335,
      "learning_rate": 3.547586342229199e-05,
      "loss": 0.6052,
      "step": 1480300
    },
    {
      "epoch": 23.240188383045528,
      "grad_norm": 4.313526153564453,
      "learning_rate": 3.547488226059655e-05,
      "loss": 0.5986,
      "step": 1480400
    },
    {
      "epoch": 23.24175824175824,
      "grad_norm": 4.087623119354248,
      "learning_rate": 3.54739010989011e-05,
      "loss": 0.5959,
      "step": 1480500
    },
    {
      "epoch": 23.243328100470958,
      "grad_norm": 4.349498748779297,
      "learning_rate": 3.547291993720565e-05,
      "loss": 0.5939,
      "step": 1480600
    },
    {
      "epoch": 23.244897959183675,
      "grad_norm": 5.061687469482422,
      "learning_rate": 3.54719387755102e-05,
      "loss": 0.5896,
      "step": 1480700
    },
    {
      "epoch": 23.246467817896388,
      "grad_norm": 4.977355480194092,
      "learning_rate": 3.547095761381476e-05,
      "loss": 0.5879,
      "step": 1480800
    },
    {
      "epoch": 23.248037676609105,
      "grad_norm": 4.7397589683532715,
      "learning_rate": 3.546997645211931e-05,
      "loss": 0.6261,
      "step": 1480900
    },
    {
      "epoch": 23.24960753532182,
      "grad_norm": 3.7844150066375732,
      "learning_rate": 3.546899529042386e-05,
      "loss": 0.5915,
      "step": 1481000
    },
    {
      "epoch": 23.25117739403454,
      "grad_norm": 4.230565071105957,
      "learning_rate": 3.546801412872841e-05,
      "loss": 0.5752,
      "step": 1481100
    },
    {
      "epoch": 23.252747252747252,
      "grad_norm": 4.021892070770264,
      "learning_rate": 3.546703296703297e-05,
      "loss": 0.5571,
      "step": 1481200
    },
    {
      "epoch": 23.25431711145997,
      "grad_norm": 3.830124855041504,
      "learning_rate": 3.546605180533752e-05,
      "loss": 0.5873,
      "step": 1481300
    },
    {
      "epoch": 23.255886970172686,
      "grad_norm": 3.5499794483184814,
      "learning_rate": 3.546507064364207e-05,
      "loss": 0.6264,
      "step": 1481400
    },
    {
      "epoch": 23.2574568288854,
      "grad_norm": 5.137767314910889,
      "learning_rate": 3.546408948194663e-05,
      "loss": 0.6148,
      "step": 1481500
    },
    {
      "epoch": 23.259026687598116,
      "grad_norm": 4.458335876464844,
      "learning_rate": 3.546310832025118e-05,
      "loss": 0.5561,
      "step": 1481600
    },
    {
      "epoch": 23.260596546310833,
      "grad_norm": 3.0019099712371826,
      "learning_rate": 3.546212715855573e-05,
      "loss": 0.5821,
      "step": 1481700
    },
    {
      "epoch": 23.26216640502355,
      "grad_norm": 3.591087818145752,
      "learning_rate": 3.5461145996860284e-05,
      "loss": 0.6062,
      "step": 1481800
    },
    {
      "epoch": 23.263736263736263,
      "grad_norm": 5.707192897796631,
      "learning_rate": 3.546016483516484e-05,
      "loss": 0.6388,
      "step": 1481900
    },
    {
      "epoch": 23.26530612244898,
      "grad_norm": 2.172839403152466,
      "learning_rate": 3.5459183673469385e-05,
      "loss": 0.6064,
      "step": 1482000
    },
    {
      "epoch": 23.266875981161697,
      "grad_norm": 3.63250994682312,
      "learning_rate": 3.545820251177394e-05,
      "loss": 0.5923,
      "step": 1482100
    },
    {
      "epoch": 23.26844583987441,
      "grad_norm": 3.9658257961273193,
      "learning_rate": 3.5457221350078494e-05,
      "loss": 0.5656,
      "step": 1482200
    },
    {
      "epoch": 23.270015698587127,
      "grad_norm": 3.500511884689331,
      "learning_rate": 3.545624018838305e-05,
      "loss": 0.5989,
      "step": 1482300
    },
    {
      "epoch": 23.271585557299844,
      "grad_norm": 4.136331558227539,
      "learning_rate": 3.5455259026687596e-05,
      "loss": 0.5723,
      "step": 1482400
    },
    {
      "epoch": 23.27315541601256,
      "grad_norm": 4.538179874420166,
      "learning_rate": 3.5454277864992154e-05,
      "loss": 0.6119,
      "step": 1482500
    },
    {
      "epoch": 23.274725274725274,
      "grad_norm": 3.6094601154327393,
      "learning_rate": 3.5453296703296705e-05,
      "loss": 0.6072,
      "step": 1482600
    },
    {
      "epoch": 23.27629513343799,
      "grad_norm": 3.830418348312378,
      "learning_rate": 3.5452315541601256e-05,
      "loss": 0.5855,
      "step": 1482700
    },
    {
      "epoch": 23.277864992150707,
      "grad_norm": 4.321265697479248,
      "learning_rate": 3.545133437990581e-05,
      "loss": 0.5741,
      "step": 1482800
    },
    {
      "epoch": 23.27943485086342,
      "grad_norm": 4.273164749145508,
      "learning_rate": 3.5450353218210365e-05,
      "loss": 0.6259,
      "step": 1482900
    },
    {
      "epoch": 23.281004709576138,
      "grad_norm": 4.211009502410889,
      "learning_rate": 3.5449372056514916e-05,
      "loss": 0.5996,
      "step": 1483000
    },
    {
      "epoch": 23.282574568288855,
      "grad_norm": 3.690222978591919,
      "learning_rate": 3.5448390894819467e-05,
      "loss": 0.5986,
      "step": 1483100
    },
    {
      "epoch": 23.28414442700157,
      "grad_norm": 3.835620880126953,
      "learning_rate": 3.544740973312402e-05,
      "loss": 0.569,
      "step": 1483200
    },
    {
      "epoch": 23.285714285714285,
      "grad_norm": 4.240279674530029,
      "learning_rate": 3.5446428571428575e-05,
      "loss": 0.6095,
      "step": 1483300
    },
    {
      "epoch": 23.287284144427,
      "grad_norm": 4.753834247589111,
      "learning_rate": 3.5445447409733126e-05,
      "loss": 0.6022,
      "step": 1483400
    },
    {
      "epoch": 23.28885400313972,
      "grad_norm": 3.2397541999816895,
      "learning_rate": 3.544446624803768e-05,
      "loss": 0.6206,
      "step": 1483500
    },
    {
      "epoch": 23.29042386185243,
      "grad_norm": 3.875091552734375,
      "learning_rate": 3.5443485086342235e-05,
      "loss": 0.5933,
      "step": 1483600
    },
    {
      "epoch": 23.29199372056515,
      "grad_norm": 4.028794765472412,
      "learning_rate": 3.5442503924646786e-05,
      "loss": 0.6119,
      "step": 1483700
    },
    {
      "epoch": 23.293563579277865,
      "grad_norm": 4.806656360626221,
      "learning_rate": 3.544152276295134e-05,
      "loss": 0.6222,
      "step": 1483800
    },
    {
      "epoch": 23.295133437990582,
      "grad_norm": 3.9332170486450195,
      "learning_rate": 3.544054160125589e-05,
      "loss": 0.6313,
      "step": 1483900
    },
    {
      "epoch": 23.296703296703296,
      "grad_norm": 4.560600280761719,
      "learning_rate": 3.5439560439560446e-05,
      "loss": 0.6104,
      "step": 1484000
    },
    {
      "epoch": 23.298273155416013,
      "grad_norm": 4.494727611541748,
      "learning_rate": 3.543857927786499e-05,
      "loss": 0.5982,
      "step": 1484100
    },
    {
      "epoch": 23.29984301412873,
      "grad_norm": 4.361104965209961,
      "learning_rate": 3.543759811616955e-05,
      "loss": 0.5992,
      "step": 1484200
    },
    {
      "epoch": 23.301412872841443,
      "grad_norm": 4.082643985748291,
      "learning_rate": 3.54366169544741e-05,
      "loss": 0.6147,
      "step": 1484300
    },
    {
      "epoch": 23.30298273155416,
      "grad_norm": 4.311190128326416,
      "learning_rate": 3.5435635792778656e-05,
      "loss": 0.6073,
      "step": 1484400
    },
    {
      "epoch": 23.304552590266876,
      "grad_norm": 2.603020668029785,
      "learning_rate": 3.54346546310832e-05,
      "loss": 0.5764,
      "step": 1484500
    },
    {
      "epoch": 23.306122448979593,
      "grad_norm": 2.7905328273773193,
      "learning_rate": 3.543367346938776e-05,
      "loss": 0.597,
      "step": 1484600
    },
    {
      "epoch": 23.307692307692307,
      "grad_norm": 3.183150291442871,
      "learning_rate": 3.543269230769231e-05,
      "loss": 0.602,
      "step": 1484700
    },
    {
      "epoch": 23.309262166405023,
      "grad_norm": 4.160304069519043,
      "learning_rate": 3.543171114599686e-05,
      "loss": 0.5844,
      "step": 1484800
    },
    {
      "epoch": 23.31083202511774,
      "grad_norm": 4.6695733070373535,
      "learning_rate": 3.543072998430141e-05,
      "loss": 0.6123,
      "step": 1484900
    },
    {
      "epoch": 23.312401883830454,
      "grad_norm": 4.256138324737549,
      "learning_rate": 3.542974882260597e-05,
      "loss": 0.6081,
      "step": 1485000
    },
    {
      "epoch": 23.31397174254317,
      "grad_norm": 3.558727264404297,
      "learning_rate": 3.542876766091052e-05,
      "loss": 0.6064,
      "step": 1485100
    },
    {
      "epoch": 23.315541601255887,
      "grad_norm": 4.2730889320373535,
      "learning_rate": 3.542778649921507e-05,
      "loss": 0.6313,
      "step": 1485200
    },
    {
      "epoch": 23.317111459968604,
      "grad_norm": 3.7116103172302246,
      "learning_rate": 3.542680533751962e-05,
      "loss": 0.571,
      "step": 1485300
    },
    {
      "epoch": 23.318681318681318,
      "grad_norm": 2.982125759124756,
      "learning_rate": 3.542582417582418e-05,
      "loss": 0.5794,
      "step": 1485400
    },
    {
      "epoch": 23.320251177394034,
      "grad_norm": 4.185611248016357,
      "learning_rate": 3.542484301412873e-05,
      "loss": 0.5897,
      "step": 1485500
    },
    {
      "epoch": 23.32182103610675,
      "grad_norm": 4.417385578155518,
      "learning_rate": 3.542386185243328e-05,
      "loss": 0.6185,
      "step": 1485600
    },
    {
      "epoch": 23.323390894819465,
      "grad_norm": 3.5684165954589844,
      "learning_rate": 3.542288069073784e-05,
      "loss": 0.5512,
      "step": 1485700
    },
    {
      "epoch": 23.32496075353218,
      "grad_norm": 3.741121768951416,
      "learning_rate": 3.542189952904239e-05,
      "loss": 0.6054,
      "step": 1485800
    },
    {
      "epoch": 23.3265306122449,
      "grad_norm": 3.731645107269287,
      "learning_rate": 3.542091836734694e-05,
      "loss": 0.599,
      "step": 1485900
    },
    {
      "epoch": 23.328100470957615,
      "grad_norm": 2.9316070079803467,
      "learning_rate": 3.541993720565149e-05,
      "loss": 0.6163,
      "step": 1486000
    },
    {
      "epoch": 23.32967032967033,
      "grad_norm": 3.3218882083892822,
      "learning_rate": 3.541895604395605e-05,
      "loss": 0.5748,
      "step": 1486100
    },
    {
      "epoch": 23.331240188383045,
      "grad_norm": 4.387325763702393,
      "learning_rate": 3.5417974882260594e-05,
      "loss": 0.6131,
      "step": 1486200
    },
    {
      "epoch": 23.332810047095762,
      "grad_norm": 3.301225185394287,
      "learning_rate": 3.541699372056515e-05,
      "loss": 0.5975,
      "step": 1486300
    },
    {
      "epoch": 23.334379905808476,
      "grad_norm": 3.806344509124756,
      "learning_rate": 3.54160125588697e-05,
      "loss": 0.6349,
      "step": 1486400
    },
    {
      "epoch": 23.335949764521192,
      "grad_norm": 5.099586486816406,
      "learning_rate": 3.541503139717426e-05,
      "loss": 0.6346,
      "step": 1486500
    },
    {
      "epoch": 23.33751962323391,
      "grad_norm": 3.1550097465515137,
      "learning_rate": 3.5414050235478805e-05,
      "loss": 0.6252,
      "step": 1486600
    },
    {
      "epoch": 23.339089481946626,
      "grad_norm": 4.223626136779785,
      "learning_rate": 3.541306907378336e-05,
      "loss": 0.6021,
      "step": 1486700
    },
    {
      "epoch": 23.34065934065934,
      "grad_norm": 4.075343608856201,
      "learning_rate": 3.5412087912087914e-05,
      "loss": 0.6021,
      "step": 1486800
    },
    {
      "epoch": 23.342229199372056,
      "grad_norm": 4.003810882568359,
      "learning_rate": 3.5411106750392465e-05,
      "loss": 0.5711,
      "step": 1486900
    },
    {
      "epoch": 23.343799058084773,
      "grad_norm": 4.755780220031738,
      "learning_rate": 3.5410125588697016e-05,
      "loss": 0.5647,
      "step": 1487000
    },
    {
      "epoch": 23.345368916797486,
      "grad_norm": 4.524943828582764,
      "learning_rate": 3.5409144427001574e-05,
      "loss": 0.5905,
      "step": 1487100
    },
    {
      "epoch": 23.346938775510203,
      "grad_norm": 4.550577163696289,
      "learning_rate": 3.5408163265306125e-05,
      "loss": 0.6055,
      "step": 1487200
    },
    {
      "epoch": 23.34850863422292,
      "grad_norm": 4.479662895202637,
      "learning_rate": 3.5407182103610676e-05,
      "loss": 0.6091,
      "step": 1487300
    },
    {
      "epoch": 23.350078492935637,
      "grad_norm": 2.975008487701416,
      "learning_rate": 3.5406200941915227e-05,
      "loss": 0.58,
      "step": 1487400
    },
    {
      "epoch": 23.35164835164835,
      "grad_norm": 4.150959491729736,
      "learning_rate": 3.5405219780219784e-05,
      "loss": 0.5951,
      "step": 1487500
    },
    {
      "epoch": 23.353218210361067,
      "grad_norm": 4.02985143661499,
      "learning_rate": 3.5404238618524335e-05,
      "loss": 0.5992,
      "step": 1487600
    },
    {
      "epoch": 23.354788069073784,
      "grad_norm": 4.111391067504883,
      "learning_rate": 3.5403257456828886e-05,
      "loss": 0.5682,
      "step": 1487700
    },
    {
      "epoch": 23.356357927786497,
      "grad_norm": 4.344870567321777,
      "learning_rate": 3.5402276295133444e-05,
      "loss": 0.5834,
      "step": 1487800
    },
    {
      "epoch": 23.357927786499214,
      "grad_norm": 2.9513394832611084,
      "learning_rate": 3.5401295133437995e-05,
      "loss": 0.6197,
      "step": 1487900
    },
    {
      "epoch": 23.35949764521193,
      "grad_norm": 4.800144195556641,
      "learning_rate": 3.5400313971742546e-05,
      "loss": 0.5633,
      "step": 1488000
    },
    {
      "epoch": 23.361067503924648,
      "grad_norm": 3.2371485233306885,
      "learning_rate": 3.53993328100471e-05,
      "loss": 0.6122,
      "step": 1488100
    },
    {
      "epoch": 23.36263736263736,
      "grad_norm": 3.4544780254364014,
      "learning_rate": 3.5398351648351655e-05,
      "loss": 0.6123,
      "step": 1488200
    },
    {
      "epoch": 23.364207221350078,
      "grad_norm": 4.626296520233154,
      "learning_rate": 3.53973704866562e-05,
      "loss": 0.5641,
      "step": 1488300
    },
    {
      "epoch": 23.365777080062795,
      "grad_norm": 4.415689468383789,
      "learning_rate": 3.539638932496076e-05,
      "loss": 0.5824,
      "step": 1488400
    },
    {
      "epoch": 23.367346938775512,
      "grad_norm": 4.12349271774292,
      "learning_rate": 3.539540816326531e-05,
      "loss": 0.5821,
      "step": 1488500
    },
    {
      "epoch": 23.368916797488225,
      "grad_norm": 2.2915737628936768,
      "learning_rate": 3.5394427001569865e-05,
      "loss": 0.6016,
      "step": 1488600
    },
    {
      "epoch": 23.370486656200942,
      "grad_norm": 3.5023863315582275,
      "learning_rate": 3.539344583987441e-05,
      "loss": 0.58,
      "step": 1488700
    },
    {
      "epoch": 23.37205651491366,
      "grad_norm": 4.862963676452637,
      "learning_rate": 3.539246467817897e-05,
      "loss": 0.6339,
      "step": 1488800
    },
    {
      "epoch": 23.373626373626372,
      "grad_norm": 4.429329872131348,
      "learning_rate": 3.539148351648352e-05,
      "loss": 0.6049,
      "step": 1488900
    },
    {
      "epoch": 23.37519623233909,
      "grad_norm": 2.5138306617736816,
      "learning_rate": 3.539050235478807e-05,
      "loss": 0.6191,
      "step": 1489000
    },
    {
      "epoch": 23.376766091051806,
      "grad_norm": 3.899632692337036,
      "learning_rate": 3.538952119309262e-05,
      "loss": 0.57,
      "step": 1489100
    },
    {
      "epoch": 23.378335949764523,
      "grad_norm": 4.615126609802246,
      "learning_rate": 3.538854003139718e-05,
      "loss": 0.5991,
      "step": 1489200
    },
    {
      "epoch": 23.379905808477236,
      "grad_norm": 3.8862059116363525,
      "learning_rate": 3.538755886970173e-05,
      "loss": 0.5787,
      "step": 1489300
    },
    {
      "epoch": 23.381475667189953,
      "grad_norm": 4.322640419006348,
      "learning_rate": 3.538657770800628e-05,
      "loss": 0.5959,
      "step": 1489400
    },
    {
      "epoch": 23.38304552590267,
      "grad_norm": 2.620635986328125,
      "learning_rate": 3.538559654631083e-05,
      "loss": 0.5897,
      "step": 1489500
    },
    {
      "epoch": 23.384615384615383,
      "grad_norm": 3.8271048069000244,
      "learning_rate": 3.538461538461539e-05,
      "loss": 0.5991,
      "step": 1489600
    },
    {
      "epoch": 23.3861852433281,
      "grad_norm": 3.8653647899627686,
      "learning_rate": 3.538363422291994e-05,
      "loss": 0.6171,
      "step": 1489700
    },
    {
      "epoch": 23.387755102040817,
      "grad_norm": 3.3404483795166016,
      "learning_rate": 3.538265306122449e-05,
      "loss": 0.6324,
      "step": 1489800
    },
    {
      "epoch": 23.389324960753534,
      "grad_norm": 4.0481343269348145,
      "learning_rate": 3.538167189952905e-05,
      "loss": 0.5858,
      "step": 1489900
    },
    {
      "epoch": 23.390894819466247,
      "grad_norm": 3.705540180206299,
      "learning_rate": 3.53806907378336e-05,
      "loss": 0.6222,
      "step": 1490000
    },
    {
      "epoch": 23.392464678178964,
      "grad_norm": 2.728848457336426,
      "learning_rate": 3.537970957613815e-05,
      "loss": 0.5802,
      "step": 1490100
    },
    {
      "epoch": 23.39403453689168,
      "grad_norm": 5.132652282714844,
      "learning_rate": 3.53787284144427e-05,
      "loss": 0.5998,
      "step": 1490200
    },
    {
      "epoch": 23.395604395604394,
      "grad_norm": 6.627206802368164,
      "learning_rate": 3.537774725274726e-05,
      "loss": 0.5288,
      "step": 1490300
    },
    {
      "epoch": 23.39717425431711,
      "grad_norm": 4.321118354797363,
      "learning_rate": 3.5376766091051803e-05,
      "loss": 0.6012,
      "step": 1490400
    },
    {
      "epoch": 23.398744113029828,
      "grad_norm": 4.164492130279541,
      "learning_rate": 3.537578492935636e-05,
      "loss": 0.5756,
      "step": 1490500
    },
    {
      "epoch": 23.400313971742545,
      "grad_norm": 2.7184910774230957,
      "learning_rate": 3.537480376766091e-05,
      "loss": 0.6029,
      "step": 1490600
    },
    {
      "epoch": 23.401883830455258,
      "grad_norm": 3.7491660118103027,
      "learning_rate": 3.537382260596546e-05,
      "loss": 0.6009,
      "step": 1490700
    },
    {
      "epoch": 23.403453689167975,
      "grad_norm": 2.654371500015259,
      "learning_rate": 3.5372841444270014e-05,
      "loss": 0.5882,
      "step": 1490800
    },
    {
      "epoch": 23.405023547880692,
      "grad_norm": 3.136875867843628,
      "learning_rate": 3.537186028257457e-05,
      "loss": 0.5829,
      "step": 1490900
    },
    {
      "epoch": 23.406593406593405,
      "grad_norm": 3.910276174545288,
      "learning_rate": 3.537087912087912e-05,
      "loss": 0.5772,
      "step": 1491000
    },
    {
      "epoch": 23.408163265306122,
      "grad_norm": 5.4023356437683105,
      "learning_rate": 3.5369897959183674e-05,
      "loss": 0.6327,
      "step": 1491100
    },
    {
      "epoch": 23.40973312401884,
      "grad_norm": 3.3855843544006348,
      "learning_rate": 3.5368916797488225e-05,
      "loss": 0.5741,
      "step": 1491200
    },
    {
      "epoch": 23.411302982731556,
      "grad_norm": 2.7318599224090576,
      "learning_rate": 3.536793563579278e-05,
      "loss": 0.5793,
      "step": 1491300
    },
    {
      "epoch": 23.41287284144427,
      "grad_norm": 4.621216297149658,
      "learning_rate": 3.5366954474097334e-05,
      "loss": 0.6377,
      "step": 1491400
    },
    {
      "epoch": 23.414442700156986,
      "grad_norm": 3.39680814743042,
      "learning_rate": 3.5365973312401885e-05,
      "loss": 0.5996,
      "step": 1491500
    },
    {
      "epoch": 23.416012558869703,
      "grad_norm": 4.462893009185791,
      "learning_rate": 3.5364992150706436e-05,
      "loss": 0.5934,
      "step": 1491600
    },
    {
      "epoch": 23.417582417582416,
      "grad_norm": 4.0605788230896,
      "learning_rate": 3.536401098901099e-05,
      "loss": 0.5862,
      "step": 1491700
    },
    {
      "epoch": 23.419152276295133,
      "grad_norm": 4.822083473205566,
      "learning_rate": 3.5363029827315544e-05,
      "loss": 0.5815,
      "step": 1491800
    },
    {
      "epoch": 23.42072213500785,
      "grad_norm": 4.592281341552734,
      "learning_rate": 3.5362048665620095e-05,
      "loss": 0.596,
      "step": 1491900
    },
    {
      "epoch": 23.422291993720567,
      "grad_norm": 3.1280319690704346,
      "learning_rate": 3.536106750392465e-05,
      "loss": 0.6381,
      "step": 1492000
    },
    {
      "epoch": 23.42386185243328,
      "grad_norm": 3.3230583667755127,
      "learning_rate": 3.53600863422292e-05,
      "loss": 0.5916,
      "step": 1492100
    },
    {
      "epoch": 23.425431711145997,
      "grad_norm": 3.403212070465088,
      "learning_rate": 3.5359105180533755e-05,
      "loss": 0.6186,
      "step": 1492200
    },
    {
      "epoch": 23.427001569858714,
      "grad_norm": 3.339306354522705,
      "learning_rate": 3.5358124018838306e-05,
      "loss": 0.6307,
      "step": 1492300
    },
    {
      "epoch": 23.428571428571427,
      "grad_norm": 4.135434150695801,
      "learning_rate": 3.5357142857142864e-05,
      "loss": 0.615,
      "step": 1492400
    },
    {
      "epoch": 23.430141287284144,
      "grad_norm": 3.2705788612365723,
      "learning_rate": 3.535616169544741e-05,
      "loss": 0.5834,
      "step": 1492500
    },
    {
      "epoch": 23.43171114599686,
      "grad_norm": 4.056328296661377,
      "learning_rate": 3.5355180533751966e-05,
      "loss": 0.5604,
      "step": 1492600
    },
    {
      "epoch": 23.433281004709578,
      "grad_norm": 4.6966118812561035,
      "learning_rate": 3.535419937205652e-05,
      "loss": 0.6014,
      "step": 1492700
    },
    {
      "epoch": 23.43485086342229,
      "grad_norm": 3.8041305541992188,
      "learning_rate": 3.535321821036107e-05,
      "loss": 0.6056,
      "step": 1492800
    },
    {
      "epoch": 23.436420722135008,
      "grad_norm": 3.515077590942383,
      "learning_rate": 3.535223704866562e-05,
      "loss": 0.5593,
      "step": 1492900
    },
    {
      "epoch": 23.437990580847725,
      "grad_norm": 4.018861293792725,
      "learning_rate": 3.5351255886970176e-05,
      "loss": 0.5609,
      "step": 1493000
    },
    {
      "epoch": 23.439560439560438,
      "grad_norm": 3.5131752490997314,
      "learning_rate": 3.535027472527473e-05,
      "loss": 0.5928,
      "step": 1493100
    },
    {
      "epoch": 23.441130298273155,
      "grad_norm": 4.170898914337158,
      "learning_rate": 3.534929356357928e-05,
      "loss": 0.5966,
      "step": 1493200
    },
    {
      "epoch": 23.44270015698587,
      "grad_norm": 3.3553435802459717,
      "learning_rate": 3.534831240188383e-05,
      "loss": 0.5861,
      "step": 1493300
    },
    {
      "epoch": 23.44427001569859,
      "grad_norm": 4.3275556564331055,
      "learning_rate": 3.534733124018839e-05,
      "loss": 0.632,
      "step": 1493400
    },
    {
      "epoch": 23.445839874411302,
      "grad_norm": 3.0937459468841553,
      "learning_rate": 3.534635007849293e-05,
      "loss": 0.5923,
      "step": 1493500
    },
    {
      "epoch": 23.44740973312402,
      "grad_norm": 4.0588202476501465,
      "learning_rate": 3.534536891679749e-05,
      "loss": 0.6266,
      "step": 1493600
    },
    {
      "epoch": 23.448979591836736,
      "grad_norm": 3.377575397491455,
      "learning_rate": 3.534438775510204e-05,
      "loss": 0.5948,
      "step": 1493700
    },
    {
      "epoch": 23.45054945054945,
      "grad_norm": 4.332143306732178,
      "learning_rate": 3.53434065934066e-05,
      "loss": 0.5968,
      "step": 1493800
    },
    {
      "epoch": 23.452119309262166,
      "grad_norm": 3.255258083343506,
      "learning_rate": 3.534242543171115e-05,
      "loss": 0.5604,
      "step": 1493900
    },
    {
      "epoch": 23.453689167974883,
      "grad_norm": 3.5094857215881348,
      "learning_rate": 3.53414442700157e-05,
      "loss": 0.6052,
      "step": 1494000
    },
    {
      "epoch": 23.4552590266876,
      "grad_norm": 3.702414035797119,
      "learning_rate": 3.534046310832026e-05,
      "loss": 0.6177,
      "step": 1494100
    },
    {
      "epoch": 23.456828885400313,
      "grad_norm": 5.094047546386719,
      "learning_rate": 3.53394819466248e-05,
      "loss": 0.6361,
      "step": 1494200
    },
    {
      "epoch": 23.45839874411303,
      "grad_norm": 3.990731716156006,
      "learning_rate": 3.533850078492936e-05,
      "loss": 0.5721,
      "step": 1494300
    },
    {
      "epoch": 23.459968602825747,
      "grad_norm": 4.074774742126465,
      "learning_rate": 3.533751962323391e-05,
      "loss": 0.5892,
      "step": 1494400
    },
    {
      "epoch": 23.46153846153846,
      "grad_norm": 3.534101724624634,
      "learning_rate": 3.533653846153847e-05,
      "loss": 0.5981,
      "step": 1494500
    },
    {
      "epoch": 23.463108320251177,
      "grad_norm": 3.53467059135437,
      "learning_rate": 3.533555729984301e-05,
      "loss": 0.5582,
      "step": 1494600
    },
    {
      "epoch": 23.464678178963894,
      "grad_norm": 3.2093961238861084,
      "learning_rate": 3.533457613814757e-05,
      "loss": 0.6043,
      "step": 1494700
    },
    {
      "epoch": 23.46624803767661,
      "grad_norm": 5.07493782043457,
      "learning_rate": 3.533359497645212e-05,
      "loss": 0.6414,
      "step": 1494800
    },
    {
      "epoch": 23.467817896389324,
      "grad_norm": 4.608471870422363,
      "learning_rate": 3.533261381475667e-05,
      "loss": 0.5758,
      "step": 1494900
    },
    {
      "epoch": 23.46938775510204,
      "grad_norm": 3.6347227096557617,
      "learning_rate": 3.533163265306122e-05,
      "loss": 0.6023,
      "step": 1495000
    },
    {
      "epoch": 23.470957613814758,
      "grad_norm": 5.016494274139404,
      "learning_rate": 3.533065149136578e-05,
      "loss": 0.601,
      "step": 1495100
    },
    {
      "epoch": 23.47252747252747,
      "grad_norm": 3.438471794128418,
      "learning_rate": 3.532967032967033e-05,
      "loss": 0.584,
      "step": 1495200
    },
    {
      "epoch": 23.474097331240188,
      "grad_norm": 4.416881084442139,
      "learning_rate": 3.532868916797488e-05,
      "loss": 0.6458,
      "step": 1495300
    },
    {
      "epoch": 23.475667189952905,
      "grad_norm": 3.5357682704925537,
      "learning_rate": 3.5327708006279434e-05,
      "loss": 0.6414,
      "step": 1495400
    },
    {
      "epoch": 23.47723704866562,
      "grad_norm": 4.224280834197998,
      "learning_rate": 3.532672684458399e-05,
      "loss": 0.6217,
      "step": 1495500
    },
    {
      "epoch": 23.478806907378335,
      "grad_norm": 4.528097629547119,
      "learning_rate": 3.5325745682888536e-05,
      "loss": 0.5704,
      "step": 1495600
    },
    {
      "epoch": 23.48037676609105,
      "grad_norm": 2.54278564453125,
      "learning_rate": 3.5324764521193094e-05,
      "loss": 0.5958,
      "step": 1495700
    },
    {
      "epoch": 23.48194662480377,
      "grad_norm": 3.5285067558288574,
      "learning_rate": 3.5323783359497645e-05,
      "loss": 0.6431,
      "step": 1495800
    },
    {
      "epoch": 23.483516483516482,
      "grad_norm": 5.014585018157959,
      "learning_rate": 3.53228021978022e-05,
      "loss": 0.5919,
      "step": 1495900
    },
    {
      "epoch": 23.4850863422292,
      "grad_norm": 4.011673927307129,
      "learning_rate": 3.532182103610675e-05,
      "loss": 0.655,
      "step": 1496000
    },
    {
      "epoch": 23.486656200941916,
      "grad_norm": 3.9496614933013916,
      "learning_rate": 3.5320839874411304e-05,
      "loss": 0.6074,
      "step": 1496100
    },
    {
      "epoch": 23.488226059654632,
      "grad_norm": 3.8730225563049316,
      "learning_rate": 3.531985871271586e-05,
      "loss": 0.6369,
      "step": 1496200
    },
    {
      "epoch": 23.489795918367346,
      "grad_norm": 3.055720806121826,
      "learning_rate": 3.5318877551020406e-05,
      "loss": 0.5837,
      "step": 1496300
    },
    {
      "epoch": 23.491365777080063,
      "grad_norm": 4.092249870300293,
      "learning_rate": 3.5317896389324964e-05,
      "loss": 0.6234,
      "step": 1496400
    },
    {
      "epoch": 23.49293563579278,
      "grad_norm": 4.69085168838501,
      "learning_rate": 3.5316915227629515e-05,
      "loss": 0.5831,
      "step": 1496500
    },
    {
      "epoch": 23.494505494505496,
      "grad_norm": 3.320849895477295,
      "learning_rate": 3.531593406593407e-05,
      "loss": 0.5837,
      "step": 1496600
    },
    {
      "epoch": 23.49607535321821,
      "grad_norm": 3.081784725189209,
      "learning_rate": 3.531495290423862e-05,
      "loss": 0.626,
      "step": 1496700
    },
    {
      "epoch": 23.497645211930926,
      "grad_norm": 5.873775959014893,
      "learning_rate": 3.5313971742543175e-05,
      "loss": 0.6218,
      "step": 1496800
    },
    {
      "epoch": 23.499215070643643,
      "grad_norm": 3.5891430377960205,
      "learning_rate": 3.5312990580847726e-05,
      "loss": 0.6098,
      "step": 1496900
    },
    {
      "epoch": 23.500784929356357,
      "grad_norm": 3.649325370788574,
      "learning_rate": 3.531200941915228e-05,
      "loss": 0.6253,
      "step": 1497000
    },
    {
      "epoch": 23.502354788069074,
      "grad_norm": 4.339881420135498,
      "learning_rate": 3.531102825745683e-05,
      "loss": 0.607,
      "step": 1497100
    },
    {
      "epoch": 23.50392464678179,
      "grad_norm": 3.973757028579712,
      "learning_rate": 3.5310047095761385e-05,
      "loss": 0.6179,
      "step": 1497200
    },
    {
      "epoch": 23.505494505494504,
      "grad_norm": 2.6169626712799072,
      "learning_rate": 3.5309065934065936e-05,
      "loss": 0.556,
      "step": 1497300
    },
    {
      "epoch": 23.50706436420722,
      "grad_norm": 3.8461148738861084,
      "learning_rate": 3.530808477237049e-05,
      "loss": 0.6575,
      "step": 1497400
    },
    {
      "epoch": 23.508634222919937,
      "grad_norm": 2.9500977993011475,
      "learning_rate": 3.530710361067504e-05,
      "loss": 0.5605,
      "step": 1497500
    },
    {
      "epoch": 23.510204081632654,
      "grad_norm": 3.4162917137145996,
      "learning_rate": 3.5306122448979596e-05,
      "loss": 0.6326,
      "step": 1497600
    },
    {
      "epoch": 23.511773940345368,
      "grad_norm": 4.086432456970215,
      "learning_rate": 3.530514128728414e-05,
      "loss": 0.6584,
      "step": 1497700
    },
    {
      "epoch": 23.513343799058084,
      "grad_norm": 2.5211338996887207,
      "learning_rate": 3.53041601255887e-05,
      "loss": 0.6509,
      "step": 1497800
    },
    {
      "epoch": 23.5149136577708,
      "grad_norm": 2.1403064727783203,
      "learning_rate": 3.530317896389325e-05,
      "loss": 0.5793,
      "step": 1497900
    },
    {
      "epoch": 23.516483516483518,
      "grad_norm": 3.266852378845215,
      "learning_rate": 3.530219780219781e-05,
      "loss": 0.5787,
      "step": 1498000
    },
    {
      "epoch": 23.51805337519623,
      "grad_norm": 2.9742655754089355,
      "learning_rate": 3.530121664050236e-05,
      "loss": 0.5875,
      "step": 1498100
    },
    {
      "epoch": 23.51962323390895,
      "grad_norm": 3.425346612930298,
      "learning_rate": 3.530023547880691e-05,
      "loss": 0.559,
      "step": 1498200
    },
    {
      "epoch": 23.521193092621665,
      "grad_norm": 2.9716289043426514,
      "learning_rate": 3.5299254317111466e-05,
      "loss": 0.5778,
      "step": 1498300
    },
    {
      "epoch": 23.52276295133438,
      "grad_norm": 2.907196283340454,
      "learning_rate": 3.529827315541601e-05,
      "loss": 0.637,
      "step": 1498400
    },
    {
      "epoch": 23.524332810047095,
      "grad_norm": 3.282317876815796,
      "learning_rate": 3.529729199372057e-05,
      "loss": 0.6552,
      "step": 1498500
    },
    {
      "epoch": 23.525902668759812,
      "grad_norm": 4.90058708190918,
      "learning_rate": 3.529631083202512e-05,
      "loss": 0.6133,
      "step": 1498600
    },
    {
      "epoch": 23.52747252747253,
      "grad_norm": 3.0555331707000732,
      "learning_rate": 3.529532967032968e-05,
      "loss": 0.5755,
      "step": 1498700
    },
    {
      "epoch": 23.529042386185242,
      "grad_norm": 2.045679807662964,
      "learning_rate": 3.529434850863422e-05,
      "loss": 0.592,
      "step": 1498800
    },
    {
      "epoch": 23.53061224489796,
      "grad_norm": 4.04893684387207,
      "learning_rate": 3.529336734693878e-05,
      "loss": 0.5768,
      "step": 1498900
    },
    {
      "epoch": 23.532182103610676,
      "grad_norm": 4.115157127380371,
      "learning_rate": 3.529238618524333e-05,
      "loss": 0.5843,
      "step": 1499000
    },
    {
      "epoch": 23.53375196232339,
      "grad_norm": 4.570311069488525,
      "learning_rate": 3.529140502354788e-05,
      "loss": 0.6152,
      "step": 1499100
    },
    {
      "epoch": 23.535321821036106,
      "grad_norm": 3.3450655937194824,
      "learning_rate": 3.529042386185243e-05,
      "loss": 0.6028,
      "step": 1499200
    },
    {
      "epoch": 23.536891679748823,
      "grad_norm": 2.151395797729492,
      "learning_rate": 3.528944270015699e-05,
      "loss": 0.6234,
      "step": 1499300
    },
    {
      "epoch": 23.53846153846154,
      "grad_norm": 3.724940776824951,
      "learning_rate": 3.528846153846154e-05,
      "loss": 0.5845,
      "step": 1499400
    },
    {
      "epoch": 23.540031397174253,
      "grad_norm": 2.635960102081299,
      "learning_rate": 3.528748037676609e-05,
      "loss": 0.5915,
      "step": 1499500
    },
    {
      "epoch": 23.54160125588697,
      "grad_norm": 3.692699432373047,
      "learning_rate": 3.528649921507064e-05,
      "loss": 0.5588,
      "step": 1499600
    },
    {
      "epoch": 23.543171114599687,
      "grad_norm": 3.707301139831543,
      "learning_rate": 3.52855180533752e-05,
      "loss": 0.5617,
      "step": 1499700
    },
    {
      "epoch": 23.5447409733124,
      "grad_norm": 2.837522029876709,
      "learning_rate": 3.5284536891679745e-05,
      "loss": 0.6168,
      "step": 1499800
    },
    {
      "epoch": 23.546310832025117,
      "grad_norm": 4.068040370941162,
      "learning_rate": 3.52835557299843e-05,
      "loss": 0.6434,
      "step": 1499900
    },
    {
      "epoch": 23.547880690737834,
      "grad_norm": 3.324291944503784,
      "learning_rate": 3.5282574568288854e-05,
      "loss": 0.6079,
      "step": 1500000
    },
    {
      "epoch": 23.54945054945055,
      "grad_norm": 4.437765598297119,
      "learning_rate": 3.528159340659341e-05,
      "loss": 0.612,
      "step": 1500100
    },
    {
      "epoch": 23.551020408163264,
      "grad_norm": 4.2403154373168945,
      "learning_rate": 3.528061224489796e-05,
      "loss": 0.6289,
      "step": 1500200
    },
    {
      "epoch": 23.55259026687598,
      "grad_norm": 3.6914210319519043,
      "learning_rate": 3.527963108320251e-05,
      "loss": 0.6103,
      "step": 1500300
    },
    {
      "epoch": 23.554160125588698,
      "grad_norm": 3.446371078491211,
      "learning_rate": 3.527864992150707e-05,
      "loss": 0.5956,
      "step": 1500400
    },
    {
      "epoch": 23.55572998430141,
      "grad_norm": 2.933459997177124,
      "learning_rate": 3.5277668759811615e-05,
      "loss": 0.6094,
      "step": 1500500
    },
    {
      "epoch": 23.55729984301413,
      "grad_norm": 4.559535026550293,
      "learning_rate": 3.527668759811617e-05,
      "loss": 0.6082,
      "step": 1500600
    },
    {
      "epoch": 23.558869701726845,
      "grad_norm": 5.016356468200684,
      "learning_rate": 3.5275706436420724e-05,
      "loss": 0.6026,
      "step": 1500700
    },
    {
      "epoch": 23.560439560439562,
      "grad_norm": 4.679256439208984,
      "learning_rate": 3.527472527472528e-05,
      "loss": 0.6119,
      "step": 1500800
    },
    {
      "epoch": 23.562009419152275,
      "grad_norm": 3.9838287830352783,
      "learning_rate": 3.5273744113029826e-05,
      "loss": 0.6102,
      "step": 1500900
    },
    {
      "epoch": 23.563579277864992,
      "grad_norm": 4.997274398803711,
      "learning_rate": 3.5272762951334384e-05,
      "loss": 0.5632,
      "step": 1501000
    },
    {
      "epoch": 23.56514913657771,
      "grad_norm": 3.470123767852783,
      "learning_rate": 3.5271781789638935e-05,
      "loss": 0.6161,
      "step": 1501100
    },
    {
      "epoch": 23.566718995290422,
      "grad_norm": 2.637223720550537,
      "learning_rate": 3.5270800627943486e-05,
      "loss": 0.6172,
      "step": 1501200
    },
    {
      "epoch": 23.56828885400314,
      "grad_norm": 3.304600715637207,
      "learning_rate": 3.5269819466248037e-05,
      "loss": 0.5868,
      "step": 1501300
    },
    {
      "epoch": 23.569858712715856,
      "grad_norm": 3.544764280319214,
      "learning_rate": 3.5268838304552594e-05,
      "loss": 0.6348,
      "step": 1501400
    },
    {
      "epoch": 23.571428571428573,
      "grad_norm": 4.733949184417725,
      "learning_rate": 3.5267857142857145e-05,
      "loss": 0.6184,
      "step": 1501500
    },
    {
      "epoch": 23.572998430141286,
      "grad_norm": 4.926715850830078,
      "learning_rate": 3.5266875981161696e-05,
      "loss": 0.5762,
      "step": 1501600
    },
    {
      "epoch": 23.574568288854003,
      "grad_norm": 2.9615578651428223,
      "learning_rate": 3.526589481946625e-05,
      "loss": 0.597,
      "step": 1501700
    },
    {
      "epoch": 23.57613814756672,
      "grad_norm": 3.618440866470337,
      "learning_rate": 3.5264913657770805e-05,
      "loss": 0.6039,
      "step": 1501800
    },
    {
      "epoch": 23.577708006279433,
      "grad_norm": 4.2843523025512695,
      "learning_rate": 3.526393249607535e-05,
      "loss": 0.6227,
      "step": 1501900
    },
    {
      "epoch": 23.57927786499215,
      "grad_norm": 4.146285057067871,
      "learning_rate": 3.526295133437991e-05,
      "loss": 0.5759,
      "step": 1502000
    },
    {
      "epoch": 23.580847723704867,
      "grad_norm": 2.624394655227661,
      "learning_rate": 3.526197017268446e-05,
      "loss": 0.5842,
      "step": 1502100
    },
    {
      "epoch": 23.582417582417584,
      "grad_norm": 3.678185224533081,
      "learning_rate": 3.5260989010989016e-05,
      "loss": 0.5872,
      "step": 1502200
    },
    {
      "epoch": 23.583987441130297,
      "grad_norm": 4.15968132019043,
      "learning_rate": 3.526000784929357e-05,
      "loss": 0.6074,
      "step": 1502300
    },
    {
      "epoch": 23.585557299843014,
      "grad_norm": 3.495227098464966,
      "learning_rate": 3.525902668759812e-05,
      "loss": 0.5945,
      "step": 1502400
    },
    {
      "epoch": 23.58712715855573,
      "grad_norm": 2.5745046138763428,
      "learning_rate": 3.525804552590267e-05,
      "loss": 0.5836,
      "step": 1502500
    },
    {
      "epoch": 23.588697017268444,
      "grad_norm": 3.721967935562134,
      "learning_rate": 3.525706436420722e-05,
      "loss": 0.5845,
      "step": 1502600
    },
    {
      "epoch": 23.59026687598116,
      "grad_norm": 3.6523280143737793,
      "learning_rate": 3.525608320251178e-05,
      "loss": 0.5883,
      "step": 1502700
    },
    {
      "epoch": 23.591836734693878,
      "grad_norm": 3.5012876987457275,
      "learning_rate": 3.525510204081633e-05,
      "loss": 0.6211,
      "step": 1502800
    },
    {
      "epoch": 23.593406593406595,
      "grad_norm": 3.964495897293091,
      "learning_rate": 3.5254120879120886e-05,
      "loss": 0.5961,
      "step": 1502900
    },
    {
      "epoch": 23.594976452119308,
      "grad_norm": 4.228833198547363,
      "learning_rate": 3.525313971742543e-05,
      "loss": 0.5825,
      "step": 1503000
    },
    {
      "epoch": 23.596546310832025,
      "grad_norm": 3.2382302284240723,
      "learning_rate": 3.525215855572999e-05,
      "loss": 0.5871,
      "step": 1503100
    },
    {
      "epoch": 23.598116169544742,
      "grad_norm": 5.0528693199157715,
      "learning_rate": 3.525117739403454e-05,
      "loss": 0.5882,
      "step": 1503200
    },
    {
      "epoch": 23.599686028257455,
      "grad_norm": 4.281792163848877,
      "learning_rate": 3.525019623233909e-05,
      "loss": 0.5916,
      "step": 1503300
    },
    {
      "epoch": 23.601255886970172,
      "grad_norm": 3.1349542140960693,
      "learning_rate": 3.524921507064364e-05,
      "loss": 0.5891,
      "step": 1503400
    },
    {
      "epoch": 23.60282574568289,
      "grad_norm": 4.3636932373046875,
      "learning_rate": 3.52482339089482e-05,
      "loss": 0.5706,
      "step": 1503500
    },
    {
      "epoch": 23.604395604395606,
      "grad_norm": 3.9611945152282715,
      "learning_rate": 3.524725274725275e-05,
      "loss": 0.6056,
      "step": 1503600
    },
    {
      "epoch": 23.60596546310832,
      "grad_norm": 4.188823223114014,
      "learning_rate": 3.52462715855573e-05,
      "loss": 0.5739,
      "step": 1503700
    },
    {
      "epoch": 23.607535321821036,
      "grad_norm": 4.658782005310059,
      "learning_rate": 3.524529042386185e-05,
      "loss": 0.5895,
      "step": 1503800
    },
    {
      "epoch": 23.609105180533753,
      "grad_norm": 3.6517131328582764,
      "learning_rate": 3.524430926216641e-05,
      "loss": 0.6468,
      "step": 1503900
    },
    {
      "epoch": 23.610675039246466,
      "grad_norm": 3.8319272994995117,
      "learning_rate": 3.5243328100470954e-05,
      "loss": 0.6075,
      "step": 1504000
    },
    {
      "epoch": 23.612244897959183,
      "grad_norm": 3.3420543670654297,
      "learning_rate": 3.524234693877551e-05,
      "loss": 0.6144,
      "step": 1504100
    },
    {
      "epoch": 23.6138147566719,
      "grad_norm": 4.422047138214111,
      "learning_rate": 3.524136577708006e-05,
      "loss": 0.6168,
      "step": 1504200
    },
    {
      "epoch": 23.615384615384617,
      "grad_norm": 3.8022379875183105,
      "learning_rate": 3.524038461538462e-05,
      "loss": 0.5418,
      "step": 1504300
    },
    {
      "epoch": 23.61695447409733,
      "grad_norm": 3.0966460704803467,
      "learning_rate": 3.523940345368917e-05,
      "loss": 0.5811,
      "step": 1504400
    },
    {
      "epoch": 23.618524332810047,
      "grad_norm": 4.942202568054199,
      "learning_rate": 3.523842229199372e-05,
      "loss": 0.6136,
      "step": 1504500
    },
    {
      "epoch": 23.620094191522764,
      "grad_norm": 4.952357292175293,
      "learning_rate": 3.523744113029827e-05,
      "loss": 0.6139,
      "step": 1504600
    },
    {
      "epoch": 23.621664050235477,
      "grad_norm": 3.2953219413757324,
      "learning_rate": 3.5236459968602824e-05,
      "loss": 0.6086,
      "step": 1504700
    },
    {
      "epoch": 23.623233908948194,
      "grad_norm": 3.228985071182251,
      "learning_rate": 3.523547880690738e-05,
      "loss": 0.5915,
      "step": 1504800
    },
    {
      "epoch": 23.62480376766091,
      "grad_norm": 3.613391637802124,
      "learning_rate": 3.523449764521193e-05,
      "loss": 0.6174,
      "step": 1504900
    },
    {
      "epoch": 23.626373626373628,
      "grad_norm": 4.609078407287598,
      "learning_rate": 3.523351648351649e-05,
      "loss": 0.5908,
      "step": 1505000
    },
    {
      "epoch": 23.62794348508634,
      "grad_norm": 3.77184796333313,
      "learning_rate": 3.5232535321821035e-05,
      "loss": 0.6101,
      "step": 1505100
    },
    {
      "epoch": 23.629513343799058,
      "grad_norm": 3.497657537460327,
      "learning_rate": 3.523155416012559e-05,
      "loss": 0.596,
      "step": 1505200
    },
    {
      "epoch": 23.631083202511775,
      "grad_norm": 4.228639125823975,
      "learning_rate": 3.5230572998430144e-05,
      "loss": 0.5909,
      "step": 1505300
    },
    {
      "epoch": 23.632653061224488,
      "grad_norm": 5.31800651550293,
      "learning_rate": 3.5229591836734695e-05,
      "loss": 0.6123,
      "step": 1505400
    },
    {
      "epoch": 23.634222919937205,
      "grad_norm": 3.5993361473083496,
      "learning_rate": 3.5228610675039246e-05,
      "loss": 0.59,
      "step": 1505500
    },
    {
      "epoch": 23.635792778649922,
      "grad_norm": 4.657898902893066,
      "learning_rate": 3.52276295133438e-05,
      "loss": 0.6091,
      "step": 1505600
    },
    {
      "epoch": 23.63736263736264,
      "grad_norm": 4.58591365814209,
      "learning_rate": 3.5226648351648354e-05,
      "loss": 0.6068,
      "step": 1505700
    },
    {
      "epoch": 23.638932496075352,
      "grad_norm": 4.15269660949707,
      "learning_rate": 3.5225667189952905e-05,
      "loss": 0.6045,
      "step": 1505800
    },
    {
      "epoch": 23.64050235478807,
      "grad_norm": 4.014693260192871,
      "learning_rate": 3.5224686028257456e-05,
      "loss": 0.6129,
      "step": 1505900
    },
    {
      "epoch": 23.642072213500786,
      "grad_norm": 3.543597936630249,
      "learning_rate": 3.5223704866562014e-05,
      "loss": 0.5706,
      "step": 1506000
    },
    {
      "epoch": 23.643642072213503,
      "grad_norm": 3.0851950645446777,
      "learning_rate": 3.522272370486656e-05,
      "loss": 0.6151,
      "step": 1506100
    },
    {
      "epoch": 23.645211930926216,
      "grad_norm": 3.881117582321167,
      "learning_rate": 3.5221742543171116e-05,
      "loss": 0.5884,
      "step": 1506200
    },
    {
      "epoch": 23.646781789638933,
      "grad_norm": 2.5167346000671387,
      "learning_rate": 3.522076138147567e-05,
      "loss": 0.5876,
      "step": 1506300
    },
    {
      "epoch": 23.64835164835165,
      "grad_norm": 4.159181594848633,
      "learning_rate": 3.5219780219780225e-05,
      "loss": 0.5898,
      "step": 1506400
    },
    {
      "epoch": 23.649921507064363,
      "grad_norm": 3.1846210956573486,
      "learning_rate": 3.5218799058084776e-05,
      "loss": 0.6593,
      "step": 1506500
    },
    {
      "epoch": 23.65149136577708,
      "grad_norm": 4.634568691253662,
      "learning_rate": 3.521781789638933e-05,
      "loss": 0.6085,
      "step": 1506600
    },
    {
      "epoch": 23.653061224489797,
      "grad_norm": 4.414454936981201,
      "learning_rate": 3.521683673469388e-05,
      "loss": 0.5889,
      "step": 1506700
    },
    {
      "epoch": 23.65463108320251,
      "grad_norm": 3.6353259086608887,
      "learning_rate": 3.521585557299843e-05,
      "loss": 0.6179,
      "step": 1506800
    },
    {
      "epoch": 23.656200941915227,
      "grad_norm": 4.134925365447998,
      "learning_rate": 3.5214874411302986e-05,
      "loss": 0.6063,
      "step": 1506900
    },
    {
      "epoch": 23.657770800627944,
      "grad_norm": 4.270654678344727,
      "learning_rate": 3.521389324960754e-05,
      "loss": 0.6104,
      "step": 1507000
    },
    {
      "epoch": 23.65934065934066,
      "grad_norm": 4.919889450073242,
      "learning_rate": 3.5212912087912095e-05,
      "loss": 0.6119,
      "step": 1507100
    },
    {
      "epoch": 23.660910518053374,
      "grad_norm": 3.781162977218628,
      "learning_rate": 3.521193092621664e-05,
      "loss": 0.6395,
      "step": 1507200
    },
    {
      "epoch": 23.66248037676609,
      "grad_norm": 3.2976932525634766,
      "learning_rate": 3.52109497645212e-05,
      "loss": 0.5916,
      "step": 1507300
    },
    {
      "epoch": 23.664050235478808,
      "grad_norm": 4.0896897315979,
      "learning_rate": 3.520996860282575e-05,
      "loss": 0.6187,
      "step": 1507400
    },
    {
      "epoch": 23.665620094191524,
      "grad_norm": 4.1819000244140625,
      "learning_rate": 3.52089874411303e-05,
      "loss": 0.6045,
      "step": 1507500
    },
    {
      "epoch": 23.667189952904238,
      "grad_norm": 3.6607327461242676,
      "learning_rate": 3.520800627943485e-05,
      "loss": 0.6227,
      "step": 1507600
    },
    {
      "epoch": 23.668759811616955,
      "grad_norm": 4.084552764892578,
      "learning_rate": 3.520702511773941e-05,
      "loss": 0.6106,
      "step": 1507700
    },
    {
      "epoch": 23.67032967032967,
      "grad_norm": 3.8461661338806152,
      "learning_rate": 3.520604395604396e-05,
      "loss": 0.5668,
      "step": 1507800
    },
    {
      "epoch": 23.671899529042385,
      "grad_norm": 4.188234329223633,
      "learning_rate": 3.520506279434851e-05,
      "loss": 0.6033,
      "step": 1507900
    },
    {
      "epoch": 23.6734693877551,
      "grad_norm": 4.171773433685303,
      "learning_rate": 3.520408163265306e-05,
      "loss": 0.6046,
      "step": 1508000
    },
    {
      "epoch": 23.67503924646782,
      "grad_norm": 3.748668670654297,
      "learning_rate": 3.520310047095762e-05,
      "loss": 0.576,
      "step": 1508100
    },
    {
      "epoch": 23.676609105180535,
      "grad_norm": 4.088199138641357,
      "learning_rate": 3.520211930926216e-05,
      "loss": 0.6195,
      "step": 1508200
    },
    {
      "epoch": 23.67817896389325,
      "grad_norm": 3.7438840866088867,
      "learning_rate": 3.520113814756672e-05,
      "loss": 0.6036,
      "step": 1508300
    },
    {
      "epoch": 23.679748822605966,
      "grad_norm": 3.317983627319336,
      "learning_rate": 3.520015698587127e-05,
      "loss": 0.5891,
      "step": 1508400
    },
    {
      "epoch": 23.681318681318682,
      "grad_norm": 3.6141717433929443,
      "learning_rate": 3.519917582417583e-05,
      "loss": 0.586,
      "step": 1508500
    },
    {
      "epoch": 23.682888540031396,
      "grad_norm": 4.4393439292907715,
      "learning_rate": 3.519819466248038e-05,
      "loss": 0.6056,
      "step": 1508600
    },
    {
      "epoch": 23.684458398744113,
      "grad_norm": 4.269619941711426,
      "learning_rate": 3.519721350078493e-05,
      "loss": 0.5917,
      "step": 1508700
    },
    {
      "epoch": 23.68602825745683,
      "grad_norm": 3.9783411026000977,
      "learning_rate": 3.519623233908948e-05,
      "loss": 0.6334,
      "step": 1508800
    },
    {
      "epoch": 23.687598116169546,
      "grad_norm": 3.104520082473755,
      "learning_rate": 3.519525117739403e-05,
      "loss": 0.5967,
      "step": 1508900
    },
    {
      "epoch": 23.68916797488226,
      "grad_norm": 3.6493546962738037,
      "learning_rate": 3.519427001569859e-05,
      "loss": 0.627,
      "step": 1509000
    },
    {
      "epoch": 23.690737833594977,
      "grad_norm": 3.3361620903015137,
      "learning_rate": 3.519328885400314e-05,
      "loss": 0.5715,
      "step": 1509100
    },
    {
      "epoch": 23.692307692307693,
      "grad_norm": 3.0252633094787598,
      "learning_rate": 3.51923076923077e-05,
      "loss": 0.5761,
      "step": 1509200
    },
    {
      "epoch": 23.693877551020407,
      "grad_norm": 3.2078216075897217,
      "learning_rate": 3.5191326530612244e-05,
      "loss": 0.6339,
      "step": 1509300
    },
    {
      "epoch": 23.695447409733124,
      "grad_norm": 3.895477294921875,
      "learning_rate": 3.51903453689168e-05,
      "loss": 0.6031,
      "step": 1509400
    },
    {
      "epoch": 23.69701726844584,
      "grad_norm": 4.710012912750244,
      "learning_rate": 3.518936420722135e-05,
      "loss": 0.565,
      "step": 1509500
    },
    {
      "epoch": 23.698587127158557,
      "grad_norm": 3.7407472133636475,
      "learning_rate": 3.5188383045525904e-05,
      "loss": 0.6254,
      "step": 1509600
    },
    {
      "epoch": 23.70015698587127,
      "grad_norm": 3.1156160831451416,
      "learning_rate": 3.5187401883830455e-05,
      "loss": 0.5957,
      "step": 1509700
    },
    {
      "epoch": 23.701726844583987,
      "grad_norm": 4.36075496673584,
      "learning_rate": 3.518642072213501e-05,
      "loss": 0.5936,
      "step": 1509800
    },
    {
      "epoch": 23.703296703296704,
      "grad_norm": 4.444388389587402,
      "learning_rate": 3.518543956043956e-05,
      "loss": 0.5799,
      "step": 1509900
    },
    {
      "epoch": 23.704866562009418,
      "grad_norm": 2.241781711578369,
      "learning_rate": 3.5184458398744114e-05,
      "loss": 0.5872,
      "step": 1510000
    },
    {
      "epoch": 23.706436420722135,
      "grad_norm": 4.68044900894165,
      "learning_rate": 3.5183477237048665e-05,
      "loss": 0.6053,
      "step": 1510100
    },
    {
      "epoch": 23.70800627943485,
      "grad_norm": 4.363903522491455,
      "learning_rate": 3.518249607535322e-05,
      "loss": 0.5484,
      "step": 1510200
    },
    {
      "epoch": 23.70957613814757,
      "grad_norm": 4.595197677612305,
      "learning_rate": 3.518151491365777e-05,
      "loss": 0.5719,
      "step": 1510300
    },
    {
      "epoch": 23.71114599686028,
      "grad_norm": 3.9238739013671875,
      "learning_rate": 3.5180533751962325e-05,
      "loss": 0.5619,
      "step": 1510400
    },
    {
      "epoch": 23.712715855573,
      "grad_norm": 4.226848602294922,
      "learning_rate": 3.5179552590266876e-05,
      "loss": 0.6202,
      "step": 1510500
    },
    {
      "epoch": 23.714285714285715,
      "grad_norm": 4.237524509429932,
      "learning_rate": 3.5178571428571434e-05,
      "loss": 0.6133,
      "step": 1510600
    },
    {
      "epoch": 23.71585557299843,
      "grad_norm": 4.200763702392578,
      "learning_rate": 3.5177590266875985e-05,
      "loss": 0.5847,
      "step": 1510700
    },
    {
      "epoch": 23.717425431711145,
      "grad_norm": 3.198823928833008,
      "learning_rate": 3.5176609105180536e-05,
      "loss": 0.6078,
      "step": 1510800
    },
    {
      "epoch": 23.718995290423862,
      "grad_norm": 4.080323696136475,
      "learning_rate": 3.517562794348509e-05,
      "loss": 0.602,
      "step": 1510900
    },
    {
      "epoch": 23.72056514913658,
      "grad_norm": 4.407846927642822,
      "learning_rate": 3.517464678178964e-05,
      "loss": 0.6043,
      "step": 1511000
    },
    {
      "epoch": 23.722135007849293,
      "grad_norm": 3.9344482421875,
      "learning_rate": 3.5173665620094195e-05,
      "loss": 0.5973,
      "step": 1511100
    },
    {
      "epoch": 23.72370486656201,
      "grad_norm": 3.385798931121826,
      "learning_rate": 3.5172684458398746e-05,
      "loss": 0.6147,
      "step": 1511200
    },
    {
      "epoch": 23.725274725274726,
      "grad_norm": 2.870419502258301,
      "learning_rate": 3.5171703296703304e-05,
      "loss": 0.5442,
      "step": 1511300
    },
    {
      "epoch": 23.72684458398744,
      "grad_norm": 4.287156581878662,
      "learning_rate": 3.517072213500785e-05,
      "loss": 0.5961,
      "step": 1511400
    },
    {
      "epoch": 23.728414442700156,
      "grad_norm": 4.054600715637207,
      "learning_rate": 3.5169740973312406e-05,
      "loss": 0.6273,
      "step": 1511500
    },
    {
      "epoch": 23.729984301412873,
      "grad_norm": 4.554259300231934,
      "learning_rate": 3.516875981161696e-05,
      "loss": 0.5862,
      "step": 1511600
    },
    {
      "epoch": 23.73155416012559,
      "grad_norm": 4.889159202575684,
      "learning_rate": 3.516777864992151e-05,
      "loss": 0.5516,
      "step": 1511700
    },
    {
      "epoch": 23.733124018838303,
      "grad_norm": 3.967294692993164,
      "learning_rate": 3.516679748822606e-05,
      "loss": 0.6275,
      "step": 1511800
    },
    {
      "epoch": 23.73469387755102,
      "grad_norm": 4.643531322479248,
      "learning_rate": 3.516581632653062e-05,
      "loss": 0.6107,
      "step": 1511900
    },
    {
      "epoch": 23.736263736263737,
      "grad_norm": 4.112185001373291,
      "learning_rate": 3.516483516483517e-05,
      "loss": 0.6247,
      "step": 1512000
    },
    {
      "epoch": 23.73783359497645,
      "grad_norm": 3.7992191314697266,
      "learning_rate": 3.516385400313972e-05,
      "loss": 0.6417,
      "step": 1512100
    },
    {
      "epoch": 23.739403453689167,
      "grad_norm": 3.4970455169677734,
      "learning_rate": 3.516287284144427e-05,
      "loss": 0.5883,
      "step": 1512200
    },
    {
      "epoch": 23.740973312401884,
      "grad_norm": 3.2778069972991943,
      "learning_rate": 3.516189167974883e-05,
      "loss": 0.5825,
      "step": 1512300
    },
    {
      "epoch": 23.7425431711146,
      "grad_norm": 4.520603179931641,
      "learning_rate": 3.516091051805337e-05,
      "loss": 0.5333,
      "step": 1512400
    },
    {
      "epoch": 23.744113029827314,
      "grad_norm": 3.8379158973693848,
      "learning_rate": 3.515992935635793e-05,
      "loss": 0.6379,
      "step": 1512500
    },
    {
      "epoch": 23.74568288854003,
      "grad_norm": 4.638036251068115,
      "learning_rate": 3.515894819466248e-05,
      "loss": 0.64,
      "step": 1512600
    },
    {
      "epoch": 23.747252747252748,
      "grad_norm": 4.17878532409668,
      "learning_rate": 3.515796703296704e-05,
      "loss": 0.6023,
      "step": 1512700
    },
    {
      "epoch": 23.74882260596546,
      "grad_norm": 4.4221014976501465,
      "learning_rate": 3.515698587127159e-05,
      "loss": 0.5922,
      "step": 1512800
    },
    {
      "epoch": 23.75039246467818,
      "grad_norm": 4.084635257720947,
      "learning_rate": 3.515600470957614e-05,
      "loss": 0.5681,
      "step": 1512900
    },
    {
      "epoch": 23.751962323390895,
      "grad_norm": 4.228014945983887,
      "learning_rate": 3.515502354788069e-05,
      "loss": 0.6187,
      "step": 1513000
    },
    {
      "epoch": 23.753532182103612,
      "grad_norm": 4.320562362670898,
      "learning_rate": 3.515404238618524e-05,
      "loss": 0.6017,
      "step": 1513100
    },
    {
      "epoch": 23.755102040816325,
      "grad_norm": 4.130138397216797,
      "learning_rate": 3.51530612244898e-05,
      "loss": 0.5999,
      "step": 1513200
    },
    {
      "epoch": 23.756671899529042,
      "grad_norm": 3.573124647140503,
      "learning_rate": 3.515208006279435e-05,
      "loss": 0.599,
      "step": 1513300
    },
    {
      "epoch": 23.75824175824176,
      "grad_norm": 3.956285238265991,
      "learning_rate": 3.51510989010989e-05,
      "loss": 0.6279,
      "step": 1513400
    },
    {
      "epoch": 23.759811616954472,
      "grad_norm": 4.095343589782715,
      "learning_rate": 3.515011773940345e-05,
      "loss": 0.6216,
      "step": 1513500
    },
    {
      "epoch": 23.76138147566719,
      "grad_norm": 3.472097635269165,
      "learning_rate": 3.514913657770801e-05,
      "loss": 0.647,
      "step": 1513600
    },
    {
      "epoch": 23.762951334379906,
      "grad_norm": 4.160706043243408,
      "learning_rate": 3.514815541601256e-05,
      "loss": 0.6292,
      "step": 1513700
    },
    {
      "epoch": 23.764521193092623,
      "grad_norm": 3.6213176250457764,
      "learning_rate": 3.514717425431711e-05,
      "loss": 0.5619,
      "step": 1513800
    },
    {
      "epoch": 23.766091051805336,
      "grad_norm": 3.2054569721221924,
      "learning_rate": 3.5146193092621664e-05,
      "loss": 0.6416,
      "step": 1513900
    },
    {
      "epoch": 23.767660910518053,
      "grad_norm": 4.8779449462890625,
      "learning_rate": 3.514521193092622e-05,
      "loss": 0.6068,
      "step": 1514000
    },
    {
      "epoch": 23.76923076923077,
      "grad_norm": 4.223155498504639,
      "learning_rate": 3.5144230769230766e-05,
      "loss": 0.6252,
      "step": 1514100
    },
    {
      "epoch": 23.770800627943487,
      "grad_norm": 3.34663462638855,
      "learning_rate": 3.514324960753532e-05,
      "loss": 0.566,
      "step": 1514200
    },
    {
      "epoch": 23.7723704866562,
      "grad_norm": 3.9001896381378174,
      "learning_rate": 3.5142268445839874e-05,
      "loss": 0.623,
      "step": 1514300
    },
    {
      "epoch": 23.773940345368917,
      "grad_norm": 2.443246841430664,
      "learning_rate": 3.514128728414443e-05,
      "loss": 0.6048,
      "step": 1514400
    },
    {
      "epoch": 23.775510204081634,
      "grad_norm": 4.6730570793151855,
      "learning_rate": 3.5140306122448976e-05,
      "loss": 0.5496,
      "step": 1514500
    },
    {
      "epoch": 23.777080062794347,
      "grad_norm": 3.413973331451416,
      "learning_rate": 3.5139324960753534e-05,
      "loss": 0.5783,
      "step": 1514600
    },
    {
      "epoch": 23.778649921507064,
      "grad_norm": 4.011854648590088,
      "learning_rate": 3.5138343799058085e-05,
      "loss": 0.5841,
      "step": 1514700
    },
    {
      "epoch": 23.78021978021978,
      "grad_norm": 3.5704526901245117,
      "learning_rate": 3.5137362637362636e-05,
      "loss": 0.6211,
      "step": 1514800
    },
    {
      "epoch": 23.781789638932494,
      "grad_norm": 3.818267345428467,
      "learning_rate": 3.5136381475667194e-05,
      "loss": 0.5754,
      "step": 1514900
    },
    {
      "epoch": 23.78335949764521,
      "grad_norm": 2.631795883178711,
      "learning_rate": 3.5135400313971745e-05,
      "loss": 0.5815,
      "step": 1515000
    },
    {
      "epoch": 23.784929356357928,
      "grad_norm": 3.1730618476867676,
      "learning_rate": 3.5134419152276296e-05,
      "loss": 0.5936,
      "step": 1515100
    },
    {
      "epoch": 23.786499215070645,
      "grad_norm": 2.6081058979034424,
      "learning_rate": 3.513343799058085e-05,
      "loss": 0.5735,
      "step": 1515200
    },
    {
      "epoch": 23.788069073783358,
      "grad_norm": 4.292539119720459,
      "learning_rate": 3.5132456828885404e-05,
      "loss": 0.5816,
      "step": 1515300
    },
    {
      "epoch": 23.789638932496075,
      "grad_norm": 2.8194739818573,
      "learning_rate": 3.5131475667189955e-05,
      "loss": 0.5932,
      "step": 1515400
    },
    {
      "epoch": 23.791208791208792,
      "grad_norm": 3.325875997543335,
      "learning_rate": 3.5130494505494506e-05,
      "loss": 0.6351,
      "step": 1515500
    },
    {
      "epoch": 23.79277864992151,
      "grad_norm": 4.153275012969971,
      "learning_rate": 3.512951334379906e-05,
      "loss": 0.6228,
      "step": 1515600
    },
    {
      "epoch": 23.794348508634222,
      "grad_norm": 2.3965835571289062,
      "learning_rate": 3.5128532182103615e-05,
      "loss": 0.5887,
      "step": 1515700
    },
    {
      "epoch": 23.79591836734694,
      "grad_norm": 4.068901538848877,
      "learning_rate": 3.5127551020408166e-05,
      "loss": 0.6398,
      "step": 1515800
    },
    {
      "epoch": 23.797488226059656,
      "grad_norm": 3.731231212615967,
      "learning_rate": 3.512656985871272e-05,
      "loss": 0.594,
      "step": 1515900
    },
    {
      "epoch": 23.79905808477237,
      "grad_norm": 4.6064910888671875,
      "learning_rate": 3.512558869701727e-05,
      "loss": 0.5887,
      "step": 1516000
    },
    {
      "epoch": 23.800627943485086,
      "grad_norm": 3.6720926761627197,
      "learning_rate": 3.5124607535321826e-05,
      "loss": 0.5818,
      "step": 1516100
    },
    {
      "epoch": 23.802197802197803,
      "grad_norm": 4.065590858459473,
      "learning_rate": 3.512362637362637e-05,
      "loss": 0.6016,
      "step": 1516200
    },
    {
      "epoch": 23.80376766091052,
      "grad_norm": 3.8744149208068848,
      "learning_rate": 3.512264521193093e-05,
      "loss": 0.577,
      "step": 1516300
    },
    {
      "epoch": 23.805337519623233,
      "grad_norm": 3.9890894889831543,
      "learning_rate": 3.512166405023548e-05,
      "loss": 0.5638,
      "step": 1516400
    },
    {
      "epoch": 23.80690737833595,
      "grad_norm": 3.958609104156494,
      "learning_rate": 3.5120682888540036e-05,
      "loss": 0.6449,
      "step": 1516500
    },
    {
      "epoch": 23.808477237048667,
      "grad_norm": 2.8870534896850586,
      "learning_rate": 3.511970172684458e-05,
      "loss": 0.5537,
      "step": 1516600
    },
    {
      "epoch": 23.81004709576138,
      "grad_norm": 3.0656657218933105,
      "learning_rate": 3.511872056514914e-05,
      "loss": 0.6112,
      "step": 1516700
    },
    {
      "epoch": 23.811616954474097,
      "grad_norm": 4.614062786102295,
      "learning_rate": 3.511773940345369e-05,
      "loss": 0.6017,
      "step": 1516800
    },
    {
      "epoch": 23.813186813186814,
      "grad_norm": 2.8452634811401367,
      "learning_rate": 3.511675824175824e-05,
      "loss": 0.616,
      "step": 1516900
    },
    {
      "epoch": 23.81475667189953,
      "grad_norm": 4.112659931182861,
      "learning_rate": 3.51157770800628e-05,
      "loss": 0.5588,
      "step": 1517000
    },
    {
      "epoch": 23.816326530612244,
      "grad_norm": 3.129875421524048,
      "learning_rate": 3.511479591836735e-05,
      "loss": 0.5797,
      "step": 1517100
    },
    {
      "epoch": 23.81789638932496,
      "grad_norm": 4.681676387786865,
      "learning_rate": 3.51138147566719e-05,
      "loss": 0.6142,
      "step": 1517200
    },
    {
      "epoch": 23.819466248037678,
      "grad_norm": 4.3651652336120605,
      "learning_rate": 3.511283359497645e-05,
      "loss": 0.6274,
      "step": 1517300
    },
    {
      "epoch": 23.82103610675039,
      "grad_norm": 3.5147647857666016,
      "learning_rate": 3.511185243328101e-05,
      "loss": 0.5825,
      "step": 1517400
    },
    {
      "epoch": 23.822605965463108,
      "grad_norm": 4.1733717918396,
      "learning_rate": 3.511087127158556e-05,
      "loss": 0.5798,
      "step": 1517500
    },
    {
      "epoch": 23.824175824175825,
      "grad_norm": 4.087271690368652,
      "learning_rate": 3.510989010989011e-05,
      "loss": 0.624,
      "step": 1517600
    },
    {
      "epoch": 23.82574568288854,
      "grad_norm": 3.648533344268799,
      "learning_rate": 3.510890894819466e-05,
      "loss": 0.6216,
      "step": 1517700
    },
    {
      "epoch": 23.827315541601255,
      "grad_norm": 4.273683071136475,
      "learning_rate": 3.510792778649922e-05,
      "loss": 0.6118,
      "step": 1517800
    },
    {
      "epoch": 23.828885400313972,
      "grad_norm": 2.3395004272460938,
      "learning_rate": 3.510694662480377e-05,
      "loss": 0.5545,
      "step": 1517900
    },
    {
      "epoch": 23.83045525902669,
      "grad_norm": 3.9547407627105713,
      "learning_rate": 3.510596546310832e-05,
      "loss": 0.6303,
      "step": 1518000
    },
    {
      "epoch": 23.832025117739402,
      "grad_norm": 3.7924187183380127,
      "learning_rate": 3.510498430141287e-05,
      "loss": 0.6011,
      "step": 1518100
    },
    {
      "epoch": 23.83359497645212,
      "grad_norm": 4.281452655792236,
      "learning_rate": 3.510400313971743e-05,
      "loss": 0.6346,
      "step": 1518200
    },
    {
      "epoch": 23.835164835164836,
      "grad_norm": 3.7883834838867188,
      "learning_rate": 3.5103021978021974e-05,
      "loss": 0.601,
      "step": 1518300
    },
    {
      "epoch": 23.836734693877553,
      "grad_norm": 3.192411422729492,
      "learning_rate": 3.510204081632653e-05,
      "loss": 0.5964,
      "step": 1518400
    },
    {
      "epoch": 23.838304552590266,
      "grad_norm": 3.542924165725708,
      "learning_rate": 3.510105965463108e-05,
      "loss": 0.5573,
      "step": 1518500
    },
    {
      "epoch": 23.839874411302983,
      "grad_norm": 4.945802211761475,
      "learning_rate": 3.510007849293564e-05,
      "loss": 0.6071,
      "step": 1518600
    },
    {
      "epoch": 23.8414442700157,
      "grad_norm": 3.3036787509918213,
      "learning_rate": 3.5099097331240185e-05,
      "loss": 0.621,
      "step": 1518700
    },
    {
      "epoch": 23.843014128728413,
      "grad_norm": 4.0751051902771,
      "learning_rate": 3.509811616954474e-05,
      "loss": 0.5764,
      "step": 1518800
    },
    {
      "epoch": 23.84458398744113,
      "grad_norm": 3.9658825397491455,
      "learning_rate": 3.5097135007849294e-05,
      "loss": 0.5728,
      "step": 1518900
    },
    {
      "epoch": 23.846153846153847,
      "grad_norm": 4.487339496612549,
      "learning_rate": 3.5096153846153845e-05,
      "loss": 0.5805,
      "step": 1519000
    },
    {
      "epoch": 23.847723704866564,
      "grad_norm": 3.5796711444854736,
      "learning_rate": 3.50951726844584e-05,
      "loss": 0.6016,
      "step": 1519100
    },
    {
      "epoch": 23.849293563579277,
      "grad_norm": 3.7327678203582764,
      "learning_rate": 3.5094191522762954e-05,
      "loss": 0.6332,
      "step": 1519200
    },
    {
      "epoch": 23.850863422291994,
      "grad_norm": 4.313051700592041,
      "learning_rate": 3.5093210361067505e-05,
      "loss": 0.6194,
      "step": 1519300
    },
    {
      "epoch": 23.85243328100471,
      "grad_norm": 3.7795753479003906,
      "learning_rate": 3.5092229199372056e-05,
      "loss": 0.6427,
      "step": 1519400
    },
    {
      "epoch": 23.854003139717424,
      "grad_norm": 3.5336124897003174,
      "learning_rate": 3.509124803767661e-05,
      "loss": 0.5971,
      "step": 1519500
    },
    {
      "epoch": 23.85557299843014,
      "grad_norm": 2.9534451961517334,
      "learning_rate": 3.5090266875981164e-05,
      "loss": 0.5612,
      "step": 1519600
    },
    {
      "epoch": 23.857142857142858,
      "grad_norm": 3.5930092334747314,
      "learning_rate": 3.5089285714285715e-05,
      "loss": 0.6124,
      "step": 1519700
    },
    {
      "epoch": 23.858712715855575,
      "grad_norm": 3.3888967037200928,
      "learning_rate": 3.5088304552590266e-05,
      "loss": 0.6067,
      "step": 1519800
    },
    {
      "epoch": 23.860282574568288,
      "grad_norm": 2.3777642250061035,
      "learning_rate": 3.5087323390894824e-05,
      "loss": 0.6261,
      "step": 1519900
    },
    {
      "epoch": 23.861852433281005,
      "grad_norm": 4.664103984832764,
      "learning_rate": 3.5086342229199375e-05,
      "loss": 0.6131,
      "step": 1520000
    },
    {
      "epoch": 23.86342229199372,
      "grad_norm": 3.958639144897461,
      "learning_rate": 3.5085361067503926e-05,
      "loss": 0.6274,
      "step": 1520100
    },
    {
      "epoch": 23.864992150706435,
      "grad_norm": 3.5790624618530273,
      "learning_rate": 3.508437990580848e-05,
      "loss": 0.5846,
      "step": 1520200
    },
    {
      "epoch": 23.86656200941915,
      "grad_norm": 3.838899850845337,
      "learning_rate": 3.5083398744113035e-05,
      "loss": 0.5513,
      "step": 1520300
    },
    {
      "epoch": 23.86813186813187,
      "grad_norm": 4.280240058898926,
      "learning_rate": 3.508241758241758e-05,
      "loss": 0.5903,
      "step": 1520400
    },
    {
      "epoch": 23.869701726844585,
      "grad_norm": 2.5976016521453857,
      "learning_rate": 3.508143642072214e-05,
      "loss": 0.5851,
      "step": 1520500
    },
    {
      "epoch": 23.8712715855573,
      "grad_norm": 3.56500506401062,
      "learning_rate": 3.508045525902669e-05,
      "loss": 0.6127,
      "step": 1520600
    },
    {
      "epoch": 23.872841444270016,
      "grad_norm": 2.9171483516693115,
      "learning_rate": 3.5079474097331245e-05,
      "loss": 0.5943,
      "step": 1520700
    },
    {
      "epoch": 23.874411302982733,
      "grad_norm": 3.0349433422088623,
      "learning_rate": 3.507849293563579e-05,
      "loss": 0.599,
      "step": 1520800
    },
    {
      "epoch": 23.875981161695446,
      "grad_norm": 3.6758296489715576,
      "learning_rate": 3.507751177394035e-05,
      "loss": 0.6093,
      "step": 1520900
    },
    {
      "epoch": 23.877551020408163,
      "grad_norm": 3.8542609214782715,
      "learning_rate": 3.50765306122449e-05,
      "loss": 0.6197,
      "step": 1521000
    },
    {
      "epoch": 23.87912087912088,
      "grad_norm": 2.7274105548858643,
      "learning_rate": 3.507554945054945e-05,
      "loss": 0.6011,
      "step": 1521100
    },
    {
      "epoch": 23.880690737833596,
      "grad_norm": 3.5130746364593506,
      "learning_rate": 3.507456828885401e-05,
      "loss": 0.6282,
      "step": 1521200
    },
    {
      "epoch": 23.88226059654631,
      "grad_norm": 3.720014810562134,
      "learning_rate": 3.507358712715856e-05,
      "loss": 0.6228,
      "step": 1521300
    },
    {
      "epoch": 23.883830455259027,
      "grad_norm": 2.9773027896881104,
      "learning_rate": 3.507260596546311e-05,
      "loss": 0.6043,
      "step": 1521400
    },
    {
      "epoch": 23.885400313971743,
      "grad_norm": 2.4626502990722656,
      "learning_rate": 3.507162480376766e-05,
      "loss": 0.5834,
      "step": 1521500
    },
    {
      "epoch": 23.886970172684457,
      "grad_norm": 4.150685787200928,
      "learning_rate": 3.507064364207222e-05,
      "loss": 0.5831,
      "step": 1521600
    },
    {
      "epoch": 23.888540031397174,
      "grad_norm": 3.5722477436065674,
      "learning_rate": 3.506966248037677e-05,
      "loss": 0.6208,
      "step": 1521700
    },
    {
      "epoch": 23.89010989010989,
      "grad_norm": 3.803462505340576,
      "learning_rate": 3.506868131868132e-05,
      "loss": 0.6251,
      "step": 1521800
    },
    {
      "epoch": 23.891679748822607,
      "grad_norm": 4.494471073150635,
      "learning_rate": 3.506770015698587e-05,
      "loss": 0.6092,
      "step": 1521900
    },
    {
      "epoch": 23.89324960753532,
      "grad_norm": 3.782844305038452,
      "learning_rate": 3.506671899529043e-05,
      "loss": 0.6548,
      "step": 1522000
    },
    {
      "epoch": 23.894819466248038,
      "grad_norm": 3.9442026615142822,
      "learning_rate": 3.506573783359498e-05,
      "loss": 0.6137,
      "step": 1522100
    },
    {
      "epoch": 23.896389324960754,
      "grad_norm": 3.254051923751831,
      "learning_rate": 3.506475667189953e-05,
      "loss": 0.558,
      "step": 1522200
    },
    {
      "epoch": 23.897959183673468,
      "grad_norm": 3.3787167072296143,
      "learning_rate": 3.506377551020408e-05,
      "loss": 0.6385,
      "step": 1522300
    },
    {
      "epoch": 23.899529042386185,
      "grad_norm": 3.5267210006713867,
      "learning_rate": 3.506279434850864e-05,
      "loss": 0.5887,
      "step": 1522400
    },
    {
      "epoch": 23.9010989010989,
      "grad_norm": 3.375105619430542,
      "learning_rate": 3.5061813186813183e-05,
      "loss": 0.6027,
      "step": 1522500
    },
    {
      "epoch": 23.90266875981162,
      "grad_norm": 3.4479987621307373,
      "learning_rate": 3.506083202511774e-05,
      "loss": 0.6283,
      "step": 1522600
    },
    {
      "epoch": 23.90423861852433,
      "grad_norm": 4.120913982391357,
      "learning_rate": 3.505985086342229e-05,
      "loss": 0.5672,
      "step": 1522700
    },
    {
      "epoch": 23.90580847723705,
      "grad_norm": 4.062084674835205,
      "learning_rate": 3.505886970172685e-05,
      "loss": 0.6284,
      "step": 1522800
    },
    {
      "epoch": 23.907378335949765,
      "grad_norm": 3.7957143783569336,
      "learning_rate": 3.5057888540031394e-05,
      "loss": 0.608,
      "step": 1522900
    },
    {
      "epoch": 23.90894819466248,
      "grad_norm": 2.6711504459381104,
      "learning_rate": 3.505690737833595e-05,
      "loss": 0.5853,
      "step": 1523000
    },
    {
      "epoch": 23.910518053375196,
      "grad_norm": 3.524944543838501,
      "learning_rate": 3.50559262166405e-05,
      "loss": 0.5992,
      "step": 1523100
    },
    {
      "epoch": 23.912087912087912,
      "grad_norm": 3.3265819549560547,
      "learning_rate": 3.5054945054945054e-05,
      "loss": 0.5679,
      "step": 1523200
    },
    {
      "epoch": 23.91365777080063,
      "grad_norm": 4.0016326904296875,
      "learning_rate": 3.505396389324961e-05,
      "loss": 0.613,
      "step": 1523300
    },
    {
      "epoch": 23.915227629513343,
      "grad_norm": 4.113158702850342,
      "learning_rate": 3.505298273155416e-05,
      "loss": 0.6408,
      "step": 1523400
    },
    {
      "epoch": 23.91679748822606,
      "grad_norm": 3.5785982608795166,
      "learning_rate": 3.5052001569858714e-05,
      "loss": 0.6139,
      "step": 1523500
    },
    {
      "epoch": 23.918367346938776,
      "grad_norm": 3.750408172607422,
      "learning_rate": 3.5051020408163265e-05,
      "loss": 0.621,
      "step": 1523600
    },
    {
      "epoch": 23.919937205651493,
      "grad_norm": 4.53010892868042,
      "learning_rate": 3.505003924646782e-05,
      "loss": 0.5728,
      "step": 1523700
    },
    {
      "epoch": 23.921507064364206,
      "grad_norm": 5.2800397872924805,
      "learning_rate": 3.504905808477237e-05,
      "loss": 0.6054,
      "step": 1523800
    },
    {
      "epoch": 23.923076923076923,
      "grad_norm": 3.822399854660034,
      "learning_rate": 3.5048076923076924e-05,
      "loss": 0.6093,
      "step": 1523900
    },
    {
      "epoch": 23.92464678178964,
      "grad_norm": 3.1378772258758545,
      "learning_rate": 3.5047095761381475e-05,
      "loss": 0.5764,
      "step": 1524000
    },
    {
      "epoch": 23.926216640502354,
      "grad_norm": 4.601251125335693,
      "learning_rate": 3.504611459968603e-05,
      "loss": 0.568,
      "step": 1524100
    },
    {
      "epoch": 23.92778649921507,
      "grad_norm": 4.145846366882324,
      "learning_rate": 3.5045133437990584e-05,
      "loss": 0.5904,
      "step": 1524200
    },
    {
      "epoch": 23.929356357927787,
      "grad_norm": 2.89728045463562,
      "learning_rate": 3.5044152276295135e-05,
      "loss": 0.5851,
      "step": 1524300
    },
    {
      "epoch": 23.9309262166405,
      "grad_norm": 2.941915988922119,
      "learning_rate": 3.5043171114599686e-05,
      "loss": 0.5823,
      "step": 1524400
    },
    {
      "epoch": 23.932496075353217,
      "grad_norm": 2.8687832355499268,
      "learning_rate": 3.5042189952904244e-05,
      "loss": 0.5924,
      "step": 1524500
    },
    {
      "epoch": 23.934065934065934,
      "grad_norm": 3.5859215259552,
      "learning_rate": 3.504120879120879e-05,
      "loss": 0.5906,
      "step": 1524600
    },
    {
      "epoch": 23.93563579277865,
      "grad_norm": 3.128230571746826,
      "learning_rate": 3.5040227629513346e-05,
      "loss": 0.5932,
      "step": 1524700
    },
    {
      "epoch": 23.937205651491364,
      "grad_norm": 3.6942431926727295,
      "learning_rate": 3.50392464678179e-05,
      "loss": 0.5812,
      "step": 1524800
    },
    {
      "epoch": 23.93877551020408,
      "grad_norm": 3.984523057937622,
      "learning_rate": 3.5038265306122454e-05,
      "loss": 0.6107,
      "step": 1524900
    },
    {
      "epoch": 23.940345368916798,
      "grad_norm": 3.754304885864258,
      "learning_rate": 3.5037284144427e-05,
      "loss": 0.612,
      "step": 1525000
    },
    {
      "epoch": 23.941915227629515,
      "grad_norm": 5.22713041305542,
      "learning_rate": 3.5036302982731556e-05,
      "loss": 0.657,
      "step": 1525100
    },
    {
      "epoch": 23.94348508634223,
      "grad_norm": 3.574672222137451,
      "learning_rate": 3.503532182103611e-05,
      "loss": 0.6306,
      "step": 1525200
    },
    {
      "epoch": 23.945054945054945,
      "grad_norm": 3.1565520763397217,
      "learning_rate": 3.503434065934066e-05,
      "loss": 0.6079,
      "step": 1525300
    },
    {
      "epoch": 23.946624803767662,
      "grad_norm": 3.185201644897461,
      "learning_rate": 3.5033359497645216e-05,
      "loss": 0.5934,
      "step": 1525400
    },
    {
      "epoch": 23.948194662480375,
      "grad_norm": 4.488581657409668,
      "learning_rate": 3.503237833594977e-05,
      "loss": 0.609,
      "step": 1525500
    },
    {
      "epoch": 23.949764521193092,
      "grad_norm": 3.66865611076355,
      "learning_rate": 3.503139717425432e-05,
      "loss": 0.5911,
      "step": 1525600
    },
    {
      "epoch": 23.95133437990581,
      "grad_norm": 2.895451068878174,
      "learning_rate": 3.503041601255887e-05,
      "loss": 0.5696,
      "step": 1525700
    },
    {
      "epoch": 23.952904238618526,
      "grad_norm": 2.9774303436279297,
      "learning_rate": 3.502943485086343e-05,
      "loss": 0.5827,
      "step": 1525800
    },
    {
      "epoch": 23.95447409733124,
      "grad_norm": 3.563161849975586,
      "learning_rate": 3.502845368916798e-05,
      "loss": 0.5987,
      "step": 1525900
    },
    {
      "epoch": 23.956043956043956,
      "grad_norm": 4.570770263671875,
      "learning_rate": 3.502747252747253e-05,
      "loss": 0.6323,
      "step": 1526000
    },
    {
      "epoch": 23.957613814756673,
      "grad_norm": 4.022747039794922,
      "learning_rate": 3.502649136577708e-05,
      "loss": 0.6138,
      "step": 1526100
    },
    {
      "epoch": 23.959183673469386,
      "grad_norm": 3.1233744621276855,
      "learning_rate": 3.502551020408164e-05,
      "loss": 0.6075,
      "step": 1526200
    },
    {
      "epoch": 23.960753532182103,
      "grad_norm": 5.33236026763916,
      "learning_rate": 3.502452904238619e-05,
      "loss": 0.5773,
      "step": 1526300
    },
    {
      "epoch": 23.96232339089482,
      "grad_norm": 4.1589765548706055,
      "learning_rate": 3.502354788069074e-05,
      "loss": 0.5864,
      "step": 1526400
    },
    {
      "epoch": 23.963893249607537,
      "grad_norm": 4.2723894119262695,
      "learning_rate": 3.502256671899529e-05,
      "loss": 0.6408,
      "step": 1526500
    },
    {
      "epoch": 23.96546310832025,
      "grad_norm": 3.947420358657837,
      "learning_rate": 3.502158555729985e-05,
      "loss": 0.6379,
      "step": 1526600
    },
    {
      "epoch": 23.967032967032967,
      "grad_norm": 3.6424827575683594,
      "learning_rate": 3.502060439560439e-05,
      "loss": 0.5998,
      "step": 1526700
    },
    {
      "epoch": 23.968602825745684,
      "grad_norm": 3.839749574661255,
      "learning_rate": 3.501962323390895e-05,
      "loss": 0.6328,
      "step": 1526800
    },
    {
      "epoch": 23.970172684458397,
      "grad_norm": 3.6382529735565186,
      "learning_rate": 3.50186420722135e-05,
      "loss": 0.6015,
      "step": 1526900
    },
    {
      "epoch": 23.971742543171114,
      "grad_norm": 3.6148526668548584,
      "learning_rate": 3.501766091051806e-05,
      "loss": 0.5839,
      "step": 1527000
    },
    {
      "epoch": 23.97331240188383,
      "grad_norm": 4.040655612945557,
      "learning_rate": 3.50166797488226e-05,
      "loss": 0.5938,
      "step": 1527100
    },
    {
      "epoch": 23.974882260596548,
      "grad_norm": 3.592040777206421,
      "learning_rate": 3.501569858712716e-05,
      "loss": 0.6124,
      "step": 1527200
    },
    {
      "epoch": 23.97645211930926,
      "grad_norm": 3.0257842540740967,
      "learning_rate": 3.501471742543171e-05,
      "loss": 0.5738,
      "step": 1527300
    },
    {
      "epoch": 23.978021978021978,
      "grad_norm": 3.4251904487609863,
      "learning_rate": 3.501373626373626e-05,
      "loss": 0.5968,
      "step": 1527400
    },
    {
      "epoch": 23.979591836734695,
      "grad_norm": 2.6895182132720947,
      "learning_rate": 3.501275510204082e-05,
      "loss": 0.5634,
      "step": 1527500
    },
    {
      "epoch": 23.98116169544741,
      "grad_norm": 4.712470531463623,
      "learning_rate": 3.501177394034537e-05,
      "loss": 0.5873,
      "step": 1527600
    },
    {
      "epoch": 23.982731554160125,
      "grad_norm": 3.2184274196624756,
      "learning_rate": 3.501079277864992e-05,
      "loss": 0.5641,
      "step": 1527700
    },
    {
      "epoch": 23.984301412872842,
      "grad_norm": 4.295354843139648,
      "learning_rate": 3.5009811616954474e-05,
      "loss": 0.6042,
      "step": 1527800
    },
    {
      "epoch": 23.98587127158556,
      "grad_norm": 3.7165403366088867,
      "learning_rate": 3.500883045525903e-05,
      "loss": 0.5991,
      "step": 1527900
    },
    {
      "epoch": 23.987441130298272,
      "grad_norm": 4.045955181121826,
      "learning_rate": 3.500784929356358e-05,
      "loss": 0.597,
      "step": 1528000
    },
    {
      "epoch": 23.98901098901099,
      "grad_norm": 3.1813981533050537,
      "learning_rate": 3.500686813186813e-05,
      "loss": 0.5593,
      "step": 1528100
    },
    {
      "epoch": 23.990580847723706,
      "grad_norm": 4.213946342468262,
      "learning_rate": 3.5005886970172684e-05,
      "loss": 0.556,
      "step": 1528200
    },
    {
      "epoch": 23.99215070643642,
      "grad_norm": 3.7498834133148193,
      "learning_rate": 3.500490580847724e-05,
      "loss": 0.6023,
      "step": 1528300
    },
    {
      "epoch": 23.993720565149136,
      "grad_norm": 4.086485385894775,
      "learning_rate": 3.500392464678179e-05,
      "loss": 0.593,
      "step": 1528400
    },
    {
      "epoch": 23.995290423861853,
      "grad_norm": 3.939225912094116,
      "learning_rate": 3.5002943485086344e-05,
      "loss": 0.5921,
      "step": 1528500
    },
    {
      "epoch": 23.99686028257457,
      "grad_norm": 3.5520358085632324,
      "learning_rate": 3.5001962323390895e-05,
      "loss": 0.5997,
      "step": 1528600
    },
    {
      "epoch": 23.998430141287283,
      "grad_norm": 3.684194803237915,
      "learning_rate": 3.500098116169545e-05,
      "loss": 0.5938,
      "step": 1528700
    },
    {
      "epoch": 24.0,
      "grad_norm": 4.623461723327637,
      "learning_rate": 3.5e-05,
      "loss": 0.5793,
      "step": 1528800
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.04185950756073,
      "eval_runtime": 15.6705,
      "eval_samples_per_second": 213.969,
      "eval_steps_per_second": 213.969,
      "step": 1528800
    },
    {
      "epoch": 24.0,
      "eval_loss": 0.46527099609375,
      "eval_runtime": 264.6418,
      "eval_samples_per_second": 240.703,
      "eval_steps_per_second": 240.703,
      "step": 1528800
    },
    {
      "epoch": 24.001569858712717,
      "grad_norm": 3.582460641860962,
      "learning_rate": 3.4999018838304555e-05,
      "loss": 0.5707,
      "step": 1528900
    },
    {
      "epoch": 24.00313971742543,
      "grad_norm": 4.6117730140686035,
      "learning_rate": 3.4998037676609106e-05,
      "loss": 0.568,
      "step": 1529000
    },
    {
      "epoch": 24.004709576138147,
      "grad_norm": 2.563533067703247,
      "learning_rate": 3.4997056514913663e-05,
      "loss": 0.5444,
      "step": 1529100
    },
    {
      "epoch": 24.006279434850864,
      "grad_norm": 3.9936327934265137,
      "learning_rate": 3.499607535321821e-05,
      "loss": 0.5563,
      "step": 1529200
    },
    {
      "epoch": 24.00784929356358,
      "grad_norm": 3.8728108406066895,
      "learning_rate": 3.4995094191522765e-05,
      "loss": 0.5905,
      "step": 1529300
    },
    {
      "epoch": 24.009419152276294,
      "grad_norm": 4.176590919494629,
      "learning_rate": 3.4994113029827316e-05,
      "loss": 0.5828,
      "step": 1529400
    },
    {
      "epoch": 24.01098901098901,
      "grad_norm": 1.9605576992034912,
      "learning_rate": 3.499313186813187e-05,
      "loss": 0.6169,
      "step": 1529500
    },
    {
      "epoch": 24.012558869701728,
      "grad_norm": 3.8982739448547363,
      "learning_rate": 3.4992150706436425e-05,
      "loss": 0.5778,
      "step": 1529600
    },
    {
      "epoch": 24.01412872841444,
      "grad_norm": 2.6955294609069824,
      "learning_rate": 3.4991169544740976e-05,
      "loss": 0.5484,
      "step": 1529700
    },
    {
      "epoch": 24.015698587127158,
      "grad_norm": 3.9552042484283447,
      "learning_rate": 3.499018838304553e-05,
      "loss": 0.5611,
      "step": 1529800
    },
    {
      "epoch": 24.017268445839875,
      "grad_norm": 3.3823559284210205,
      "learning_rate": 3.498920722135008e-05,
      "loss": 0.5788,
      "step": 1529900
    },
    {
      "epoch": 24.01883830455259,
      "grad_norm": 3.652153491973877,
      "learning_rate": 3.4988226059654636e-05,
      "loss": 0.5963,
      "step": 1530000
    },
    {
      "epoch": 24.020408163265305,
      "grad_norm": 2.8365843296051025,
      "learning_rate": 3.498724489795919e-05,
      "loss": 0.6003,
      "step": 1530100
    },
    {
      "epoch": 24.021978021978022,
      "grad_norm": 5.369234561920166,
      "learning_rate": 3.498626373626374e-05,
      "loss": 0.5982,
      "step": 1530200
    },
    {
      "epoch": 24.02354788069074,
      "grad_norm": 3.8832156658172607,
      "learning_rate": 3.498528257456829e-05,
      "loss": 0.574,
      "step": 1530300
    },
    {
      "epoch": 24.025117739403452,
      "grad_norm": 4.1798481941223145,
      "learning_rate": 3.4984301412872847e-05,
      "loss": 0.6205,
      "step": 1530400
    },
    {
      "epoch": 24.02668759811617,
      "grad_norm": 4.660796642303467,
      "learning_rate": 3.49833202511774e-05,
      "loss": 0.596,
      "step": 1530500
    },
    {
      "epoch": 24.028257456828886,
      "grad_norm": 4.305117130279541,
      "learning_rate": 3.498233908948195e-05,
      "loss": 0.5982,
      "step": 1530600
    },
    {
      "epoch": 24.029827315541603,
      "grad_norm": 3.3136239051818848,
      "learning_rate": 3.49813579277865e-05,
      "loss": 0.5935,
      "step": 1530700
    },
    {
      "epoch": 24.031397174254316,
      "grad_norm": 3.6660194396972656,
      "learning_rate": 3.498037676609106e-05,
      "loss": 0.5829,
      "step": 1530800
    },
    {
      "epoch": 24.032967032967033,
      "grad_norm": 3.2421646118164062,
      "learning_rate": 3.49793956043956e-05,
      "loss": 0.5649,
      "step": 1530900
    },
    {
      "epoch": 24.03453689167975,
      "grad_norm": 2.3642075061798096,
      "learning_rate": 3.497841444270016e-05,
      "loss": 0.6531,
      "step": 1531000
    },
    {
      "epoch": 24.036106750392463,
      "grad_norm": 3.702542781829834,
      "learning_rate": 3.497743328100471e-05,
      "loss": 0.6223,
      "step": 1531100
    },
    {
      "epoch": 24.03767660910518,
      "grad_norm": 2.7616238594055176,
      "learning_rate": 3.497645211930927e-05,
      "loss": 0.6389,
      "step": 1531200
    },
    {
      "epoch": 24.039246467817897,
      "grad_norm": 4.183910369873047,
      "learning_rate": 3.497547095761381e-05,
      "loss": 0.5858,
      "step": 1531300
    },
    {
      "epoch": 24.040816326530614,
      "grad_norm": 4.1814398765563965,
      "learning_rate": 3.497448979591837e-05,
      "loss": 0.6346,
      "step": 1531400
    },
    {
      "epoch": 24.042386185243327,
      "grad_norm": 4.711364269256592,
      "learning_rate": 3.497350863422292e-05,
      "loss": 0.5391,
      "step": 1531500
    },
    {
      "epoch": 24.043956043956044,
      "grad_norm": 5.200077533721924,
      "learning_rate": 3.497252747252747e-05,
      "loss": 0.5952,
      "step": 1531600
    },
    {
      "epoch": 24.04552590266876,
      "grad_norm": 4.2467732429504395,
      "learning_rate": 3.497154631083203e-05,
      "loss": 0.6053,
      "step": 1531700
    },
    {
      "epoch": 24.047095761381474,
      "grad_norm": 3.584559679031372,
      "learning_rate": 3.497056514913658e-05,
      "loss": 0.5976,
      "step": 1531800
    },
    {
      "epoch": 24.04866562009419,
      "grad_norm": 4.028826713562012,
      "learning_rate": 3.496958398744113e-05,
      "loss": 0.6094,
      "step": 1531900
    },
    {
      "epoch": 24.050235478806908,
      "grad_norm": 4.78374719619751,
      "learning_rate": 3.496860282574568e-05,
      "loss": 0.6239,
      "step": 1532000
    },
    {
      "epoch": 24.051805337519625,
      "grad_norm": 4.747953414916992,
      "learning_rate": 3.496762166405024e-05,
      "loss": 0.5679,
      "step": 1532100
    },
    {
      "epoch": 24.053375196232338,
      "grad_norm": 2.703549385070801,
      "learning_rate": 3.496664050235479e-05,
      "loss": 0.621,
      "step": 1532200
    },
    {
      "epoch": 24.054945054945055,
      "grad_norm": 2.563957929611206,
      "learning_rate": 3.496565934065934e-05,
      "loss": 0.6388,
      "step": 1532300
    },
    {
      "epoch": 24.05651491365777,
      "grad_norm": 4.809113502502441,
      "learning_rate": 3.496467817896389e-05,
      "loss": 0.593,
      "step": 1532400
    },
    {
      "epoch": 24.058084772370485,
      "grad_norm": 3.9299306869506836,
      "learning_rate": 3.496369701726845e-05,
      "loss": 0.5646,
      "step": 1532500
    },
    {
      "epoch": 24.059654631083202,
      "grad_norm": 3.958735227584839,
      "learning_rate": 3.4962715855573e-05,
      "loss": 0.595,
      "step": 1532600
    },
    {
      "epoch": 24.06122448979592,
      "grad_norm": 3.4275341033935547,
      "learning_rate": 3.496173469387755e-05,
      "loss": 0.5691,
      "step": 1532700
    },
    {
      "epoch": 24.062794348508636,
      "grad_norm": 2.710853338241577,
      "learning_rate": 3.4960753532182104e-05,
      "loss": 0.5923,
      "step": 1532800
    },
    {
      "epoch": 24.06436420722135,
      "grad_norm": 3.885749340057373,
      "learning_rate": 3.495977237048666e-05,
      "loss": 0.6236,
      "step": 1532900
    },
    {
      "epoch": 24.065934065934066,
      "grad_norm": 3.3003475666046143,
      "learning_rate": 3.4958791208791206e-05,
      "loss": 0.5518,
      "step": 1533000
    },
    {
      "epoch": 24.067503924646783,
      "grad_norm": 4.1130876541137695,
      "learning_rate": 3.4957810047095764e-05,
      "loss": 0.5716,
      "step": 1533100
    },
    {
      "epoch": 24.069073783359496,
      "grad_norm": 3.6380181312561035,
      "learning_rate": 3.4956828885400315e-05,
      "loss": 0.5698,
      "step": 1533200
    },
    {
      "epoch": 24.070643642072213,
      "grad_norm": 3.6983482837677,
      "learning_rate": 3.495584772370487e-05,
      "loss": 0.5899,
      "step": 1533300
    },
    {
      "epoch": 24.07221350078493,
      "grad_norm": 3.1557483673095703,
      "learning_rate": 3.495486656200942e-05,
      "loss": 0.6546,
      "step": 1533400
    },
    {
      "epoch": 24.073783359497646,
      "grad_norm": 3.6443803310394287,
      "learning_rate": 3.4953885400313974e-05,
      "loss": 0.5851,
      "step": 1533500
    },
    {
      "epoch": 24.07535321821036,
      "grad_norm": 4.632476806640625,
      "learning_rate": 3.4952904238618525e-05,
      "loss": 0.5937,
      "step": 1533600
    },
    {
      "epoch": 24.076923076923077,
      "grad_norm": 4.161469459533691,
      "learning_rate": 3.4951923076923076e-05,
      "loss": 0.6,
      "step": 1533700
    },
    {
      "epoch": 24.078492935635794,
      "grad_norm": 3.041546106338501,
      "learning_rate": 3.4950941915227634e-05,
      "loss": 0.5762,
      "step": 1533800
    },
    {
      "epoch": 24.08006279434851,
      "grad_norm": 3.3552908897399902,
      "learning_rate": 3.4949960753532185e-05,
      "loss": 0.6018,
      "step": 1533900
    },
    {
      "epoch": 24.081632653061224,
      "grad_norm": 2.34781813621521,
      "learning_rate": 3.4948979591836736e-05,
      "loss": 0.6239,
      "step": 1534000
    },
    {
      "epoch": 24.08320251177394,
      "grad_norm": 5.090051174163818,
      "learning_rate": 3.494799843014129e-05,
      "loss": 0.5959,
      "step": 1534100
    },
    {
      "epoch": 24.084772370486657,
      "grad_norm": 3.618833303451538,
      "learning_rate": 3.4947017268445845e-05,
      "loss": 0.5934,
      "step": 1534200
    },
    {
      "epoch": 24.08634222919937,
      "grad_norm": 4.239527702331543,
      "learning_rate": 3.4946036106750396e-05,
      "loss": 0.5728,
      "step": 1534300
    },
    {
      "epoch": 24.087912087912088,
      "grad_norm": 4.510344982147217,
      "learning_rate": 3.494505494505495e-05,
      "loss": 0.5654,
      "step": 1534400
    },
    {
      "epoch": 24.089481946624804,
      "grad_norm": 3.1867127418518066,
      "learning_rate": 3.49440737833595e-05,
      "loss": 0.6458,
      "step": 1534500
    },
    {
      "epoch": 24.09105180533752,
      "grad_norm": 4.412155628204346,
      "learning_rate": 3.4943092621664055e-05,
      "loss": 0.6226,
      "step": 1534600
    },
    {
      "epoch": 24.092621664050235,
      "grad_norm": 4.239893913269043,
      "learning_rate": 3.4942111459968606e-05,
      "loss": 0.5703,
      "step": 1534700
    },
    {
      "epoch": 24.09419152276295,
      "grad_norm": 3.682978868484497,
      "learning_rate": 3.494113029827316e-05,
      "loss": 0.6313,
      "step": 1534800
    },
    {
      "epoch": 24.09576138147567,
      "grad_norm": 4.516195297241211,
      "learning_rate": 3.494014913657771e-05,
      "loss": 0.5868,
      "step": 1534900
    },
    {
      "epoch": 24.09733124018838,
      "grad_norm": 3.4683072566986084,
      "learning_rate": 3.4939167974882266e-05,
      "loss": 0.5616,
      "step": 1535000
    },
    {
      "epoch": 24.0989010989011,
      "grad_norm": 4.393988132476807,
      "learning_rate": 3.493818681318681e-05,
      "loss": 0.6149,
      "step": 1535100
    },
    {
      "epoch": 24.100470957613815,
      "grad_norm": 4.353365898132324,
      "learning_rate": 3.493720565149137e-05,
      "loss": 0.6525,
      "step": 1535200
    },
    {
      "epoch": 24.102040816326532,
      "grad_norm": 3.459167957305908,
      "learning_rate": 3.493622448979592e-05,
      "loss": 0.6042,
      "step": 1535300
    },
    {
      "epoch": 24.103610675039246,
      "grad_norm": 5.120932102203369,
      "learning_rate": 3.493524332810048e-05,
      "loss": 0.6137,
      "step": 1535400
    },
    {
      "epoch": 24.105180533751962,
      "grad_norm": 3.5067431926727295,
      "learning_rate": 3.493426216640502e-05,
      "loss": 0.5873,
      "step": 1535500
    },
    {
      "epoch": 24.10675039246468,
      "grad_norm": 3.91754150390625,
      "learning_rate": 3.493328100470958e-05,
      "loss": 0.6044,
      "step": 1535600
    },
    {
      "epoch": 24.108320251177393,
      "grad_norm": 2.979732036590576,
      "learning_rate": 3.493229984301413e-05,
      "loss": 0.5917,
      "step": 1535700
    },
    {
      "epoch": 24.10989010989011,
      "grad_norm": 4.609533309936523,
      "learning_rate": 3.493131868131868e-05,
      "loss": 0.6551,
      "step": 1535800
    },
    {
      "epoch": 24.111459968602826,
      "grad_norm": 4.491593360900879,
      "learning_rate": 3.493033751962323e-05,
      "loss": 0.6195,
      "step": 1535900
    },
    {
      "epoch": 24.113029827315543,
      "grad_norm": 2.6688554286956787,
      "learning_rate": 3.492935635792779e-05,
      "loss": 0.6083,
      "step": 1536000
    },
    {
      "epoch": 24.114599686028257,
      "grad_norm": 4.817562103271484,
      "learning_rate": 3.492837519623234e-05,
      "loss": 0.6123,
      "step": 1536100
    },
    {
      "epoch": 24.116169544740973,
      "grad_norm": 3.905613899230957,
      "learning_rate": 3.492739403453689e-05,
      "loss": 0.5822,
      "step": 1536200
    },
    {
      "epoch": 24.11773940345369,
      "grad_norm": 5.351590156555176,
      "learning_rate": 3.492641287284145e-05,
      "loss": 0.5758,
      "step": 1536300
    },
    {
      "epoch": 24.119309262166404,
      "grad_norm": 4.089953422546387,
      "learning_rate": 3.4925431711146e-05,
      "loss": 0.5876,
      "step": 1536400
    },
    {
      "epoch": 24.12087912087912,
      "grad_norm": 3.2580809593200684,
      "learning_rate": 3.492445054945055e-05,
      "loss": 0.5829,
      "step": 1536500
    },
    {
      "epoch": 24.122448979591837,
      "grad_norm": 3.7493629455566406,
      "learning_rate": 3.49234693877551e-05,
      "loss": 0.6023,
      "step": 1536600
    },
    {
      "epoch": 24.124018838304554,
      "grad_norm": 4.954977035522461,
      "learning_rate": 3.492248822605966e-05,
      "loss": 0.5816,
      "step": 1536700
    },
    {
      "epoch": 24.125588697017267,
      "grad_norm": 4.748043060302734,
      "learning_rate": 3.4921507064364204e-05,
      "loss": 0.5585,
      "step": 1536800
    },
    {
      "epoch": 24.127158555729984,
      "grad_norm": 2.664541721343994,
      "learning_rate": 3.492052590266876e-05,
      "loss": 0.596,
      "step": 1536900
    },
    {
      "epoch": 24.1287284144427,
      "grad_norm": 3.7809972763061523,
      "learning_rate": 3.491954474097331e-05,
      "loss": 0.6183,
      "step": 1537000
    },
    {
      "epoch": 24.130298273155415,
      "grad_norm": 3.9891037940979004,
      "learning_rate": 3.491856357927787e-05,
      "loss": 0.6146,
      "step": 1537100
    },
    {
      "epoch": 24.13186813186813,
      "grad_norm": 3.9643516540527344,
      "learning_rate": 3.4917582417582415e-05,
      "loss": 0.567,
      "step": 1537200
    },
    {
      "epoch": 24.13343799058085,
      "grad_norm": 2.8695578575134277,
      "learning_rate": 3.491660125588697e-05,
      "loss": 0.6037,
      "step": 1537300
    },
    {
      "epoch": 24.135007849293565,
      "grad_norm": 4.6337714195251465,
      "learning_rate": 3.4915620094191524e-05,
      "loss": 0.615,
      "step": 1537400
    },
    {
      "epoch": 24.13657770800628,
      "grad_norm": 3.7769532203674316,
      "learning_rate": 3.4914638932496075e-05,
      "loss": 0.5898,
      "step": 1537500
    },
    {
      "epoch": 24.138147566718995,
      "grad_norm": 3.7959580421447754,
      "learning_rate": 3.4913657770800626e-05,
      "loss": 0.6009,
      "step": 1537600
    },
    {
      "epoch": 24.139717425431712,
      "grad_norm": 3.903811454772949,
      "learning_rate": 3.491267660910518e-05,
      "loss": 0.6004,
      "step": 1537700
    },
    {
      "epoch": 24.141287284144425,
      "grad_norm": 3.0035924911499023,
      "learning_rate": 3.4911695447409734e-05,
      "loss": 0.5929,
      "step": 1537800
    },
    {
      "epoch": 24.142857142857142,
      "grad_norm": 3.870300531387329,
      "learning_rate": 3.4910714285714285e-05,
      "loss": 0.6249,
      "step": 1537900
    },
    {
      "epoch": 24.14442700156986,
      "grad_norm": 4.490640640258789,
      "learning_rate": 3.4909733124018836e-05,
      "loss": 0.6528,
      "step": 1538000
    },
    {
      "epoch": 24.145996860282576,
      "grad_norm": 4.0763983726501465,
      "learning_rate": 3.4908751962323394e-05,
      "loss": 0.6128,
      "step": 1538100
    },
    {
      "epoch": 24.14756671899529,
      "grad_norm": 3.7335193157196045,
      "learning_rate": 3.4907770800627945e-05,
      "loss": 0.6236,
      "step": 1538200
    },
    {
      "epoch": 24.149136577708006,
      "grad_norm": 3.2368388175964355,
      "learning_rate": 3.4906789638932496e-05,
      "loss": 0.5726,
      "step": 1538300
    },
    {
      "epoch": 24.150706436420723,
      "grad_norm": 4.0494256019592285,
      "learning_rate": 3.4905808477237054e-05,
      "loss": 0.5887,
      "step": 1538400
    },
    {
      "epoch": 24.152276295133436,
      "grad_norm": 3.2708141803741455,
      "learning_rate": 3.4904827315541605e-05,
      "loss": 0.5758,
      "step": 1538500
    },
    {
      "epoch": 24.153846153846153,
      "grad_norm": 4.644728660583496,
      "learning_rate": 3.4903846153846156e-05,
      "loss": 0.5855,
      "step": 1538600
    },
    {
      "epoch": 24.15541601255887,
      "grad_norm": 4.662303924560547,
      "learning_rate": 3.490286499215071e-05,
      "loss": 0.6334,
      "step": 1538700
    },
    {
      "epoch": 24.156985871271587,
      "grad_norm": 4.4116315841674805,
      "learning_rate": 3.4901883830455264e-05,
      "loss": 0.5616,
      "step": 1538800
    },
    {
      "epoch": 24.1585557299843,
      "grad_norm": 4.6265177726745605,
      "learning_rate": 3.490090266875981e-05,
      "loss": 0.5884,
      "step": 1538900
    },
    {
      "epoch": 24.160125588697017,
      "grad_norm": 5.434906482696533,
      "learning_rate": 3.4899921507064366e-05,
      "loss": 0.5933,
      "step": 1539000
    },
    {
      "epoch": 24.161695447409734,
      "grad_norm": 4.046765327453613,
      "learning_rate": 3.489894034536892e-05,
      "loss": 0.5934,
      "step": 1539100
    },
    {
      "epoch": 24.163265306122447,
      "grad_norm": 4.266005516052246,
      "learning_rate": 3.4897959183673475e-05,
      "loss": 0.6149,
      "step": 1539200
    },
    {
      "epoch": 24.164835164835164,
      "grad_norm": 3.3865106105804443,
      "learning_rate": 3.489697802197802e-05,
      "loss": 0.6201,
      "step": 1539300
    },
    {
      "epoch": 24.16640502354788,
      "grad_norm": 2.8302109241485596,
      "learning_rate": 3.489599686028258e-05,
      "loss": 0.5656,
      "step": 1539400
    },
    {
      "epoch": 24.167974882260598,
      "grad_norm": 3.21734619140625,
      "learning_rate": 3.489501569858713e-05,
      "loss": 0.6133,
      "step": 1539500
    },
    {
      "epoch": 24.16954474097331,
      "grad_norm": 4.545544624328613,
      "learning_rate": 3.489403453689168e-05,
      "loss": 0.6081,
      "step": 1539600
    },
    {
      "epoch": 24.171114599686028,
      "grad_norm": 4.182473659515381,
      "learning_rate": 3.489305337519623e-05,
      "loss": 0.5995,
      "step": 1539700
    },
    {
      "epoch": 24.172684458398745,
      "grad_norm": 3.766451358795166,
      "learning_rate": 3.489207221350079e-05,
      "loss": 0.5634,
      "step": 1539800
    },
    {
      "epoch": 24.17425431711146,
      "grad_norm": 3.86316180229187,
      "learning_rate": 3.489109105180534e-05,
      "loss": 0.5908,
      "step": 1539900
    },
    {
      "epoch": 24.175824175824175,
      "grad_norm": 3.2028143405914307,
      "learning_rate": 3.489010989010989e-05,
      "loss": 0.5857,
      "step": 1540000
    },
    {
      "epoch": 24.177394034536892,
      "grad_norm": 3.267780303955078,
      "learning_rate": 3.488912872841444e-05,
      "loss": 0.584,
      "step": 1540100
    },
    {
      "epoch": 24.17896389324961,
      "grad_norm": 4.285400867462158,
      "learning_rate": 3.4888147566719e-05,
      "loss": 0.6219,
      "step": 1540200
    },
    {
      "epoch": 24.180533751962322,
      "grad_norm": 4.623331546783447,
      "learning_rate": 3.488716640502355e-05,
      "loss": 0.6001,
      "step": 1540300
    },
    {
      "epoch": 24.18210361067504,
      "grad_norm": 3.0491955280303955,
      "learning_rate": 3.48861852433281e-05,
      "loss": 0.5923,
      "step": 1540400
    },
    {
      "epoch": 24.183673469387756,
      "grad_norm": 3.4786694049835205,
      "learning_rate": 3.488520408163266e-05,
      "loss": 0.5967,
      "step": 1540500
    },
    {
      "epoch": 24.18524332810047,
      "grad_norm": 2.522125720977783,
      "learning_rate": 3.488422291993721e-05,
      "loss": 0.5955,
      "step": 1540600
    },
    {
      "epoch": 24.186813186813186,
      "grad_norm": 4.370066165924072,
      "learning_rate": 3.488324175824176e-05,
      "loss": 0.6125,
      "step": 1540700
    },
    {
      "epoch": 24.188383045525903,
      "grad_norm": 3.654489040374756,
      "learning_rate": 3.488226059654631e-05,
      "loss": 0.5575,
      "step": 1540800
    },
    {
      "epoch": 24.18995290423862,
      "grad_norm": 4.243753910064697,
      "learning_rate": 3.488127943485087e-05,
      "loss": 0.5925,
      "step": 1540900
    },
    {
      "epoch": 24.191522762951333,
      "grad_norm": 3.2263166904449463,
      "learning_rate": 3.488029827315541e-05,
      "loss": 0.6146,
      "step": 1541000
    },
    {
      "epoch": 24.19309262166405,
      "grad_norm": 4.080502033233643,
      "learning_rate": 3.487931711145997e-05,
      "loss": 0.5573,
      "step": 1541100
    },
    {
      "epoch": 24.194662480376767,
      "grad_norm": 4.348377704620361,
      "learning_rate": 3.487833594976452e-05,
      "loss": 0.6326,
      "step": 1541200
    },
    {
      "epoch": 24.19623233908948,
      "grad_norm": 2.328002452850342,
      "learning_rate": 3.487735478806908e-05,
      "loss": 0.6003,
      "step": 1541300
    },
    {
      "epoch": 24.197802197802197,
      "grad_norm": 3.8937392234802246,
      "learning_rate": 3.4876373626373624e-05,
      "loss": 0.5884,
      "step": 1541400
    },
    {
      "epoch": 24.199372056514914,
      "grad_norm": 4.624122142791748,
      "learning_rate": 3.487539246467818e-05,
      "loss": 0.6424,
      "step": 1541500
    },
    {
      "epoch": 24.20094191522763,
      "grad_norm": 3.330282211303711,
      "learning_rate": 3.487441130298273e-05,
      "loss": 0.6131,
      "step": 1541600
    },
    {
      "epoch": 24.202511773940344,
      "grad_norm": 4.795215606689453,
      "learning_rate": 3.4873430141287284e-05,
      "loss": 0.6261,
      "step": 1541700
    },
    {
      "epoch": 24.20408163265306,
      "grad_norm": 4.7593231201171875,
      "learning_rate": 3.4872448979591835e-05,
      "loss": 0.6268,
      "step": 1541800
    },
    {
      "epoch": 24.205651491365778,
      "grad_norm": 3.963381290435791,
      "learning_rate": 3.487146781789639e-05,
      "loss": 0.5888,
      "step": 1541900
    },
    {
      "epoch": 24.20722135007849,
      "grad_norm": 4.690705299377441,
      "learning_rate": 3.487048665620094e-05,
      "loss": 0.5921,
      "step": 1542000
    },
    {
      "epoch": 24.208791208791208,
      "grad_norm": 3.7431392669677734,
      "learning_rate": 3.4869505494505494e-05,
      "loss": 0.5948,
      "step": 1542100
    },
    {
      "epoch": 24.210361067503925,
      "grad_norm": 3.5858426094055176,
      "learning_rate": 3.4868524332810045e-05,
      "loss": 0.5749,
      "step": 1542200
    },
    {
      "epoch": 24.211930926216642,
      "grad_norm": 4.210327625274658,
      "learning_rate": 3.48675431711146e-05,
      "loss": 0.5884,
      "step": 1542300
    },
    {
      "epoch": 24.213500784929355,
      "grad_norm": 3.8593127727508545,
      "learning_rate": 3.4866562009419154e-05,
      "loss": 0.597,
      "step": 1542400
    },
    {
      "epoch": 24.215070643642072,
      "grad_norm": 4.7752485275268555,
      "learning_rate": 3.4865580847723705e-05,
      "loss": 0.5651,
      "step": 1542500
    },
    {
      "epoch": 24.21664050235479,
      "grad_norm": 3.9212512969970703,
      "learning_rate": 3.486459968602826e-05,
      "loss": 0.5672,
      "step": 1542600
    },
    {
      "epoch": 24.218210361067506,
      "grad_norm": 3.1489291191101074,
      "learning_rate": 3.4863618524332814e-05,
      "loss": 0.6049,
      "step": 1542700
    },
    {
      "epoch": 24.21978021978022,
      "grad_norm": 2.3204479217529297,
      "learning_rate": 3.4862637362637365e-05,
      "loss": 0.5498,
      "step": 1542800
    },
    {
      "epoch": 24.221350078492936,
      "grad_norm": 4.8212456703186035,
      "learning_rate": 3.4861656200941916e-05,
      "loss": 0.568,
      "step": 1542900
    },
    {
      "epoch": 24.222919937205653,
      "grad_norm": 3.3158633708953857,
      "learning_rate": 3.4860675039246473e-05,
      "loss": 0.5369,
      "step": 1543000
    },
    {
      "epoch": 24.224489795918366,
      "grad_norm": 4.592608451843262,
      "learning_rate": 3.485969387755102e-05,
      "loss": 0.6008,
      "step": 1543100
    },
    {
      "epoch": 24.226059654631083,
      "grad_norm": 4.20449161529541,
      "learning_rate": 3.4858712715855575e-05,
      "loss": 0.5895,
      "step": 1543200
    },
    {
      "epoch": 24.2276295133438,
      "grad_norm": 3.6648449897766113,
      "learning_rate": 3.4857731554160126e-05,
      "loss": 0.5998,
      "step": 1543300
    },
    {
      "epoch": 24.229199372056517,
      "grad_norm": 3.1804049015045166,
      "learning_rate": 3.4856750392464684e-05,
      "loss": 0.611,
      "step": 1543400
    },
    {
      "epoch": 24.23076923076923,
      "grad_norm": 3.882934093475342,
      "learning_rate": 3.485576923076923e-05,
      "loss": 0.5944,
      "step": 1543500
    },
    {
      "epoch": 24.232339089481947,
      "grad_norm": 3.876934289932251,
      "learning_rate": 3.4854788069073786e-05,
      "loss": 0.6476,
      "step": 1543600
    },
    {
      "epoch": 24.233908948194664,
      "grad_norm": 3.3205819129943848,
      "learning_rate": 3.485380690737834e-05,
      "loss": 0.612,
      "step": 1543700
    },
    {
      "epoch": 24.235478806907377,
      "grad_norm": 3.49889874458313,
      "learning_rate": 3.485282574568289e-05,
      "loss": 0.6059,
      "step": 1543800
    },
    {
      "epoch": 24.237048665620094,
      "grad_norm": 4.1104631423950195,
      "learning_rate": 3.485184458398744e-05,
      "loss": 0.5677,
      "step": 1543900
    },
    {
      "epoch": 24.23861852433281,
      "grad_norm": 4.324333667755127,
      "learning_rate": 3.4850863422292e-05,
      "loss": 0.5965,
      "step": 1544000
    },
    {
      "epoch": 24.240188383045528,
      "grad_norm": 4.679329872131348,
      "learning_rate": 3.484988226059655e-05,
      "loss": 0.5595,
      "step": 1544100
    },
    {
      "epoch": 24.24175824175824,
      "grad_norm": 3.343167304992676,
      "learning_rate": 3.48489010989011e-05,
      "loss": 0.6143,
      "step": 1544200
    },
    {
      "epoch": 24.243328100470958,
      "grad_norm": 4.845849990844727,
      "learning_rate": 3.484791993720565e-05,
      "loss": 0.5721,
      "step": 1544300
    },
    {
      "epoch": 24.244897959183675,
      "grad_norm": 4.3696088790893555,
      "learning_rate": 3.484693877551021e-05,
      "loss": 0.625,
      "step": 1544400
    },
    {
      "epoch": 24.246467817896388,
      "grad_norm": 3.1182854175567627,
      "learning_rate": 3.484595761381476e-05,
      "loss": 0.6033,
      "step": 1544500
    },
    {
      "epoch": 24.248037676609105,
      "grad_norm": 3.2736587524414062,
      "learning_rate": 3.484497645211931e-05,
      "loss": 0.5977,
      "step": 1544600
    },
    {
      "epoch": 24.24960753532182,
      "grad_norm": 4.112000942230225,
      "learning_rate": 3.484399529042387e-05,
      "loss": 0.5742,
      "step": 1544700
    },
    {
      "epoch": 24.25117739403454,
      "grad_norm": 3.5827627182006836,
      "learning_rate": 3.484301412872842e-05,
      "loss": 0.6106,
      "step": 1544800
    },
    {
      "epoch": 24.252747252747252,
      "grad_norm": 2.6419148445129395,
      "learning_rate": 3.484203296703297e-05,
      "loss": 0.6325,
      "step": 1544900
    },
    {
      "epoch": 24.25431711145997,
      "grad_norm": 4.202693462371826,
      "learning_rate": 3.484105180533752e-05,
      "loss": 0.5645,
      "step": 1545000
    },
    {
      "epoch": 24.255886970172686,
      "grad_norm": 4.23911190032959,
      "learning_rate": 3.484007064364208e-05,
      "loss": 0.6107,
      "step": 1545100
    },
    {
      "epoch": 24.2574568288854,
      "grad_norm": 4.501416206359863,
      "learning_rate": 3.483908948194662e-05,
      "loss": 0.5783,
      "step": 1545200
    },
    {
      "epoch": 24.259026687598116,
      "grad_norm": 3.7919998168945312,
      "learning_rate": 3.483810832025118e-05,
      "loss": 0.5566,
      "step": 1545300
    },
    {
      "epoch": 24.260596546310833,
      "grad_norm": 3.762263536453247,
      "learning_rate": 3.483712715855573e-05,
      "loss": 0.6101,
      "step": 1545400
    },
    {
      "epoch": 24.26216640502355,
      "grad_norm": 3.086414337158203,
      "learning_rate": 3.483614599686029e-05,
      "loss": 0.5857,
      "step": 1545500
    },
    {
      "epoch": 24.263736263736263,
      "grad_norm": 4.879527568817139,
      "learning_rate": 3.483516483516483e-05,
      "loss": 0.6206,
      "step": 1545600
    },
    {
      "epoch": 24.26530612244898,
      "grad_norm": 5.762522220611572,
      "learning_rate": 3.483418367346939e-05,
      "loss": 0.6324,
      "step": 1545700
    },
    {
      "epoch": 24.266875981161697,
      "grad_norm": 3.2690892219543457,
      "learning_rate": 3.483320251177394e-05,
      "loss": 0.6033,
      "step": 1545800
    },
    {
      "epoch": 24.26844583987441,
      "grad_norm": 2.8125081062316895,
      "learning_rate": 3.483222135007849e-05,
      "loss": 0.5943,
      "step": 1545900
    },
    {
      "epoch": 24.270015698587127,
      "grad_norm": 4.215839862823486,
      "learning_rate": 3.4831240188383044e-05,
      "loss": 0.5844,
      "step": 1546000
    },
    {
      "epoch": 24.271585557299844,
      "grad_norm": 3.8863682746887207,
      "learning_rate": 3.48302590266876e-05,
      "loss": 0.5668,
      "step": 1546100
    },
    {
      "epoch": 24.27315541601256,
      "grad_norm": 2.901214838027954,
      "learning_rate": 3.482927786499215e-05,
      "loss": 0.5532,
      "step": 1546200
    },
    {
      "epoch": 24.274725274725274,
      "grad_norm": 4.868537902832031,
      "learning_rate": 3.48282967032967e-05,
      "loss": 0.6117,
      "step": 1546300
    },
    {
      "epoch": 24.27629513343799,
      "grad_norm": 2.463538646697998,
      "learning_rate": 3.4827315541601254e-05,
      "loss": 0.5904,
      "step": 1546400
    },
    {
      "epoch": 24.277864992150707,
      "grad_norm": 3.434332847595215,
      "learning_rate": 3.482633437990581e-05,
      "loss": 0.6025,
      "step": 1546500
    },
    {
      "epoch": 24.27943485086342,
      "grad_norm": 4.727006912231445,
      "learning_rate": 3.482535321821036e-05,
      "loss": 0.5985,
      "step": 1546600
    },
    {
      "epoch": 24.281004709576138,
      "grad_norm": 3.494547128677368,
      "learning_rate": 3.4824372056514914e-05,
      "loss": 0.6153,
      "step": 1546700
    },
    {
      "epoch": 24.282574568288855,
      "grad_norm": 4.866305828094482,
      "learning_rate": 3.482339089481947e-05,
      "loss": 0.5934,
      "step": 1546800
    },
    {
      "epoch": 24.28414442700157,
      "grad_norm": 3.552807092666626,
      "learning_rate": 3.482240973312402e-05,
      "loss": 0.5799,
      "step": 1546900
    },
    {
      "epoch": 24.285714285714285,
      "grad_norm": 4.1556291580200195,
      "learning_rate": 3.4821428571428574e-05,
      "loss": 0.5967,
      "step": 1547000
    },
    {
      "epoch": 24.287284144427,
      "grad_norm": 3.220881938934326,
      "learning_rate": 3.4820447409733125e-05,
      "loss": 0.5699,
      "step": 1547100
    },
    {
      "epoch": 24.28885400313972,
      "grad_norm": 3.7666454315185547,
      "learning_rate": 3.481946624803768e-05,
      "loss": 0.6164,
      "step": 1547200
    },
    {
      "epoch": 24.29042386185243,
      "grad_norm": 3.5822572708129883,
      "learning_rate": 3.481848508634223e-05,
      "loss": 0.5572,
      "step": 1547300
    },
    {
      "epoch": 24.29199372056515,
      "grad_norm": 4.579113960266113,
      "learning_rate": 3.4817503924646784e-05,
      "loss": 0.6023,
      "step": 1547400
    },
    {
      "epoch": 24.293563579277865,
      "grad_norm": 4.059473514556885,
      "learning_rate": 3.4816522762951335e-05,
      "loss": 0.5836,
      "step": 1547500
    },
    {
      "epoch": 24.295133437990582,
      "grad_norm": 5.1872172355651855,
      "learning_rate": 3.481554160125589e-05,
      "loss": 0.5952,
      "step": 1547600
    },
    {
      "epoch": 24.296703296703296,
      "grad_norm": 3.233729600906372,
      "learning_rate": 3.481456043956044e-05,
      "loss": 0.5785,
      "step": 1547700
    },
    {
      "epoch": 24.298273155416013,
      "grad_norm": 4.346722602844238,
      "learning_rate": 3.4813579277864995e-05,
      "loss": 0.6147,
      "step": 1547800
    },
    {
      "epoch": 24.29984301412873,
      "grad_norm": 3.6369781494140625,
      "learning_rate": 3.4812598116169546e-05,
      "loss": 0.5687,
      "step": 1547900
    },
    {
      "epoch": 24.301412872841443,
      "grad_norm": 4.130122184753418,
      "learning_rate": 3.48116169544741e-05,
      "loss": 0.6122,
      "step": 1548000
    },
    {
      "epoch": 24.30298273155416,
      "grad_norm": 3.620678424835205,
      "learning_rate": 3.481063579277865e-05,
      "loss": 0.6129,
      "step": 1548100
    },
    {
      "epoch": 24.304552590266876,
      "grad_norm": 4.119309425354004,
      "learning_rate": 3.4809654631083206e-05,
      "loss": 0.5851,
      "step": 1548200
    },
    {
      "epoch": 24.306122448979593,
      "grad_norm": 3.7013466358184814,
      "learning_rate": 3.480867346938776e-05,
      "loss": 0.6113,
      "step": 1548300
    },
    {
      "epoch": 24.307692307692307,
      "grad_norm": 4.843142509460449,
      "learning_rate": 3.480769230769231e-05,
      "loss": 0.6367,
      "step": 1548400
    },
    {
      "epoch": 24.309262166405023,
      "grad_norm": 2.709425210952759,
      "learning_rate": 3.480671114599686e-05,
      "loss": 0.5828,
      "step": 1548500
    },
    {
      "epoch": 24.31083202511774,
      "grad_norm": 4.478354454040527,
      "learning_rate": 3.4805729984301417e-05,
      "loss": 0.5637,
      "step": 1548600
    },
    {
      "epoch": 24.312401883830454,
      "grad_norm": 4.568445205688477,
      "learning_rate": 3.480474882260597e-05,
      "loss": 0.5826,
      "step": 1548700
    },
    {
      "epoch": 24.31397174254317,
      "grad_norm": 4.2055559158325195,
      "learning_rate": 3.480376766091052e-05,
      "loss": 0.6164,
      "step": 1548800
    },
    {
      "epoch": 24.315541601255887,
      "grad_norm": 2.016911745071411,
      "learning_rate": 3.4802786499215076e-05,
      "loss": 0.6221,
      "step": 1548900
    },
    {
      "epoch": 24.317111459968604,
      "grad_norm": 4.188059329986572,
      "learning_rate": 3.480180533751963e-05,
      "loss": 0.6025,
      "step": 1549000
    },
    {
      "epoch": 24.318681318681318,
      "grad_norm": 4.133340835571289,
      "learning_rate": 3.480082417582418e-05,
      "loss": 0.5731,
      "step": 1549100
    },
    {
      "epoch": 24.320251177394034,
      "grad_norm": 3.7266995906829834,
      "learning_rate": 3.479984301412873e-05,
      "loss": 0.5945,
      "step": 1549200
    },
    {
      "epoch": 24.32182103610675,
      "grad_norm": 3.822650194168091,
      "learning_rate": 3.479886185243329e-05,
      "loss": 0.6112,
      "step": 1549300
    },
    {
      "epoch": 24.323390894819465,
      "grad_norm": 3.585575819015503,
      "learning_rate": 3.479788069073783e-05,
      "loss": 0.5828,
      "step": 1549400
    },
    {
      "epoch": 24.32496075353218,
      "grad_norm": 2.686033010482788,
      "learning_rate": 3.479689952904239e-05,
      "loss": 0.5733,
      "step": 1549500
    },
    {
      "epoch": 24.3265306122449,
      "grad_norm": 4.174138069152832,
      "learning_rate": 3.479591836734694e-05,
      "loss": 0.6291,
      "step": 1549600
    },
    {
      "epoch": 24.328100470957615,
      "grad_norm": 2.646923065185547,
      "learning_rate": 3.47949372056515e-05,
      "loss": 0.5992,
      "step": 1549700
    },
    {
      "epoch": 24.32967032967033,
      "grad_norm": 3.883946180343628,
      "learning_rate": 3.479395604395604e-05,
      "loss": 0.6262,
      "step": 1549800
    },
    {
      "epoch": 24.331240188383045,
      "grad_norm": 3.9434027671813965,
      "learning_rate": 3.47929748822606e-05,
      "loss": 0.5895,
      "step": 1549900
    },
    {
      "epoch": 24.332810047095762,
      "grad_norm": 3.015331983566284,
      "learning_rate": 3.479199372056515e-05,
      "loss": 0.5863,
      "step": 1550000
    },
    {
      "epoch": 24.334379905808476,
      "grad_norm": 4.489453315734863,
      "learning_rate": 3.47910125588697e-05,
      "loss": 0.6058,
      "step": 1550100
    },
    {
      "epoch": 24.335949764521192,
      "grad_norm": 4.54892110824585,
      "learning_rate": 3.479003139717425e-05,
      "loss": 0.5614,
      "step": 1550200
    },
    {
      "epoch": 24.33751962323391,
      "grad_norm": 3.8839685916900635,
      "learning_rate": 3.478905023547881e-05,
      "loss": 0.6249,
      "step": 1550300
    },
    {
      "epoch": 24.339089481946626,
      "grad_norm": 2.773467540740967,
      "learning_rate": 3.478806907378336e-05,
      "loss": 0.5876,
      "step": 1550400
    },
    {
      "epoch": 24.34065934065934,
      "grad_norm": 2.1787211894989014,
      "learning_rate": 3.478708791208791e-05,
      "loss": 0.5659,
      "step": 1550500
    },
    {
      "epoch": 24.342229199372056,
      "grad_norm": 3.3072011470794678,
      "learning_rate": 3.478610675039246e-05,
      "loss": 0.622,
      "step": 1550600
    },
    {
      "epoch": 24.343799058084773,
      "grad_norm": 4.798628330230713,
      "learning_rate": 3.478512558869702e-05,
      "loss": 0.5817,
      "step": 1550700
    },
    {
      "epoch": 24.345368916797486,
      "grad_norm": 2.75667142868042,
      "learning_rate": 3.478414442700157e-05,
      "loss": 0.622,
      "step": 1550800
    },
    {
      "epoch": 24.346938775510203,
      "grad_norm": 4.603583335876465,
      "learning_rate": 3.478316326530612e-05,
      "loss": 0.6202,
      "step": 1550900
    },
    {
      "epoch": 24.34850863422292,
      "grad_norm": 2.5442495346069336,
      "learning_rate": 3.478218210361068e-05,
      "loss": 0.5973,
      "step": 1551000
    },
    {
      "epoch": 24.350078492935637,
      "grad_norm": 3.7371747493743896,
      "learning_rate": 3.478120094191523e-05,
      "loss": 0.5788,
      "step": 1551100
    },
    {
      "epoch": 24.35164835164835,
      "grad_norm": 3.3528850078582764,
      "learning_rate": 3.478021978021978e-05,
      "loss": 0.62,
      "step": 1551200
    },
    {
      "epoch": 24.353218210361067,
      "grad_norm": 3.071354627609253,
      "learning_rate": 3.4779238618524334e-05,
      "loss": 0.5955,
      "step": 1551300
    },
    {
      "epoch": 24.354788069073784,
      "grad_norm": 5.213931083679199,
      "learning_rate": 3.477825745682889e-05,
      "loss": 0.6022,
      "step": 1551400
    },
    {
      "epoch": 24.356357927786497,
      "grad_norm": 2.399906873703003,
      "learning_rate": 3.4777276295133436e-05,
      "loss": 0.5986,
      "step": 1551500
    },
    {
      "epoch": 24.357927786499214,
      "grad_norm": 4.210982322692871,
      "learning_rate": 3.477629513343799e-05,
      "loss": 0.6059,
      "step": 1551600
    },
    {
      "epoch": 24.35949764521193,
      "grad_norm": 3.6354167461395264,
      "learning_rate": 3.4775313971742544e-05,
      "loss": 0.6167,
      "step": 1551700
    },
    {
      "epoch": 24.361067503924648,
      "grad_norm": 3.6096692085266113,
      "learning_rate": 3.47743328100471e-05,
      "loss": 0.6027,
      "step": 1551800
    },
    {
      "epoch": 24.36263736263736,
      "grad_norm": 5.455696105957031,
      "learning_rate": 3.4773351648351646e-05,
      "loss": 0.6282,
      "step": 1551900
    },
    {
      "epoch": 24.364207221350078,
      "grad_norm": 3.717296838760376,
      "learning_rate": 3.4772370486656204e-05,
      "loss": 0.6333,
      "step": 1552000
    },
    {
      "epoch": 24.365777080062795,
      "grad_norm": 4.010329723358154,
      "learning_rate": 3.4771389324960755e-05,
      "loss": 0.57,
      "step": 1552100
    },
    {
      "epoch": 24.367346938775512,
      "grad_norm": 3.694279432296753,
      "learning_rate": 3.4770408163265306e-05,
      "loss": 0.5722,
      "step": 1552200
    },
    {
      "epoch": 24.368916797488225,
      "grad_norm": 3.939336061477661,
      "learning_rate": 3.476942700156986e-05,
      "loss": 0.6035,
      "step": 1552300
    },
    {
      "epoch": 24.370486656200942,
      "grad_norm": 3.971719980239868,
      "learning_rate": 3.4768445839874415e-05,
      "loss": 0.6177,
      "step": 1552400
    },
    {
      "epoch": 24.37205651491366,
      "grad_norm": 2.8524580001831055,
      "learning_rate": 3.4767464678178966e-05,
      "loss": 0.567,
      "step": 1552500
    },
    {
      "epoch": 24.373626373626372,
      "grad_norm": 3.6468350887298584,
      "learning_rate": 3.476648351648352e-05,
      "loss": 0.5726,
      "step": 1552600
    },
    {
      "epoch": 24.37519623233909,
      "grad_norm": 3.1742851734161377,
      "learning_rate": 3.476550235478807e-05,
      "loss": 0.5881,
      "step": 1552700
    },
    {
      "epoch": 24.376766091051806,
      "grad_norm": 3.783052444458008,
      "learning_rate": 3.4764521193092625e-05,
      "loss": 0.6181,
      "step": 1552800
    },
    {
      "epoch": 24.378335949764523,
      "grad_norm": 2.860948085784912,
      "learning_rate": 3.4763540031397176e-05,
      "loss": 0.5738,
      "step": 1552900
    },
    {
      "epoch": 24.379905808477236,
      "grad_norm": 3.9982223510742188,
      "learning_rate": 3.476255886970173e-05,
      "loss": 0.608,
      "step": 1553000
    },
    {
      "epoch": 24.381475667189953,
      "grad_norm": 4.506100654602051,
      "learning_rate": 3.4761577708006285e-05,
      "loss": 0.5552,
      "step": 1553100
    },
    {
      "epoch": 24.38304552590267,
      "grad_norm": 4.888779163360596,
      "learning_rate": 3.4760596546310836e-05,
      "loss": 0.5772,
      "step": 1553200
    },
    {
      "epoch": 24.384615384615383,
      "grad_norm": 4.8011674880981445,
      "learning_rate": 3.475961538461539e-05,
      "loss": 0.5701,
      "step": 1553300
    },
    {
      "epoch": 24.3861852433281,
      "grad_norm": 4.4513773918151855,
      "learning_rate": 3.475863422291994e-05,
      "loss": 0.6078,
      "step": 1553400
    },
    {
      "epoch": 24.387755102040817,
      "grad_norm": 5.154036998748779,
      "learning_rate": 3.4757653061224496e-05,
      "loss": 0.6018,
      "step": 1553500
    },
    {
      "epoch": 24.389324960753534,
      "grad_norm": 3.2173333168029785,
      "learning_rate": 3.475667189952904e-05,
      "loss": 0.6016,
      "step": 1553600
    },
    {
      "epoch": 24.390894819466247,
      "grad_norm": 3.1937673091888428,
      "learning_rate": 3.47556907378336e-05,
      "loss": 0.5825,
      "step": 1553700
    },
    {
      "epoch": 24.392464678178964,
      "grad_norm": 4.478264331817627,
      "learning_rate": 3.475470957613815e-05,
      "loss": 0.6081,
      "step": 1553800
    },
    {
      "epoch": 24.39403453689168,
      "grad_norm": 3.969780921936035,
      "learning_rate": 3.475372841444271e-05,
      "loss": 0.5955,
      "step": 1553900
    },
    {
      "epoch": 24.395604395604394,
      "grad_norm": 3.7269229888916016,
      "learning_rate": 3.475274725274725e-05,
      "loss": 0.6114,
      "step": 1554000
    },
    {
      "epoch": 24.39717425431711,
      "grad_norm": 3.4997010231018066,
      "learning_rate": 3.475176609105181e-05,
      "loss": 0.6086,
      "step": 1554100
    },
    {
      "epoch": 24.398744113029828,
      "grad_norm": 3.7959160804748535,
      "learning_rate": 3.475078492935636e-05,
      "loss": 0.6021,
      "step": 1554200
    },
    {
      "epoch": 24.400313971742545,
      "grad_norm": 4.13592004776001,
      "learning_rate": 3.474980376766091e-05,
      "loss": 0.6558,
      "step": 1554300
    },
    {
      "epoch": 24.401883830455258,
      "grad_norm": 3.0080084800720215,
      "learning_rate": 3.474882260596546e-05,
      "loss": 0.6164,
      "step": 1554400
    },
    {
      "epoch": 24.403453689167975,
      "grad_norm": 4.269444942474365,
      "learning_rate": 3.474784144427002e-05,
      "loss": 0.618,
      "step": 1554500
    },
    {
      "epoch": 24.405023547880692,
      "grad_norm": 3.2584030628204346,
      "learning_rate": 3.474686028257457e-05,
      "loss": 0.6243,
      "step": 1554600
    },
    {
      "epoch": 24.406593406593405,
      "grad_norm": 4.128249645233154,
      "learning_rate": 3.474587912087912e-05,
      "loss": 0.6145,
      "step": 1554700
    },
    {
      "epoch": 24.408163265306122,
      "grad_norm": 4.017021179199219,
      "learning_rate": 3.474489795918367e-05,
      "loss": 0.5774,
      "step": 1554800
    },
    {
      "epoch": 24.40973312401884,
      "grad_norm": 3.871981620788574,
      "learning_rate": 3.474391679748823e-05,
      "loss": 0.5806,
      "step": 1554900
    },
    {
      "epoch": 24.411302982731556,
      "grad_norm": 4.586923599243164,
      "learning_rate": 3.474293563579278e-05,
      "loss": 0.6052,
      "step": 1555000
    },
    {
      "epoch": 24.41287284144427,
      "grad_norm": 3.7773194313049316,
      "learning_rate": 3.474195447409733e-05,
      "loss": 0.5695,
      "step": 1555100
    },
    {
      "epoch": 24.414442700156986,
      "grad_norm": 4.169010639190674,
      "learning_rate": 3.474097331240189e-05,
      "loss": 0.6014,
      "step": 1555200
    },
    {
      "epoch": 24.416012558869703,
      "grad_norm": 3.8974990844726562,
      "learning_rate": 3.473999215070644e-05,
      "loss": 0.6314,
      "step": 1555300
    },
    {
      "epoch": 24.417582417582416,
      "grad_norm": 2.5448715686798096,
      "learning_rate": 3.473901098901099e-05,
      "loss": 0.604,
      "step": 1555400
    },
    {
      "epoch": 24.419152276295133,
      "grad_norm": 4.006743907928467,
      "learning_rate": 3.473802982731554e-05,
      "loss": 0.6121,
      "step": 1555500
    },
    {
      "epoch": 24.42072213500785,
      "grad_norm": 2.6470274925231934,
      "learning_rate": 3.47370486656201e-05,
      "loss": 0.6259,
      "step": 1555600
    },
    {
      "epoch": 24.422291993720567,
      "grad_norm": 3.96820068359375,
      "learning_rate": 3.4736067503924645e-05,
      "loss": 0.5874,
      "step": 1555700
    },
    {
      "epoch": 24.42386185243328,
      "grad_norm": 4.233266830444336,
      "learning_rate": 3.47350863422292e-05,
      "loss": 0.57,
      "step": 1555800
    },
    {
      "epoch": 24.425431711145997,
      "grad_norm": 3.2746825218200684,
      "learning_rate": 3.473410518053375e-05,
      "loss": 0.6122,
      "step": 1555900
    },
    {
      "epoch": 24.427001569858714,
      "grad_norm": 4.910562992095947,
      "learning_rate": 3.473312401883831e-05,
      "loss": 0.6161,
      "step": 1556000
    },
    {
      "epoch": 24.428571428571427,
      "grad_norm": 3.7529091835021973,
      "learning_rate": 3.4732142857142855e-05,
      "loss": 0.5953,
      "step": 1556100
    },
    {
      "epoch": 24.430141287284144,
      "grad_norm": 2.1428849697113037,
      "learning_rate": 3.473116169544741e-05,
      "loss": 0.5497,
      "step": 1556200
    },
    {
      "epoch": 24.43171114599686,
      "grad_norm": 3.7040319442749023,
      "learning_rate": 3.4730180533751964e-05,
      "loss": 0.572,
      "step": 1556300
    },
    {
      "epoch": 24.433281004709578,
      "grad_norm": 4.023891925811768,
      "learning_rate": 3.4729199372056515e-05,
      "loss": 0.6034,
      "step": 1556400
    },
    {
      "epoch": 24.43485086342229,
      "grad_norm": 4.1505560874938965,
      "learning_rate": 3.4728218210361066e-05,
      "loss": 0.5814,
      "step": 1556500
    },
    {
      "epoch": 24.436420722135008,
      "grad_norm": 4.221733570098877,
      "learning_rate": 3.4727237048665624e-05,
      "loss": 0.6322,
      "step": 1556600
    },
    {
      "epoch": 24.437990580847725,
      "grad_norm": 3.0658771991729736,
      "learning_rate": 3.4726255886970175e-05,
      "loss": 0.5881,
      "step": 1556700
    },
    {
      "epoch": 24.439560439560438,
      "grad_norm": 3.6783783435821533,
      "learning_rate": 3.4725274725274726e-05,
      "loss": 0.564,
      "step": 1556800
    },
    {
      "epoch": 24.441130298273155,
      "grad_norm": 2.811210870742798,
      "learning_rate": 3.472429356357928e-05,
      "loss": 0.5878,
      "step": 1556900
    },
    {
      "epoch": 24.44270015698587,
      "grad_norm": 3.89389705657959,
      "learning_rate": 3.4723312401883834e-05,
      "loss": 0.5854,
      "step": 1557000
    },
    {
      "epoch": 24.44427001569859,
      "grad_norm": 3.958754777908325,
      "learning_rate": 3.4722331240188385e-05,
      "loss": 0.6029,
      "step": 1557100
    },
    {
      "epoch": 24.445839874411302,
      "grad_norm": 3.337873935699463,
      "learning_rate": 3.4721350078492936e-05,
      "loss": 0.5893,
      "step": 1557200
    },
    {
      "epoch": 24.44740973312402,
      "grad_norm": 4.212792873382568,
      "learning_rate": 3.4720368916797494e-05,
      "loss": 0.5724,
      "step": 1557300
    },
    {
      "epoch": 24.448979591836736,
      "grad_norm": 4.590014934539795,
      "learning_rate": 3.4719387755102045e-05,
      "loss": 0.5952,
      "step": 1557400
    },
    {
      "epoch": 24.45054945054945,
      "grad_norm": 3.871143341064453,
      "learning_rate": 3.4718406593406596e-05,
      "loss": 0.5636,
      "step": 1557500
    },
    {
      "epoch": 24.452119309262166,
      "grad_norm": 4.194477081298828,
      "learning_rate": 3.471742543171115e-05,
      "loss": 0.5633,
      "step": 1557600
    },
    {
      "epoch": 24.453689167974883,
      "grad_norm": 4.5852766036987305,
      "learning_rate": 3.4716444270015705e-05,
      "loss": 0.6222,
      "step": 1557700
    },
    {
      "epoch": 24.4552590266876,
      "grad_norm": 3.5583441257476807,
      "learning_rate": 3.471546310832025e-05,
      "loss": 0.5875,
      "step": 1557800
    },
    {
      "epoch": 24.456828885400313,
      "grad_norm": 3.455749988555908,
      "learning_rate": 3.471448194662481e-05,
      "loss": 0.6081,
      "step": 1557900
    },
    {
      "epoch": 24.45839874411303,
      "grad_norm": 4.259921073913574,
      "learning_rate": 3.471350078492936e-05,
      "loss": 0.5911,
      "step": 1558000
    },
    {
      "epoch": 24.459968602825747,
      "grad_norm": 3.65809965133667,
      "learning_rate": 3.4712519623233916e-05,
      "loss": 0.6004,
      "step": 1558100
    },
    {
      "epoch": 24.46153846153846,
      "grad_norm": 3.609933614730835,
      "learning_rate": 3.471153846153846e-05,
      "loss": 0.6069,
      "step": 1558200
    },
    {
      "epoch": 24.463108320251177,
      "grad_norm": 4.395817756652832,
      "learning_rate": 3.471055729984302e-05,
      "loss": 0.6061,
      "step": 1558300
    },
    {
      "epoch": 24.464678178963894,
      "grad_norm": 4.004317760467529,
      "learning_rate": 3.470957613814757e-05,
      "loss": 0.6116,
      "step": 1558400
    },
    {
      "epoch": 24.46624803767661,
      "grad_norm": 3.4133636951446533,
      "learning_rate": 3.470859497645212e-05,
      "loss": 0.5937,
      "step": 1558500
    },
    {
      "epoch": 24.467817896389324,
      "grad_norm": 3.6427345275878906,
      "learning_rate": 3.470761381475667e-05,
      "loss": 0.554,
      "step": 1558600
    },
    {
      "epoch": 24.46938775510204,
      "grad_norm": 3.2861878871917725,
      "learning_rate": 3.470663265306123e-05,
      "loss": 0.5904,
      "step": 1558700
    },
    {
      "epoch": 24.470957613814758,
      "grad_norm": 3.756420612335205,
      "learning_rate": 3.470565149136578e-05,
      "loss": 0.6015,
      "step": 1558800
    },
    {
      "epoch": 24.47252747252747,
      "grad_norm": 3.951000928878784,
      "learning_rate": 3.470467032967033e-05,
      "loss": 0.5965,
      "step": 1558900
    },
    {
      "epoch": 24.474097331240188,
      "grad_norm": 4.379130840301514,
      "learning_rate": 3.470368916797488e-05,
      "loss": 0.6163,
      "step": 1559000
    },
    {
      "epoch": 24.475667189952905,
      "grad_norm": 3.9243342876434326,
      "learning_rate": 3.470270800627944e-05,
      "loss": 0.5693,
      "step": 1559100
    },
    {
      "epoch": 24.47723704866562,
      "grad_norm": 3.9838507175445557,
      "learning_rate": 3.470172684458399e-05,
      "loss": 0.5765,
      "step": 1559200
    },
    {
      "epoch": 24.478806907378335,
      "grad_norm": 3.3635783195495605,
      "learning_rate": 3.470074568288854e-05,
      "loss": 0.5992,
      "step": 1559300
    },
    {
      "epoch": 24.48037676609105,
      "grad_norm": 3.8564350605010986,
      "learning_rate": 3.46997645211931e-05,
      "loss": 0.5907,
      "step": 1559400
    },
    {
      "epoch": 24.48194662480377,
      "grad_norm": 4.169804096221924,
      "learning_rate": 3.469878335949764e-05,
      "loss": 0.6053,
      "step": 1559500
    },
    {
      "epoch": 24.483516483516482,
      "grad_norm": 4.03087854385376,
      "learning_rate": 3.46978021978022e-05,
      "loss": 0.5789,
      "step": 1559600
    },
    {
      "epoch": 24.4850863422292,
      "grad_norm": 4.025427341461182,
      "learning_rate": 3.469682103610675e-05,
      "loss": 0.5488,
      "step": 1559700
    },
    {
      "epoch": 24.486656200941916,
      "grad_norm": 4.7393622398376465,
      "learning_rate": 3.469583987441131e-05,
      "loss": 0.5615,
      "step": 1559800
    },
    {
      "epoch": 24.488226059654632,
      "grad_norm": 3.3780858516693115,
      "learning_rate": 3.4694858712715854e-05,
      "loss": 0.6367,
      "step": 1559900
    },
    {
      "epoch": 24.489795918367346,
      "grad_norm": 4.111473560333252,
      "learning_rate": 3.469387755102041e-05,
      "loss": 0.6122,
      "step": 1560000
    },
    {
      "epoch": 24.491365777080063,
      "grad_norm": 3.920103073120117,
      "learning_rate": 3.469289638932496e-05,
      "loss": 0.6615,
      "step": 1560100
    },
    {
      "epoch": 24.49293563579278,
      "grad_norm": 3.0111305713653564,
      "learning_rate": 3.469191522762951e-05,
      "loss": 0.5922,
      "step": 1560200
    },
    {
      "epoch": 24.494505494505496,
      "grad_norm": 4.273398399353027,
      "learning_rate": 3.4690934065934064e-05,
      "loss": 0.5915,
      "step": 1560300
    },
    {
      "epoch": 24.49607535321821,
      "grad_norm": 4.058172225952148,
      "learning_rate": 3.468995290423862e-05,
      "loss": 0.6388,
      "step": 1560400
    },
    {
      "epoch": 24.497645211930926,
      "grad_norm": 3.2763538360595703,
      "learning_rate": 3.468897174254317e-05,
      "loss": 0.5865,
      "step": 1560500
    },
    {
      "epoch": 24.499215070643643,
      "grad_norm": 2.8646717071533203,
      "learning_rate": 3.4687990580847724e-05,
      "loss": 0.5913,
      "step": 1560600
    },
    {
      "epoch": 24.500784929356357,
      "grad_norm": 4.086243152618408,
      "learning_rate": 3.4687009419152275e-05,
      "loss": 0.586,
      "step": 1560700
    },
    {
      "epoch": 24.502354788069074,
      "grad_norm": 4.204165458679199,
      "learning_rate": 3.468602825745683e-05,
      "loss": 0.5755,
      "step": 1560800
    },
    {
      "epoch": 24.50392464678179,
      "grad_norm": 3.4842541217803955,
      "learning_rate": 3.4685047095761384e-05,
      "loss": 0.5837,
      "step": 1560900
    },
    {
      "epoch": 24.505494505494504,
      "grad_norm": 4.941135406494141,
      "learning_rate": 3.4684065934065935e-05,
      "loss": 0.5867,
      "step": 1561000
    },
    {
      "epoch": 24.50706436420722,
      "grad_norm": 4.250301361083984,
      "learning_rate": 3.4683084772370486e-05,
      "loss": 0.6159,
      "step": 1561100
    },
    {
      "epoch": 24.508634222919937,
      "grad_norm": 4.652501583099365,
      "learning_rate": 3.4682103610675043e-05,
      "loss": 0.5638,
      "step": 1561200
    },
    {
      "epoch": 24.510204081632654,
      "grad_norm": 4.544336795806885,
      "learning_rate": 3.4681122448979594e-05,
      "loss": 0.6026,
      "step": 1561300
    },
    {
      "epoch": 24.511773940345368,
      "grad_norm": 4.5711164474487305,
      "learning_rate": 3.4680141287284145e-05,
      "loss": 0.591,
      "step": 1561400
    },
    {
      "epoch": 24.513343799058084,
      "grad_norm": 3.6890525817871094,
      "learning_rate": 3.46791601255887e-05,
      "loss": 0.5847,
      "step": 1561500
    },
    {
      "epoch": 24.5149136577708,
      "grad_norm": 3.116334915161133,
      "learning_rate": 3.467817896389325e-05,
      "loss": 0.6153,
      "step": 1561600
    },
    {
      "epoch": 24.516483516483518,
      "grad_norm": 3.8207244873046875,
      "learning_rate": 3.4677197802197805e-05,
      "loss": 0.6224,
      "step": 1561700
    },
    {
      "epoch": 24.51805337519623,
      "grad_norm": 3.5653061866760254,
      "learning_rate": 3.4676216640502356e-05,
      "loss": 0.6014,
      "step": 1561800
    },
    {
      "epoch": 24.51962323390895,
      "grad_norm": 3.919080972671509,
      "learning_rate": 3.4675235478806914e-05,
      "loss": 0.5768,
      "step": 1561900
    },
    {
      "epoch": 24.521193092621665,
      "grad_norm": 4.097172737121582,
      "learning_rate": 3.467425431711146e-05,
      "loss": 0.6009,
      "step": 1562000
    },
    {
      "epoch": 24.52276295133438,
      "grad_norm": 4.256183624267578,
      "learning_rate": 3.4673273155416016e-05,
      "loss": 0.6166,
      "step": 1562100
    },
    {
      "epoch": 24.524332810047095,
      "grad_norm": 4.884783744812012,
      "learning_rate": 3.467229199372057e-05,
      "loss": 0.6034,
      "step": 1562200
    },
    {
      "epoch": 24.525902668759812,
      "grad_norm": 3.3295671939849854,
      "learning_rate": 3.467131083202512e-05,
      "loss": 0.5701,
      "step": 1562300
    },
    {
      "epoch": 24.52747252747253,
      "grad_norm": 4.202511310577393,
      "learning_rate": 3.467032967032967e-05,
      "loss": 0.6209,
      "step": 1562400
    },
    {
      "epoch": 24.529042386185242,
      "grad_norm": 4.41221284866333,
      "learning_rate": 3.4669348508634227e-05,
      "loss": 0.6169,
      "step": 1562500
    },
    {
      "epoch": 24.53061224489796,
      "grad_norm": 5.365384578704834,
      "learning_rate": 3.466836734693878e-05,
      "loss": 0.632,
      "step": 1562600
    },
    {
      "epoch": 24.532182103610676,
      "grad_norm": 4.470519065856934,
      "learning_rate": 3.466738618524333e-05,
      "loss": 0.5721,
      "step": 1562700
    },
    {
      "epoch": 24.53375196232339,
      "grad_norm": 3.141343593597412,
      "learning_rate": 3.466640502354788e-05,
      "loss": 0.5753,
      "step": 1562800
    },
    {
      "epoch": 24.535321821036106,
      "grad_norm": 4.736033916473389,
      "learning_rate": 3.466542386185244e-05,
      "loss": 0.6162,
      "step": 1562900
    },
    {
      "epoch": 24.536891679748823,
      "grad_norm": 4.455970287322998,
      "learning_rate": 3.466444270015699e-05,
      "loss": 0.6302,
      "step": 1563000
    },
    {
      "epoch": 24.53846153846154,
      "grad_norm": 3.6333184242248535,
      "learning_rate": 3.466346153846154e-05,
      "loss": 0.5769,
      "step": 1563100
    },
    {
      "epoch": 24.540031397174253,
      "grad_norm": 3.8835482597351074,
      "learning_rate": 3.466248037676609e-05,
      "loss": 0.6041,
      "step": 1563200
    },
    {
      "epoch": 24.54160125588697,
      "grad_norm": 4.1584672927856445,
      "learning_rate": 3.466149921507065e-05,
      "loss": 0.5501,
      "step": 1563300
    },
    {
      "epoch": 24.543171114599687,
      "grad_norm": 3.4683680534362793,
      "learning_rate": 3.46605180533752e-05,
      "loss": 0.5685,
      "step": 1563400
    },
    {
      "epoch": 24.5447409733124,
      "grad_norm": 4.607250690460205,
      "learning_rate": 3.465953689167975e-05,
      "loss": 0.5888,
      "step": 1563500
    },
    {
      "epoch": 24.546310832025117,
      "grad_norm": 3.9130659103393555,
      "learning_rate": 3.465855572998431e-05,
      "loss": 0.6261,
      "step": 1563600
    },
    {
      "epoch": 24.547880690737834,
      "grad_norm": 3.0718069076538086,
      "learning_rate": 3.465757456828885e-05,
      "loss": 0.5835,
      "step": 1563700
    },
    {
      "epoch": 24.54945054945055,
      "grad_norm": 3.2139458656311035,
      "learning_rate": 3.465659340659341e-05,
      "loss": 0.5978,
      "step": 1563800
    },
    {
      "epoch": 24.551020408163264,
      "grad_norm": 4.230025291442871,
      "learning_rate": 3.465561224489796e-05,
      "loss": 0.6074,
      "step": 1563900
    },
    {
      "epoch": 24.55259026687598,
      "grad_norm": 4.303138256072998,
      "learning_rate": 3.465463108320252e-05,
      "loss": 0.6292,
      "step": 1564000
    },
    {
      "epoch": 24.554160125588698,
      "grad_norm": 4.471408843994141,
      "learning_rate": 3.465364992150706e-05,
      "loss": 0.6106,
      "step": 1564100
    },
    {
      "epoch": 24.55572998430141,
      "grad_norm": 4.1865315437316895,
      "learning_rate": 3.465266875981162e-05,
      "loss": 0.6308,
      "step": 1564200
    },
    {
      "epoch": 24.55729984301413,
      "grad_norm": 4.3935933113098145,
      "learning_rate": 3.465168759811617e-05,
      "loss": 0.5851,
      "step": 1564300
    },
    {
      "epoch": 24.558869701726845,
      "grad_norm": 3.9809694290161133,
      "learning_rate": 3.465070643642072e-05,
      "loss": 0.5932,
      "step": 1564400
    },
    {
      "epoch": 24.560439560439562,
      "grad_norm": 4.6121110916137695,
      "learning_rate": 3.464972527472527e-05,
      "loss": 0.6031,
      "step": 1564500
    },
    {
      "epoch": 24.562009419152275,
      "grad_norm": 4.349410057067871,
      "learning_rate": 3.464874411302983e-05,
      "loss": 0.6229,
      "step": 1564600
    },
    {
      "epoch": 24.563579277864992,
      "grad_norm": 3.882709503173828,
      "learning_rate": 3.464776295133438e-05,
      "loss": 0.5948,
      "step": 1564700
    },
    {
      "epoch": 24.56514913657771,
      "grad_norm": 4.790631294250488,
      "learning_rate": 3.464678178963893e-05,
      "loss": 0.6252,
      "step": 1564800
    },
    {
      "epoch": 24.566718995290422,
      "grad_norm": 4.182052135467529,
      "learning_rate": 3.4645800627943484e-05,
      "loss": 0.6165,
      "step": 1564900
    },
    {
      "epoch": 24.56828885400314,
      "grad_norm": 4.2685370445251465,
      "learning_rate": 3.464481946624804e-05,
      "loss": 0.6049,
      "step": 1565000
    },
    {
      "epoch": 24.569858712715856,
      "grad_norm": 5.836944580078125,
      "learning_rate": 3.464383830455259e-05,
      "loss": 0.62,
      "step": 1565100
    },
    {
      "epoch": 24.571428571428573,
      "grad_norm": 4.089735984802246,
      "learning_rate": 3.4642857142857144e-05,
      "loss": 0.5962,
      "step": 1565200
    },
    {
      "epoch": 24.572998430141286,
      "grad_norm": 4.273133277893066,
      "learning_rate": 3.4641875981161695e-05,
      "loss": 0.6215,
      "step": 1565300
    },
    {
      "epoch": 24.574568288854003,
      "grad_norm": 2.6493160724639893,
      "learning_rate": 3.464089481946625e-05,
      "loss": 0.6022,
      "step": 1565400
    },
    {
      "epoch": 24.57613814756672,
      "grad_norm": 2.1048011779785156,
      "learning_rate": 3.4639913657770803e-05,
      "loss": 0.5731,
      "step": 1565500
    },
    {
      "epoch": 24.577708006279433,
      "grad_norm": 3.561129093170166,
      "learning_rate": 3.4638932496075354e-05,
      "loss": 0.5757,
      "step": 1565600
    },
    {
      "epoch": 24.57927786499215,
      "grad_norm": 3.0178799629211426,
      "learning_rate": 3.463795133437991e-05,
      "loss": 0.595,
      "step": 1565700
    },
    {
      "epoch": 24.580847723704867,
      "grad_norm": 5.167877197265625,
      "learning_rate": 3.4636970172684456e-05,
      "loss": 0.6241,
      "step": 1565800
    },
    {
      "epoch": 24.582417582417584,
      "grad_norm": 3.313533067703247,
      "learning_rate": 3.4635989010989014e-05,
      "loss": 0.5831,
      "step": 1565900
    },
    {
      "epoch": 24.583987441130297,
      "grad_norm": 3.966477394104004,
      "learning_rate": 3.4635007849293565e-05,
      "loss": 0.5822,
      "step": 1566000
    },
    {
      "epoch": 24.585557299843014,
      "grad_norm": 4.140654563903809,
      "learning_rate": 3.463402668759812e-05,
      "loss": 0.6255,
      "step": 1566100
    },
    {
      "epoch": 24.58712715855573,
      "grad_norm": 2.5873897075653076,
      "learning_rate": 3.463304552590267e-05,
      "loss": 0.5806,
      "step": 1566200
    },
    {
      "epoch": 24.588697017268444,
      "grad_norm": 4.123408794403076,
      "learning_rate": 3.4632064364207225e-05,
      "loss": 0.6161,
      "step": 1566300
    },
    {
      "epoch": 24.59026687598116,
      "grad_norm": 3.623666286468506,
      "learning_rate": 3.4631083202511776e-05,
      "loss": 0.6227,
      "step": 1566400
    },
    {
      "epoch": 24.591836734693878,
      "grad_norm": 3.5340144634246826,
      "learning_rate": 3.463010204081633e-05,
      "loss": 0.5867,
      "step": 1566500
    },
    {
      "epoch": 24.593406593406595,
      "grad_norm": 3.2956581115722656,
      "learning_rate": 3.462912087912088e-05,
      "loss": 0.6238,
      "step": 1566600
    },
    {
      "epoch": 24.594976452119308,
      "grad_norm": 3.770071268081665,
      "learning_rate": 3.4628139717425436e-05,
      "loss": 0.5978,
      "step": 1566700
    },
    {
      "epoch": 24.596546310832025,
      "grad_norm": 3.736582040786743,
      "learning_rate": 3.4627158555729987e-05,
      "loss": 0.6091,
      "step": 1566800
    },
    {
      "epoch": 24.598116169544742,
      "grad_norm": 3.2672677040100098,
      "learning_rate": 3.462617739403454e-05,
      "loss": 0.5755,
      "step": 1566900
    },
    {
      "epoch": 24.599686028257455,
      "grad_norm": 3.6871886253356934,
      "learning_rate": 3.462519623233909e-05,
      "loss": 0.622,
      "step": 1567000
    },
    {
      "epoch": 24.601255886970172,
      "grad_norm": 3.144704818725586,
      "learning_rate": 3.4624215070643646e-05,
      "loss": 0.6397,
      "step": 1567100
    },
    {
      "epoch": 24.60282574568289,
      "grad_norm": 2.3747761249542236,
      "learning_rate": 3.46232339089482e-05,
      "loss": 0.5633,
      "step": 1567200
    },
    {
      "epoch": 24.604395604395606,
      "grad_norm": 4.318355560302734,
      "learning_rate": 3.462225274725275e-05,
      "loss": 0.5768,
      "step": 1567300
    },
    {
      "epoch": 24.60596546310832,
      "grad_norm": 3.7552685737609863,
      "learning_rate": 3.46212715855573e-05,
      "loss": 0.5978,
      "step": 1567400
    },
    {
      "epoch": 24.607535321821036,
      "grad_norm": 3.7039034366607666,
      "learning_rate": 3.462029042386186e-05,
      "loss": 0.5631,
      "step": 1567500
    },
    {
      "epoch": 24.609105180533753,
      "grad_norm": 2.1725687980651855,
      "learning_rate": 3.461930926216641e-05,
      "loss": 0.5817,
      "step": 1567600
    },
    {
      "epoch": 24.610675039246466,
      "grad_norm": 3.2887966632843018,
      "learning_rate": 3.461832810047096e-05,
      "loss": 0.5849,
      "step": 1567700
    },
    {
      "epoch": 24.612244897959183,
      "grad_norm": 4.317409515380859,
      "learning_rate": 3.461734693877552e-05,
      "loss": 0.6251,
      "step": 1567800
    },
    {
      "epoch": 24.6138147566719,
      "grad_norm": 3.959851026535034,
      "learning_rate": 3.461636577708006e-05,
      "loss": 0.5999,
      "step": 1567900
    },
    {
      "epoch": 24.615384615384617,
      "grad_norm": 3.8941690921783447,
      "learning_rate": 3.461538461538462e-05,
      "loss": 0.5832,
      "step": 1568000
    },
    {
      "epoch": 24.61695447409733,
      "grad_norm": 2.7227213382720947,
      "learning_rate": 3.461440345368917e-05,
      "loss": 0.5753,
      "step": 1568100
    },
    {
      "epoch": 24.618524332810047,
      "grad_norm": 3.353294610977173,
      "learning_rate": 3.461342229199373e-05,
      "loss": 0.6318,
      "step": 1568200
    },
    {
      "epoch": 24.620094191522764,
      "grad_norm": 4.439691543579102,
      "learning_rate": 3.461244113029827e-05,
      "loss": 0.605,
      "step": 1568300
    },
    {
      "epoch": 24.621664050235477,
      "grad_norm": 2.5218899250030518,
      "learning_rate": 3.461145996860283e-05,
      "loss": 0.5925,
      "step": 1568400
    },
    {
      "epoch": 24.623233908948194,
      "grad_norm": 3.800081729888916,
      "learning_rate": 3.461047880690738e-05,
      "loss": 0.5816,
      "step": 1568500
    },
    {
      "epoch": 24.62480376766091,
      "grad_norm": 3.0688388347625732,
      "learning_rate": 3.460949764521193e-05,
      "loss": 0.6254,
      "step": 1568600
    },
    {
      "epoch": 24.626373626373628,
      "grad_norm": 4.089532852172852,
      "learning_rate": 3.460851648351648e-05,
      "loss": 0.6297,
      "step": 1568700
    },
    {
      "epoch": 24.62794348508634,
      "grad_norm": 2.7185370922088623,
      "learning_rate": 3.460753532182104e-05,
      "loss": 0.5856,
      "step": 1568800
    },
    {
      "epoch": 24.629513343799058,
      "grad_norm": 4.788237571716309,
      "learning_rate": 3.460655416012559e-05,
      "loss": 0.6158,
      "step": 1568900
    },
    {
      "epoch": 24.631083202511775,
      "grad_norm": 3.379271984100342,
      "learning_rate": 3.460557299843014e-05,
      "loss": 0.5679,
      "step": 1569000
    },
    {
      "epoch": 24.632653061224488,
      "grad_norm": 3.8790457248687744,
      "learning_rate": 3.460459183673469e-05,
      "loss": 0.6668,
      "step": 1569100
    },
    {
      "epoch": 24.634222919937205,
      "grad_norm": 3.8812179565429688,
      "learning_rate": 3.460361067503925e-05,
      "loss": 0.5688,
      "step": 1569200
    },
    {
      "epoch": 24.635792778649922,
      "grad_norm": 3.416980266571045,
      "learning_rate": 3.46026295133438e-05,
      "loss": 0.5777,
      "step": 1569300
    },
    {
      "epoch": 24.63736263736264,
      "grad_norm": 2.9661755561828613,
      "learning_rate": 3.460164835164835e-05,
      "loss": 0.5891,
      "step": 1569400
    },
    {
      "epoch": 24.638932496075352,
      "grad_norm": 4.238080978393555,
      "learning_rate": 3.4600667189952904e-05,
      "loss": 0.6237,
      "step": 1569500
    },
    {
      "epoch": 24.64050235478807,
      "grad_norm": 2.5771262645721436,
      "learning_rate": 3.459968602825746e-05,
      "loss": 0.6073,
      "step": 1569600
    },
    {
      "epoch": 24.642072213500786,
      "grad_norm": 4.058448791503906,
      "learning_rate": 3.459870486656201e-05,
      "loss": 0.6075,
      "step": 1569700
    },
    {
      "epoch": 24.643642072213503,
      "grad_norm": 3.3250627517700195,
      "learning_rate": 3.459772370486656e-05,
      "loss": 0.6009,
      "step": 1569800
    },
    {
      "epoch": 24.645211930926216,
      "grad_norm": 3.3053998947143555,
      "learning_rate": 3.459674254317112e-05,
      "loss": 0.5899,
      "step": 1569900
    },
    {
      "epoch": 24.646781789638933,
      "grad_norm": 4.136819839477539,
      "learning_rate": 3.4595761381475665e-05,
      "loss": 0.6238,
      "step": 1570000
    },
    {
      "epoch": 24.64835164835165,
      "grad_norm": 3.431905508041382,
      "learning_rate": 3.459478021978022e-05,
      "loss": 0.5829,
      "step": 1570100
    },
    {
      "epoch": 24.649921507064363,
      "grad_norm": 4.28926944732666,
      "learning_rate": 3.4593799058084774e-05,
      "loss": 0.5858,
      "step": 1570200
    },
    {
      "epoch": 24.65149136577708,
      "grad_norm": 5.16705846786499,
      "learning_rate": 3.459281789638933e-05,
      "loss": 0.5771,
      "step": 1570300
    },
    {
      "epoch": 24.653061224489797,
      "grad_norm": 3.537778854370117,
      "learning_rate": 3.4591836734693876e-05,
      "loss": 0.6201,
      "step": 1570400
    },
    {
      "epoch": 24.65463108320251,
      "grad_norm": 4.33098840713501,
      "learning_rate": 3.4590855572998434e-05,
      "loss": 0.6366,
      "step": 1570500
    },
    {
      "epoch": 24.656200941915227,
      "grad_norm": 3.9266223907470703,
      "learning_rate": 3.4589874411302985e-05,
      "loss": 0.5699,
      "step": 1570600
    },
    {
      "epoch": 24.657770800627944,
      "grad_norm": 4.738710880279541,
      "learning_rate": 3.4588893249607536e-05,
      "loss": 0.5957,
      "step": 1570700
    },
    {
      "epoch": 24.65934065934066,
      "grad_norm": 2.560793399810791,
      "learning_rate": 3.458791208791209e-05,
      "loss": 0.5714,
      "step": 1570800
    },
    {
      "epoch": 24.660910518053374,
      "grad_norm": 3.80562162399292,
      "learning_rate": 3.4586930926216645e-05,
      "loss": 0.6019,
      "step": 1570900
    },
    {
      "epoch": 24.66248037676609,
      "grad_norm": 3.9633193016052246,
      "learning_rate": 3.4585949764521195e-05,
      "loss": 0.6018,
      "step": 1571000
    },
    {
      "epoch": 24.664050235478808,
      "grad_norm": 2.8569748401641846,
      "learning_rate": 3.4584968602825746e-05,
      "loss": 0.5571,
      "step": 1571100
    },
    {
      "epoch": 24.665620094191524,
      "grad_norm": 3.7669618129730225,
      "learning_rate": 3.45839874411303e-05,
      "loss": 0.5864,
      "step": 1571200
    },
    {
      "epoch": 24.667189952904238,
      "grad_norm": 4.785506248474121,
      "learning_rate": 3.4583006279434855e-05,
      "loss": 0.6452,
      "step": 1571300
    },
    {
      "epoch": 24.668759811616955,
      "grad_norm": 3.698643445968628,
      "learning_rate": 3.45820251177394e-05,
      "loss": 0.62,
      "step": 1571400
    },
    {
      "epoch": 24.67032967032967,
      "grad_norm": 3.0698821544647217,
      "learning_rate": 3.458104395604396e-05,
      "loss": 0.5872,
      "step": 1571500
    },
    {
      "epoch": 24.671899529042385,
      "grad_norm": 3.9493701457977295,
      "learning_rate": 3.458006279434851e-05,
      "loss": 0.5931,
      "step": 1571600
    },
    {
      "epoch": 24.6734693877551,
      "grad_norm": 4.780935764312744,
      "learning_rate": 3.4579081632653066e-05,
      "loss": 0.6331,
      "step": 1571700
    },
    {
      "epoch": 24.67503924646782,
      "grad_norm": 3.7226827144622803,
      "learning_rate": 3.457810047095762e-05,
      "loss": 0.5942,
      "step": 1571800
    },
    {
      "epoch": 24.676609105180535,
      "grad_norm": 3.508915424346924,
      "learning_rate": 3.457711930926217e-05,
      "loss": 0.6416,
      "step": 1571900
    },
    {
      "epoch": 24.67817896389325,
      "grad_norm": 4.142926216125488,
      "learning_rate": 3.4576138147566726e-05,
      "loss": 0.5856,
      "step": 1572000
    },
    {
      "epoch": 24.679748822605966,
      "grad_norm": 3.4944028854370117,
      "learning_rate": 3.457515698587127e-05,
      "loss": 0.6052,
      "step": 1572100
    },
    {
      "epoch": 24.681318681318682,
      "grad_norm": 3.4926154613494873,
      "learning_rate": 3.457417582417583e-05,
      "loss": 0.6074,
      "step": 1572200
    },
    {
      "epoch": 24.682888540031396,
      "grad_norm": 2.9439306259155273,
      "learning_rate": 3.457319466248038e-05,
      "loss": 0.6297,
      "step": 1572300
    },
    {
      "epoch": 24.684458398744113,
      "grad_norm": 3.2135446071624756,
      "learning_rate": 3.4572213500784936e-05,
      "loss": 0.58,
      "step": 1572400
    },
    {
      "epoch": 24.68602825745683,
      "grad_norm": 4.141693592071533,
      "learning_rate": 3.457123233908948e-05,
      "loss": 0.6131,
      "step": 1572500
    },
    {
      "epoch": 24.687598116169546,
      "grad_norm": 4.454493045806885,
      "learning_rate": 3.457025117739404e-05,
      "loss": 0.5564,
      "step": 1572600
    },
    {
      "epoch": 24.68916797488226,
      "grad_norm": 2.9510908126831055,
      "learning_rate": 3.456927001569859e-05,
      "loss": 0.6053,
      "step": 1572700
    },
    {
      "epoch": 24.690737833594977,
      "grad_norm": 4.387185096740723,
      "learning_rate": 3.456828885400314e-05,
      "loss": 0.5871,
      "step": 1572800
    },
    {
      "epoch": 24.692307692307693,
      "grad_norm": 3.8762946128845215,
      "learning_rate": 3.456730769230769e-05,
      "loss": 0.6102,
      "step": 1572900
    },
    {
      "epoch": 24.693877551020407,
      "grad_norm": 3.7877495288848877,
      "learning_rate": 3.456632653061225e-05,
      "loss": 0.5947,
      "step": 1573000
    },
    {
      "epoch": 24.695447409733124,
      "grad_norm": 4.209219932556152,
      "learning_rate": 3.45653453689168e-05,
      "loss": 0.5989,
      "step": 1573100
    },
    {
      "epoch": 24.69701726844584,
      "grad_norm": 3.0581228733062744,
      "learning_rate": 3.456436420722135e-05,
      "loss": 0.615,
      "step": 1573200
    },
    {
      "epoch": 24.698587127158557,
      "grad_norm": 3.208832025527954,
      "learning_rate": 3.45633830455259e-05,
      "loss": 0.649,
      "step": 1573300
    },
    {
      "epoch": 24.70015698587127,
      "grad_norm": 2.9923856258392334,
      "learning_rate": 3.456240188383046e-05,
      "loss": 0.6565,
      "step": 1573400
    },
    {
      "epoch": 24.701726844583987,
      "grad_norm": 3.0012590885162354,
      "learning_rate": 3.4561420722135004e-05,
      "loss": 0.6474,
      "step": 1573500
    },
    {
      "epoch": 24.703296703296704,
      "grad_norm": 3.408083438873291,
      "learning_rate": 3.456043956043956e-05,
      "loss": 0.6053,
      "step": 1573600
    },
    {
      "epoch": 24.704866562009418,
      "grad_norm": 4.093496799468994,
      "learning_rate": 3.455945839874411e-05,
      "loss": 0.6153,
      "step": 1573700
    },
    {
      "epoch": 24.706436420722135,
      "grad_norm": 3.92083477973938,
      "learning_rate": 3.455847723704867e-05,
      "loss": 0.6308,
      "step": 1573800
    },
    {
      "epoch": 24.70800627943485,
      "grad_norm": 3.2509067058563232,
      "learning_rate": 3.455749607535322e-05,
      "loss": 0.6232,
      "step": 1573900
    },
    {
      "epoch": 24.70957613814757,
      "grad_norm": 3.0615484714508057,
      "learning_rate": 3.455651491365777e-05,
      "loss": 0.5994,
      "step": 1574000
    },
    {
      "epoch": 24.71114599686028,
      "grad_norm": 3.313997745513916,
      "learning_rate": 3.455553375196233e-05,
      "loss": 0.6005,
      "step": 1574100
    },
    {
      "epoch": 24.712715855573,
      "grad_norm": 3.320969581604004,
      "learning_rate": 3.4554552590266874e-05,
      "loss": 0.575,
      "step": 1574200
    },
    {
      "epoch": 24.714285714285715,
      "grad_norm": 3.9588074684143066,
      "learning_rate": 3.455357142857143e-05,
      "loss": 0.6221,
      "step": 1574300
    },
    {
      "epoch": 24.71585557299843,
      "grad_norm": 2.0644583702087402,
      "learning_rate": 3.455259026687598e-05,
      "loss": 0.5648,
      "step": 1574400
    },
    {
      "epoch": 24.717425431711145,
      "grad_norm": 3.433218002319336,
      "learning_rate": 3.455160910518054e-05,
      "loss": 0.5532,
      "step": 1574500
    },
    {
      "epoch": 24.718995290423862,
      "grad_norm": 3.566474199295044,
      "learning_rate": 3.4550627943485085e-05,
      "loss": 0.6293,
      "step": 1574600
    },
    {
      "epoch": 24.72056514913658,
      "grad_norm": 3.7300615310668945,
      "learning_rate": 3.454964678178964e-05,
      "loss": 0.5913,
      "step": 1574700
    },
    {
      "epoch": 24.722135007849293,
      "grad_norm": 3.41597843170166,
      "learning_rate": 3.4548665620094194e-05,
      "loss": 0.5697,
      "step": 1574800
    },
    {
      "epoch": 24.72370486656201,
      "grad_norm": 2.2064104080200195,
      "learning_rate": 3.4547684458398745e-05,
      "loss": 0.5995,
      "step": 1574900
    },
    {
      "epoch": 24.725274725274726,
      "grad_norm": 4.197728157043457,
      "learning_rate": 3.4546703296703296e-05,
      "loss": 0.5717,
      "step": 1575000
    },
    {
      "epoch": 24.72684458398744,
      "grad_norm": 3.8168187141418457,
      "learning_rate": 3.4545722135007853e-05,
      "loss": 0.6151,
      "step": 1575100
    },
    {
      "epoch": 24.728414442700156,
      "grad_norm": 3.8053343296051025,
      "learning_rate": 3.4544740973312404e-05,
      "loss": 0.6173,
      "step": 1575200
    },
    {
      "epoch": 24.729984301412873,
      "grad_norm": 4.29811954498291,
      "learning_rate": 3.4543759811616955e-05,
      "loss": 0.58,
      "step": 1575300
    },
    {
      "epoch": 24.73155416012559,
      "grad_norm": 3.4744811058044434,
      "learning_rate": 3.4542778649921506e-05,
      "loss": 0.597,
      "step": 1575400
    },
    {
      "epoch": 24.733124018838303,
      "grad_norm": 3.166729688644409,
      "learning_rate": 3.4541797488226064e-05,
      "loss": 0.565,
      "step": 1575500
    },
    {
      "epoch": 24.73469387755102,
      "grad_norm": 4.980679988861084,
      "learning_rate": 3.454081632653061e-05,
      "loss": 0.6508,
      "step": 1575600
    },
    {
      "epoch": 24.736263736263737,
      "grad_norm": 3.2327215671539307,
      "learning_rate": 3.4539835164835166e-05,
      "loss": 0.6144,
      "step": 1575700
    },
    {
      "epoch": 24.73783359497645,
      "grad_norm": 3.2612783908843994,
      "learning_rate": 3.453885400313972e-05,
      "loss": 0.6359,
      "step": 1575800
    },
    {
      "epoch": 24.739403453689167,
      "grad_norm": 3.3940823078155518,
      "learning_rate": 3.4537872841444275e-05,
      "loss": 0.6127,
      "step": 1575900
    },
    {
      "epoch": 24.740973312401884,
      "grad_norm": 3.640634775161743,
      "learning_rate": 3.4536891679748826e-05,
      "loss": 0.592,
      "step": 1576000
    },
    {
      "epoch": 24.7425431711146,
      "grad_norm": 4.230642795562744,
      "learning_rate": 3.453591051805338e-05,
      "loss": 0.639,
      "step": 1576100
    },
    {
      "epoch": 24.744113029827314,
      "grad_norm": 3.878499984741211,
      "learning_rate": 3.4534929356357935e-05,
      "loss": 0.5558,
      "step": 1576200
    },
    {
      "epoch": 24.74568288854003,
      "grad_norm": 4.0295329093933105,
      "learning_rate": 3.453394819466248e-05,
      "loss": 0.6381,
      "step": 1576300
    },
    {
      "epoch": 24.747252747252748,
      "grad_norm": 2.9928781986236572,
      "learning_rate": 3.4532967032967037e-05,
      "loss": 0.5818,
      "step": 1576400
    },
    {
      "epoch": 24.74882260596546,
      "grad_norm": 4.214115619659424,
      "learning_rate": 3.453198587127159e-05,
      "loss": 0.6049,
      "step": 1576500
    },
    {
      "epoch": 24.75039246467818,
      "grad_norm": 4.010500907897949,
      "learning_rate": 3.4531004709576145e-05,
      "loss": 0.6198,
      "step": 1576600
    },
    {
      "epoch": 24.751962323390895,
      "grad_norm": 4.3595147132873535,
      "learning_rate": 3.453002354788069e-05,
      "loss": 0.5766,
      "step": 1576700
    },
    {
      "epoch": 24.753532182103612,
      "grad_norm": 3.8090524673461914,
      "learning_rate": 3.452904238618525e-05,
      "loss": 0.5657,
      "step": 1576800
    },
    {
      "epoch": 24.755102040816325,
      "grad_norm": 3.5762710571289062,
      "learning_rate": 3.45280612244898e-05,
      "loss": 0.555,
      "step": 1576900
    },
    {
      "epoch": 24.756671899529042,
      "grad_norm": 2.843937397003174,
      "learning_rate": 3.452708006279435e-05,
      "loss": 0.6078,
      "step": 1577000
    },
    {
      "epoch": 24.75824175824176,
      "grad_norm": 3.451406955718994,
      "learning_rate": 3.45260989010989e-05,
      "loss": 0.6058,
      "step": 1577100
    },
    {
      "epoch": 24.759811616954472,
      "grad_norm": 3.1579384803771973,
      "learning_rate": 3.452511773940346e-05,
      "loss": 0.5845,
      "step": 1577200
    },
    {
      "epoch": 24.76138147566719,
      "grad_norm": 3.1498444080352783,
      "learning_rate": 3.452413657770801e-05,
      "loss": 0.5624,
      "step": 1577300
    },
    {
      "epoch": 24.762951334379906,
      "grad_norm": 2.6949682235717773,
      "learning_rate": 3.452315541601256e-05,
      "loss": 0.5992,
      "step": 1577400
    },
    {
      "epoch": 24.764521193092623,
      "grad_norm": 3.8814167976379395,
      "learning_rate": 3.452217425431711e-05,
      "loss": 0.5989,
      "step": 1577500
    },
    {
      "epoch": 24.766091051805336,
      "grad_norm": 3.968855381011963,
      "learning_rate": 3.452119309262167e-05,
      "loss": 0.5846,
      "step": 1577600
    },
    {
      "epoch": 24.767660910518053,
      "grad_norm": 3.9760262966156006,
      "learning_rate": 3.452021193092621e-05,
      "loss": 0.6362,
      "step": 1577700
    },
    {
      "epoch": 24.76923076923077,
      "grad_norm": 3.3332371711730957,
      "learning_rate": 3.451923076923077e-05,
      "loss": 0.6178,
      "step": 1577800
    },
    {
      "epoch": 24.770800627943487,
      "grad_norm": 4.722880840301514,
      "learning_rate": 3.451824960753532e-05,
      "loss": 0.5811,
      "step": 1577900
    },
    {
      "epoch": 24.7723704866562,
      "grad_norm": 3.4210526943206787,
      "learning_rate": 3.451726844583988e-05,
      "loss": 0.613,
      "step": 1578000
    },
    {
      "epoch": 24.773940345368917,
      "grad_norm": 4.259834289550781,
      "learning_rate": 3.451628728414443e-05,
      "loss": 0.5725,
      "step": 1578100
    },
    {
      "epoch": 24.775510204081634,
      "grad_norm": 4.954128265380859,
      "learning_rate": 3.451530612244898e-05,
      "loss": 0.5966,
      "step": 1578200
    },
    {
      "epoch": 24.777080062794347,
      "grad_norm": 3.8462557792663574,
      "learning_rate": 3.451432496075354e-05,
      "loss": 0.5968,
      "step": 1578300
    },
    {
      "epoch": 24.778649921507064,
      "grad_norm": 4.079736709594727,
      "learning_rate": 3.451334379905808e-05,
      "loss": 0.5974,
      "step": 1578400
    },
    {
      "epoch": 24.78021978021978,
      "grad_norm": 3.322974443435669,
      "learning_rate": 3.451236263736264e-05,
      "loss": 0.5772,
      "step": 1578500
    },
    {
      "epoch": 24.781789638932494,
      "grad_norm": 3.9608826637268066,
      "learning_rate": 3.451138147566719e-05,
      "loss": 0.5598,
      "step": 1578600
    },
    {
      "epoch": 24.78335949764521,
      "grad_norm": 3.6575429439544678,
      "learning_rate": 3.451040031397175e-05,
      "loss": 0.5686,
      "step": 1578700
    },
    {
      "epoch": 24.784929356357928,
      "grad_norm": 4.139311790466309,
      "learning_rate": 3.4509419152276294e-05,
      "loss": 0.5817,
      "step": 1578800
    },
    {
      "epoch": 24.786499215070645,
      "grad_norm": 5.090368270874023,
      "learning_rate": 3.450843799058085e-05,
      "loss": 0.6021,
      "step": 1578900
    },
    {
      "epoch": 24.788069073783358,
      "grad_norm": 3.4735944271087646,
      "learning_rate": 3.45074568288854e-05,
      "loss": 0.6095,
      "step": 1579000
    },
    {
      "epoch": 24.789638932496075,
      "grad_norm": 3.5306732654571533,
      "learning_rate": 3.4506475667189954e-05,
      "loss": 0.6148,
      "step": 1579100
    },
    {
      "epoch": 24.791208791208792,
      "grad_norm": 3.9835588932037354,
      "learning_rate": 3.4505494505494505e-05,
      "loss": 0.598,
      "step": 1579200
    },
    {
      "epoch": 24.79277864992151,
      "grad_norm": 5.309844970703125,
      "learning_rate": 3.450451334379906e-05,
      "loss": 0.6089,
      "step": 1579300
    },
    {
      "epoch": 24.794348508634222,
      "grad_norm": 4.403220176696777,
      "learning_rate": 3.4503532182103613e-05,
      "loss": 0.5994,
      "step": 1579400
    },
    {
      "epoch": 24.79591836734694,
      "grad_norm": 3.9305918216705322,
      "learning_rate": 3.4502551020408164e-05,
      "loss": 0.55,
      "step": 1579500
    },
    {
      "epoch": 24.797488226059656,
      "grad_norm": 3.159499406814575,
      "learning_rate": 3.4501569858712715e-05,
      "loss": 0.6085,
      "step": 1579600
    },
    {
      "epoch": 24.79905808477237,
      "grad_norm": 4.779597759246826,
      "learning_rate": 3.450058869701727e-05,
      "loss": 0.5969,
      "step": 1579700
    },
    {
      "epoch": 24.800627943485086,
      "grad_norm": 3.622232675552368,
      "learning_rate": 3.449960753532182e-05,
      "loss": 0.576,
      "step": 1579800
    },
    {
      "epoch": 24.802197802197803,
      "grad_norm": 3.417649745941162,
      "learning_rate": 3.4498626373626375e-05,
      "loss": 0.6534,
      "step": 1579900
    },
    {
      "epoch": 24.80376766091052,
      "grad_norm": 4.343930244445801,
      "learning_rate": 3.4497645211930926e-05,
      "loss": 0.6019,
      "step": 1580000
    },
    {
      "epoch": 24.805337519623233,
      "grad_norm": 4.2511773109436035,
      "learning_rate": 3.4496664050235484e-05,
      "loss": 0.6021,
      "step": 1580100
    },
    {
      "epoch": 24.80690737833595,
      "grad_norm": 4.612208366394043,
      "learning_rate": 3.4495682888540035e-05,
      "loss": 0.5627,
      "step": 1580200
    },
    {
      "epoch": 24.808477237048667,
      "grad_norm": 3.953876256942749,
      "learning_rate": 3.4494701726844586e-05,
      "loss": 0.5733,
      "step": 1580300
    },
    {
      "epoch": 24.81004709576138,
      "grad_norm": 2.2687573432922363,
      "learning_rate": 3.449372056514914e-05,
      "loss": 0.5765,
      "step": 1580400
    },
    {
      "epoch": 24.811616954474097,
      "grad_norm": 3.7224888801574707,
      "learning_rate": 3.449273940345369e-05,
      "loss": 0.5722,
      "step": 1580500
    },
    {
      "epoch": 24.813186813186814,
      "grad_norm": 3.784907341003418,
      "learning_rate": 3.4491758241758246e-05,
      "loss": 0.5791,
      "step": 1580600
    },
    {
      "epoch": 24.81475667189953,
      "grad_norm": 4.8299055099487305,
      "learning_rate": 3.4490777080062797e-05,
      "loss": 0.6002,
      "step": 1580700
    },
    {
      "epoch": 24.816326530612244,
      "grad_norm": 3.1708199977874756,
      "learning_rate": 3.4489795918367354e-05,
      "loss": 0.617,
      "step": 1580800
    },
    {
      "epoch": 24.81789638932496,
      "grad_norm": 3.3699593544006348,
      "learning_rate": 3.44888147566719e-05,
      "loss": 0.5839,
      "step": 1580900
    },
    {
      "epoch": 24.819466248037678,
      "grad_norm": 4.249979019165039,
      "learning_rate": 3.4487833594976456e-05,
      "loss": 0.6333,
      "step": 1581000
    },
    {
      "epoch": 24.82103610675039,
      "grad_norm": 4.224925518035889,
      "learning_rate": 3.448685243328101e-05,
      "loss": 0.588,
      "step": 1581100
    },
    {
      "epoch": 24.822605965463108,
      "grad_norm": 3.9400932788848877,
      "learning_rate": 3.448587127158556e-05,
      "loss": 0.6079,
      "step": 1581200
    },
    {
      "epoch": 24.824175824175825,
      "grad_norm": 3.391319990158081,
      "learning_rate": 3.448489010989011e-05,
      "loss": 0.62,
      "step": 1581300
    },
    {
      "epoch": 24.82574568288854,
      "grad_norm": 3.767096757888794,
      "learning_rate": 3.448390894819467e-05,
      "loss": 0.6213,
      "step": 1581400
    },
    {
      "epoch": 24.827315541601255,
      "grad_norm": 4.949525833129883,
      "learning_rate": 3.448292778649921e-05,
      "loss": 0.6137,
      "step": 1581500
    },
    {
      "epoch": 24.828885400313972,
      "grad_norm": 3.644728422164917,
      "learning_rate": 3.448194662480377e-05,
      "loss": 0.5728,
      "step": 1581600
    },
    {
      "epoch": 24.83045525902669,
      "grad_norm": 2.8536012172698975,
      "learning_rate": 3.448096546310832e-05,
      "loss": 0.5668,
      "step": 1581700
    },
    {
      "epoch": 24.832025117739402,
      "grad_norm": 3.6138548851013184,
      "learning_rate": 3.447998430141288e-05,
      "loss": 0.5756,
      "step": 1581800
    },
    {
      "epoch": 24.83359497645212,
      "grad_norm": 4.26783561706543,
      "learning_rate": 3.447900313971742e-05,
      "loss": 0.6161,
      "step": 1581900
    },
    {
      "epoch": 24.835164835164836,
      "grad_norm": 4.370704174041748,
      "learning_rate": 3.447802197802198e-05,
      "loss": 0.5993,
      "step": 1582000
    },
    {
      "epoch": 24.836734693877553,
      "grad_norm": 2.6506338119506836,
      "learning_rate": 3.447704081632653e-05,
      "loss": 0.5721,
      "step": 1582100
    },
    {
      "epoch": 24.838304552590266,
      "grad_norm": 4.060159206390381,
      "learning_rate": 3.447605965463108e-05,
      "loss": 0.6141,
      "step": 1582200
    },
    {
      "epoch": 24.839874411302983,
      "grad_norm": 4.533186435699463,
      "learning_rate": 3.447507849293564e-05,
      "loss": 0.5689,
      "step": 1582300
    },
    {
      "epoch": 24.8414442700157,
      "grad_norm": 3.318347930908203,
      "learning_rate": 3.447409733124019e-05,
      "loss": 0.6391,
      "step": 1582400
    },
    {
      "epoch": 24.843014128728413,
      "grad_norm": 4.383402347564697,
      "learning_rate": 3.447311616954474e-05,
      "loss": 0.6135,
      "step": 1582500
    },
    {
      "epoch": 24.84458398744113,
      "grad_norm": 3.3476076126098633,
      "learning_rate": 3.447213500784929e-05,
      "loss": 0.5698,
      "step": 1582600
    },
    {
      "epoch": 24.846153846153847,
      "grad_norm": 3.730616331100464,
      "learning_rate": 3.447115384615385e-05,
      "loss": 0.6131,
      "step": 1582700
    },
    {
      "epoch": 24.847723704866564,
      "grad_norm": 2.7614197731018066,
      "learning_rate": 3.44701726844584e-05,
      "loss": 0.5906,
      "step": 1582800
    },
    {
      "epoch": 24.849293563579277,
      "grad_norm": 3.087493419647217,
      "learning_rate": 3.446919152276295e-05,
      "loss": 0.5621,
      "step": 1582900
    },
    {
      "epoch": 24.850863422291994,
      "grad_norm": 3.8393638134002686,
      "learning_rate": 3.44682103610675e-05,
      "loss": 0.5826,
      "step": 1583000
    },
    {
      "epoch": 24.85243328100471,
      "grad_norm": 3.140105724334717,
      "learning_rate": 3.446722919937206e-05,
      "loss": 0.5633,
      "step": 1583100
    },
    {
      "epoch": 24.854003139717424,
      "grad_norm": 2.897505283355713,
      "learning_rate": 3.446624803767661e-05,
      "loss": 0.5983,
      "step": 1583200
    },
    {
      "epoch": 24.85557299843014,
      "grad_norm": 3.153204917907715,
      "learning_rate": 3.446526687598116e-05,
      "loss": 0.5691,
      "step": 1583300
    },
    {
      "epoch": 24.857142857142858,
      "grad_norm": 4.018843173980713,
      "learning_rate": 3.4464285714285714e-05,
      "loss": 0.5923,
      "step": 1583400
    },
    {
      "epoch": 24.858712715855575,
      "grad_norm": 4.2460737228393555,
      "learning_rate": 3.446330455259027e-05,
      "loss": 0.5978,
      "step": 1583500
    },
    {
      "epoch": 24.860282574568288,
      "grad_norm": 4.279719352722168,
      "learning_rate": 3.4462323390894816e-05,
      "loss": 0.6122,
      "step": 1583600
    },
    {
      "epoch": 24.861852433281005,
      "grad_norm": 4.251806259155273,
      "learning_rate": 3.4461342229199373e-05,
      "loss": 0.6307,
      "step": 1583700
    },
    {
      "epoch": 24.86342229199372,
      "grad_norm": 2.6110117435455322,
      "learning_rate": 3.4460361067503924e-05,
      "loss": 0.5791,
      "step": 1583800
    },
    {
      "epoch": 24.864992150706435,
      "grad_norm": 4.497589588165283,
      "learning_rate": 3.445937990580848e-05,
      "loss": 0.5921,
      "step": 1583900
    },
    {
      "epoch": 24.86656200941915,
      "grad_norm": 3.9095609188079834,
      "learning_rate": 3.4458398744113026e-05,
      "loss": 0.5983,
      "step": 1584000
    },
    {
      "epoch": 24.86813186813187,
      "grad_norm": 3.5099589824676514,
      "learning_rate": 3.4457417582417584e-05,
      "loss": 0.5735,
      "step": 1584100
    },
    {
      "epoch": 24.869701726844585,
      "grad_norm": 3.7321486473083496,
      "learning_rate": 3.4456436420722135e-05,
      "loss": 0.616,
      "step": 1584200
    },
    {
      "epoch": 24.8712715855573,
      "grad_norm": 4.422101974487305,
      "learning_rate": 3.4455455259026686e-05,
      "loss": 0.5564,
      "step": 1584300
    },
    {
      "epoch": 24.872841444270016,
      "grad_norm": 3.6695749759674072,
      "learning_rate": 3.4454474097331244e-05,
      "loss": 0.5986,
      "step": 1584400
    },
    {
      "epoch": 24.874411302982733,
      "grad_norm": 3.7359673976898193,
      "learning_rate": 3.4453492935635795e-05,
      "loss": 0.6391,
      "step": 1584500
    },
    {
      "epoch": 24.875981161695446,
      "grad_norm": 3.539734363555908,
      "learning_rate": 3.4452511773940346e-05,
      "loss": 0.5749,
      "step": 1584600
    },
    {
      "epoch": 24.877551020408163,
      "grad_norm": 3.183371067047119,
      "learning_rate": 3.44515306122449e-05,
      "loss": 0.6136,
      "step": 1584700
    },
    {
      "epoch": 24.87912087912088,
      "grad_norm": 3.856081008911133,
      "learning_rate": 3.4450549450549455e-05,
      "loss": 0.5602,
      "step": 1584800
    },
    {
      "epoch": 24.880690737833596,
      "grad_norm": 4.1685380935668945,
      "learning_rate": 3.4449568288854006e-05,
      "loss": 0.5953,
      "step": 1584900
    },
    {
      "epoch": 24.88226059654631,
      "grad_norm": 3.56931471824646,
      "learning_rate": 3.4448587127158556e-05,
      "loss": 0.5578,
      "step": 1585000
    },
    {
      "epoch": 24.883830455259027,
      "grad_norm": 3.522393226623535,
      "learning_rate": 3.444760596546311e-05,
      "loss": 0.5895,
      "step": 1585100
    },
    {
      "epoch": 24.885400313971743,
      "grad_norm": 4.298951625823975,
      "learning_rate": 3.4446624803767665e-05,
      "loss": 0.6019,
      "step": 1585200
    },
    {
      "epoch": 24.886970172684457,
      "grad_norm": 2.3967907428741455,
      "learning_rate": 3.4445643642072216e-05,
      "loss": 0.5915,
      "step": 1585300
    },
    {
      "epoch": 24.888540031397174,
      "grad_norm": 4.614340305328369,
      "learning_rate": 3.444466248037677e-05,
      "loss": 0.6033,
      "step": 1585400
    },
    {
      "epoch": 24.89010989010989,
      "grad_norm": 4.705072402954102,
      "learning_rate": 3.444368131868132e-05,
      "loss": 0.5936,
      "step": 1585500
    },
    {
      "epoch": 24.891679748822607,
      "grad_norm": 4.7496724128723145,
      "learning_rate": 3.4442700156985876e-05,
      "loss": 0.5952,
      "step": 1585600
    },
    {
      "epoch": 24.89324960753532,
      "grad_norm": 4.233424663543701,
      "learning_rate": 3.444171899529042e-05,
      "loss": 0.6136,
      "step": 1585700
    },
    {
      "epoch": 24.894819466248038,
      "grad_norm": 3.406146287918091,
      "learning_rate": 3.444073783359498e-05,
      "loss": 0.6335,
      "step": 1585800
    },
    {
      "epoch": 24.896389324960754,
      "grad_norm": 2.2962098121643066,
      "learning_rate": 3.443975667189953e-05,
      "loss": 0.6012,
      "step": 1585900
    },
    {
      "epoch": 24.897959183673468,
      "grad_norm": 3.5310919284820557,
      "learning_rate": 3.443877551020409e-05,
      "loss": 0.5922,
      "step": 1586000
    },
    {
      "epoch": 24.899529042386185,
      "grad_norm": 3.687347173690796,
      "learning_rate": 3.443779434850863e-05,
      "loss": 0.6125,
      "step": 1586100
    },
    {
      "epoch": 24.9010989010989,
      "grad_norm": 5.073283672332764,
      "learning_rate": 3.443681318681319e-05,
      "loss": 0.6145,
      "step": 1586200
    },
    {
      "epoch": 24.90266875981162,
      "grad_norm": 3.386859893798828,
      "learning_rate": 3.443583202511774e-05,
      "loss": 0.5811,
      "step": 1586300
    },
    {
      "epoch": 24.90423861852433,
      "grad_norm": 2.769787311553955,
      "learning_rate": 3.443485086342229e-05,
      "loss": 0.596,
      "step": 1586400
    },
    {
      "epoch": 24.90580847723705,
      "grad_norm": 4.245790481567383,
      "learning_rate": 3.443386970172685e-05,
      "loss": 0.6325,
      "step": 1586500
    },
    {
      "epoch": 24.907378335949765,
      "grad_norm": 4.483903884887695,
      "learning_rate": 3.44328885400314e-05,
      "loss": 0.5719,
      "step": 1586600
    },
    {
      "epoch": 24.90894819466248,
      "grad_norm": 3.6036651134490967,
      "learning_rate": 3.443190737833595e-05,
      "loss": 0.6095,
      "step": 1586700
    },
    {
      "epoch": 24.910518053375196,
      "grad_norm": 2.9500296115875244,
      "learning_rate": 3.44309262166405e-05,
      "loss": 0.6103,
      "step": 1586800
    },
    {
      "epoch": 24.912087912087912,
      "grad_norm": 3.8319754600524902,
      "learning_rate": 3.442994505494506e-05,
      "loss": 0.6037,
      "step": 1586900
    },
    {
      "epoch": 24.91365777080063,
      "grad_norm": 4.355278491973877,
      "learning_rate": 3.442896389324961e-05,
      "loss": 0.5853,
      "step": 1587000
    },
    {
      "epoch": 24.915227629513343,
      "grad_norm": 2.8368589878082275,
      "learning_rate": 3.442798273155416e-05,
      "loss": 0.6094,
      "step": 1587100
    },
    {
      "epoch": 24.91679748822606,
      "grad_norm": 3.167663097381592,
      "learning_rate": 3.442700156985871e-05,
      "loss": 0.5882,
      "step": 1587200
    },
    {
      "epoch": 24.918367346938776,
      "grad_norm": 3.8665194511413574,
      "learning_rate": 3.442602040816327e-05,
      "loss": 0.6369,
      "step": 1587300
    },
    {
      "epoch": 24.919937205651493,
      "grad_norm": 3.348316192626953,
      "learning_rate": 3.442503924646782e-05,
      "loss": 0.5944,
      "step": 1587400
    },
    {
      "epoch": 24.921507064364206,
      "grad_norm": 3.0869102478027344,
      "learning_rate": 3.442405808477237e-05,
      "loss": 0.5998,
      "step": 1587500
    },
    {
      "epoch": 24.923076923076923,
      "grad_norm": 4.6597900390625,
      "learning_rate": 3.442307692307692e-05,
      "loss": 0.6164,
      "step": 1587600
    },
    {
      "epoch": 24.92464678178964,
      "grad_norm": 3.698452949523926,
      "learning_rate": 3.442209576138148e-05,
      "loss": 0.6234,
      "step": 1587700
    },
    {
      "epoch": 24.926216640502354,
      "grad_norm": 4.04765510559082,
      "learning_rate": 3.4421114599686025e-05,
      "loss": 0.6148,
      "step": 1587800
    },
    {
      "epoch": 24.92778649921507,
      "grad_norm": 4.44963264465332,
      "learning_rate": 3.442013343799058e-05,
      "loss": 0.5986,
      "step": 1587900
    },
    {
      "epoch": 24.929356357927787,
      "grad_norm": 3.978635549545288,
      "learning_rate": 3.441915227629513e-05,
      "loss": 0.6268,
      "step": 1588000
    },
    {
      "epoch": 24.9309262166405,
      "grad_norm": 4.641765594482422,
      "learning_rate": 3.441817111459969e-05,
      "loss": 0.6281,
      "step": 1588100
    },
    {
      "epoch": 24.932496075353217,
      "grad_norm": 2.882471799850464,
      "learning_rate": 3.4417189952904235e-05,
      "loss": 0.5823,
      "step": 1588200
    },
    {
      "epoch": 24.934065934065934,
      "grad_norm": 3.7053186893463135,
      "learning_rate": 3.441620879120879e-05,
      "loss": 0.6168,
      "step": 1588300
    },
    {
      "epoch": 24.93563579277865,
      "grad_norm": 3.4106502532958984,
      "learning_rate": 3.4415227629513344e-05,
      "loss": 0.591,
      "step": 1588400
    },
    {
      "epoch": 24.937205651491364,
      "grad_norm": 3.5191638469696045,
      "learning_rate": 3.4414246467817895e-05,
      "loss": 0.5879,
      "step": 1588500
    },
    {
      "epoch": 24.93877551020408,
      "grad_norm": 3.4086337089538574,
      "learning_rate": 3.441326530612245e-05,
      "loss": 0.5662,
      "step": 1588600
    },
    {
      "epoch": 24.940345368916798,
      "grad_norm": 3.9141921997070312,
      "learning_rate": 3.4412284144427004e-05,
      "loss": 0.577,
      "step": 1588700
    },
    {
      "epoch": 24.941915227629515,
      "grad_norm": 3.642901659011841,
      "learning_rate": 3.4411302982731555e-05,
      "loss": 0.5808,
      "step": 1588800
    },
    {
      "epoch": 24.94348508634223,
      "grad_norm": 3.1101877689361572,
      "learning_rate": 3.4410321821036106e-05,
      "loss": 0.5724,
      "step": 1588900
    },
    {
      "epoch": 24.945054945054945,
      "grad_norm": 3.4126839637756348,
      "learning_rate": 3.4409340659340664e-05,
      "loss": 0.5716,
      "step": 1589000
    },
    {
      "epoch": 24.946624803767662,
      "grad_norm": 2.6794114112854004,
      "learning_rate": 3.4408359497645214e-05,
      "loss": 0.627,
      "step": 1589100
    },
    {
      "epoch": 24.948194662480375,
      "grad_norm": 4.042174816131592,
      "learning_rate": 3.4407378335949765e-05,
      "loss": 0.625,
      "step": 1589200
    },
    {
      "epoch": 24.949764521193092,
      "grad_norm": 3.3193166255950928,
      "learning_rate": 3.4406397174254316e-05,
      "loss": 0.5608,
      "step": 1589300
    },
    {
      "epoch": 24.95133437990581,
      "grad_norm": 3.091318368911743,
      "learning_rate": 3.4405416012558874e-05,
      "loss": 0.6012,
      "step": 1589400
    },
    {
      "epoch": 24.952904238618526,
      "grad_norm": 2.7533578872680664,
      "learning_rate": 3.4404434850863425e-05,
      "loss": 0.5918,
      "step": 1589500
    },
    {
      "epoch": 24.95447409733124,
      "grad_norm": 3.492901563644409,
      "learning_rate": 3.4403453689167976e-05,
      "loss": 0.6412,
      "step": 1589600
    },
    {
      "epoch": 24.956043956043956,
      "grad_norm": 4.294338703155518,
      "learning_rate": 3.440247252747253e-05,
      "loss": 0.6231,
      "step": 1589700
    },
    {
      "epoch": 24.957613814756673,
      "grad_norm": 3.65901517868042,
      "learning_rate": 3.4401491365777085e-05,
      "loss": 0.5954,
      "step": 1589800
    },
    {
      "epoch": 24.959183673469386,
      "grad_norm": 5.255119323730469,
      "learning_rate": 3.440051020408163e-05,
      "loss": 0.5841,
      "step": 1589900
    },
    {
      "epoch": 24.960753532182103,
      "grad_norm": 5.6404290199279785,
      "learning_rate": 3.439952904238619e-05,
      "loss": 0.6161,
      "step": 1590000
    },
    {
      "epoch": 24.96232339089482,
      "grad_norm": 4.0588250160217285,
      "learning_rate": 3.439854788069074e-05,
      "loss": 0.6158,
      "step": 1590100
    },
    {
      "epoch": 24.963893249607537,
      "grad_norm": 2.502312660217285,
      "learning_rate": 3.4397566718995296e-05,
      "loss": 0.5979,
      "step": 1590200
    },
    {
      "epoch": 24.96546310832025,
      "grad_norm": 3.848640203475952,
      "learning_rate": 3.439658555729984e-05,
      "loss": 0.5846,
      "step": 1590300
    },
    {
      "epoch": 24.967032967032967,
      "grad_norm": 3.5625510215759277,
      "learning_rate": 3.43956043956044e-05,
      "loss": 0.6026,
      "step": 1590400
    },
    {
      "epoch": 24.968602825745684,
      "grad_norm": 3.399751901626587,
      "learning_rate": 3.439462323390895e-05,
      "loss": 0.5907,
      "step": 1590500
    },
    {
      "epoch": 24.970172684458397,
      "grad_norm": 3.5568411350250244,
      "learning_rate": 3.43936420722135e-05,
      "loss": 0.5895,
      "step": 1590600
    },
    {
      "epoch": 24.971742543171114,
      "grad_norm": 4.061985969543457,
      "learning_rate": 3.439266091051806e-05,
      "loss": 0.5944,
      "step": 1590700
    },
    {
      "epoch": 24.97331240188383,
      "grad_norm": 3.3119053840637207,
      "learning_rate": 3.439167974882261e-05,
      "loss": 0.6064,
      "step": 1590800
    },
    {
      "epoch": 24.974882260596548,
      "grad_norm": 2.544678211212158,
      "learning_rate": 3.439069858712716e-05,
      "loss": 0.6209,
      "step": 1590900
    },
    {
      "epoch": 24.97645211930926,
      "grad_norm": 3.5948610305786133,
      "learning_rate": 3.438971742543171e-05,
      "loss": 0.579,
      "step": 1591000
    },
    {
      "epoch": 24.978021978021978,
      "grad_norm": 3.4905431270599365,
      "learning_rate": 3.438873626373627e-05,
      "loss": 0.6013,
      "step": 1591100
    },
    {
      "epoch": 24.979591836734695,
      "grad_norm": 4.198214054107666,
      "learning_rate": 3.438775510204082e-05,
      "loss": 0.5857,
      "step": 1591200
    },
    {
      "epoch": 24.98116169544741,
      "grad_norm": 2.9897589683532715,
      "learning_rate": 3.438677394034537e-05,
      "loss": 0.5674,
      "step": 1591300
    },
    {
      "epoch": 24.982731554160125,
      "grad_norm": 4.877976894378662,
      "learning_rate": 3.438579277864992e-05,
      "loss": 0.6022,
      "step": 1591400
    },
    {
      "epoch": 24.984301412872842,
      "grad_norm": 3.696256637573242,
      "learning_rate": 3.438481161695448e-05,
      "loss": 0.6421,
      "step": 1591500
    },
    {
      "epoch": 24.98587127158556,
      "grad_norm": 3.908111333847046,
      "learning_rate": 3.438383045525903e-05,
      "loss": 0.5859,
      "step": 1591600
    },
    {
      "epoch": 24.987441130298272,
      "grad_norm": 3.4333388805389404,
      "learning_rate": 3.438284929356358e-05,
      "loss": 0.6032,
      "step": 1591700
    },
    {
      "epoch": 24.98901098901099,
      "grad_norm": 3.4542534351348877,
      "learning_rate": 3.438186813186813e-05,
      "loss": 0.5838,
      "step": 1591800
    },
    {
      "epoch": 24.990580847723706,
      "grad_norm": 4.253023147583008,
      "learning_rate": 3.438088697017269e-05,
      "loss": 0.6232,
      "step": 1591900
    },
    {
      "epoch": 24.99215070643642,
      "grad_norm": 3.6648542881011963,
      "learning_rate": 3.4379905808477234e-05,
      "loss": 0.5559,
      "step": 1592000
    },
    {
      "epoch": 24.993720565149136,
      "grad_norm": 3.807969570159912,
      "learning_rate": 3.437892464678179e-05,
      "loss": 0.5717,
      "step": 1592100
    },
    {
      "epoch": 24.995290423861853,
      "grad_norm": 4.374392986297607,
      "learning_rate": 3.437794348508634e-05,
      "loss": 0.5933,
      "step": 1592200
    },
    {
      "epoch": 24.99686028257457,
      "grad_norm": 3.3232030868530273,
      "learning_rate": 3.43769623233909e-05,
      "loss": 0.6081,
      "step": 1592300
    },
    {
      "epoch": 24.998430141287283,
      "grad_norm": 3.8515267372131348,
      "learning_rate": 3.4375981161695444e-05,
      "loss": 0.62,
      "step": 1592400
    },
    {
      "epoch": 25.0,
      "grad_norm": 3.698754072189331,
      "learning_rate": 3.4375e-05,
      "loss": 0.5983,
      "step": 1592500
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.0268239974975586,
      "eval_runtime": 18.0217,
      "eval_samples_per_second": 186.053,
      "eval_steps_per_second": 186.053,
      "step": 1592500
    },
    {
      "epoch": 25.0,
      "eval_loss": 0.4597136378288269,
      "eval_runtime": 343.3754,
      "eval_samples_per_second": 185.511,
      "eval_steps_per_second": 185.511,
      "step": 1592500
    },
    {
      "epoch": 25.001569858712717,
      "grad_norm": 3.517158031463623,
      "learning_rate": 3.437401883830455e-05,
      "loss": 0.6121,
      "step": 1592600
    },
    {
      "epoch": 25.00313971742543,
      "grad_norm": 3.7143161296844482,
      "learning_rate": 3.4373037676609104e-05,
      "loss": 0.5601,
      "step": 1592700
    },
    {
      "epoch": 25.004709576138147,
      "grad_norm": 3.867414712905884,
      "learning_rate": 3.437205651491366e-05,
      "loss": 0.6016,
      "step": 1592800
    },
    {
      "epoch": 25.006279434850864,
      "grad_norm": 3.450435161590576,
      "learning_rate": 3.437107535321821e-05,
      "loss": 0.5854,
      "step": 1592900
    },
    {
      "epoch": 25.00784929356358,
      "grad_norm": 3.8386309146881104,
      "learning_rate": 3.4370094191522764e-05,
      "loss": 0.5992,
      "step": 1593000
    },
    {
      "epoch": 25.009419152276294,
      "grad_norm": 4.290835857391357,
      "learning_rate": 3.4369113029827315e-05,
      "loss": 0.6222,
      "step": 1593100
    },
    {
      "epoch": 25.01098901098901,
      "grad_norm": 4.195199966430664,
      "learning_rate": 3.436813186813187e-05,
      "loss": 0.5903,
      "step": 1593200
    },
    {
      "epoch": 25.012558869701728,
      "grad_norm": 4.035722732543945,
      "learning_rate": 3.4367150706436423e-05,
      "loss": 0.5742,
      "step": 1593300
    },
    {
      "epoch": 25.01412872841444,
      "grad_norm": 2.5654337406158447,
      "learning_rate": 3.4366169544740974e-05,
      "loss": 0.6017,
      "step": 1593400
    },
    {
      "epoch": 25.015698587127158,
      "grad_norm": 3.7912986278533936,
      "learning_rate": 3.4365188383045525e-05,
      "loss": 0.5497,
      "step": 1593500
    },
    {
      "epoch": 25.017268445839875,
      "grad_norm": 3.5058701038360596,
      "learning_rate": 3.436420722135008e-05,
      "loss": 0.5976,
      "step": 1593600
    },
    {
      "epoch": 25.01883830455259,
      "grad_norm": 3.407531499862671,
      "learning_rate": 3.4363226059654634e-05,
      "loss": 0.5722,
      "step": 1593700
    },
    {
      "epoch": 25.020408163265305,
      "grad_norm": 2.728673219680786,
      "learning_rate": 3.4362244897959185e-05,
      "loss": 0.6083,
      "step": 1593800
    },
    {
      "epoch": 25.021978021978022,
      "grad_norm": 3.4943861961364746,
      "learning_rate": 3.4361263736263736e-05,
      "loss": 0.5997,
      "step": 1593900
    },
    {
      "epoch": 25.02354788069074,
      "grad_norm": 4.727126598358154,
      "learning_rate": 3.4360282574568294e-05,
      "loss": 0.6076,
      "step": 1594000
    },
    {
      "epoch": 25.025117739403452,
      "grad_norm": 4.741266250610352,
      "learning_rate": 3.435930141287284e-05,
      "loss": 0.5864,
      "step": 1594100
    },
    {
      "epoch": 25.02668759811617,
      "grad_norm": 3.1570560932159424,
      "learning_rate": 3.4358320251177396e-05,
      "loss": 0.6185,
      "step": 1594200
    },
    {
      "epoch": 25.028257456828886,
      "grad_norm": 3.300048828125,
      "learning_rate": 3.435733908948195e-05,
      "loss": 0.5621,
      "step": 1594300
    },
    {
      "epoch": 25.029827315541603,
      "grad_norm": 4.204316139221191,
      "learning_rate": 3.4356357927786505e-05,
      "loss": 0.5556,
      "step": 1594400
    },
    {
      "epoch": 25.031397174254316,
      "grad_norm": 3.2503457069396973,
      "learning_rate": 3.435537676609105e-05,
      "loss": 0.615,
      "step": 1594500
    },
    {
      "epoch": 25.032967032967033,
      "grad_norm": 3.2919552326202393,
      "learning_rate": 3.4354395604395607e-05,
      "loss": 0.5797,
      "step": 1594600
    },
    {
      "epoch": 25.03453689167975,
      "grad_norm": 3.012160539627075,
      "learning_rate": 3.435341444270016e-05,
      "loss": 0.5996,
      "step": 1594700
    },
    {
      "epoch": 25.036106750392463,
      "grad_norm": 3.9823577404022217,
      "learning_rate": 3.435243328100471e-05,
      "loss": 0.5945,
      "step": 1594800
    },
    {
      "epoch": 25.03767660910518,
      "grad_norm": 2.531008720397949,
      "learning_rate": 3.4351452119309266e-05,
      "loss": 0.5772,
      "step": 1594900
    },
    {
      "epoch": 25.039246467817897,
      "grad_norm": 3.573145866394043,
      "learning_rate": 3.435047095761382e-05,
      "loss": 0.5785,
      "step": 1595000
    },
    {
      "epoch": 25.040816326530614,
      "grad_norm": 4.60502290725708,
      "learning_rate": 3.434948979591837e-05,
      "loss": 0.5593,
      "step": 1595100
    },
    {
      "epoch": 25.042386185243327,
      "grad_norm": 4.0058159828186035,
      "learning_rate": 3.434850863422292e-05,
      "loss": 0.5737,
      "step": 1595200
    },
    {
      "epoch": 25.043956043956044,
      "grad_norm": 3.5049266815185547,
      "learning_rate": 3.434752747252748e-05,
      "loss": 0.6031,
      "step": 1595300
    },
    {
      "epoch": 25.04552590266876,
      "grad_norm": 3.714022159576416,
      "learning_rate": 3.434654631083203e-05,
      "loss": 0.6075,
      "step": 1595400
    },
    {
      "epoch": 25.047095761381474,
      "grad_norm": 3.3607895374298096,
      "learning_rate": 3.434556514913658e-05,
      "loss": 0.5654,
      "step": 1595500
    },
    {
      "epoch": 25.04866562009419,
      "grad_norm": 4.137118339538574,
      "learning_rate": 3.434458398744113e-05,
      "loss": 0.5939,
      "step": 1595600
    },
    {
      "epoch": 25.050235478806908,
      "grad_norm": 3.361342430114746,
      "learning_rate": 3.434360282574569e-05,
      "loss": 0.5896,
      "step": 1595700
    },
    {
      "epoch": 25.051805337519625,
      "grad_norm": 4.64078950881958,
      "learning_rate": 3.434262166405024e-05,
      "loss": 0.5973,
      "step": 1595800
    },
    {
      "epoch": 25.053375196232338,
      "grad_norm": 3.7254202365875244,
      "learning_rate": 3.434164050235479e-05,
      "loss": 0.6266,
      "step": 1595900
    },
    {
      "epoch": 25.054945054945055,
      "grad_norm": 4.764688491821289,
      "learning_rate": 3.434065934065934e-05,
      "loss": 0.6154,
      "step": 1596000
    },
    {
      "epoch": 25.05651491365777,
      "grad_norm": 4.0161967277526855,
      "learning_rate": 3.43396781789639e-05,
      "loss": 0.5819,
      "step": 1596100
    },
    {
      "epoch": 25.058084772370485,
      "grad_norm": 4.716852188110352,
      "learning_rate": 3.433869701726844e-05,
      "loss": 0.5907,
      "step": 1596200
    },
    {
      "epoch": 25.059654631083202,
      "grad_norm": 4.407571315765381,
      "learning_rate": 3.4337715855573e-05,
      "loss": 0.5787,
      "step": 1596300
    },
    {
      "epoch": 25.06122448979592,
      "grad_norm": 4.848697185516357,
      "learning_rate": 3.433673469387755e-05,
      "loss": 0.5489,
      "step": 1596400
    },
    {
      "epoch": 25.062794348508636,
      "grad_norm": 3.371267080307007,
      "learning_rate": 3.433575353218211e-05,
      "loss": 0.5974,
      "step": 1596500
    },
    {
      "epoch": 25.06436420722135,
      "grad_norm": 2.426032304763794,
      "learning_rate": 3.433477237048665e-05,
      "loss": 0.6009,
      "step": 1596600
    },
    {
      "epoch": 25.065934065934066,
      "grad_norm": 3.2395997047424316,
      "learning_rate": 3.433379120879121e-05,
      "loss": 0.601,
      "step": 1596700
    },
    {
      "epoch": 25.067503924646783,
      "grad_norm": 3.937592029571533,
      "learning_rate": 3.433281004709576e-05,
      "loss": 0.5739,
      "step": 1596800
    },
    {
      "epoch": 25.069073783359496,
      "grad_norm": 2.2642128467559814,
      "learning_rate": 3.433182888540031e-05,
      "loss": 0.5988,
      "step": 1596900
    },
    {
      "epoch": 25.070643642072213,
      "grad_norm": 3.362426280975342,
      "learning_rate": 3.433084772370487e-05,
      "loss": 0.5883,
      "step": 1597000
    },
    {
      "epoch": 25.07221350078493,
      "grad_norm": 4.186890602111816,
      "learning_rate": 3.432986656200942e-05,
      "loss": 0.5923,
      "step": 1597100
    },
    {
      "epoch": 25.073783359497646,
      "grad_norm": 4.151948928833008,
      "learning_rate": 3.432888540031397e-05,
      "loss": 0.5694,
      "step": 1597200
    },
    {
      "epoch": 25.07535321821036,
      "grad_norm": 4.022858142852783,
      "learning_rate": 3.4327904238618524e-05,
      "loss": 0.6023,
      "step": 1597300
    },
    {
      "epoch": 25.076923076923077,
      "grad_norm": 3.2166850566864014,
      "learning_rate": 3.432692307692308e-05,
      "loss": 0.5917,
      "step": 1597400
    },
    {
      "epoch": 25.078492935635794,
      "grad_norm": 4.032317638397217,
      "learning_rate": 3.432594191522763e-05,
      "loss": 0.5871,
      "step": 1597500
    },
    {
      "epoch": 25.08006279434851,
      "grad_norm": 3.331172227859497,
      "learning_rate": 3.4324960753532183e-05,
      "loss": 0.6083,
      "step": 1597600
    },
    {
      "epoch": 25.081632653061224,
      "grad_norm": 3.6344196796417236,
      "learning_rate": 3.4323979591836734e-05,
      "loss": 0.5979,
      "step": 1597700
    },
    {
      "epoch": 25.08320251177394,
      "grad_norm": 3.4158899784088135,
      "learning_rate": 3.432299843014129e-05,
      "loss": 0.5851,
      "step": 1597800
    },
    {
      "epoch": 25.084772370486657,
      "grad_norm": 4.2933125495910645,
      "learning_rate": 3.432201726844584e-05,
      "loss": 0.5971,
      "step": 1597900
    },
    {
      "epoch": 25.08634222919937,
      "grad_norm": 3.8867568969726562,
      "learning_rate": 3.4321036106750394e-05,
      "loss": 0.5833,
      "step": 1598000
    },
    {
      "epoch": 25.087912087912088,
      "grad_norm": 3.0554912090301514,
      "learning_rate": 3.4320054945054945e-05,
      "loss": 0.5908,
      "step": 1598100
    },
    {
      "epoch": 25.089481946624804,
      "grad_norm": 4.999337673187256,
      "learning_rate": 3.43190737833595e-05,
      "loss": 0.5705,
      "step": 1598200
    },
    {
      "epoch": 25.09105180533752,
      "grad_norm": 3.2792725563049316,
      "learning_rate": 3.431809262166405e-05,
      "loss": 0.5478,
      "step": 1598300
    },
    {
      "epoch": 25.092621664050235,
      "grad_norm": 3.0313870906829834,
      "learning_rate": 3.4317111459968605e-05,
      "loss": 0.554,
      "step": 1598400
    },
    {
      "epoch": 25.09419152276295,
      "grad_norm": 3.664686441421509,
      "learning_rate": 3.4316130298273156e-05,
      "loss": 0.5929,
      "step": 1598500
    },
    {
      "epoch": 25.09576138147567,
      "grad_norm": 2.103844404220581,
      "learning_rate": 3.4315149136577714e-05,
      "loss": 0.5267,
      "step": 1598600
    },
    {
      "epoch": 25.09733124018838,
      "grad_norm": 3.3910253047943115,
      "learning_rate": 3.431416797488226e-05,
      "loss": 0.6002,
      "step": 1598700
    },
    {
      "epoch": 25.0989010989011,
      "grad_norm": 3.7045106887817383,
      "learning_rate": 3.4313186813186816e-05,
      "loss": 0.5945,
      "step": 1598800
    },
    {
      "epoch": 25.100470957613815,
      "grad_norm": 3.924704074859619,
      "learning_rate": 3.4312205651491367e-05,
      "loss": 0.6058,
      "step": 1598900
    },
    {
      "epoch": 25.102040816326532,
      "grad_norm": 3.989570140838623,
      "learning_rate": 3.431122448979592e-05,
      "loss": 0.6222,
      "step": 1599000
    },
    {
      "epoch": 25.103610675039246,
      "grad_norm": 4.708388328552246,
      "learning_rate": 3.4310243328100475e-05,
      "loss": 0.5878,
      "step": 1599100
    },
    {
      "epoch": 25.105180533751962,
      "grad_norm": 3.5005130767822266,
      "learning_rate": 3.4309262166405026e-05,
      "loss": 0.5666,
      "step": 1599200
    },
    {
      "epoch": 25.10675039246468,
      "grad_norm": 4.004904747009277,
      "learning_rate": 3.430828100470958e-05,
      "loss": 0.5676,
      "step": 1599300
    },
    {
      "epoch": 25.108320251177393,
      "grad_norm": 2.773040533065796,
      "learning_rate": 3.430729984301413e-05,
      "loss": 0.6166,
      "step": 1599400
    },
    {
      "epoch": 25.10989010989011,
      "grad_norm": 3.9819421768188477,
      "learning_rate": 3.4306318681318686e-05,
      "loss": 0.5759,
      "step": 1599500
    },
    {
      "epoch": 25.111459968602826,
      "grad_norm": 4.407771587371826,
      "learning_rate": 3.430533751962324e-05,
      "loss": 0.5761,
      "step": 1599600
    },
    {
      "epoch": 25.113029827315543,
      "grad_norm": 3.7517800331115723,
      "learning_rate": 3.430435635792779e-05,
      "loss": 0.5777,
      "step": 1599700
    },
    {
      "epoch": 25.114599686028257,
      "grad_norm": 4.023574352264404,
      "learning_rate": 3.430337519623234e-05,
      "loss": 0.6423,
      "step": 1599800
    },
    {
      "epoch": 25.116169544740973,
      "grad_norm": 3.6079158782958984,
      "learning_rate": 3.43023940345369e-05,
      "loss": 0.5617,
      "step": 1599900
    },
    {
      "epoch": 25.11773940345369,
      "grad_norm": 4.8208160400390625,
      "learning_rate": 3.430141287284145e-05,
      "loss": 0.5973,
      "step": 1600000
    },
    {
      "epoch": 25.119309262166404,
      "grad_norm": 3.5799174308776855,
      "learning_rate": 3.4300431711146e-05,
      "loss": 0.6231,
      "step": 1600100
    },
    {
      "epoch": 25.12087912087912,
      "grad_norm": 3.6884584426879883,
      "learning_rate": 3.429945054945055e-05,
      "loss": 0.5642,
      "step": 1600200
    },
    {
      "epoch": 25.122448979591837,
      "grad_norm": 2.9891250133514404,
      "learning_rate": 3.429846938775511e-05,
      "loss": 0.6109,
      "step": 1600300
    },
    {
      "epoch": 25.124018838304554,
      "grad_norm": 3.789860486984253,
      "learning_rate": 3.429748822605965e-05,
      "loss": 0.5667,
      "step": 1600400
    },
    {
      "epoch": 25.125588697017267,
      "grad_norm": 4.346722602844238,
      "learning_rate": 3.429650706436421e-05,
      "loss": 0.5809,
      "step": 1600500
    },
    {
      "epoch": 25.127158555729984,
      "grad_norm": 3.925478219985962,
      "learning_rate": 3.429552590266876e-05,
      "loss": 0.6054,
      "step": 1600600
    },
    {
      "epoch": 25.1287284144427,
      "grad_norm": 4.030716896057129,
      "learning_rate": 3.429454474097332e-05,
      "loss": 0.5892,
      "step": 1600700
    },
    {
      "epoch": 25.130298273155415,
      "grad_norm": 4.636513710021973,
      "learning_rate": 3.429356357927786e-05,
      "loss": 0.5582,
      "step": 1600800
    },
    {
      "epoch": 25.13186813186813,
      "grad_norm": 3.5577855110168457,
      "learning_rate": 3.429258241758242e-05,
      "loss": 0.6011,
      "step": 1600900
    },
    {
      "epoch": 25.13343799058085,
      "grad_norm": 4.573520660400391,
      "learning_rate": 3.429160125588697e-05,
      "loss": 0.6036,
      "step": 1601000
    },
    {
      "epoch": 25.135007849293565,
      "grad_norm": 3.4463248252868652,
      "learning_rate": 3.429062009419152e-05,
      "loss": 0.5858,
      "step": 1601100
    },
    {
      "epoch": 25.13657770800628,
      "grad_norm": 4.206508636474609,
      "learning_rate": 3.428963893249608e-05,
      "loss": 0.5988,
      "step": 1601200
    },
    {
      "epoch": 25.138147566718995,
      "grad_norm": 4.178844928741455,
      "learning_rate": 3.428865777080063e-05,
      "loss": 0.5852,
      "step": 1601300
    },
    {
      "epoch": 25.139717425431712,
      "grad_norm": 3.4791030883789062,
      "learning_rate": 3.428767660910518e-05,
      "loss": 0.6115,
      "step": 1601400
    },
    {
      "epoch": 25.141287284144425,
      "grad_norm": 3.9308910369873047,
      "learning_rate": 3.428669544740973e-05,
      "loss": 0.5719,
      "step": 1601500
    },
    {
      "epoch": 25.142857142857142,
      "grad_norm": 4.193243503570557,
      "learning_rate": 3.428571428571429e-05,
      "loss": 0.5851,
      "step": 1601600
    },
    {
      "epoch": 25.14442700156986,
      "grad_norm": 3.0089447498321533,
      "learning_rate": 3.428473312401884e-05,
      "loss": 0.6058,
      "step": 1601700
    },
    {
      "epoch": 25.145996860282576,
      "grad_norm": 3.632032871246338,
      "learning_rate": 3.428375196232339e-05,
      "loss": 0.5849,
      "step": 1601800
    },
    {
      "epoch": 25.14756671899529,
      "grad_norm": 3.5999410152435303,
      "learning_rate": 3.4282770800627943e-05,
      "loss": 0.576,
      "step": 1601900
    },
    {
      "epoch": 25.149136577708006,
      "grad_norm": 3.7927989959716797,
      "learning_rate": 3.42817896389325e-05,
      "loss": 0.5679,
      "step": 1602000
    },
    {
      "epoch": 25.150706436420723,
      "grad_norm": 3.6777474880218506,
      "learning_rate": 3.428080847723705e-05,
      "loss": 0.6168,
      "step": 1602100
    },
    {
      "epoch": 25.152276295133436,
      "grad_norm": 2.5044589042663574,
      "learning_rate": 3.42798273155416e-05,
      "loss": 0.602,
      "step": 1602200
    },
    {
      "epoch": 25.153846153846153,
      "grad_norm": 2.7408089637756348,
      "learning_rate": 3.4278846153846154e-05,
      "loss": 0.5833,
      "step": 1602300
    },
    {
      "epoch": 25.15541601255887,
      "grad_norm": 2.917762279510498,
      "learning_rate": 3.427786499215071e-05,
      "loss": 0.5819,
      "step": 1602400
    },
    {
      "epoch": 25.156985871271587,
      "grad_norm": 4.451940536499023,
      "learning_rate": 3.4276883830455256e-05,
      "loss": 0.5998,
      "step": 1602500
    },
    {
      "epoch": 25.1585557299843,
      "grad_norm": 3.815317153930664,
      "learning_rate": 3.4275902668759814e-05,
      "loss": 0.6141,
      "step": 1602600
    },
    {
      "epoch": 25.160125588697017,
      "grad_norm": 4.534448146820068,
      "learning_rate": 3.4274921507064365e-05,
      "loss": 0.6106,
      "step": 1602700
    },
    {
      "epoch": 25.161695447409734,
      "grad_norm": 2.7997734546661377,
      "learning_rate": 3.427394034536892e-05,
      "loss": 0.5907,
      "step": 1602800
    },
    {
      "epoch": 25.163265306122447,
      "grad_norm": 3.3131871223449707,
      "learning_rate": 3.427295918367347e-05,
      "loss": 0.5868,
      "step": 1602900
    },
    {
      "epoch": 25.164835164835164,
      "grad_norm": 3.1738569736480713,
      "learning_rate": 3.4271978021978025e-05,
      "loss": 0.6074,
      "step": 1603000
    },
    {
      "epoch": 25.16640502354788,
      "grad_norm": 4.542996883392334,
      "learning_rate": 3.4270996860282576e-05,
      "loss": 0.6058,
      "step": 1603100
    },
    {
      "epoch": 25.167974882260598,
      "grad_norm": 4.08264684677124,
      "learning_rate": 3.4270015698587126e-05,
      "loss": 0.605,
      "step": 1603200
    },
    {
      "epoch": 25.16954474097331,
      "grad_norm": 2.6953790187835693,
      "learning_rate": 3.4269034536891684e-05,
      "loss": 0.5863,
      "step": 1603300
    },
    {
      "epoch": 25.171114599686028,
      "grad_norm": 3.7885243892669678,
      "learning_rate": 3.4268053375196235e-05,
      "loss": 0.5916,
      "step": 1603400
    },
    {
      "epoch": 25.172684458398745,
      "grad_norm": 3.5239334106445312,
      "learning_rate": 3.4267072213500786e-05,
      "loss": 0.5808,
      "step": 1603500
    },
    {
      "epoch": 25.17425431711146,
      "grad_norm": 4.389895915985107,
      "learning_rate": 3.426609105180534e-05,
      "loss": 0.5995,
      "step": 1603600
    },
    {
      "epoch": 25.175824175824175,
      "grad_norm": 5.041322231292725,
      "learning_rate": 3.4265109890109895e-05,
      "loss": 0.6063,
      "step": 1603700
    },
    {
      "epoch": 25.177394034536892,
      "grad_norm": 3.9891605377197266,
      "learning_rate": 3.4264128728414446e-05,
      "loss": 0.636,
      "step": 1603800
    },
    {
      "epoch": 25.17896389324961,
      "grad_norm": 3.595484733581543,
      "learning_rate": 3.4263147566719e-05,
      "loss": 0.595,
      "step": 1603900
    },
    {
      "epoch": 25.180533751962322,
      "grad_norm": 4.486154079437256,
      "learning_rate": 3.426216640502355e-05,
      "loss": 0.6187,
      "step": 1604000
    },
    {
      "epoch": 25.18210361067504,
      "grad_norm": 4.1773223876953125,
      "learning_rate": 3.4261185243328106e-05,
      "loss": 0.5885,
      "step": 1604100
    },
    {
      "epoch": 25.183673469387756,
      "grad_norm": 3.3154897689819336,
      "learning_rate": 3.426020408163265e-05,
      "loss": 0.5932,
      "step": 1604200
    },
    {
      "epoch": 25.18524332810047,
      "grad_norm": 3.638972282409668,
      "learning_rate": 3.425922291993721e-05,
      "loss": 0.6135,
      "step": 1604300
    },
    {
      "epoch": 25.186813186813186,
      "grad_norm": 3.1395962238311768,
      "learning_rate": 3.425824175824176e-05,
      "loss": 0.5845,
      "step": 1604400
    },
    {
      "epoch": 25.188383045525903,
      "grad_norm": 4.30223274230957,
      "learning_rate": 3.4257260596546316e-05,
      "loss": 0.6109,
      "step": 1604500
    },
    {
      "epoch": 25.18995290423862,
      "grad_norm": 3.4583170413970947,
      "learning_rate": 3.425627943485086e-05,
      "loss": 0.5769,
      "step": 1604600
    },
    {
      "epoch": 25.191522762951333,
      "grad_norm": 3.154287338256836,
      "learning_rate": 3.425529827315542e-05,
      "loss": 0.5849,
      "step": 1604700
    },
    {
      "epoch": 25.19309262166405,
      "grad_norm": 3.0394976139068604,
      "learning_rate": 3.425431711145997e-05,
      "loss": 0.553,
      "step": 1604800
    },
    {
      "epoch": 25.194662480376767,
      "grad_norm": 3.2275948524475098,
      "learning_rate": 3.425333594976452e-05,
      "loss": 0.6138,
      "step": 1604900
    },
    {
      "epoch": 25.19623233908948,
      "grad_norm": 3.2969443798065186,
      "learning_rate": 3.425235478806907e-05,
      "loss": 0.6215,
      "step": 1605000
    },
    {
      "epoch": 25.197802197802197,
      "grad_norm": 4.185300827026367,
      "learning_rate": 3.425137362637363e-05,
      "loss": 0.6408,
      "step": 1605100
    },
    {
      "epoch": 25.199372056514914,
      "grad_norm": 3.988797426223755,
      "learning_rate": 3.425039246467818e-05,
      "loss": 0.6011,
      "step": 1605200
    },
    {
      "epoch": 25.20094191522763,
      "grad_norm": 2.919788122177124,
      "learning_rate": 3.424941130298273e-05,
      "loss": 0.6198,
      "step": 1605300
    },
    {
      "epoch": 25.202511773940344,
      "grad_norm": 3.6742749214172363,
      "learning_rate": 3.424843014128729e-05,
      "loss": 0.5846,
      "step": 1605400
    },
    {
      "epoch": 25.20408163265306,
      "grad_norm": 3.611553430557251,
      "learning_rate": 3.424744897959184e-05,
      "loss": 0.5942,
      "step": 1605500
    },
    {
      "epoch": 25.205651491365778,
      "grad_norm": 4.6598005294799805,
      "learning_rate": 3.424646781789639e-05,
      "loss": 0.6106,
      "step": 1605600
    },
    {
      "epoch": 25.20722135007849,
      "grad_norm": 4.601868152618408,
      "learning_rate": 3.424548665620094e-05,
      "loss": 0.5949,
      "step": 1605700
    },
    {
      "epoch": 25.208791208791208,
      "grad_norm": 4.414025783538818,
      "learning_rate": 3.42445054945055e-05,
      "loss": 0.5795,
      "step": 1605800
    },
    {
      "epoch": 25.210361067503925,
      "grad_norm": 5.705151557922363,
      "learning_rate": 3.424352433281005e-05,
      "loss": 0.6061,
      "step": 1605900
    },
    {
      "epoch": 25.211930926216642,
      "grad_norm": 4.49969482421875,
      "learning_rate": 3.42425431711146e-05,
      "loss": 0.5984,
      "step": 1606000
    },
    {
      "epoch": 25.213500784929355,
      "grad_norm": 3.159090518951416,
      "learning_rate": 3.424156200941915e-05,
      "loss": 0.6189,
      "step": 1606100
    },
    {
      "epoch": 25.215070643642072,
      "grad_norm": 3.039872407913208,
      "learning_rate": 3.424058084772371e-05,
      "loss": 0.5768,
      "step": 1606200
    },
    {
      "epoch": 25.21664050235479,
      "grad_norm": 3.005803108215332,
      "learning_rate": 3.4239599686028254e-05,
      "loss": 0.5658,
      "step": 1606300
    },
    {
      "epoch": 25.218210361067506,
      "grad_norm": 4.07505464553833,
      "learning_rate": 3.423861852433281e-05,
      "loss": 0.606,
      "step": 1606400
    },
    {
      "epoch": 25.21978021978022,
      "grad_norm": 4.035532474517822,
      "learning_rate": 3.423763736263736e-05,
      "loss": 0.5828,
      "step": 1606500
    },
    {
      "epoch": 25.221350078492936,
      "grad_norm": 3.875183343887329,
      "learning_rate": 3.423665620094192e-05,
      "loss": 0.5911,
      "step": 1606600
    },
    {
      "epoch": 25.222919937205653,
      "grad_norm": 5.434495449066162,
      "learning_rate": 3.4235675039246465e-05,
      "loss": 0.5688,
      "step": 1606700
    },
    {
      "epoch": 25.224489795918366,
      "grad_norm": 3.9765331745147705,
      "learning_rate": 3.423469387755102e-05,
      "loss": 0.6151,
      "step": 1606800
    },
    {
      "epoch": 25.226059654631083,
      "grad_norm": 3.755939245223999,
      "learning_rate": 3.4233712715855574e-05,
      "loss": 0.6102,
      "step": 1606900
    },
    {
      "epoch": 25.2276295133438,
      "grad_norm": 3.7166736125946045,
      "learning_rate": 3.4232731554160125e-05,
      "loss": 0.605,
      "step": 1607000
    },
    {
      "epoch": 25.229199372056517,
      "grad_norm": 4.60721492767334,
      "learning_rate": 3.4231750392464676e-05,
      "loss": 0.5981,
      "step": 1607100
    },
    {
      "epoch": 25.23076923076923,
      "grad_norm": 4.125119209289551,
      "learning_rate": 3.4230769230769234e-05,
      "loss": 0.603,
      "step": 1607200
    },
    {
      "epoch": 25.232339089481947,
      "grad_norm": 3.089621067047119,
      "learning_rate": 3.4229788069073784e-05,
      "loss": 0.5867,
      "step": 1607300
    },
    {
      "epoch": 25.233908948194664,
      "grad_norm": 3.515812873840332,
      "learning_rate": 3.4228806907378335e-05,
      "loss": 0.6109,
      "step": 1607400
    },
    {
      "epoch": 25.235478806907377,
      "grad_norm": 3.514503240585327,
      "learning_rate": 3.422782574568289e-05,
      "loss": 0.6038,
      "step": 1607500
    },
    {
      "epoch": 25.237048665620094,
      "grad_norm": 5.561020374298096,
      "learning_rate": 3.4226844583987444e-05,
      "loss": 0.6107,
      "step": 1607600
    },
    {
      "epoch": 25.23861852433281,
      "grad_norm": 4.399458408355713,
      "learning_rate": 3.4225863422291995e-05,
      "loss": 0.6324,
      "step": 1607700
    },
    {
      "epoch": 25.240188383045528,
      "grad_norm": 3.694718599319458,
      "learning_rate": 3.4224882260596546e-05,
      "loss": 0.587,
      "step": 1607800
    },
    {
      "epoch": 25.24175824175824,
      "grad_norm": 3.6312758922576904,
      "learning_rate": 3.4223901098901104e-05,
      "loss": 0.5639,
      "step": 1607900
    },
    {
      "epoch": 25.243328100470958,
      "grad_norm": 4.0325541496276855,
      "learning_rate": 3.4222919937205655e-05,
      "loss": 0.6117,
      "step": 1608000
    },
    {
      "epoch": 25.244897959183675,
      "grad_norm": 3.832820177078247,
      "learning_rate": 3.4221938775510206e-05,
      "loss": 0.6048,
      "step": 1608100
    },
    {
      "epoch": 25.246467817896388,
      "grad_norm": 3.408620834350586,
      "learning_rate": 3.422095761381476e-05,
      "loss": 0.5552,
      "step": 1608200
    },
    {
      "epoch": 25.248037676609105,
      "grad_norm": 2.9440596103668213,
      "learning_rate": 3.4219976452119315e-05,
      "loss": 0.6035,
      "step": 1608300
    },
    {
      "epoch": 25.24960753532182,
      "grad_norm": 2.686209201812744,
      "learning_rate": 3.421899529042386e-05,
      "loss": 0.6247,
      "step": 1608400
    },
    {
      "epoch": 25.25117739403454,
      "grad_norm": 3.7764604091644287,
      "learning_rate": 3.4218014128728417e-05,
      "loss": 0.625,
      "step": 1608500
    },
    {
      "epoch": 25.252747252747252,
      "grad_norm": 3.6549620628356934,
      "learning_rate": 3.421703296703297e-05,
      "loss": 0.5548,
      "step": 1608600
    },
    {
      "epoch": 25.25431711145997,
      "grad_norm": 3.190021514892578,
      "learning_rate": 3.4216051805337525e-05,
      "loss": 0.6282,
      "step": 1608700
    },
    {
      "epoch": 25.255886970172686,
      "grad_norm": 4.348318576812744,
      "learning_rate": 3.421507064364207e-05,
      "loss": 0.5761,
      "step": 1608800
    },
    {
      "epoch": 25.2574568288854,
      "grad_norm": 4.7941083908081055,
      "learning_rate": 3.421408948194663e-05,
      "loss": 0.6123,
      "step": 1608900
    },
    {
      "epoch": 25.259026687598116,
      "grad_norm": 3.1845011711120605,
      "learning_rate": 3.421310832025118e-05,
      "loss": 0.5924,
      "step": 1609000
    },
    {
      "epoch": 25.260596546310833,
      "grad_norm": 3.7986059188842773,
      "learning_rate": 3.421212715855573e-05,
      "loss": 0.595,
      "step": 1609100
    },
    {
      "epoch": 25.26216640502355,
      "grad_norm": 3.6287643909454346,
      "learning_rate": 3.421114599686028e-05,
      "loss": 0.5964,
      "step": 1609200
    },
    {
      "epoch": 25.263736263736263,
      "grad_norm": 3.9231855869293213,
      "learning_rate": 3.421016483516484e-05,
      "loss": 0.6128,
      "step": 1609300
    },
    {
      "epoch": 25.26530612244898,
      "grad_norm": 4.260738372802734,
      "learning_rate": 3.420918367346939e-05,
      "loss": 0.5951,
      "step": 1609400
    },
    {
      "epoch": 25.266875981161697,
      "grad_norm": 4.8869242668151855,
      "learning_rate": 3.420820251177394e-05,
      "loss": 0.6231,
      "step": 1609500
    },
    {
      "epoch": 25.26844583987441,
      "grad_norm": 2.363966941833496,
      "learning_rate": 3.42072213500785e-05,
      "loss": 0.6068,
      "step": 1609600
    },
    {
      "epoch": 25.270015698587127,
      "grad_norm": 4.683803081512451,
      "learning_rate": 3.420624018838305e-05,
      "loss": 0.5774,
      "step": 1609700
    },
    {
      "epoch": 25.271585557299844,
      "grad_norm": 3.629371166229248,
      "learning_rate": 3.42052590266876e-05,
      "loss": 0.6353,
      "step": 1609800
    },
    {
      "epoch": 25.27315541601256,
      "grad_norm": 3.6421666145324707,
      "learning_rate": 3.420427786499215e-05,
      "loss": 0.5918,
      "step": 1609900
    },
    {
      "epoch": 25.274725274725274,
      "grad_norm": 4.1719279289245605,
      "learning_rate": 3.420329670329671e-05,
      "loss": 0.5468,
      "step": 1610000
    },
    {
      "epoch": 25.27629513343799,
      "grad_norm": 1.994862675666809,
      "learning_rate": 3.420231554160126e-05,
      "loss": 0.5592,
      "step": 1610100
    },
    {
      "epoch": 25.277864992150707,
      "grad_norm": 2.9980428218841553,
      "learning_rate": 3.420133437990581e-05,
      "loss": 0.5876,
      "step": 1610200
    },
    {
      "epoch": 25.27943485086342,
      "grad_norm": 3.0967659950256348,
      "learning_rate": 3.420035321821036e-05,
      "loss": 0.5651,
      "step": 1610300
    },
    {
      "epoch": 25.281004709576138,
      "grad_norm": 2.504743814468384,
      "learning_rate": 3.419937205651492e-05,
      "loss": 0.6057,
      "step": 1610400
    },
    {
      "epoch": 25.282574568288855,
      "grad_norm": 3.8050975799560547,
      "learning_rate": 3.419839089481946e-05,
      "loss": 0.568,
      "step": 1610500
    },
    {
      "epoch": 25.28414442700157,
      "grad_norm": 4.1748366355896,
      "learning_rate": 3.419740973312402e-05,
      "loss": 0.6018,
      "step": 1610600
    },
    {
      "epoch": 25.285714285714285,
      "grad_norm": 4.317450523376465,
      "learning_rate": 3.419642857142857e-05,
      "loss": 0.6351,
      "step": 1610700
    },
    {
      "epoch": 25.287284144427,
      "grad_norm": 3.710522413253784,
      "learning_rate": 3.419544740973313e-05,
      "loss": 0.5845,
      "step": 1610800
    },
    {
      "epoch": 25.28885400313972,
      "grad_norm": 3.7166037559509277,
      "learning_rate": 3.4194466248037674e-05,
      "loss": 0.5919,
      "step": 1610900
    },
    {
      "epoch": 25.29042386185243,
      "grad_norm": 3.8349530696868896,
      "learning_rate": 3.419348508634223e-05,
      "loss": 0.5929,
      "step": 1611000
    },
    {
      "epoch": 25.29199372056515,
      "grad_norm": 4.833755016326904,
      "learning_rate": 3.419250392464678e-05,
      "loss": 0.5867,
      "step": 1611100
    },
    {
      "epoch": 25.293563579277865,
      "grad_norm": 4.047725677490234,
      "learning_rate": 3.4191522762951334e-05,
      "loss": 0.5654,
      "step": 1611200
    },
    {
      "epoch": 25.295133437990582,
      "grad_norm": 3.686070442199707,
      "learning_rate": 3.4190541601255885e-05,
      "loss": 0.6172,
      "step": 1611300
    },
    {
      "epoch": 25.296703296703296,
      "grad_norm": 3.727922201156616,
      "learning_rate": 3.418956043956044e-05,
      "loss": 0.6213,
      "step": 1611400
    },
    {
      "epoch": 25.298273155416013,
      "grad_norm": 2.3270034790039062,
      "learning_rate": 3.4188579277864993e-05,
      "loss": 0.6086,
      "step": 1611500
    },
    {
      "epoch": 25.29984301412873,
      "grad_norm": 3.511920690536499,
      "learning_rate": 3.4187598116169544e-05,
      "loss": 0.5766,
      "step": 1611600
    },
    {
      "epoch": 25.301412872841443,
      "grad_norm": 4.977877140045166,
      "learning_rate": 3.41866169544741e-05,
      "loss": 0.5744,
      "step": 1611700
    },
    {
      "epoch": 25.30298273155416,
      "grad_norm": 4.115850448608398,
      "learning_rate": 3.418563579277865e-05,
      "loss": 0.5699,
      "step": 1611800
    },
    {
      "epoch": 25.304552590266876,
      "grad_norm": 1.9262595176696777,
      "learning_rate": 3.4184654631083204e-05,
      "loss": 0.5724,
      "step": 1611900
    },
    {
      "epoch": 25.306122448979593,
      "grad_norm": 3.9706954956054688,
      "learning_rate": 3.4183673469387755e-05,
      "loss": 0.5648,
      "step": 1612000
    },
    {
      "epoch": 25.307692307692307,
      "grad_norm": 4.892937660217285,
      "learning_rate": 3.418269230769231e-05,
      "loss": 0.6245,
      "step": 1612100
    },
    {
      "epoch": 25.309262166405023,
      "grad_norm": 2.800725221633911,
      "learning_rate": 3.4181711145996864e-05,
      "loss": 0.648,
      "step": 1612200
    },
    {
      "epoch": 25.31083202511774,
      "grad_norm": 3.18182373046875,
      "learning_rate": 3.4180729984301415e-05,
      "loss": 0.5671,
      "step": 1612300
    },
    {
      "epoch": 25.312401883830454,
      "grad_norm": 3.858543634414673,
      "learning_rate": 3.4179748822605966e-05,
      "loss": 0.5668,
      "step": 1612400
    },
    {
      "epoch": 25.31397174254317,
      "grad_norm": 4.615871906280518,
      "learning_rate": 3.4178767660910524e-05,
      "loss": 0.5745,
      "step": 1612500
    },
    {
      "epoch": 25.315541601255887,
      "grad_norm": 3.9980318546295166,
      "learning_rate": 3.417778649921507e-05,
      "loss": 0.5399,
      "step": 1612600
    },
    {
      "epoch": 25.317111459968604,
      "grad_norm": 3.3401122093200684,
      "learning_rate": 3.4176805337519626e-05,
      "loss": 0.59,
      "step": 1612700
    },
    {
      "epoch": 25.318681318681318,
      "grad_norm": 3.547102451324463,
      "learning_rate": 3.4175824175824177e-05,
      "loss": 0.5483,
      "step": 1612800
    },
    {
      "epoch": 25.320251177394034,
      "grad_norm": 4.041865825653076,
      "learning_rate": 3.4174843014128734e-05,
      "loss": 0.5989,
      "step": 1612900
    },
    {
      "epoch": 25.32182103610675,
      "grad_norm": 2.832341432571411,
      "learning_rate": 3.417386185243328e-05,
      "loss": 0.5938,
      "step": 1613000
    },
    {
      "epoch": 25.323390894819465,
      "grad_norm": 4.276915550231934,
      "learning_rate": 3.4172880690737836e-05,
      "loss": 0.5764,
      "step": 1613100
    },
    {
      "epoch": 25.32496075353218,
      "grad_norm": 3.240046501159668,
      "learning_rate": 3.417189952904239e-05,
      "loss": 0.5593,
      "step": 1613200
    },
    {
      "epoch": 25.3265306122449,
      "grad_norm": 3.4590532779693604,
      "learning_rate": 3.417091836734694e-05,
      "loss": 0.5864,
      "step": 1613300
    },
    {
      "epoch": 25.328100470957615,
      "grad_norm": 3.7307088375091553,
      "learning_rate": 3.416993720565149e-05,
      "loss": 0.6371,
      "step": 1613400
    },
    {
      "epoch": 25.32967032967033,
      "grad_norm": 3.882004976272583,
      "learning_rate": 3.416895604395605e-05,
      "loss": 0.6108,
      "step": 1613500
    },
    {
      "epoch": 25.331240188383045,
      "grad_norm": 3.6571154594421387,
      "learning_rate": 3.41679748822606e-05,
      "loss": 0.5944,
      "step": 1613600
    },
    {
      "epoch": 25.332810047095762,
      "grad_norm": 3.871115207672119,
      "learning_rate": 3.416699372056515e-05,
      "loss": 0.6285,
      "step": 1613700
    },
    {
      "epoch": 25.334379905808476,
      "grad_norm": 4.761051654815674,
      "learning_rate": 3.416601255886971e-05,
      "loss": 0.5858,
      "step": 1613800
    },
    {
      "epoch": 25.335949764521192,
      "grad_norm": 3.348240375518799,
      "learning_rate": 3.416503139717426e-05,
      "loss": 0.5919,
      "step": 1613900
    },
    {
      "epoch": 25.33751962323391,
      "grad_norm": 2.8874764442443848,
      "learning_rate": 3.416405023547881e-05,
      "loss": 0.5738,
      "step": 1614000
    },
    {
      "epoch": 25.339089481946626,
      "grad_norm": 3.6647427082061768,
      "learning_rate": 3.416306907378336e-05,
      "loss": 0.5916,
      "step": 1614100
    },
    {
      "epoch": 25.34065934065934,
      "grad_norm": 3.76047945022583,
      "learning_rate": 3.416208791208792e-05,
      "loss": 0.6233,
      "step": 1614200
    },
    {
      "epoch": 25.342229199372056,
      "grad_norm": 3.787003517150879,
      "learning_rate": 3.416110675039247e-05,
      "loss": 0.6127,
      "step": 1614300
    },
    {
      "epoch": 25.343799058084773,
      "grad_norm": 3.7129111289978027,
      "learning_rate": 3.416012558869702e-05,
      "loss": 0.6073,
      "step": 1614400
    },
    {
      "epoch": 25.345368916797486,
      "grad_norm": 3.580350875854492,
      "learning_rate": 3.415914442700157e-05,
      "loss": 0.6052,
      "step": 1614500
    },
    {
      "epoch": 25.346938775510203,
      "grad_norm": 4.061481475830078,
      "learning_rate": 3.415816326530613e-05,
      "loss": 0.6099,
      "step": 1614600
    },
    {
      "epoch": 25.34850863422292,
      "grad_norm": 3.241173028945923,
      "learning_rate": 3.415718210361067e-05,
      "loss": 0.6093,
      "step": 1614700
    },
    {
      "epoch": 25.350078492935637,
      "grad_norm": 4.21982479095459,
      "learning_rate": 3.415620094191523e-05,
      "loss": 0.5954,
      "step": 1614800
    },
    {
      "epoch": 25.35164835164835,
      "grad_norm": 4.643993377685547,
      "learning_rate": 3.415521978021978e-05,
      "loss": 0.5947,
      "step": 1614900
    },
    {
      "epoch": 25.353218210361067,
      "grad_norm": 2.8383829593658447,
      "learning_rate": 3.415423861852434e-05,
      "loss": 0.6017,
      "step": 1615000
    },
    {
      "epoch": 25.354788069073784,
      "grad_norm": 4.5497307777404785,
      "learning_rate": 3.415325745682888e-05,
      "loss": 0.6316,
      "step": 1615100
    },
    {
      "epoch": 25.356357927786497,
      "grad_norm": 4.129776954650879,
      "learning_rate": 3.415227629513344e-05,
      "loss": 0.6475,
      "step": 1615200
    },
    {
      "epoch": 25.357927786499214,
      "grad_norm": 3.237539529800415,
      "learning_rate": 3.415129513343799e-05,
      "loss": 0.5817,
      "step": 1615300
    },
    {
      "epoch": 25.35949764521193,
      "grad_norm": 3.6619760990142822,
      "learning_rate": 3.415031397174254e-05,
      "loss": 0.6026,
      "step": 1615400
    },
    {
      "epoch": 25.361067503924648,
      "grad_norm": 4.045431137084961,
      "learning_rate": 3.4149332810047094e-05,
      "loss": 0.604,
      "step": 1615500
    },
    {
      "epoch": 25.36263736263736,
      "grad_norm": 4.051958084106445,
      "learning_rate": 3.414835164835165e-05,
      "loss": 0.5614,
      "step": 1615600
    },
    {
      "epoch": 25.364207221350078,
      "grad_norm": 4.186385631561279,
      "learning_rate": 3.41473704866562e-05,
      "loss": 0.631,
      "step": 1615700
    },
    {
      "epoch": 25.365777080062795,
      "grad_norm": 2.6289403438568115,
      "learning_rate": 3.4146389324960753e-05,
      "loss": 0.6014,
      "step": 1615800
    },
    {
      "epoch": 25.367346938775512,
      "grad_norm": 2.500735282897949,
      "learning_rate": 3.4145408163265304e-05,
      "loss": 0.5698,
      "step": 1615900
    },
    {
      "epoch": 25.368916797488225,
      "grad_norm": 3.3866820335388184,
      "learning_rate": 3.414442700156986e-05,
      "loss": 0.6304,
      "step": 1616000
    },
    {
      "epoch": 25.370486656200942,
      "grad_norm": 3.34751558303833,
      "learning_rate": 3.414344583987441e-05,
      "loss": 0.6329,
      "step": 1616100
    },
    {
      "epoch": 25.37205651491366,
      "grad_norm": 3.494980573654175,
      "learning_rate": 3.4142464678178964e-05,
      "loss": 0.5955,
      "step": 1616200
    },
    {
      "epoch": 25.373626373626372,
      "grad_norm": 4.694647789001465,
      "learning_rate": 3.414148351648352e-05,
      "loss": 0.5937,
      "step": 1616300
    },
    {
      "epoch": 25.37519623233909,
      "grad_norm": 4.359497547149658,
      "learning_rate": 3.414050235478807e-05,
      "loss": 0.598,
      "step": 1616400
    },
    {
      "epoch": 25.376766091051806,
      "grad_norm": 3.9749326705932617,
      "learning_rate": 3.4139521193092624e-05,
      "loss": 0.6119,
      "step": 1616500
    },
    {
      "epoch": 25.378335949764523,
      "grad_norm": 2.71519136428833,
      "learning_rate": 3.4138540031397175e-05,
      "loss": 0.6043,
      "step": 1616600
    },
    {
      "epoch": 25.379905808477236,
      "grad_norm": 3.9731664657592773,
      "learning_rate": 3.413755886970173e-05,
      "loss": 0.6231,
      "step": 1616700
    },
    {
      "epoch": 25.381475667189953,
      "grad_norm": 4.156713485717773,
      "learning_rate": 3.413657770800628e-05,
      "loss": 0.6254,
      "step": 1616800
    },
    {
      "epoch": 25.38304552590267,
      "grad_norm": 4.414957523345947,
      "learning_rate": 3.4135596546310835e-05,
      "loss": 0.6053,
      "step": 1616900
    },
    {
      "epoch": 25.384615384615383,
      "grad_norm": 3.993441343307495,
      "learning_rate": 3.4134615384615386e-05,
      "loss": 0.5787,
      "step": 1617000
    },
    {
      "epoch": 25.3861852433281,
      "grad_norm": 2.9612929821014404,
      "learning_rate": 3.413363422291994e-05,
      "loss": 0.6059,
      "step": 1617100
    },
    {
      "epoch": 25.387755102040817,
      "grad_norm": 3.9714560508728027,
      "learning_rate": 3.413265306122449e-05,
      "loss": 0.5739,
      "step": 1617200
    },
    {
      "epoch": 25.389324960753534,
      "grad_norm": 4.752776622772217,
      "learning_rate": 3.4131671899529045e-05,
      "loss": 0.6392,
      "step": 1617300
    },
    {
      "epoch": 25.390894819466247,
      "grad_norm": 3.5157575607299805,
      "learning_rate": 3.4130690737833596e-05,
      "loss": 0.6032,
      "step": 1617400
    },
    {
      "epoch": 25.392464678178964,
      "grad_norm": 3.1716115474700928,
      "learning_rate": 3.412970957613815e-05,
      "loss": 0.6112,
      "step": 1617500
    },
    {
      "epoch": 25.39403453689168,
      "grad_norm": 2.9718050956726074,
      "learning_rate": 3.41287284144427e-05,
      "loss": 0.6145,
      "step": 1617600
    },
    {
      "epoch": 25.395604395604394,
      "grad_norm": 3.8552494049072266,
      "learning_rate": 3.4127747252747256e-05,
      "loss": 0.6273,
      "step": 1617700
    },
    {
      "epoch": 25.39717425431711,
      "grad_norm": 3.421771764755249,
      "learning_rate": 3.412676609105181e-05,
      "loss": 0.6014,
      "step": 1617800
    },
    {
      "epoch": 25.398744113029828,
      "grad_norm": 4.466493129730225,
      "learning_rate": 3.412578492935636e-05,
      "loss": 0.5827,
      "step": 1617900
    },
    {
      "epoch": 25.400313971742545,
      "grad_norm": 3.453047752380371,
      "learning_rate": 3.412480376766091e-05,
      "loss": 0.5801,
      "step": 1618000
    },
    {
      "epoch": 25.401883830455258,
      "grad_norm": 4.385765552520752,
      "learning_rate": 3.412382260596547e-05,
      "loss": 0.5679,
      "step": 1618100
    },
    {
      "epoch": 25.403453689167975,
      "grad_norm": 4.0926618576049805,
      "learning_rate": 3.412284144427002e-05,
      "loss": 0.5951,
      "step": 1618200
    },
    {
      "epoch": 25.405023547880692,
      "grad_norm": 4.593708038330078,
      "learning_rate": 3.412186028257457e-05,
      "loss": 0.6123,
      "step": 1618300
    },
    {
      "epoch": 25.406593406593405,
      "grad_norm": 3.051175117492676,
      "learning_rate": 3.4120879120879126e-05,
      "loss": 0.5924,
      "step": 1618400
    },
    {
      "epoch": 25.408163265306122,
      "grad_norm": 4.598356246948242,
      "learning_rate": 3.411989795918368e-05,
      "loss": 0.5805,
      "step": 1618500
    },
    {
      "epoch": 25.40973312401884,
      "grad_norm": 3.481126070022583,
      "learning_rate": 3.411891679748823e-05,
      "loss": 0.6096,
      "step": 1618600
    },
    {
      "epoch": 25.411302982731556,
      "grad_norm": 2.1229827404022217,
      "learning_rate": 3.411793563579278e-05,
      "loss": 0.5778,
      "step": 1618700
    },
    {
      "epoch": 25.41287284144427,
      "grad_norm": 4.100364685058594,
      "learning_rate": 3.411695447409734e-05,
      "loss": 0.5992,
      "step": 1618800
    },
    {
      "epoch": 25.414442700156986,
      "grad_norm": 3.903717041015625,
      "learning_rate": 3.411597331240188e-05,
      "loss": 0.6137,
      "step": 1618900
    },
    {
      "epoch": 25.416012558869703,
      "grad_norm": 2.1347553730010986,
      "learning_rate": 3.411499215070644e-05,
      "loss": 0.5681,
      "step": 1619000
    },
    {
      "epoch": 25.417582417582416,
      "grad_norm": 4.082356929779053,
      "learning_rate": 3.411401098901099e-05,
      "loss": 0.5883,
      "step": 1619100
    },
    {
      "epoch": 25.419152276295133,
      "grad_norm": 3.005185842514038,
      "learning_rate": 3.411302982731555e-05,
      "loss": 0.6056,
      "step": 1619200
    },
    {
      "epoch": 25.42072213500785,
      "grad_norm": 4.05364990234375,
      "learning_rate": 3.411204866562009e-05,
      "loss": 0.5902,
      "step": 1619300
    },
    {
      "epoch": 25.422291993720567,
      "grad_norm": 4.199110507965088,
      "learning_rate": 3.411106750392465e-05,
      "loss": 0.6448,
      "step": 1619400
    },
    {
      "epoch": 25.42386185243328,
      "grad_norm": 4.592870712280273,
      "learning_rate": 3.41100863422292e-05,
      "loss": 0.6053,
      "step": 1619500
    },
    {
      "epoch": 25.425431711145997,
      "grad_norm": 3.863164186477661,
      "learning_rate": 3.410910518053375e-05,
      "loss": 0.6158,
      "step": 1619600
    },
    {
      "epoch": 25.427001569858714,
      "grad_norm": 3.620115041732788,
      "learning_rate": 3.41081240188383e-05,
      "loss": 0.6173,
      "step": 1619700
    },
    {
      "epoch": 25.428571428571427,
      "grad_norm": 3.560866355895996,
      "learning_rate": 3.410714285714286e-05,
      "loss": 0.6106,
      "step": 1619800
    },
    {
      "epoch": 25.430141287284144,
      "grad_norm": 4.014209270477295,
      "learning_rate": 3.410616169544741e-05,
      "loss": 0.6318,
      "step": 1619900
    },
    {
      "epoch": 25.43171114599686,
      "grad_norm": 4.69635534286499,
      "learning_rate": 3.410518053375196e-05,
      "loss": 0.6037,
      "step": 1620000
    },
    {
      "epoch": 25.433281004709578,
      "grad_norm": 4.178924560546875,
      "learning_rate": 3.4104199372056513e-05,
      "loss": 0.6049,
      "step": 1620100
    },
    {
      "epoch": 25.43485086342229,
      "grad_norm": 2.0870182514190674,
      "learning_rate": 3.410321821036107e-05,
      "loss": 0.5714,
      "step": 1620200
    },
    {
      "epoch": 25.436420722135008,
      "grad_norm": 5.48107385635376,
      "learning_rate": 3.410223704866562e-05,
      "loss": 0.566,
      "step": 1620300
    },
    {
      "epoch": 25.437990580847725,
      "grad_norm": 4.131595134735107,
      "learning_rate": 3.410125588697017e-05,
      "loss": 0.6069,
      "step": 1620400
    },
    {
      "epoch": 25.439560439560438,
      "grad_norm": 3.81573486328125,
      "learning_rate": 3.410027472527473e-05,
      "loss": 0.5745,
      "step": 1620500
    },
    {
      "epoch": 25.441130298273155,
      "grad_norm": 3.837467908859253,
      "learning_rate": 3.409929356357928e-05,
      "loss": 0.5967,
      "step": 1620600
    },
    {
      "epoch": 25.44270015698587,
      "grad_norm": 2.1347391605377197,
      "learning_rate": 3.409831240188383e-05,
      "loss": 0.6039,
      "step": 1620700
    },
    {
      "epoch": 25.44427001569859,
      "grad_norm": 4.214718818664551,
      "learning_rate": 3.4097331240188384e-05,
      "loss": 0.5652,
      "step": 1620800
    },
    {
      "epoch": 25.445839874411302,
      "grad_norm": 3.5175931453704834,
      "learning_rate": 3.409635007849294e-05,
      "loss": 0.5446,
      "step": 1620900
    },
    {
      "epoch": 25.44740973312402,
      "grad_norm": 4.1947922706604,
      "learning_rate": 3.4095368916797486e-05,
      "loss": 0.6108,
      "step": 1621000
    },
    {
      "epoch": 25.448979591836736,
      "grad_norm": 4.122785568237305,
      "learning_rate": 3.4094387755102044e-05,
      "loss": 0.6019,
      "step": 1621100
    },
    {
      "epoch": 25.45054945054945,
      "grad_norm": 5.031954288482666,
      "learning_rate": 3.4093406593406595e-05,
      "loss": 0.5932,
      "step": 1621200
    },
    {
      "epoch": 25.452119309262166,
      "grad_norm": 3.789140462875366,
      "learning_rate": 3.409242543171115e-05,
      "loss": 0.5787,
      "step": 1621300
    },
    {
      "epoch": 25.453689167974883,
      "grad_norm": 4.294856548309326,
      "learning_rate": 3.4091444270015696e-05,
      "loss": 0.6233,
      "step": 1621400
    },
    {
      "epoch": 25.4552590266876,
      "grad_norm": 3.9560134410858154,
      "learning_rate": 3.4090463108320254e-05,
      "loss": 0.5568,
      "step": 1621500
    },
    {
      "epoch": 25.456828885400313,
      "grad_norm": 4.093652725219727,
      "learning_rate": 3.4089481946624805e-05,
      "loss": 0.6152,
      "step": 1621600
    },
    {
      "epoch": 25.45839874411303,
      "grad_norm": 4.779117107391357,
      "learning_rate": 3.4088500784929356e-05,
      "loss": 0.6034,
      "step": 1621700
    },
    {
      "epoch": 25.459968602825747,
      "grad_norm": 4.072740077972412,
      "learning_rate": 3.408751962323391e-05,
      "loss": 0.583,
      "step": 1621800
    },
    {
      "epoch": 25.46153846153846,
      "grad_norm": 4.547005653381348,
      "learning_rate": 3.4086538461538465e-05,
      "loss": 0.6001,
      "step": 1621900
    },
    {
      "epoch": 25.463108320251177,
      "grad_norm": 3.8036510944366455,
      "learning_rate": 3.4085557299843016e-05,
      "loss": 0.6169,
      "step": 1622000
    },
    {
      "epoch": 25.464678178963894,
      "grad_norm": 4.428869247436523,
      "learning_rate": 3.408457613814757e-05,
      "loss": 0.5907,
      "step": 1622100
    },
    {
      "epoch": 25.46624803767661,
      "grad_norm": 5.10392427444458,
      "learning_rate": 3.408359497645212e-05,
      "loss": 0.6121,
      "step": 1622200
    },
    {
      "epoch": 25.467817896389324,
      "grad_norm": 3.215104579925537,
      "learning_rate": 3.4082613814756676e-05,
      "loss": 0.5923,
      "step": 1622300
    },
    {
      "epoch": 25.46938775510204,
      "grad_norm": 3.227881908416748,
      "learning_rate": 3.408163265306123e-05,
      "loss": 0.5875,
      "step": 1622400
    },
    {
      "epoch": 25.470957613814758,
      "grad_norm": 3.05515193939209,
      "learning_rate": 3.408065149136578e-05,
      "loss": 0.5965,
      "step": 1622500
    },
    {
      "epoch": 25.47252747252747,
      "grad_norm": 4.3697028160095215,
      "learning_rate": 3.4079670329670335e-05,
      "loss": 0.595,
      "step": 1622600
    },
    {
      "epoch": 25.474097331240188,
      "grad_norm": 4.046289920806885,
      "learning_rate": 3.4078689167974886e-05,
      "loss": 0.5778,
      "step": 1622700
    },
    {
      "epoch": 25.475667189952905,
      "grad_norm": 3.6059889793395996,
      "learning_rate": 3.407770800627944e-05,
      "loss": 0.6095,
      "step": 1622800
    },
    {
      "epoch": 25.47723704866562,
      "grad_norm": 3.3674237728118896,
      "learning_rate": 3.407672684458399e-05,
      "loss": 0.6039,
      "step": 1622900
    },
    {
      "epoch": 25.478806907378335,
      "grad_norm": 3.7463369369506836,
      "learning_rate": 3.4075745682888546e-05,
      "loss": 0.5626,
      "step": 1623000
    },
    {
      "epoch": 25.48037676609105,
      "grad_norm": 4.599933624267578,
      "learning_rate": 3.407476452119309e-05,
      "loss": 0.6075,
      "step": 1623100
    },
    {
      "epoch": 25.48194662480377,
      "grad_norm": 3.9692234992980957,
      "learning_rate": 3.407378335949765e-05,
      "loss": 0.5746,
      "step": 1623200
    },
    {
      "epoch": 25.483516483516482,
      "grad_norm": 4.188003063201904,
      "learning_rate": 3.40728021978022e-05,
      "loss": 0.5985,
      "step": 1623300
    },
    {
      "epoch": 25.4850863422292,
      "grad_norm": 4.777912616729736,
      "learning_rate": 3.407182103610676e-05,
      "loss": 0.5797,
      "step": 1623400
    },
    {
      "epoch": 25.486656200941916,
      "grad_norm": 3.5252277851104736,
      "learning_rate": 3.40708398744113e-05,
      "loss": 0.5963,
      "step": 1623500
    },
    {
      "epoch": 25.488226059654632,
      "grad_norm": 4.084995746612549,
      "learning_rate": 3.406985871271586e-05,
      "loss": 0.5993,
      "step": 1623600
    },
    {
      "epoch": 25.489795918367346,
      "grad_norm": 3.557051420211792,
      "learning_rate": 3.406887755102041e-05,
      "loss": 0.6061,
      "step": 1623700
    },
    {
      "epoch": 25.491365777080063,
      "grad_norm": 3.6668593883514404,
      "learning_rate": 3.406789638932496e-05,
      "loss": 0.5619,
      "step": 1623800
    },
    {
      "epoch": 25.49293563579278,
      "grad_norm": 3.354414224624634,
      "learning_rate": 3.406691522762951e-05,
      "loss": 0.612,
      "step": 1623900
    },
    {
      "epoch": 25.494505494505496,
      "grad_norm": 3.8438258171081543,
      "learning_rate": 3.406593406593407e-05,
      "loss": 0.5922,
      "step": 1624000
    },
    {
      "epoch": 25.49607535321821,
      "grad_norm": 3.466550588607788,
      "learning_rate": 3.406495290423862e-05,
      "loss": 0.5922,
      "step": 1624100
    },
    {
      "epoch": 25.497645211930926,
      "grad_norm": 4.067805767059326,
      "learning_rate": 3.406397174254317e-05,
      "loss": 0.5713,
      "step": 1624200
    },
    {
      "epoch": 25.499215070643643,
      "grad_norm": 3.169907808303833,
      "learning_rate": 3.406299058084772e-05,
      "loss": 0.6248,
      "step": 1624300
    },
    {
      "epoch": 25.500784929356357,
      "grad_norm": 4.284638404846191,
      "learning_rate": 3.406200941915228e-05,
      "loss": 0.533,
      "step": 1624400
    },
    {
      "epoch": 25.502354788069074,
      "grad_norm": 3.291656017303467,
      "learning_rate": 3.406102825745683e-05,
      "loss": 0.5998,
      "step": 1624500
    },
    {
      "epoch": 25.50392464678179,
      "grad_norm": 3.397521495819092,
      "learning_rate": 3.406004709576138e-05,
      "loss": 0.5797,
      "step": 1624600
    },
    {
      "epoch": 25.505494505494504,
      "grad_norm": 3.224069833755493,
      "learning_rate": 3.405906593406594e-05,
      "loss": 0.6074,
      "step": 1624700
    },
    {
      "epoch": 25.50706436420722,
      "grad_norm": 2.917102813720703,
      "learning_rate": 3.405808477237049e-05,
      "loss": 0.5658,
      "step": 1624800
    },
    {
      "epoch": 25.508634222919937,
      "grad_norm": 4.117812633514404,
      "learning_rate": 3.405710361067504e-05,
      "loss": 0.5808,
      "step": 1624900
    },
    {
      "epoch": 25.510204081632654,
      "grad_norm": 3.826549530029297,
      "learning_rate": 3.405612244897959e-05,
      "loss": 0.5858,
      "step": 1625000
    },
    {
      "epoch": 25.511773940345368,
      "grad_norm": 3.003493309020996,
      "learning_rate": 3.405514128728415e-05,
      "loss": 0.592,
      "step": 1625100
    },
    {
      "epoch": 25.513343799058084,
      "grad_norm": 4.643886566162109,
      "learning_rate": 3.4054160125588695e-05,
      "loss": 0.6113,
      "step": 1625200
    },
    {
      "epoch": 25.5149136577708,
      "grad_norm": 4.230160236358643,
      "learning_rate": 3.405317896389325e-05,
      "loss": 0.576,
      "step": 1625300
    },
    {
      "epoch": 25.516483516483518,
      "grad_norm": 3.1378092765808105,
      "learning_rate": 3.4052197802197803e-05,
      "loss": 0.6233,
      "step": 1625400
    },
    {
      "epoch": 25.51805337519623,
      "grad_norm": 3.991919755935669,
      "learning_rate": 3.405121664050236e-05,
      "loss": 0.5769,
      "step": 1625500
    },
    {
      "epoch": 25.51962323390895,
      "grad_norm": 5.033273220062256,
      "learning_rate": 3.4050235478806905e-05,
      "loss": 0.5853,
      "step": 1625600
    },
    {
      "epoch": 25.521193092621665,
      "grad_norm": 4.06673002243042,
      "learning_rate": 3.404925431711146e-05,
      "loss": 0.5897,
      "step": 1625700
    },
    {
      "epoch": 25.52276295133438,
      "grad_norm": 3.249157190322876,
      "learning_rate": 3.4048273155416014e-05,
      "loss": 0.5929,
      "step": 1625800
    },
    {
      "epoch": 25.524332810047095,
      "grad_norm": 3.6578705310821533,
      "learning_rate": 3.4047291993720565e-05,
      "loss": 0.6258,
      "step": 1625900
    },
    {
      "epoch": 25.525902668759812,
      "grad_norm": 2.7297329902648926,
      "learning_rate": 3.4046310832025116e-05,
      "loss": 0.6362,
      "step": 1626000
    },
    {
      "epoch": 25.52747252747253,
      "grad_norm": 4.472781658172607,
      "learning_rate": 3.4045329670329674e-05,
      "loss": 0.5694,
      "step": 1626100
    },
    {
      "epoch": 25.529042386185242,
      "grad_norm": 3.8874850273132324,
      "learning_rate": 3.4044348508634225e-05,
      "loss": 0.5813,
      "step": 1626200
    },
    {
      "epoch": 25.53061224489796,
      "grad_norm": 4.130614280700684,
      "learning_rate": 3.4043367346938776e-05,
      "loss": 0.6188,
      "step": 1626300
    },
    {
      "epoch": 25.532182103610676,
      "grad_norm": 3.781726360321045,
      "learning_rate": 3.404238618524333e-05,
      "loss": 0.6321,
      "step": 1626400
    },
    {
      "epoch": 25.53375196232339,
      "grad_norm": 4.287862300872803,
      "learning_rate": 3.4041405023547885e-05,
      "loss": 0.6035,
      "step": 1626500
    },
    {
      "epoch": 25.535321821036106,
      "grad_norm": 4.958221912384033,
      "learning_rate": 3.4040423861852436e-05,
      "loss": 0.5496,
      "step": 1626600
    },
    {
      "epoch": 25.536891679748823,
      "grad_norm": 3.9697983264923096,
      "learning_rate": 3.4039442700156987e-05,
      "loss": 0.6012,
      "step": 1626700
    },
    {
      "epoch": 25.53846153846154,
      "grad_norm": 4.091310024261475,
      "learning_rate": 3.4038461538461544e-05,
      "loss": 0.5895,
      "step": 1626800
    },
    {
      "epoch": 25.540031397174253,
      "grad_norm": 3.8534457683563232,
      "learning_rate": 3.403748037676609e-05,
      "loss": 0.5852,
      "step": 1626900
    },
    {
      "epoch": 25.54160125588697,
      "grad_norm": 3.908205270767212,
      "learning_rate": 3.4036499215070646e-05,
      "loss": 0.5993,
      "step": 1627000
    },
    {
      "epoch": 25.543171114599687,
      "grad_norm": 3.2993452548980713,
      "learning_rate": 3.40355180533752e-05,
      "loss": 0.5893,
      "step": 1627100
    },
    {
      "epoch": 25.5447409733124,
      "grad_norm": 3.8194398880004883,
      "learning_rate": 3.4034536891679755e-05,
      "loss": 0.6288,
      "step": 1627200
    },
    {
      "epoch": 25.546310832025117,
      "grad_norm": 2.9853343963623047,
      "learning_rate": 3.40335557299843e-05,
      "loss": 0.6483,
      "step": 1627300
    },
    {
      "epoch": 25.547880690737834,
      "grad_norm": 2.4504964351654053,
      "learning_rate": 3.403257456828886e-05,
      "loss": 0.5702,
      "step": 1627400
    },
    {
      "epoch": 25.54945054945055,
      "grad_norm": 4.539188385009766,
      "learning_rate": 3.403159340659341e-05,
      "loss": 0.6049,
      "step": 1627500
    },
    {
      "epoch": 25.551020408163264,
      "grad_norm": 3.4506354331970215,
      "learning_rate": 3.403061224489796e-05,
      "loss": 0.5511,
      "step": 1627600
    },
    {
      "epoch": 25.55259026687598,
      "grad_norm": 3.8970329761505127,
      "learning_rate": 3.402963108320251e-05,
      "loss": 0.6179,
      "step": 1627700
    },
    {
      "epoch": 25.554160125588698,
      "grad_norm": 3.0488550662994385,
      "learning_rate": 3.402864992150707e-05,
      "loss": 0.594,
      "step": 1627800
    },
    {
      "epoch": 25.55572998430141,
      "grad_norm": 4.65718936920166,
      "learning_rate": 3.402766875981162e-05,
      "loss": 0.6024,
      "step": 1627900
    },
    {
      "epoch": 25.55729984301413,
      "grad_norm": 3.3413000106811523,
      "learning_rate": 3.402668759811617e-05,
      "loss": 0.5838,
      "step": 1628000
    },
    {
      "epoch": 25.558869701726845,
      "grad_norm": 3.3575825691223145,
      "learning_rate": 3.402570643642072e-05,
      "loss": 0.6173,
      "step": 1628100
    },
    {
      "epoch": 25.560439560439562,
      "grad_norm": 4.716169834136963,
      "learning_rate": 3.402472527472528e-05,
      "loss": 0.5888,
      "step": 1628200
    },
    {
      "epoch": 25.562009419152275,
      "grad_norm": 3.8156046867370605,
      "learning_rate": 3.402374411302983e-05,
      "loss": 0.5992,
      "step": 1628300
    },
    {
      "epoch": 25.563579277864992,
      "grad_norm": 5.042995929718018,
      "learning_rate": 3.402276295133438e-05,
      "loss": 0.5836,
      "step": 1628400
    },
    {
      "epoch": 25.56514913657771,
      "grad_norm": 4.095941066741943,
      "learning_rate": 3.402178178963893e-05,
      "loss": 0.6302,
      "step": 1628500
    },
    {
      "epoch": 25.566718995290422,
      "grad_norm": 3.2180986404418945,
      "learning_rate": 3.402080062794349e-05,
      "loss": 0.5948,
      "step": 1628600
    },
    {
      "epoch": 25.56828885400314,
      "grad_norm": 4.48105525970459,
      "learning_rate": 3.401981946624804e-05,
      "loss": 0.6059,
      "step": 1628700
    },
    {
      "epoch": 25.569858712715856,
      "grad_norm": 3.311086893081665,
      "learning_rate": 3.401883830455259e-05,
      "loss": 0.5966,
      "step": 1628800
    },
    {
      "epoch": 25.571428571428573,
      "grad_norm": 4.09602165222168,
      "learning_rate": 3.401785714285715e-05,
      "loss": 0.6166,
      "step": 1628900
    },
    {
      "epoch": 25.572998430141286,
      "grad_norm": 4.9743452072143555,
      "learning_rate": 3.401687598116169e-05,
      "loss": 0.5535,
      "step": 1629000
    },
    {
      "epoch": 25.574568288854003,
      "grad_norm": 5.102124214172363,
      "learning_rate": 3.401589481946625e-05,
      "loss": 0.5551,
      "step": 1629100
    },
    {
      "epoch": 25.57613814756672,
      "grad_norm": 4.086362838745117,
      "learning_rate": 3.40149136577708e-05,
      "loss": 0.6177,
      "step": 1629200
    },
    {
      "epoch": 25.577708006279433,
      "grad_norm": 3.764662504196167,
      "learning_rate": 3.401393249607536e-05,
      "loss": 0.61,
      "step": 1629300
    },
    {
      "epoch": 25.57927786499215,
      "grad_norm": 3.455322027206421,
      "learning_rate": 3.4012951334379904e-05,
      "loss": 0.5955,
      "step": 1629400
    },
    {
      "epoch": 25.580847723704867,
      "grad_norm": 5.787237644195557,
      "learning_rate": 3.401197017268446e-05,
      "loss": 0.5608,
      "step": 1629500
    },
    {
      "epoch": 25.582417582417584,
      "grad_norm": 3.5235068798065186,
      "learning_rate": 3.401098901098901e-05,
      "loss": 0.6344,
      "step": 1629600
    },
    {
      "epoch": 25.583987441130297,
      "grad_norm": 3.4627795219421387,
      "learning_rate": 3.4010007849293563e-05,
      "loss": 0.6297,
      "step": 1629700
    },
    {
      "epoch": 25.585557299843014,
      "grad_norm": 4.512975215911865,
      "learning_rate": 3.4009026687598114e-05,
      "loss": 0.5801,
      "step": 1629800
    },
    {
      "epoch": 25.58712715855573,
      "grad_norm": 2.6122405529022217,
      "learning_rate": 3.400804552590267e-05,
      "loss": 0.5953,
      "step": 1629900
    },
    {
      "epoch": 25.588697017268444,
      "grad_norm": 3.72419810295105,
      "learning_rate": 3.400706436420722e-05,
      "loss": 0.5956,
      "step": 1630000
    },
    {
      "epoch": 25.59026687598116,
      "grad_norm": 4.412764549255371,
      "learning_rate": 3.4006083202511774e-05,
      "loss": 0.6012,
      "step": 1630100
    },
    {
      "epoch": 25.591836734693878,
      "grad_norm": 2.774266004562378,
      "learning_rate": 3.4005102040816325e-05,
      "loss": 0.5861,
      "step": 1630200
    },
    {
      "epoch": 25.593406593406595,
      "grad_norm": 4.269073486328125,
      "learning_rate": 3.400412087912088e-05,
      "loss": 0.5796,
      "step": 1630300
    },
    {
      "epoch": 25.594976452119308,
      "grad_norm": 4.710610866546631,
      "learning_rate": 3.4003139717425434e-05,
      "loss": 0.6088,
      "step": 1630400
    },
    {
      "epoch": 25.596546310832025,
      "grad_norm": 4.122522830963135,
      "learning_rate": 3.4002158555729985e-05,
      "loss": 0.5732,
      "step": 1630500
    },
    {
      "epoch": 25.598116169544742,
      "grad_norm": 3.4960031509399414,
      "learning_rate": 3.4001177394034536e-05,
      "loss": 0.6067,
      "step": 1630600
    },
    {
      "epoch": 25.599686028257455,
      "grad_norm": 3.6869137287139893,
      "learning_rate": 3.4000196232339094e-05,
      "loss": 0.6065,
      "step": 1630700
    },
    {
      "epoch": 25.601255886970172,
      "grad_norm": 3.4889724254608154,
      "learning_rate": 3.3999215070643645e-05,
      "loss": 0.5979,
      "step": 1630800
    },
    {
      "epoch": 25.60282574568289,
      "grad_norm": 3.5662729740142822,
      "learning_rate": 3.3998233908948196e-05,
      "loss": 0.5761,
      "step": 1630900
    },
    {
      "epoch": 25.604395604395606,
      "grad_norm": 2.649568796157837,
      "learning_rate": 3.399725274725275e-05,
      "loss": 0.6288,
      "step": 1631000
    },
    {
      "epoch": 25.60596546310832,
      "grad_norm": 2.9082369804382324,
      "learning_rate": 3.39962715855573e-05,
      "loss": 0.6195,
      "step": 1631100
    },
    {
      "epoch": 25.607535321821036,
      "grad_norm": 3.816307783126831,
      "learning_rate": 3.3995290423861855e-05,
      "loss": 0.6064,
      "step": 1631200
    },
    {
      "epoch": 25.609105180533753,
      "grad_norm": 4.619772434234619,
      "learning_rate": 3.3994309262166406e-05,
      "loss": 0.5761,
      "step": 1631300
    },
    {
      "epoch": 25.610675039246466,
      "grad_norm": 4.223491191864014,
      "learning_rate": 3.3993328100470964e-05,
      "loss": 0.6337,
      "step": 1631400
    },
    {
      "epoch": 25.612244897959183,
      "grad_norm": 4.08820104598999,
      "learning_rate": 3.399234693877551e-05,
      "loss": 0.6007,
      "step": 1631500
    },
    {
      "epoch": 25.6138147566719,
      "grad_norm": 3.5022690296173096,
      "learning_rate": 3.3991365777080066e-05,
      "loss": 0.6255,
      "step": 1631600
    },
    {
      "epoch": 25.615384615384617,
      "grad_norm": 3.9762964248657227,
      "learning_rate": 3.399038461538462e-05,
      "loss": 0.5836,
      "step": 1631700
    },
    {
      "epoch": 25.61695447409733,
      "grad_norm": 2.6919806003570557,
      "learning_rate": 3.398940345368917e-05,
      "loss": 0.5775,
      "step": 1631800
    },
    {
      "epoch": 25.618524332810047,
      "grad_norm": 3.5031421184539795,
      "learning_rate": 3.398842229199372e-05,
      "loss": 0.5702,
      "step": 1631900
    },
    {
      "epoch": 25.620094191522764,
      "grad_norm": 3.247380495071411,
      "learning_rate": 3.398744113029828e-05,
      "loss": 0.5987,
      "step": 1632000
    },
    {
      "epoch": 25.621664050235477,
      "grad_norm": 2.9009616374969482,
      "learning_rate": 3.398645996860283e-05,
      "loss": 0.613,
      "step": 1632100
    },
    {
      "epoch": 25.623233908948194,
      "grad_norm": 3.261040449142456,
      "learning_rate": 3.398547880690738e-05,
      "loss": 0.5653,
      "step": 1632200
    },
    {
      "epoch": 25.62480376766091,
      "grad_norm": 3.8856513500213623,
      "learning_rate": 3.398449764521193e-05,
      "loss": 0.5831,
      "step": 1632300
    },
    {
      "epoch": 25.626373626373628,
      "grad_norm": 4.090855121612549,
      "learning_rate": 3.398351648351649e-05,
      "loss": 0.5775,
      "step": 1632400
    },
    {
      "epoch": 25.62794348508634,
      "grad_norm": 4.08198881149292,
      "learning_rate": 3.398253532182104e-05,
      "loss": 0.6283,
      "step": 1632500
    },
    {
      "epoch": 25.629513343799058,
      "grad_norm": 5.245267868041992,
      "learning_rate": 3.398155416012559e-05,
      "loss": 0.5566,
      "step": 1632600
    },
    {
      "epoch": 25.631083202511775,
      "grad_norm": 2.05851411819458,
      "learning_rate": 3.398057299843014e-05,
      "loss": 0.6133,
      "step": 1632700
    },
    {
      "epoch": 25.632653061224488,
      "grad_norm": 3.6925766468048096,
      "learning_rate": 3.39795918367347e-05,
      "loss": 0.5743,
      "step": 1632800
    },
    {
      "epoch": 25.634222919937205,
      "grad_norm": 2.3409650325775146,
      "learning_rate": 3.397861067503925e-05,
      "loss": 0.562,
      "step": 1632900
    },
    {
      "epoch": 25.635792778649922,
      "grad_norm": 4.600785255432129,
      "learning_rate": 3.39776295133438e-05,
      "loss": 0.6356,
      "step": 1633000
    },
    {
      "epoch": 25.63736263736264,
      "grad_norm": 3.4966161251068115,
      "learning_rate": 3.397664835164836e-05,
      "loss": 0.6011,
      "step": 1633100
    },
    {
      "epoch": 25.638932496075352,
      "grad_norm": 3.8398497104644775,
      "learning_rate": 3.39756671899529e-05,
      "loss": 0.5924,
      "step": 1633200
    },
    {
      "epoch": 25.64050235478807,
      "grad_norm": 4.19726037979126,
      "learning_rate": 3.397468602825746e-05,
      "loss": 0.5671,
      "step": 1633300
    },
    {
      "epoch": 25.642072213500786,
      "grad_norm": 4.746540546417236,
      "learning_rate": 3.397370486656201e-05,
      "loss": 0.5879,
      "step": 1633400
    },
    {
      "epoch": 25.643642072213503,
      "grad_norm": 2.9350945949554443,
      "learning_rate": 3.397272370486657e-05,
      "loss": 0.5749,
      "step": 1633500
    },
    {
      "epoch": 25.645211930926216,
      "grad_norm": 4.236404895782471,
      "learning_rate": 3.397174254317111e-05,
      "loss": 0.5923,
      "step": 1633600
    },
    {
      "epoch": 25.646781789638933,
      "grad_norm": 3.9538159370422363,
      "learning_rate": 3.397076138147567e-05,
      "loss": 0.6368,
      "step": 1633700
    },
    {
      "epoch": 25.64835164835165,
      "grad_norm": 2.712001323699951,
      "learning_rate": 3.396978021978022e-05,
      "loss": 0.5628,
      "step": 1633800
    },
    {
      "epoch": 25.649921507064363,
      "grad_norm": 4.3594770431518555,
      "learning_rate": 3.396879905808477e-05,
      "loss": 0.6369,
      "step": 1633900
    },
    {
      "epoch": 25.65149136577708,
      "grad_norm": 4.9483795166015625,
      "learning_rate": 3.3967817896389323e-05,
      "loss": 0.5867,
      "step": 1634000
    },
    {
      "epoch": 25.653061224489797,
      "grad_norm": 3.944782018661499,
      "learning_rate": 3.396683673469388e-05,
      "loss": 0.584,
      "step": 1634100
    },
    {
      "epoch": 25.65463108320251,
      "grad_norm": 3.7435953617095947,
      "learning_rate": 3.396585557299843e-05,
      "loss": 0.5884,
      "step": 1634200
    },
    {
      "epoch": 25.656200941915227,
      "grad_norm": 3.996400833129883,
      "learning_rate": 3.396487441130298e-05,
      "loss": 0.5668,
      "step": 1634300
    },
    {
      "epoch": 25.657770800627944,
      "grad_norm": 2.6287217140197754,
      "learning_rate": 3.3963893249607534e-05,
      "loss": 0.5741,
      "step": 1634400
    },
    {
      "epoch": 25.65934065934066,
      "grad_norm": 5.684201240539551,
      "learning_rate": 3.396291208791209e-05,
      "loss": 0.5933,
      "step": 1634500
    },
    {
      "epoch": 25.660910518053374,
      "grad_norm": 3.3485183715820312,
      "learning_rate": 3.396193092621664e-05,
      "loss": 0.5827,
      "step": 1634600
    },
    {
      "epoch": 25.66248037676609,
      "grad_norm": 2.2868175506591797,
      "learning_rate": 3.3960949764521194e-05,
      "loss": 0.6045,
      "step": 1634700
    },
    {
      "epoch": 25.664050235478808,
      "grad_norm": 4.49569845199585,
      "learning_rate": 3.3959968602825745e-05,
      "loss": 0.5424,
      "step": 1634800
    },
    {
      "epoch": 25.665620094191524,
      "grad_norm": 4.453610897064209,
      "learning_rate": 3.39589874411303e-05,
      "loss": 0.5884,
      "step": 1634900
    },
    {
      "epoch": 25.667189952904238,
      "grad_norm": 3.9660449028015137,
      "learning_rate": 3.3958006279434854e-05,
      "loss": 0.5975,
      "step": 1635000
    },
    {
      "epoch": 25.668759811616955,
      "grad_norm": 4.6974196434021,
      "learning_rate": 3.3957025117739405e-05,
      "loss": 0.5973,
      "step": 1635100
    },
    {
      "epoch": 25.67032967032967,
      "grad_norm": 4.223546981811523,
      "learning_rate": 3.395604395604396e-05,
      "loss": 0.6055,
      "step": 1635200
    },
    {
      "epoch": 25.671899529042385,
      "grad_norm": 3.6560776233673096,
      "learning_rate": 3.3955062794348507e-05,
      "loss": 0.5903,
      "step": 1635300
    },
    {
      "epoch": 25.6734693877551,
      "grad_norm": 4.096777439117432,
      "learning_rate": 3.3954081632653064e-05,
      "loss": 0.5878,
      "step": 1635400
    },
    {
      "epoch": 25.67503924646782,
      "grad_norm": 3.56295108795166,
      "learning_rate": 3.3953100470957615e-05,
      "loss": 0.6101,
      "step": 1635500
    },
    {
      "epoch": 25.676609105180535,
      "grad_norm": 4.3657917976379395,
      "learning_rate": 3.395211930926217e-05,
      "loss": 0.5669,
      "step": 1635600
    },
    {
      "epoch": 25.67817896389325,
      "grad_norm": 4.6251678466796875,
      "learning_rate": 3.395113814756672e-05,
      "loss": 0.5814,
      "step": 1635700
    },
    {
      "epoch": 25.679748822605966,
      "grad_norm": 4.3377685546875,
      "learning_rate": 3.3950156985871275e-05,
      "loss": 0.5585,
      "step": 1635800
    },
    {
      "epoch": 25.681318681318682,
      "grad_norm": 4.364567756652832,
      "learning_rate": 3.3949175824175826e-05,
      "loss": 0.5836,
      "step": 1635900
    },
    {
      "epoch": 25.682888540031396,
      "grad_norm": 3.2320327758789062,
      "learning_rate": 3.394819466248038e-05,
      "loss": 0.594,
      "step": 1636000
    },
    {
      "epoch": 25.684458398744113,
      "grad_norm": 3.25125789642334,
      "learning_rate": 3.394721350078493e-05,
      "loss": 0.6123,
      "step": 1636100
    },
    {
      "epoch": 25.68602825745683,
      "grad_norm": 3.171290397644043,
      "learning_rate": 3.3946232339089486e-05,
      "loss": 0.5673,
      "step": 1636200
    },
    {
      "epoch": 25.687598116169546,
      "grad_norm": 4.680252552032471,
      "learning_rate": 3.394525117739404e-05,
      "loss": 0.6496,
      "step": 1636300
    },
    {
      "epoch": 25.68916797488226,
      "grad_norm": 4.046574592590332,
      "learning_rate": 3.394427001569859e-05,
      "loss": 0.6006,
      "step": 1636400
    },
    {
      "epoch": 25.690737833594977,
      "grad_norm": 3.6069536209106445,
      "learning_rate": 3.394328885400314e-05,
      "loss": 0.5906,
      "step": 1636500
    },
    {
      "epoch": 25.692307692307693,
      "grad_norm": 3.5881733894348145,
      "learning_rate": 3.3942307692307696e-05,
      "loss": 0.6158,
      "step": 1636600
    },
    {
      "epoch": 25.693877551020407,
      "grad_norm": 4.653615951538086,
      "learning_rate": 3.394132653061225e-05,
      "loss": 0.584,
      "step": 1636700
    },
    {
      "epoch": 25.695447409733124,
      "grad_norm": 4.190183639526367,
      "learning_rate": 3.39403453689168e-05,
      "loss": 0.5989,
      "step": 1636800
    },
    {
      "epoch": 25.69701726844584,
      "grad_norm": 3.4738309383392334,
      "learning_rate": 3.393936420722135e-05,
      "loss": 0.6291,
      "step": 1636900
    },
    {
      "epoch": 25.698587127158557,
      "grad_norm": 3.1250805854797363,
      "learning_rate": 3.393838304552591e-05,
      "loss": 0.6322,
      "step": 1637000
    },
    {
      "epoch": 25.70015698587127,
      "grad_norm": 4.305736064910889,
      "learning_rate": 3.393740188383046e-05,
      "loss": 0.6149,
      "step": 1637100
    },
    {
      "epoch": 25.701726844583987,
      "grad_norm": 3.3120155334472656,
      "learning_rate": 3.393642072213501e-05,
      "loss": 0.6282,
      "step": 1637200
    },
    {
      "epoch": 25.703296703296704,
      "grad_norm": 3.0356953144073486,
      "learning_rate": 3.393543956043957e-05,
      "loss": 0.6393,
      "step": 1637300
    },
    {
      "epoch": 25.704866562009418,
      "grad_norm": 4.072258472442627,
      "learning_rate": 3.393445839874411e-05,
      "loss": 0.6349,
      "step": 1637400
    },
    {
      "epoch": 25.706436420722135,
      "grad_norm": 3.2471821308135986,
      "learning_rate": 3.393347723704867e-05,
      "loss": 0.5682,
      "step": 1637500
    },
    {
      "epoch": 25.70800627943485,
      "grad_norm": 2.7573347091674805,
      "learning_rate": 3.393249607535322e-05,
      "loss": 0.587,
      "step": 1637600
    },
    {
      "epoch": 25.70957613814757,
      "grad_norm": 3.6821465492248535,
      "learning_rate": 3.393151491365778e-05,
      "loss": 0.6301,
      "step": 1637700
    },
    {
      "epoch": 25.71114599686028,
      "grad_norm": 4.0617146492004395,
      "learning_rate": 3.393053375196232e-05,
      "loss": 0.6017,
      "step": 1637800
    },
    {
      "epoch": 25.712715855573,
      "grad_norm": 4.352565288543701,
      "learning_rate": 3.392955259026688e-05,
      "loss": 0.5697,
      "step": 1637900
    },
    {
      "epoch": 25.714285714285715,
      "grad_norm": 4.1985039710998535,
      "learning_rate": 3.392857142857143e-05,
      "loss": 0.5902,
      "step": 1638000
    },
    {
      "epoch": 25.71585557299843,
      "grad_norm": 3.8512141704559326,
      "learning_rate": 3.392759026687598e-05,
      "loss": 0.5458,
      "step": 1638100
    },
    {
      "epoch": 25.717425431711145,
      "grad_norm": 4.1357831954956055,
      "learning_rate": 3.392660910518053e-05,
      "loss": 0.6323,
      "step": 1638200
    },
    {
      "epoch": 25.718995290423862,
      "grad_norm": 3.9131901264190674,
      "learning_rate": 3.392562794348509e-05,
      "loss": 0.6444,
      "step": 1638300
    },
    {
      "epoch": 25.72056514913658,
      "grad_norm": 3.7911734580993652,
      "learning_rate": 3.392464678178964e-05,
      "loss": 0.5884,
      "step": 1638400
    },
    {
      "epoch": 25.722135007849293,
      "grad_norm": 5.021249771118164,
      "learning_rate": 3.392366562009419e-05,
      "loss": 0.5851,
      "step": 1638500
    },
    {
      "epoch": 25.72370486656201,
      "grad_norm": 4.319635391235352,
      "learning_rate": 3.392268445839874e-05,
      "loss": 0.6098,
      "step": 1638600
    },
    {
      "epoch": 25.725274725274726,
      "grad_norm": 4.224245548248291,
      "learning_rate": 3.39217032967033e-05,
      "loss": 0.5813,
      "step": 1638700
    },
    {
      "epoch": 25.72684458398744,
      "grad_norm": 4.196056842803955,
      "learning_rate": 3.392072213500785e-05,
      "loss": 0.5984,
      "step": 1638800
    },
    {
      "epoch": 25.728414442700156,
      "grad_norm": 3.426443576812744,
      "learning_rate": 3.39197409733124e-05,
      "loss": 0.5652,
      "step": 1638900
    },
    {
      "epoch": 25.729984301412873,
      "grad_norm": 3.0407707691192627,
      "learning_rate": 3.3918759811616954e-05,
      "loss": 0.5927,
      "step": 1639000
    },
    {
      "epoch": 25.73155416012559,
      "grad_norm": 3.1143150329589844,
      "learning_rate": 3.391777864992151e-05,
      "loss": 0.6015,
      "step": 1639100
    },
    {
      "epoch": 25.733124018838303,
      "grad_norm": 3.690472364425659,
      "learning_rate": 3.391679748822606e-05,
      "loss": 0.6005,
      "step": 1639200
    },
    {
      "epoch": 25.73469387755102,
      "grad_norm": 4.570993900299072,
      "learning_rate": 3.3915816326530614e-05,
      "loss": 0.6157,
      "step": 1639300
    },
    {
      "epoch": 25.736263736263737,
      "grad_norm": 3.2985782623291016,
      "learning_rate": 3.391483516483517e-05,
      "loss": 0.625,
      "step": 1639400
    },
    {
      "epoch": 25.73783359497645,
      "grad_norm": 3.7343411445617676,
      "learning_rate": 3.3913854003139715e-05,
      "loss": 0.5857,
      "step": 1639500
    },
    {
      "epoch": 25.739403453689167,
      "grad_norm": 3.6248202323913574,
      "learning_rate": 3.391287284144427e-05,
      "loss": 0.581,
      "step": 1639600
    },
    {
      "epoch": 25.740973312401884,
      "grad_norm": 4.627712726593018,
      "learning_rate": 3.3911891679748824e-05,
      "loss": 0.6541,
      "step": 1639700
    },
    {
      "epoch": 25.7425431711146,
      "grad_norm": 4.190062522888184,
      "learning_rate": 3.391091051805338e-05,
      "loss": 0.6183,
      "step": 1639800
    },
    {
      "epoch": 25.744113029827314,
      "grad_norm": 3.1807777881622314,
      "learning_rate": 3.3909929356357926e-05,
      "loss": 0.5757,
      "step": 1639900
    },
    {
      "epoch": 25.74568288854003,
      "grad_norm": 4.541217803955078,
      "learning_rate": 3.3908948194662484e-05,
      "loss": 0.6317,
      "step": 1640000
    },
    {
      "epoch": 25.747252747252748,
      "grad_norm": 5.1251606941223145,
      "learning_rate": 3.3907967032967035e-05,
      "loss": 0.6193,
      "step": 1640100
    },
    {
      "epoch": 25.74882260596546,
      "grad_norm": 4.145642280578613,
      "learning_rate": 3.3906985871271586e-05,
      "loss": 0.5918,
      "step": 1640200
    },
    {
      "epoch": 25.75039246467818,
      "grad_norm": 4.093105792999268,
      "learning_rate": 3.390600470957614e-05,
      "loss": 0.6487,
      "step": 1640300
    },
    {
      "epoch": 25.751962323390895,
      "grad_norm": 3.6939303874969482,
      "learning_rate": 3.3905023547880695e-05,
      "loss": 0.5988,
      "step": 1640400
    },
    {
      "epoch": 25.753532182103612,
      "grad_norm": 5.099016189575195,
      "learning_rate": 3.3904042386185246e-05,
      "loss": 0.5662,
      "step": 1640500
    },
    {
      "epoch": 25.755102040816325,
      "grad_norm": 3.3239688873291016,
      "learning_rate": 3.39030612244898e-05,
      "loss": 0.6558,
      "step": 1640600
    },
    {
      "epoch": 25.756671899529042,
      "grad_norm": 4.132504463195801,
      "learning_rate": 3.390208006279435e-05,
      "loss": 0.5855,
      "step": 1640700
    },
    {
      "epoch": 25.75824175824176,
      "grad_norm": 4.1582183837890625,
      "learning_rate": 3.3901098901098905e-05,
      "loss": 0.6054,
      "step": 1640800
    },
    {
      "epoch": 25.759811616954472,
      "grad_norm": 3.456930637359619,
      "learning_rate": 3.3900117739403456e-05,
      "loss": 0.594,
      "step": 1640900
    },
    {
      "epoch": 25.76138147566719,
      "grad_norm": 2.9837491512298584,
      "learning_rate": 3.389913657770801e-05,
      "loss": 0.584,
      "step": 1641000
    },
    {
      "epoch": 25.762951334379906,
      "grad_norm": 3.821751356124878,
      "learning_rate": 3.389815541601256e-05,
      "loss": 0.5939,
      "step": 1641100
    },
    {
      "epoch": 25.764521193092623,
      "grad_norm": 3.9181606769561768,
      "learning_rate": 3.3897174254317116e-05,
      "loss": 0.6093,
      "step": 1641200
    },
    {
      "epoch": 25.766091051805336,
      "grad_norm": 3.861337661743164,
      "learning_rate": 3.389619309262167e-05,
      "loss": 0.5782,
      "step": 1641300
    },
    {
      "epoch": 25.767660910518053,
      "grad_norm": 4.582871437072754,
      "learning_rate": 3.389521193092622e-05,
      "loss": 0.5975,
      "step": 1641400
    },
    {
      "epoch": 25.76923076923077,
      "grad_norm": 3.7176589965820312,
      "learning_rate": 3.3894230769230776e-05,
      "loss": 0.6181,
      "step": 1641500
    },
    {
      "epoch": 25.770800627943487,
      "grad_norm": 4.440351486206055,
      "learning_rate": 3.389324960753532e-05,
      "loss": 0.5757,
      "step": 1641600
    },
    {
      "epoch": 25.7723704866562,
      "grad_norm": 4.293990135192871,
      "learning_rate": 3.389226844583988e-05,
      "loss": 0.5839,
      "step": 1641700
    },
    {
      "epoch": 25.773940345368917,
      "grad_norm": 4.370734214782715,
      "learning_rate": 3.389128728414443e-05,
      "loss": 0.6019,
      "step": 1641800
    },
    {
      "epoch": 25.775510204081634,
      "grad_norm": 3.7160558700561523,
      "learning_rate": 3.3890306122448986e-05,
      "loss": 0.5888,
      "step": 1641900
    },
    {
      "epoch": 25.777080062794347,
      "grad_norm": 4.88389778137207,
      "learning_rate": 3.388932496075353e-05,
      "loss": 0.6006,
      "step": 1642000
    },
    {
      "epoch": 25.778649921507064,
      "grad_norm": 3.6278185844421387,
      "learning_rate": 3.388834379905809e-05,
      "loss": 0.5812,
      "step": 1642100
    },
    {
      "epoch": 25.78021978021978,
      "grad_norm": 3.9141883850097656,
      "learning_rate": 3.388736263736264e-05,
      "loss": 0.602,
      "step": 1642200
    },
    {
      "epoch": 25.781789638932494,
      "grad_norm": 2.9619903564453125,
      "learning_rate": 3.388638147566719e-05,
      "loss": 0.6274,
      "step": 1642300
    },
    {
      "epoch": 25.78335949764521,
      "grad_norm": 3.725144624710083,
      "learning_rate": 3.388540031397174e-05,
      "loss": 0.5612,
      "step": 1642400
    },
    {
      "epoch": 25.784929356357928,
      "grad_norm": 3.944561719894409,
      "learning_rate": 3.38844191522763e-05,
      "loss": 0.585,
      "step": 1642500
    },
    {
      "epoch": 25.786499215070645,
      "grad_norm": 2.4465203285217285,
      "learning_rate": 3.388343799058085e-05,
      "loss": 0.5948,
      "step": 1642600
    },
    {
      "epoch": 25.788069073783358,
      "grad_norm": 3.6973657608032227,
      "learning_rate": 3.38824568288854e-05,
      "loss": 0.6046,
      "step": 1642700
    },
    {
      "epoch": 25.789638932496075,
      "grad_norm": 4.406917572021484,
      "learning_rate": 3.388147566718995e-05,
      "loss": 0.6212,
      "step": 1642800
    },
    {
      "epoch": 25.791208791208792,
      "grad_norm": 2.902505397796631,
      "learning_rate": 3.388049450549451e-05,
      "loss": 0.5741,
      "step": 1642900
    },
    {
      "epoch": 25.79277864992151,
      "grad_norm": 2.2433207035064697,
      "learning_rate": 3.387951334379906e-05,
      "loss": 0.6163,
      "step": 1643000
    },
    {
      "epoch": 25.794348508634222,
      "grad_norm": 3.2911646366119385,
      "learning_rate": 3.387853218210361e-05,
      "loss": 0.5821,
      "step": 1643100
    },
    {
      "epoch": 25.79591836734694,
      "grad_norm": 2.4627254009246826,
      "learning_rate": 3.387755102040816e-05,
      "loss": 0.6291,
      "step": 1643200
    },
    {
      "epoch": 25.797488226059656,
      "grad_norm": 3.4297077655792236,
      "learning_rate": 3.387656985871272e-05,
      "loss": 0.6188,
      "step": 1643300
    },
    {
      "epoch": 25.79905808477237,
      "grad_norm": 2.4714717864990234,
      "learning_rate": 3.387558869701727e-05,
      "loss": 0.5937,
      "step": 1643400
    },
    {
      "epoch": 25.800627943485086,
      "grad_norm": 2.9394803047180176,
      "learning_rate": 3.387460753532182e-05,
      "loss": 0.5807,
      "step": 1643500
    },
    {
      "epoch": 25.802197802197803,
      "grad_norm": 3.040616750717163,
      "learning_rate": 3.387362637362638e-05,
      "loss": 0.6007,
      "step": 1643600
    },
    {
      "epoch": 25.80376766091052,
      "grad_norm": 3.3631932735443115,
      "learning_rate": 3.3872645211930924e-05,
      "loss": 0.5682,
      "step": 1643700
    },
    {
      "epoch": 25.805337519623233,
      "grad_norm": 4.6157426834106445,
      "learning_rate": 3.387166405023548e-05,
      "loss": 0.5696,
      "step": 1643800
    },
    {
      "epoch": 25.80690737833595,
      "grad_norm": 4.382449150085449,
      "learning_rate": 3.387068288854003e-05,
      "loss": 0.6118,
      "step": 1643900
    },
    {
      "epoch": 25.808477237048667,
      "grad_norm": 4.128201484680176,
      "learning_rate": 3.386970172684459e-05,
      "loss": 0.5821,
      "step": 1644000
    },
    {
      "epoch": 25.81004709576138,
      "grad_norm": 2.4573123455047607,
      "learning_rate": 3.3868720565149135e-05,
      "loss": 0.5661,
      "step": 1644100
    },
    {
      "epoch": 25.811616954474097,
      "grad_norm": 4.213366508483887,
      "learning_rate": 3.386773940345369e-05,
      "loss": 0.5998,
      "step": 1644200
    },
    {
      "epoch": 25.813186813186814,
      "grad_norm": 3.892812490463257,
      "learning_rate": 3.3866758241758244e-05,
      "loss": 0.6069,
      "step": 1644300
    },
    {
      "epoch": 25.81475667189953,
      "grad_norm": 4.137259483337402,
      "learning_rate": 3.3865777080062795e-05,
      "loss": 0.6033,
      "step": 1644400
    },
    {
      "epoch": 25.816326530612244,
      "grad_norm": 3.186134099960327,
      "learning_rate": 3.3864795918367346e-05,
      "loss": 0.5652,
      "step": 1644500
    },
    {
      "epoch": 25.81789638932496,
      "grad_norm": 3.2210004329681396,
      "learning_rate": 3.3863814756671904e-05,
      "loss": 0.5986,
      "step": 1644600
    },
    {
      "epoch": 25.819466248037678,
      "grad_norm": 3.2083749771118164,
      "learning_rate": 3.3862833594976455e-05,
      "loss": 0.5711,
      "step": 1644700
    },
    {
      "epoch": 25.82103610675039,
      "grad_norm": 2.858853578567505,
      "learning_rate": 3.3861852433281006e-05,
      "loss": 0.6092,
      "step": 1644800
    },
    {
      "epoch": 25.822605965463108,
      "grad_norm": 3.577378034591675,
      "learning_rate": 3.3860871271585557e-05,
      "loss": 0.5709,
      "step": 1644900
    },
    {
      "epoch": 25.824175824175825,
      "grad_norm": 3.36499285697937,
      "learning_rate": 3.3859890109890114e-05,
      "loss": 0.6093,
      "step": 1645000
    },
    {
      "epoch": 25.82574568288854,
      "grad_norm": 4.176756381988525,
      "learning_rate": 3.3858908948194665e-05,
      "loss": 0.6035,
      "step": 1645100
    },
    {
      "epoch": 25.827315541601255,
      "grad_norm": 3.409430742263794,
      "learning_rate": 3.3857927786499216e-05,
      "loss": 0.5494,
      "step": 1645200
    },
    {
      "epoch": 25.828885400313972,
      "grad_norm": 3.0591344833374023,
      "learning_rate": 3.385694662480377e-05,
      "loss": 0.5932,
      "step": 1645300
    },
    {
      "epoch": 25.83045525902669,
      "grad_norm": 3.416198968887329,
      "learning_rate": 3.3855965463108325e-05,
      "loss": 0.6091,
      "step": 1645400
    },
    {
      "epoch": 25.832025117739402,
      "grad_norm": 4.140566349029541,
      "learning_rate": 3.3854984301412876e-05,
      "loss": 0.5684,
      "step": 1645500
    },
    {
      "epoch": 25.83359497645212,
      "grad_norm": 3.722940683364868,
      "learning_rate": 3.385400313971743e-05,
      "loss": 0.5928,
      "step": 1645600
    },
    {
      "epoch": 25.835164835164836,
      "grad_norm": 3.111180305480957,
      "learning_rate": 3.3853021978021985e-05,
      "loss": 0.6241,
      "step": 1645700
    },
    {
      "epoch": 25.836734693877553,
      "grad_norm": 3.697117328643799,
      "learning_rate": 3.385204081632653e-05,
      "loss": 0.6101,
      "step": 1645800
    },
    {
      "epoch": 25.838304552590266,
      "grad_norm": 7.088291645050049,
      "learning_rate": 3.385105965463109e-05,
      "loss": 0.6008,
      "step": 1645900
    },
    {
      "epoch": 25.839874411302983,
      "grad_norm": 4.14748477935791,
      "learning_rate": 3.385007849293564e-05,
      "loss": 0.6213,
      "step": 1646000
    },
    {
      "epoch": 25.8414442700157,
      "grad_norm": 4.2601237297058105,
      "learning_rate": 3.3849097331240195e-05,
      "loss": 0.5694,
      "step": 1646100
    },
    {
      "epoch": 25.843014128728413,
      "grad_norm": 3.4870002269744873,
      "learning_rate": 3.384811616954474e-05,
      "loss": 0.5864,
      "step": 1646200
    },
    {
      "epoch": 25.84458398744113,
      "grad_norm": 4.824826240539551,
      "learning_rate": 3.38471350078493e-05,
      "loss": 0.6063,
      "step": 1646300
    },
    {
      "epoch": 25.846153846153847,
      "grad_norm": 4.450788974761963,
      "learning_rate": 3.384615384615385e-05,
      "loss": 0.5899,
      "step": 1646400
    },
    {
      "epoch": 25.847723704866564,
      "grad_norm": 4.75839900970459,
      "learning_rate": 3.38451726844584e-05,
      "loss": 0.622,
      "step": 1646500
    },
    {
      "epoch": 25.849293563579277,
      "grad_norm": 4.465194225311279,
      "learning_rate": 3.384419152276295e-05,
      "loss": 0.6299,
      "step": 1646600
    },
    {
      "epoch": 25.850863422291994,
      "grad_norm": 4.146828651428223,
      "learning_rate": 3.384321036106751e-05,
      "loss": 0.6154,
      "step": 1646700
    },
    {
      "epoch": 25.85243328100471,
      "grad_norm": 4.1473307609558105,
      "learning_rate": 3.384222919937206e-05,
      "loss": 0.6164,
      "step": 1646800
    },
    {
      "epoch": 25.854003139717424,
      "grad_norm": 4.589252471923828,
      "learning_rate": 3.384124803767661e-05,
      "loss": 0.6179,
      "step": 1646900
    },
    {
      "epoch": 25.85557299843014,
      "grad_norm": 4.205417633056641,
      "learning_rate": 3.384026687598116e-05,
      "loss": 0.6489,
      "step": 1647000
    },
    {
      "epoch": 25.857142857142858,
      "grad_norm": 4.015566825866699,
      "learning_rate": 3.383928571428572e-05,
      "loss": 0.6259,
      "step": 1647100
    },
    {
      "epoch": 25.858712715855575,
      "grad_norm": 3.9676456451416016,
      "learning_rate": 3.383830455259027e-05,
      "loss": 0.624,
      "step": 1647200
    },
    {
      "epoch": 25.860282574568288,
      "grad_norm": 4.1547722816467285,
      "learning_rate": 3.383732339089482e-05,
      "loss": 0.5373,
      "step": 1647300
    },
    {
      "epoch": 25.861852433281005,
      "grad_norm": 4.292097091674805,
      "learning_rate": 3.383634222919937e-05,
      "loss": 0.6068,
      "step": 1647400
    },
    {
      "epoch": 25.86342229199372,
      "grad_norm": 3.169780969619751,
      "learning_rate": 3.383536106750393e-05,
      "loss": 0.5801,
      "step": 1647500
    },
    {
      "epoch": 25.864992150706435,
      "grad_norm": 4.025414943695068,
      "learning_rate": 3.383437990580848e-05,
      "loss": 0.6055,
      "step": 1647600
    },
    {
      "epoch": 25.86656200941915,
      "grad_norm": 3.864823579788208,
      "learning_rate": 3.383339874411303e-05,
      "loss": 0.6062,
      "step": 1647700
    },
    {
      "epoch": 25.86813186813187,
      "grad_norm": 4.489412784576416,
      "learning_rate": 3.383241758241759e-05,
      "loss": 0.5592,
      "step": 1647800
    },
    {
      "epoch": 25.869701726844585,
      "grad_norm": 3.2035961151123047,
      "learning_rate": 3.3831436420722133e-05,
      "loss": 0.575,
      "step": 1647900
    },
    {
      "epoch": 25.8712715855573,
      "grad_norm": 3.407498836517334,
      "learning_rate": 3.383045525902669e-05,
      "loss": 0.5978,
      "step": 1648000
    },
    {
      "epoch": 25.872841444270016,
      "grad_norm": 3.8755643367767334,
      "learning_rate": 3.382947409733124e-05,
      "loss": 0.6063,
      "step": 1648100
    },
    {
      "epoch": 25.874411302982733,
      "grad_norm": 3.3725898265838623,
      "learning_rate": 3.38284929356358e-05,
      "loss": 0.553,
      "step": 1648200
    },
    {
      "epoch": 25.875981161695446,
      "grad_norm": 4.104650020599365,
      "learning_rate": 3.3827511773940344e-05,
      "loss": 0.6005,
      "step": 1648300
    },
    {
      "epoch": 25.877551020408163,
      "grad_norm": 4.762728214263916,
      "learning_rate": 3.38265306122449e-05,
      "loss": 0.6085,
      "step": 1648400
    },
    {
      "epoch": 25.87912087912088,
      "grad_norm": 4.250762939453125,
      "learning_rate": 3.382554945054945e-05,
      "loss": 0.6324,
      "step": 1648500
    },
    {
      "epoch": 25.880690737833596,
      "grad_norm": 4.0330095291137695,
      "learning_rate": 3.3824568288854004e-05,
      "loss": 0.6115,
      "step": 1648600
    },
    {
      "epoch": 25.88226059654631,
      "grad_norm": 3.8578274250030518,
      "learning_rate": 3.3823587127158555e-05,
      "loss": 0.606,
      "step": 1648700
    },
    {
      "epoch": 25.883830455259027,
      "grad_norm": 2.8592429161071777,
      "learning_rate": 3.382260596546311e-05,
      "loss": 0.6305,
      "step": 1648800
    },
    {
      "epoch": 25.885400313971743,
      "grad_norm": 4.589377403259277,
      "learning_rate": 3.3821624803767664e-05,
      "loss": 0.6167,
      "step": 1648900
    },
    {
      "epoch": 25.886970172684457,
      "grad_norm": 3.594973087310791,
      "learning_rate": 3.3820643642072215e-05,
      "loss": 0.6147,
      "step": 1649000
    },
    {
      "epoch": 25.888540031397174,
      "grad_norm": 3.7324721813201904,
      "learning_rate": 3.3819662480376766e-05,
      "loss": 0.6351,
      "step": 1649100
    },
    {
      "epoch": 25.89010989010989,
      "grad_norm": 3.2882726192474365,
      "learning_rate": 3.381868131868132e-05,
      "loss": 0.6365,
      "step": 1649200
    },
    {
      "epoch": 25.891679748822607,
      "grad_norm": 2.763629198074341,
      "learning_rate": 3.381770015698587e-05,
      "loss": 0.5976,
      "step": 1649300
    },
    {
      "epoch": 25.89324960753532,
      "grad_norm": 3.718472957611084,
      "learning_rate": 3.3816718995290425e-05,
      "loss": 0.584,
      "step": 1649400
    },
    {
      "epoch": 25.894819466248038,
      "grad_norm": 4.90078067779541,
      "learning_rate": 3.3815737833594976e-05,
      "loss": 0.5799,
      "step": 1649500
    },
    {
      "epoch": 25.896389324960754,
      "grad_norm": 3.617189407348633,
      "learning_rate": 3.381475667189953e-05,
      "loss": 0.6007,
      "step": 1649600
    },
    {
      "epoch": 25.897959183673468,
      "grad_norm": 4.164400100708008,
      "learning_rate": 3.3813775510204085e-05,
      "loss": 0.6274,
      "step": 1649700
    },
    {
      "epoch": 25.899529042386185,
      "grad_norm": 3.1925241947174072,
      "learning_rate": 3.3812794348508636e-05,
      "loss": 0.5508,
      "step": 1649800
    },
    {
      "epoch": 25.9010989010989,
      "grad_norm": 2.591768980026245,
      "learning_rate": 3.3811813186813194e-05,
      "loss": 0.6015,
      "step": 1649900
    },
    {
      "epoch": 25.90266875981162,
      "grad_norm": 4.143308162689209,
      "learning_rate": 3.381083202511774e-05,
      "loss": 0.589,
      "step": 1650000
    },
    {
      "epoch": 25.90423861852433,
      "grad_norm": 2.9112801551818848,
      "learning_rate": 3.3809850863422296e-05,
      "loss": 0.5912,
      "step": 1650100
    },
    {
      "epoch": 25.90580847723705,
      "grad_norm": 5.006011962890625,
      "learning_rate": 3.380886970172685e-05,
      "loss": 0.6179,
      "step": 1650200
    },
    {
      "epoch": 25.907378335949765,
      "grad_norm": 3.7284610271453857,
      "learning_rate": 3.38078885400314e-05,
      "loss": 0.591,
      "step": 1650300
    },
    {
      "epoch": 25.90894819466248,
      "grad_norm": 3.3324785232543945,
      "learning_rate": 3.380690737833595e-05,
      "loss": 0.6014,
      "step": 1650400
    },
    {
      "epoch": 25.910518053375196,
      "grad_norm": 3.2316675186157227,
      "learning_rate": 3.3805926216640506e-05,
      "loss": 0.5854,
      "step": 1650500
    },
    {
      "epoch": 25.912087912087912,
      "grad_norm": 3.2467355728149414,
      "learning_rate": 3.380494505494506e-05,
      "loss": 0.6182,
      "step": 1650600
    },
    {
      "epoch": 25.91365777080063,
      "grad_norm": 4.21221399307251,
      "learning_rate": 3.380396389324961e-05,
      "loss": 0.6063,
      "step": 1650700
    },
    {
      "epoch": 25.915227629513343,
      "grad_norm": 3.975905656814575,
      "learning_rate": 3.380298273155416e-05,
      "loss": 0.5966,
      "step": 1650800
    },
    {
      "epoch": 25.91679748822606,
      "grad_norm": 3.62604022026062,
      "learning_rate": 3.380200156985872e-05,
      "loss": 0.5773,
      "step": 1650900
    },
    {
      "epoch": 25.918367346938776,
      "grad_norm": 2.931844711303711,
      "learning_rate": 3.380102040816326e-05,
      "loss": 0.6135,
      "step": 1651000
    },
    {
      "epoch": 25.919937205651493,
      "grad_norm": 3.381223440170288,
      "learning_rate": 3.380003924646782e-05,
      "loss": 0.5905,
      "step": 1651100
    },
    {
      "epoch": 25.921507064364206,
      "grad_norm": 4.034534454345703,
      "learning_rate": 3.379905808477237e-05,
      "loss": 0.6312,
      "step": 1651200
    },
    {
      "epoch": 25.923076923076923,
      "grad_norm": 3.0212302207946777,
      "learning_rate": 3.379807692307693e-05,
      "loss": 0.5764,
      "step": 1651300
    },
    {
      "epoch": 25.92464678178964,
      "grad_norm": 3.8290441036224365,
      "learning_rate": 3.379709576138147e-05,
      "loss": 0.5758,
      "step": 1651400
    },
    {
      "epoch": 25.926216640502354,
      "grad_norm": 3.4873104095458984,
      "learning_rate": 3.379611459968603e-05,
      "loss": 0.5756,
      "step": 1651500
    },
    {
      "epoch": 25.92778649921507,
      "grad_norm": 3.192152261734009,
      "learning_rate": 3.379513343799058e-05,
      "loss": 0.5845,
      "step": 1651600
    },
    {
      "epoch": 25.929356357927787,
      "grad_norm": 2.595984697341919,
      "learning_rate": 3.379415227629513e-05,
      "loss": 0.569,
      "step": 1651700
    },
    {
      "epoch": 25.9309262166405,
      "grad_norm": 3.345637798309326,
      "learning_rate": 3.379317111459969e-05,
      "loss": 0.5588,
      "step": 1651800
    },
    {
      "epoch": 25.932496075353217,
      "grad_norm": 3.5200908184051514,
      "learning_rate": 3.379218995290424e-05,
      "loss": 0.6217,
      "step": 1651900
    },
    {
      "epoch": 25.934065934065934,
      "grad_norm": 3.785888671875,
      "learning_rate": 3.37912087912088e-05,
      "loss": 0.5648,
      "step": 1652000
    },
    {
      "epoch": 25.93563579277865,
      "grad_norm": 3.442809581756592,
      "learning_rate": 3.379022762951334e-05,
      "loss": 0.6046,
      "step": 1652100
    },
    {
      "epoch": 25.937205651491364,
      "grad_norm": 3.6565604209899902,
      "learning_rate": 3.37892464678179e-05,
      "loss": 0.6,
      "step": 1652200
    },
    {
      "epoch": 25.93877551020408,
      "grad_norm": 2.2236573696136475,
      "learning_rate": 3.378826530612245e-05,
      "loss": 0.6016,
      "step": 1652300
    },
    {
      "epoch": 25.940345368916798,
      "grad_norm": 4.309567928314209,
      "learning_rate": 3.3787284144427e-05,
      "loss": 0.5879,
      "step": 1652400
    },
    {
      "epoch": 25.941915227629515,
      "grad_norm": 3.2946746349334717,
      "learning_rate": 3.378630298273155e-05,
      "loss": 0.6218,
      "step": 1652500
    },
    {
      "epoch": 25.94348508634223,
      "grad_norm": 3.6104490756988525,
      "learning_rate": 3.378532182103611e-05,
      "loss": 0.6255,
      "step": 1652600
    },
    {
      "epoch": 25.945054945054945,
      "grad_norm": 2.484757661819458,
      "learning_rate": 3.378434065934066e-05,
      "loss": 0.6019,
      "step": 1652700
    },
    {
      "epoch": 25.946624803767662,
      "grad_norm": 3.4055888652801514,
      "learning_rate": 3.378335949764521e-05,
      "loss": 0.6106,
      "step": 1652800
    },
    {
      "epoch": 25.948194662480375,
      "grad_norm": 4.140844345092773,
      "learning_rate": 3.3782378335949764e-05,
      "loss": 0.5969,
      "step": 1652900
    },
    {
      "epoch": 25.949764521193092,
      "grad_norm": 2.8180594444274902,
      "learning_rate": 3.378139717425432e-05,
      "loss": 0.5858,
      "step": 1653000
    },
    {
      "epoch": 25.95133437990581,
      "grad_norm": 3.044192314147949,
      "learning_rate": 3.3780416012558866e-05,
      "loss": 0.5982,
      "step": 1653100
    },
    {
      "epoch": 25.952904238618526,
      "grad_norm": 3.0368964672088623,
      "learning_rate": 3.3779434850863424e-05,
      "loss": 0.6145,
      "step": 1653200
    },
    {
      "epoch": 25.95447409733124,
      "grad_norm": 4.2264933586120605,
      "learning_rate": 3.3778453689167975e-05,
      "loss": 0.6346,
      "step": 1653300
    },
    {
      "epoch": 25.956043956043956,
      "grad_norm": 4.0293474197387695,
      "learning_rate": 3.377747252747253e-05,
      "loss": 0.5808,
      "step": 1653400
    },
    {
      "epoch": 25.957613814756673,
      "grad_norm": 3.9194891452789307,
      "learning_rate": 3.3776491365777077e-05,
      "loss": 0.5921,
      "step": 1653500
    },
    {
      "epoch": 25.959183673469386,
      "grad_norm": 3.7312591075897217,
      "learning_rate": 3.3775510204081634e-05,
      "loss": 0.6242,
      "step": 1653600
    },
    {
      "epoch": 25.960753532182103,
      "grad_norm": 3.4994254112243652,
      "learning_rate": 3.3774529042386185e-05,
      "loss": 0.5963,
      "step": 1653700
    },
    {
      "epoch": 25.96232339089482,
      "grad_norm": 3.9670121669769287,
      "learning_rate": 3.3773547880690736e-05,
      "loss": 0.6218,
      "step": 1653800
    },
    {
      "epoch": 25.963893249607537,
      "grad_norm": 3.491091728210449,
      "learning_rate": 3.3772566718995294e-05,
      "loss": 0.6342,
      "step": 1653900
    },
    {
      "epoch": 25.96546310832025,
      "grad_norm": 3.2373032569885254,
      "learning_rate": 3.3771585557299845e-05,
      "loss": 0.615,
      "step": 1654000
    },
    {
      "epoch": 25.967032967032967,
      "grad_norm": 4.159483432769775,
      "learning_rate": 3.37706043956044e-05,
      "loss": 0.5974,
      "step": 1654100
    },
    {
      "epoch": 25.968602825745684,
      "grad_norm": 3.959799289703369,
      "learning_rate": 3.376962323390895e-05,
      "loss": 0.6469,
      "step": 1654200
    },
    {
      "epoch": 25.970172684458397,
      "grad_norm": 3.202223539352417,
      "learning_rate": 3.3768642072213505e-05,
      "loss": 0.586,
      "step": 1654300
    },
    {
      "epoch": 25.971742543171114,
      "grad_norm": 2.6614067554473877,
      "learning_rate": 3.3767660910518056e-05,
      "loss": 0.5967,
      "step": 1654400
    },
    {
      "epoch": 25.97331240188383,
      "grad_norm": 2.9015204906463623,
      "learning_rate": 3.376667974882261e-05,
      "loss": 0.6061,
      "step": 1654500
    },
    {
      "epoch": 25.974882260596548,
      "grad_norm": 4.422904014587402,
      "learning_rate": 3.376569858712716e-05,
      "loss": 0.5626,
      "step": 1654600
    },
    {
      "epoch": 25.97645211930926,
      "grad_norm": 3.761751174926758,
      "learning_rate": 3.3764717425431715e-05,
      "loss": 0.5747,
      "step": 1654700
    },
    {
      "epoch": 25.978021978021978,
      "grad_norm": 3.874634265899658,
      "learning_rate": 3.3763736263736266e-05,
      "loss": 0.6507,
      "step": 1654800
    },
    {
      "epoch": 25.979591836734695,
      "grad_norm": 4.106222629547119,
      "learning_rate": 3.376275510204082e-05,
      "loss": 0.6267,
      "step": 1654900
    },
    {
      "epoch": 25.98116169544741,
      "grad_norm": 3.8050436973571777,
      "learning_rate": 3.376177394034537e-05,
      "loss": 0.5798,
      "step": 1655000
    },
    {
      "epoch": 25.982731554160125,
      "grad_norm": 3.522406816482544,
      "learning_rate": 3.3760792778649926e-05,
      "loss": 0.6145,
      "step": 1655100
    },
    {
      "epoch": 25.984301412872842,
      "grad_norm": 4.333707332611084,
      "learning_rate": 3.375981161695447e-05,
      "loss": 0.573,
      "step": 1655200
    },
    {
      "epoch": 25.98587127158556,
      "grad_norm": 3.9634909629821777,
      "learning_rate": 3.375883045525903e-05,
      "loss": 0.5964,
      "step": 1655300
    },
    {
      "epoch": 25.987441130298272,
      "grad_norm": 4.654160022735596,
      "learning_rate": 3.375784929356358e-05,
      "loss": 0.6217,
      "step": 1655400
    },
    {
      "epoch": 25.98901098901099,
      "grad_norm": 2.470214605331421,
      "learning_rate": 3.375686813186814e-05,
      "loss": 0.6274,
      "step": 1655500
    },
    {
      "epoch": 25.990580847723706,
      "grad_norm": 3.3864667415618896,
      "learning_rate": 3.375588697017268e-05,
      "loss": 0.6146,
      "step": 1655600
    },
    {
      "epoch": 25.99215070643642,
      "grad_norm": 2.9201717376708984,
      "learning_rate": 3.375490580847724e-05,
      "loss": 0.5823,
      "step": 1655700
    },
    {
      "epoch": 25.993720565149136,
      "grad_norm": 3.275007963180542,
      "learning_rate": 3.375392464678179e-05,
      "loss": 0.6083,
      "step": 1655800
    },
    {
      "epoch": 25.995290423861853,
      "grad_norm": 3.73852801322937,
      "learning_rate": 3.375294348508634e-05,
      "loss": 0.6021,
      "step": 1655900
    },
    {
      "epoch": 25.99686028257457,
      "grad_norm": 3.3337104320526123,
      "learning_rate": 3.37519623233909e-05,
      "loss": 0.6099,
      "step": 1656000
    },
    {
      "epoch": 25.998430141287283,
      "grad_norm": 4.253291130065918,
      "learning_rate": 3.375098116169545e-05,
      "loss": 0.6361,
      "step": 1656100
    },
    {
      "epoch": 26.0,
      "grad_norm": 3.806525707244873,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.598,
      "step": 1656200
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.0230871438980103,
      "eval_runtime": 17.752,
      "eval_samples_per_second": 188.88,
      "eval_steps_per_second": 188.88,
      "step": 1656200
    },
    {
      "epoch": 26.0,
      "eval_loss": 0.459146648645401,
      "eval_runtime": 342.4875,
      "eval_samples_per_second": 185.992,
      "eval_steps_per_second": 185.992,
      "step": 1656200
    },
    {
      "epoch": 26.001569858712717,
      "grad_norm": 3.7695422172546387,
      "learning_rate": 3.374901883830455e-05,
      "loss": 0.6412,
      "step": 1656300
    },
    {
      "epoch": 26.00313971742543,
      "grad_norm": 4.410360813140869,
      "learning_rate": 3.374803767660911e-05,
      "loss": 0.5723,
      "step": 1656400
    },
    {
      "epoch": 26.004709576138147,
      "grad_norm": 4.277609825134277,
      "learning_rate": 3.374705651491366e-05,
      "loss": 0.5581,
      "step": 1656500
    },
    {
      "epoch": 26.006279434850864,
      "grad_norm": 4.0569586753845215,
      "learning_rate": 3.374607535321821e-05,
      "loss": 0.56,
      "step": 1656600
    },
    {
      "epoch": 26.00784929356358,
      "grad_norm": 4.102379322052002,
      "learning_rate": 3.374509419152276e-05,
      "loss": 0.5851,
      "step": 1656700
    },
    {
      "epoch": 26.009419152276294,
      "grad_norm": 3.343183755874634,
      "learning_rate": 3.374411302982732e-05,
      "loss": 0.5556,
      "step": 1656800
    },
    {
      "epoch": 26.01098901098901,
      "grad_norm": 2.58480167388916,
      "learning_rate": 3.374313186813187e-05,
      "loss": 0.5748,
      "step": 1656900
    },
    {
      "epoch": 26.012558869701728,
      "grad_norm": 4.149324417114258,
      "learning_rate": 3.374215070643642e-05,
      "loss": 0.5915,
      "step": 1657000
    },
    {
      "epoch": 26.01412872841444,
      "grad_norm": 4.719239234924316,
      "learning_rate": 3.374116954474097e-05,
      "loss": 0.5668,
      "step": 1657100
    },
    {
      "epoch": 26.015698587127158,
      "grad_norm": 2.57389760017395,
      "learning_rate": 3.374018838304553e-05,
      "loss": 0.6462,
      "step": 1657200
    },
    {
      "epoch": 26.017268445839875,
      "grad_norm": 4.209555149078369,
      "learning_rate": 3.3739207221350075e-05,
      "loss": 0.5791,
      "step": 1657300
    },
    {
      "epoch": 26.01883830455259,
      "grad_norm": 4.133252143859863,
      "learning_rate": 3.373822605965463e-05,
      "loss": 0.6609,
      "step": 1657400
    },
    {
      "epoch": 26.020408163265305,
      "grad_norm": 3.674595594406128,
      "learning_rate": 3.3737244897959184e-05,
      "loss": 0.6017,
      "step": 1657500
    },
    {
      "epoch": 26.021978021978022,
      "grad_norm": 3.3653736114501953,
      "learning_rate": 3.373626373626374e-05,
      "loss": 0.611,
      "step": 1657600
    },
    {
      "epoch": 26.02354788069074,
      "grad_norm": 3.6674070358276367,
      "learning_rate": 3.3735282574568285e-05,
      "loss": 0.5767,
      "step": 1657700
    },
    {
      "epoch": 26.025117739403452,
      "grad_norm": 3.3941102027893066,
      "learning_rate": 3.373430141287284e-05,
      "loss": 0.5785,
      "step": 1657800
    },
    {
      "epoch": 26.02668759811617,
      "grad_norm": 4.08128547668457,
      "learning_rate": 3.3733320251177394e-05,
      "loss": 0.6056,
      "step": 1657900
    },
    {
      "epoch": 26.028257456828886,
      "grad_norm": 4.225765228271484,
      "learning_rate": 3.3732339089481945e-05,
      "loss": 0.6037,
      "step": 1658000
    },
    {
      "epoch": 26.029827315541603,
      "grad_norm": 4.94089937210083,
      "learning_rate": 3.37313579277865e-05,
      "loss": 0.5997,
      "step": 1658100
    },
    {
      "epoch": 26.031397174254316,
      "grad_norm": 5.0039381980896,
      "learning_rate": 3.3730376766091054e-05,
      "loss": 0.63,
      "step": 1658200
    },
    {
      "epoch": 26.032967032967033,
      "grad_norm": 4.111425399780273,
      "learning_rate": 3.3729395604395605e-05,
      "loss": 0.58,
      "step": 1658300
    },
    {
      "epoch": 26.03453689167975,
      "grad_norm": 3.829899787902832,
      "learning_rate": 3.3728414442700156e-05,
      "loss": 0.5725,
      "step": 1658400
    },
    {
      "epoch": 26.036106750392463,
      "grad_norm": 3.9606919288635254,
      "learning_rate": 3.3727433281004714e-05,
      "loss": 0.5956,
      "step": 1658500
    },
    {
      "epoch": 26.03767660910518,
      "grad_norm": 4.4817986488342285,
      "learning_rate": 3.3726452119309265e-05,
      "loss": 0.5984,
      "step": 1658600
    },
    {
      "epoch": 26.039246467817897,
      "grad_norm": 3.1684744358062744,
      "learning_rate": 3.3725470957613816e-05,
      "loss": 0.5998,
      "step": 1658700
    },
    {
      "epoch": 26.040816326530614,
      "grad_norm": 2.7374279499053955,
      "learning_rate": 3.3724489795918367e-05,
      "loss": 0.5709,
      "step": 1658800
    },
    {
      "epoch": 26.042386185243327,
      "grad_norm": 3.8141701221466064,
      "learning_rate": 3.3723508634222924e-05,
      "loss": 0.5568,
      "step": 1658900
    },
    {
      "epoch": 26.043956043956044,
      "grad_norm": 4.839911937713623,
      "learning_rate": 3.3722527472527475e-05,
      "loss": 0.5693,
      "step": 1659000
    },
    {
      "epoch": 26.04552590266876,
      "grad_norm": 3.8649494647979736,
      "learning_rate": 3.3721546310832026e-05,
      "loss": 0.5871,
      "step": 1659100
    },
    {
      "epoch": 26.047095761381474,
      "grad_norm": 2.633164405822754,
      "learning_rate": 3.372056514913658e-05,
      "loss": 0.5983,
      "step": 1659200
    },
    {
      "epoch": 26.04866562009419,
      "grad_norm": 3.753748655319214,
      "learning_rate": 3.3719583987441135e-05,
      "loss": 0.5865,
      "step": 1659300
    },
    {
      "epoch": 26.050235478806908,
      "grad_norm": 3.362377643585205,
      "learning_rate": 3.371860282574568e-05,
      "loss": 0.629,
      "step": 1659400
    },
    {
      "epoch": 26.051805337519625,
      "grad_norm": 3.892521858215332,
      "learning_rate": 3.371762166405024e-05,
      "loss": 0.5708,
      "step": 1659500
    },
    {
      "epoch": 26.053375196232338,
      "grad_norm": 3.519136428833008,
      "learning_rate": 3.371664050235479e-05,
      "loss": 0.5297,
      "step": 1659600
    },
    {
      "epoch": 26.054945054945055,
      "grad_norm": 4.483084678649902,
      "learning_rate": 3.3715659340659346e-05,
      "loss": 0.6064,
      "step": 1659700
    },
    {
      "epoch": 26.05651491365777,
      "grad_norm": 4.115435600280762,
      "learning_rate": 3.371467817896389e-05,
      "loss": 0.5767,
      "step": 1659800
    },
    {
      "epoch": 26.058084772370485,
      "grad_norm": 3.8341891765594482,
      "learning_rate": 3.371369701726845e-05,
      "loss": 0.5714,
      "step": 1659900
    },
    {
      "epoch": 26.059654631083202,
      "grad_norm": 2.547375440597534,
      "learning_rate": 3.3712715855573e-05,
      "loss": 0.5961,
      "step": 1660000
    },
    {
      "epoch": 26.06122448979592,
      "grad_norm": 3.8413889408111572,
      "learning_rate": 3.371173469387755e-05,
      "loss": 0.6131,
      "step": 1660100
    },
    {
      "epoch": 26.062794348508636,
      "grad_norm": 3.961413621902466,
      "learning_rate": 3.371075353218211e-05,
      "loss": 0.5808,
      "step": 1660200
    },
    {
      "epoch": 26.06436420722135,
      "grad_norm": 3.829439163208008,
      "learning_rate": 3.370977237048666e-05,
      "loss": 0.6154,
      "step": 1660300
    },
    {
      "epoch": 26.065934065934066,
      "grad_norm": 2.638187885284424,
      "learning_rate": 3.370879120879121e-05,
      "loss": 0.5909,
      "step": 1660400
    },
    {
      "epoch": 26.067503924646783,
      "grad_norm": 4.274410724639893,
      "learning_rate": 3.370781004709576e-05,
      "loss": 0.5941,
      "step": 1660500
    },
    {
      "epoch": 26.069073783359496,
      "grad_norm": 3.9269750118255615,
      "learning_rate": 3.370682888540032e-05,
      "loss": 0.5936,
      "step": 1660600
    },
    {
      "epoch": 26.070643642072213,
      "grad_norm": 2.9100425243377686,
      "learning_rate": 3.370584772370487e-05,
      "loss": 0.5653,
      "step": 1660700
    },
    {
      "epoch": 26.07221350078493,
      "grad_norm": 4.182045936584473,
      "learning_rate": 3.370486656200942e-05,
      "loss": 0.6042,
      "step": 1660800
    },
    {
      "epoch": 26.073783359497646,
      "grad_norm": 4.264972686767578,
      "learning_rate": 3.370388540031397e-05,
      "loss": 0.5897,
      "step": 1660900
    },
    {
      "epoch": 26.07535321821036,
      "grad_norm": 3.9436306953430176,
      "learning_rate": 3.370290423861853e-05,
      "loss": 0.6008,
      "step": 1661000
    },
    {
      "epoch": 26.076923076923077,
      "grad_norm": 2.912891149520874,
      "learning_rate": 3.370192307692308e-05,
      "loss": 0.6397,
      "step": 1661100
    },
    {
      "epoch": 26.078492935635794,
      "grad_norm": 3.6923060417175293,
      "learning_rate": 3.370094191522763e-05,
      "loss": 0.5741,
      "step": 1661200
    },
    {
      "epoch": 26.08006279434851,
      "grad_norm": 3.007606029510498,
      "learning_rate": 3.369996075353218e-05,
      "loss": 0.6016,
      "step": 1661300
    },
    {
      "epoch": 26.081632653061224,
      "grad_norm": 3.9343435764312744,
      "learning_rate": 3.369897959183674e-05,
      "loss": 0.5848,
      "step": 1661400
    },
    {
      "epoch": 26.08320251177394,
      "grad_norm": 3.3925979137420654,
      "learning_rate": 3.3697998430141284e-05,
      "loss": 0.5894,
      "step": 1661500
    },
    {
      "epoch": 26.084772370486657,
      "grad_norm": 3.5408668518066406,
      "learning_rate": 3.369701726844584e-05,
      "loss": 0.5542,
      "step": 1661600
    },
    {
      "epoch": 26.08634222919937,
      "grad_norm": 4.4738335609436035,
      "learning_rate": 3.369603610675039e-05,
      "loss": 0.586,
      "step": 1661700
    },
    {
      "epoch": 26.087912087912088,
      "grad_norm": 4.335615634918213,
      "learning_rate": 3.369505494505495e-05,
      "loss": 0.5679,
      "step": 1661800
    },
    {
      "epoch": 26.089481946624804,
      "grad_norm": 3.3087403774261475,
      "learning_rate": 3.3694073783359494e-05,
      "loss": 0.6329,
      "step": 1661900
    },
    {
      "epoch": 26.09105180533752,
      "grad_norm": 2.717233180999756,
      "learning_rate": 3.369309262166405e-05,
      "loss": 0.5766,
      "step": 1662000
    },
    {
      "epoch": 26.092621664050235,
      "grad_norm": 3.1297080516815186,
      "learning_rate": 3.36921114599686e-05,
      "loss": 0.5883,
      "step": 1662100
    },
    {
      "epoch": 26.09419152276295,
      "grad_norm": 2.3300530910491943,
      "learning_rate": 3.3691130298273154e-05,
      "loss": 0.6006,
      "step": 1662200
    },
    {
      "epoch": 26.09576138147567,
      "grad_norm": 2.6142871379852295,
      "learning_rate": 3.369014913657771e-05,
      "loss": 0.559,
      "step": 1662300
    },
    {
      "epoch": 26.09733124018838,
      "grad_norm": 3.325406789779663,
      "learning_rate": 3.368916797488226e-05,
      "loss": 0.5614,
      "step": 1662400
    },
    {
      "epoch": 26.0989010989011,
      "grad_norm": 3.4105165004730225,
      "learning_rate": 3.3688186813186814e-05,
      "loss": 0.5825,
      "step": 1662500
    },
    {
      "epoch": 26.100470957613815,
      "grad_norm": 3.092637538909912,
      "learning_rate": 3.3687205651491365e-05,
      "loss": 0.5822,
      "step": 1662600
    },
    {
      "epoch": 26.102040816326532,
      "grad_norm": 3.4380531311035156,
      "learning_rate": 3.368622448979592e-05,
      "loss": 0.5789,
      "step": 1662700
    },
    {
      "epoch": 26.103610675039246,
      "grad_norm": 4.292510032653809,
      "learning_rate": 3.3685243328100474e-05,
      "loss": 0.5906,
      "step": 1662800
    },
    {
      "epoch": 26.105180533751962,
      "grad_norm": 3.2007999420166016,
      "learning_rate": 3.3684262166405025e-05,
      "loss": 0.5727,
      "step": 1662900
    },
    {
      "epoch": 26.10675039246468,
      "grad_norm": 3.7306506633758545,
      "learning_rate": 3.3683281004709576e-05,
      "loss": 0.5873,
      "step": 1663000
    },
    {
      "epoch": 26.108320251177393,
      "grad_norm": 3.9983789920806885,
      "learning_rate": 3.368229984301413e-05,
      "loss": 0.5752,
      "step": 1663100
    },
    {
      "epoch": 26.10989010989011,
      "grad_norm": 3.8858790397644043,
      "learning_rate": 3.3681318681318684e-05,
      "loss": 0.5903,
      "step": 1663200
    },
    {
      "epoch": 26.111459968602826,
      "grad_norm": 4.0255842208862305,
      "learning_rate": 3.3680337519623235e-05,
      "loss": 0.615,
      "step": 1663300
    },
    {
      "epoch": 26.113029827315543,
      "grad_norm": 3.921361207962036,
      "learning_rate": 3.3679356357927786e-05,
      "loss": 0.6459,
      "step": 1663400
    },
    {
      "epoch": 26.114599686028257,
      "grad_norm": 2.499981164932251,
      "learning_rate": 3.3678375196232344e-05,
      "loss": 0.6397,
      "step": 1663500
    },
    {
      "epoch": 26.116169544740973,
      "grad_norm": 2.985698699951172,
      "learning_rate": 3.367739403453689e-05,
      "loss": 0.5671,
      "step": 1663600
    },
    {
      "epoch": 26.11773940345369,
      "grad_norm": 3.6293487548828125,
      "learning_rate": 3.3676412872841446e-05,
      "loss": 0.6419,
      "step": 1663700
    },
    {
      "epoch": 26.119309262166404,
      "grad_norm": 3.0003278255462646,
      "learning_rate": 3.3675431711146e-05,
      "loss": 0.5739,
      "step": 1663800
    },
    {
      "epoch": 26.12087912087912,
      "grad_norm": 4.45625638961792,
      "learning_rate": 3.3674450549450555e-05,
      "loss": 0.5969,
      "step": 1663900
    },
    {
      "epoch": 26.122448979591837,
      "grad_norm": 4.685013771057129,
      "learning_rate": 3.36734693877551e-05,
      "loss": 0.6072,
      "step": 1664000
    },
    {
      "epoch": 26.124018838304554,
      "grad_norm": 4.484570026397705,
      "learning_rate": 3.367248822605966e-05,
      "loss": 0.6165,
      "step": 1664100
    },
    {
      "epoch": 26.125588697017267,
      "grad_norm": 2.9901928901672363,
      "learning_rate": 3.367150706436421e-05,
      "loss": 0.5937,
      "step": 1664200
    },
    {
      "epoch": 26.127158555729984,
      "grad_norm": 3.6409285068511963,
      "learning_rate": 3.367052590266876e-05,
      "loss": 0.5703,
      "step": 1664300
    },
    {
      "epoch": 26.1287284144427,
      "grad_norm": 3.8604462146759033,
      "learning_rate": 3.3669544740973316e-05,
      "loss": 0.6079,
      "step": 1664400
    },
    {
      "epoch": 26.130298273155415,
      "grad_norm": 3.7302236557006836,
      "learning_rate": 3.366856357927787e-05,
      "loss": 0.5942,
      "step": 1664500
    },
    {
      "epoch": 26.13186813186813,
      "grad_norm": 4.020233631134033,
      "learning_rate": 3.366758241758242e-05,
      "loss": 0.5838,
      "step": 1664600
    },
    {
      "epoch": 26.13343799058085,
      "grad_norm": 2.5757551193237305,
      "learning_rate": 3.366660125588697e-05,
      "loss": 0.5788,
      "step": 1664700
    },
    {
      "epoch": 26.135007849293565,
      "grad_norm": 2.1798317432403564,
      "learning_rate": 3.366562009419153e-05,
      "loss": 0.6068,
      "step": 1664800
    },
    {
      "epoch": 26.13657770800628,
      "grad_norm": 4.238158226013184,
      "learning_rate": 3.366463893249608e-05,
      "loss": 0.5842,
      "step": 1664900
    },
    {
      "epoch": 26.138147566718995,
      "grad_norm": 2.9601893424987793,
      "learning_rate": 3.366365777080063e-05,
      "loss": 0.5995,
      "step": 1665000
    },
    {
      "epoch": 26.139717425431712,
      "grad_norm": 3.742321014404297,
      "learning_rate": 3.366267660910518e-05,
      "loss": 0.6211,
      "step": 1665100
    },
    {
      "epoch": 26.141287284144425,
      "grad_norm": 2.4888439178466797,
      "learning_rate": 3.366169544740974e-05,
      "loss": 0.5636,
      "step": 1665200
    },
    {
      "epoch": 26.142857142857142,
      "grad_norm": 4.701168537139893,
      "learning_rate": 3.366071428571429e-05,
      "loss": 0.5593,
      "step": 1665300
    },
    {
      "epoch": 26.14442700156986,
      "grad_norm": 3.242851972579956,
      "learning_rate": 3.365973312401884e-05,
      "loss": 0.6232,
      "step": 1665400
    },
    {
      "epoch": 26.145996860282576,
      "grad_norm": 4.057793617248535,
      "learning_rate": 3.365875196232339e-05,
      "loss": 0.5476,
      "step": 1665500
    },
    {
      "epoch": 26.14756671899529,
      "grad_norm": 4.543423652648926,
      "learning_rate": 3.365777080062795e-05,
      "loss": 0.5504,
      "step": 1665600
    },
    {
      "epoch": 26.149136577708006,
      "grad_norm": 2.6270060539245605,
      "learning_rate": 3.365678963893249e-05,
      "loss": 0.6295,
      "step": 1665700
    },
    {
      "epoch": 26.150706436420723,
      "grad_norm": 3.617642641067505,
      "learning_rate": 3.365580847723705e-05,
      "loss": 0.5803,
      "step": 1665800
    },
    {
      "epoch": 26.152276295133436,
      "grad_norm": 3.664931058883667,
      "learning_rate": 3.36548273155416e-05,
      "loss": 0.58,
      "step": 1665900
    },
    {
      "epoch": 26.153846153846153,
      "grad_norm": 3.8284265995025635,
      "learning_rate": 3.365384615384616e-05,
      "loss": 0.6102,
      "step": 1666000
    },
    {
      "epoch": 26.15541601255887,
      "grad_norm": 3.8900697231292725,
      "learning_rate": 3.3652864992150703e-05,
      "loss": 0.5823,
      "step": 1666100
    },
    {
      "epoch": 26.156985871271587,
      "grad_norm": 4.0521721839904785,
      "learning_rate": 3.365188383045526e-05,
      "loss": 0.5898,
      "step": 1666200
    },
    {
      "epoch": 26.1585557299843,
      "grad_norm": 2.950043201446533,
      "learning_rate": 3.365090266875981e-05,
      "loss": 0.609,
      "step": 1666300
    },
    {
      "epoch": 26.160125588697017,
      "grad_norm": 4.64678955078125,
      "learning_rate": 3.364992150706436e-05,
      "loss": 0.6103,
      "step": 1666400
    },
    {
      "epoch": 26.161695447409734,
      "grad_norm": 4.025918960571289,
      "learning_rate": 3.364894034536892e-05,
      "loss": 0.5994,
      "step": 1666500
    },
    {
      "epoch": 26.163265306122447,
      "grad_norm": 5.2074055671691895,
      "learning_rate": 3.364795918367347e-05,
      "loss": 0.5663,
      "step": 1666600
    },
    {
      "epoch": 26.164835164835164,
      "grad_norm": 4.225100517272949,
      "learning_rate": 3.364697802197802e-05,
      "loss": 0.6031,
      "step": 1666700
    },
    {
      "epoch": 26.16640502354788,
      "grad_norm": 3.734828472137451,
      "learning_rate": 3.3645996860282574e-05,
      "loss": 0.6042,
      "step": 1666800
    },
    {
      "epoch": 26.167974882260598,
      "grad_norm": 3.093461036682129,
      "learning_rate": 3.364501569858713e-05,
      "loss": 0.5837,
      "step": 1666900
    },
    {
      "epoch": 26.16954474097331,
      "grad_norm": 3.9993622303009033,
      "learning_rate": 3.364403453689168e-05,
      "loss": 0.6069,
      "step": 1667000
    },
    {
      "epoch": 26.171114599686028,
      "grad_norm": 4.185654163360596,
      "learning_rate": 3.3643053375196234e-05,
      "loss": 0.6128,
      "step": 1667100
    },
    {
      "epoch": 26.172684458398745,
      "grad_norm": 3.712880849838257,
      "learning_rate": 3.3642072213500785e-05,
      "loss": 0.6068,
      "step": 1667200
    },
    {
      "epoch": 26.17425431711146,
      "grad_norm": 3.1204843521118164,
      "learning_rate": 3.364109105180534e-05,
      "loss": 0.5731,
      "step": 1667300
    },
    {
      "epoch": 26.175824175824175,
      "grad_norm": 5.009990215301514,
      "learning_rate": 3.364010989010989e-05,
      "loss": 0.6465,
      "step": 1667400
    },
    {
      "epoch": 26.177394034536892,
      "grad_norm": 4.3660569190979,
      "learning_rate": 3.3639128728414444e-05,
      "loss": 0.6126,
      "step": 1667500
    },
    {
      "epoch": 26.17896389324961,
      "grad_norm": 4.7593278884887695,
      "learning_rate": 3.3638147566718995e-05,
      "loss": 0.5921,
      "step": 1667600
    },
    {
      "epoch": 26.180533751962322,
      "grad_norm": 4.975559711456299,
      "learning_rate": 3.363716640502355e-05,
      "loss": 0.5966,
      "step": 1667700
    },
    {
      "epoch": 26.18210361067504,
      "grad_norm": 3.4889638423919678,
      "learning_rate": 3.36361852433281e-05,
      "loss": 0.5741,
      "step": 1667800
    },
    {
      "epoch": 26.183673469387756,
      "grad_norm": 3.5158097743988037,
      "learning_rate": 3.3635204081632655e-05,
      "loss": 0.5886,
      "step": 1667900
    },
    {
      "epoch": 26.18524332810047,
      "grad_norm": 2.5424444675445557,
      "learning_rate": 3.3634222919937206e-05,
      "loss": 0.5528,
      "step": 1668000
    },
    {
      "epoch": 26.186813186813186,
      "grad_norm": 4.2303290367126465,
      "learning_rate": 3.3633241758241764e-05,
      "loss": 0.6259,
      "step": 1668100
    },
    {
      "epoch": 26.188383045525903,
      "grad_norm": 4.453586101531982,
      "learning_rate": 3.363226059654631e-05,
      "loss": 0.5843,
      "step": 1668200
    },
    {
      "epoch": 26.18995290423862,
      "grad_norm": 3.9109671115875244,
      "learning_rate": 3.3631279434850866e-05,
      "loss": 0.6008,
      "step": 1668300
    },
    {
      "epoch": 26.191522762951333,
      "grad_norm": 3.9705424308776855,
      "learning_rate": 3.363029827315542e-05,
      "loss": 0.5813,
      "step": 1668400
    },
    {
      "epoch": 26.19309262166405,
      "grad_norm": 4.167745113372803,
      "learning_rate": 3.362931711145997e-05,
      "loss": 0.5827,
      "step": 1668500
    },
    {
      "epoch": 26.194662480376767,
      "grad_norm": 4.689573287963867,
      "learning_rate": 3.3628335949764525e-05,
      "loss": 0.5848,
      "step": 1668600
    },
    {
      "epoch": 26.19623233908948,
      "grad_norm": 3.94994854927063,
      "learning_rate": 3.3627354788069076e-05,
      "loss": 0.5953,
      "step": 1668700
    },
    {
      "epoch": 26.197802197802197,
      "grad_norm": 5.6805925369262695,
      "learning_rate": 3.362637362637363e-05,
      "loss": 0.5911,
      "step": 1668800
    },
    {
      "epoch": 26.199372056514914,
      "grad_norm": 4.439835071563721,
      "learning_rate": 3.362539246467818e-05,
      "loss": 0.5658,
      "step": 1668900
    },
    {
      "epoch": 26.20094191522763,
      "grad_norm": 3.7449564933776855,
      "learning_rate": 3.3624411302982736e-05,
      "loss": 0.582,
      "step": 1669000
    },
    {
      "epoch": 26.202511773940344,
      "grad_norm": 3.7919423580169678,
      "learning_rate": 3.362343014128729e-05,
      "loss": 0.5915,
      "step": 1669100
    },
    {
      "epoch": 26.20408163265306,
      "grad_norm": 3.3836495876312256,
      "learning_rate": 3.362244897959184e-05,
      "loss": 0.5936,
      "step": 1669200
    },
    {
      "epoch": 26.205651491365778,
      "grad_norm": 3.3984696865081787,
      "learning_rate": 3.362146781789639e-05,
      "loss": 0.6117,
      "step": 1669300
    },
    {
      "epoch": 26.20722135007849,
      "grad_norm": 3.379676342010498,
      "learning_rate": 3.362048665620095e-05,
      "loss": 0.5795,
      "step": 1669400
    },
    {
      "epoch": 26.208791208791208,
      "grad_norm": 3.500235080718994,
      "learning_rate": 3.36195054945055e-05,
      "loss": 0.5997,
      "step": 1669500
    },
    {
      "epoch": 26.210361067503925,
      "grad_norm": 2.9319493770599365,
      "learning_rate": 3.361852433281005e-05,
      "loss": 0.6041,
      "step": 1669600
    },
    {
      "epoch": 26.211930926216642,
      "grad_norm": 3.6743874549865723,
      "learning_rate": 3.36175431711146e-05,
      "loss": 0.595,
      "step": 1669700
    },
    {
      "epoch": 26.213500784929355,
      "grad_norm": 4.831876277923584,
      "learning_rate": 3.361656200941916e-05,
      "loss": 0.5906,
      "step": 1669800
    },
    {
      "epoch": 26.215070643642072,
      "grad_norm": 3.060675621032715,
      "learning_rate": 3.36155808477237e-05,
      "loss": 0.5551,
      "step": 1669900
    },
    {
      "epoch": 26.21664050235479,
      "grad_norm": 4.262136936187744,
      "learning_rate": 3.361459968602826e-05,
      "loss": 0.6008,
      "step": 1670000
    },
    {
      "epoch": 26.218210361067506,
      "grad_norm": 3.5033373832702637,
      "learning_rate": 3.361361852433281e-05,
      "loss": 0.5976,
      "step": 1670100
    },
    {
      "epoch": 26.21978021978022,
      "grad_norm": 4.055347442626953,
      "learning_rate": 3.361263736263737e-05,
      "loss": 0.5774,
      "step": 1670200
    },
    {
      "epoch": 26.221350078492936,
      "grad_norm": 4.149918079376221,
      "learning_rate": 3.361165620094191e-05,
      "loss": 0.5851,
      "step": 1670300
    },
    {
      "epoch": 26.222919937205653,
      "grad_norm": 3.367988109588623,
      "learning_rate": 3.361067503924647e-05,
      "loss": 0.6002,
      "step": 1670400
    },
    {
      "epoch": 26.224489795918366,
      "grad_norm": 3.796065330505371,
      "learning_rate": 3.360969387755102e-05,
      "loss": 0.6015,
      "step": 1670500
    },
    {
      "epoch": 26.226059654631083,
      "grad_norm": 4.664609432220459,
      "learning_rate": 3.360871271585557e-05,
      "loss": 0.6477,
      "step": 1670600
    },
    {
      "epoch": 26.2276295133438,
      "grad_norm": 3.557720184326172,
      "learning_rate": 3.360773155416013e-05,
      "loss": 0.6411,
      "step": 1670700
    },
    {
      "epoch": 26.229199372056517,
      "grad_norm": 5.577698707580566,
      "learning_rate": 3.360675039246468e-05,
      "loss": 0.5829,
      "step": 1670800
    },
    {
      "epoch": 26.23076923076923,
      "grad_norm": 4.065887451171875,
      "learning_rate": 3.360576923076923e-05,
      "loss": 0.5919,
      "step": 1670900
    },
    {
      "epoch": 26.232339089481947,
      "grad_norm": 4.442407608032227,
      "learning_rate": 3.360478806907378e-05,
      "loss": 0.5826,
      "step": 1671000
    },
    {
      "epoch": 26.233908948194664,
      "grad_norm": 3.0457398891448975,
      "learning_rate": 3.360380690737834e-05,
      "loss": 0.5843,
      "step": 1671100
    },
    {
      "epoch": 26.235478806907377,
      "grad_norm": 4.806275367736816,
      "learning_rate": 3.360282574568289e-05,
      "loss": 0.6014,
      "step": 1671200
    },
    {
      "epoch": 26.237048665620094,
      "grad_norm": 3.970989942550659,
      "learning_rate": 3.360184458398744e-05,
      "loss": 0.6268,
      "step": 1671300
    },
    {
      "epoch": 26.23861852433281,
      "grad_norm": 3.515355110168457,
      "learning_rate": 3.3600863422291994e-05,
      "loss": 0.5847,
      "step": 1671400
    },
    {
      "epoch": 26.240188383045528,
      "grad_norm": 3.482172727584839,
      "learning_rate": 3.359988226059655e-05,
      "loss": 0.581,
      "step": 1671500
    },
    {
      "epoch": 26.24175824175824,
      "grad_norm": 4.336633682250977,
      "learning_rate": 3.35989010989011e-05,
      "loss": 0.6058,
      "step": 1671600
    },
    {
      "epoch": 26.243328100470958,
      "grad_norm": 4.201714992523193,
      "learning_rate": 3.359791993720565e-05,
      "loss": 0.5968,
      "step": 1671700
    },
    {
      "epoch": 26.244897959183675,
      "grad_norm": 4.484724044799805,
      "learning_rate": 3.3596938775510204e-05,
      "loss": 0.6025,
      "step": 1671800
    },
    {
      "epoch": 26.246467817896388,
      "grad_norm": 2.7013320922851562,
      "learning_rate": 3.359595761381476e-05,
      "loss": 0.5969,
      "step": 1671900
    },
    {
      "epoch": 26.248037676609105,
      "grad_norm": 3.2063186168670654,
      "learning_rate": 3.3594976452119306e-05,
      "loss": 0.5823,
      "step": 1672000
    },
    {
      "epoch": 26.24960753532182,
      "grad_norm": 5.122928619384766,
      "learning_rate": 3.3593995290423864e-05,
      "loss": 0.6083,
      "step": 1672100
    },
    {
      "epoch": 26.25117739403454,
      "grad_norm": 2.755429983139038,
      "learning_rate": 3.3593014128728415e-05,
      "loss": 0.635,
      "step": 1672200
    },
    {
      "epoch": 26.252747252747252,
      "grad_norm": 4.422508716583252,
      "learning_rate": 3.3592032967032966e-05,
      "loss": 0.6011,
      "step": 1672300
    },
    {
      "epoch": 26.25431711145997,
      "grad_norm": 4.32505989074707,
      "learning_rate": 3.359105180533752e-05,
      "loss": 0.571,
      "step": 1672400
    },
    {
      "epoch": 26.255886970172686,
      "grad_norm": 4.535886287689209,
      "learning_rate": 3.3590070643642075e-05,
      "loss": 0.6328,
      "step": 1672500
    },
    {
      "epoch": 26.2574568288854,
      "grad_norm": 3.637712240219116,
      "learning_rate": 3.3589089481946626e-05,
      "loss": 0.6189,
      "step": 1672600
    },
    {
      "epoch": 26.259026687598116,
      "grad_norm": 2.7186310291290283,
      "learning_rate": 3.358810832025118e-05,
      "loss": 0.5701,
      "step": 1672700
    },
    {
      "epoch": 26.260596546310833,
      "grad_norm": 4.0892720222473145,
      "learning_rate": 3.3587127158555734e-05,
      "loss": 0.5536,
      "step": 1672800
    },
    {
      "epoch": 26.26216640502355,
      "grad_norm": 3.787593364715576,
      "learning_rate": 3.3586145996860285e-05,
      "loss": 0.5779,
      "step": 1672900
    },
    {
      "epoch": 26.263736263736263,
      "grad_norm": 4.998143672943115,
      "learning_rate": 3.3585164835164836e-05,
      "loss": 0.5931,
      "step": 1673000
    },
    {
      "epoch": 26.26530612244898,
      "grad_norm": 3.7893340587615967,
      "learning_rate": 3.358418367346939e-05,
      "loss": 0.6347,
      "step": 1673100
    },
    {
      "epoch": 26.266875981161697,
      "grad_norm": 3.895001173019409,
      "learning_rate": 3.3583202511773945e-05,
      "loss": 0.6046,
      "step": 1673200
    },
    {
      "epoch": 26.26844583987441,
      "grad_norm": 3.192918300628662,
      "learning_rate": 3.3582221350078496e-05,
      "loss": 0.6048,
      "step": 1673300
    },
    {
      "epoch": 26.270015698587127,
      "grad_norm": 4.149493217468262,
      "learning_rate": 3.358124018838305e-05,
      "loss": 0.5924,
      "step": 1673400
    },
    {
      "epoch": 26.271585557299844,
      "grad_norm": 3.9865059852600098,
      "learning_rate": 3.35802590266876e-05,
      "loss": 0.5893,
      "step": 1673500
    },
    {
      "epoch": 26.27315541601256,
      "grad_norm": 3.917264699935913,
      "learning_rate": 3.3579277864992156e-05,
      "loss": 0.637,
      "step": 1673600
    },
    {
      "epoch": 26.274725274725274,
      "grad_norm": 4.78341817855835,
      "learning_rate": 3.35782967032967e-05,
      "loss": 0.5618,
      "step": 1673700
    },
    {
      "epoch": 26.27629513343799,
      "grad_norm": 3.7943739891052246,
      "learning_rate": 3.357731554160126e-05,
      "loss": 0.5525,
      "step": 1673800
    },
    {
      "epoch": 26.277864992150707,
      "grad_norm": 4.121670722961426,
      "learning_rate": 3.357633437990581e-05,
      "loss": 0.5867,
      "step": 1673900
    },
    {
      "epoch": 26.27943485086342,
      "grad_norm": 3.6566169261932373,
      "learning_rate": 3.3575353218210366e-05,
      "loss": 0.6043,
      "step": 1674000
    },
    {
      "epoch": 26.281004709576138,
      "grad_norm": 4.019079685211182,
      "learning_rate": 3.357437205651491e-05,
      "loss": 0.612,
      "step": 1674100
    },
    {
      "epoch": 26.282574568288855,
      "grad_norm": 4.639463901519775,
      "learning_rate": 3.357339089481947e-05,
      "loss": 0.607,
      "step": 1674200
    },
    {
      "epoch": 26.28414442700157,
      "grad_norm": 3.802032709121704,
      "learning_rate": 3.357240973312402e-05,
      "loss": 0.5881,
      "step": 1674300
    },
    {
      "epoch": 26.285714285714285,
      "grad_norm": 3.7252633571624756,
      "learning_rate": 3.357142857142857e-05,
      "loss": 0.5947,
      "step": 1674400
    },
    {
      "epoch": 26.287284144427,
      "grad_norm": 2.4545273780822754,
      "learning_rate": 3.357044740973312e-05,
      "loss": 0.5881,
      "step": 1674500
    },
    {
      "epoch": 26.28885400313972,
      "grad_norm": 3.8040366172790527,
      "learning_rate": 3.356946624803768e-05,
      "loss": 0.6378,
      "step": 1674600
    },
    {
      "epoch": 26.29042386185243,
      "grad_norm": 3.8177194595336914,
      "learning_rate": 3.356848508634223e-05,
      "loss": 0.5687,
      "step": 1674700
    },
    {
      "epoch": 26.29199372056515,
      "grad_norm": 3.815406560897827,
      "learning_rate": 3.356750392464678e-05,
      "loss": 0.6206,
      "step": 1674800
    },
    {
      "epoch": 26.293563579277865,
      "grad_norm": 4.710337162017822,
      "learning_rate": 3.356652276295134e-05,
      "loss": 0.5765,
      "step": 1674900
    },
    {
      "epoch": 26.295133437990582,
      "grad_norm": 3.0904388427734375,
      "learning_rate": 3.356554160125589e-05,
      "loss": 0.5799,
      "step": 1675000
    },
    {
      "epoch": 26.296703296703296,
      "grad_norm": 3.8213651180267334,
      "learning_rate": 3.356456043956044e-05,
      "loss": 0.5887,
      "step": 1675100
    },
    {
      "epoch": 26.298273155416013,
      "grad_norm": 3.9349842071533203,
      "learning_rate": 3.356357927786499e-05,
      "loss": 0.5836,
      "step": 1675200
    },
    {
      "epoch": 26.29984301412873,
      "grad_norm": 3.9830470085144043,
      "learning_rate": 3.356259811616955e-05,
      "loss": 0.603,
      "step": 1675300
    },
    {
      "epoch": 26.301412872841443,
      "grad_norm": 4.4763922691345215,
      "learning_rate": 3.35616169544741e-05,
      "loss": 0.6126,
      "step": 1675400
    },
    {
      "epoch": 26.30298273155416,
      "grad_norm": 4.34876823425293,
      "learning_rate": 3.356063579277865e-05,
      "loss": 0.6142,
      "step": 1675500
    },
    {
      "epoch": 26.304552590266876,
      "grad_norm": 4.756651401519775,
      "learning_rate": 3.35596546310832e-05,
      "loss": 0.5313,
      "step": 1675600
    },
    {
      "epoch": 26.306122448979593,
      "grad_norm": 3.234219551086426,
      "learning_rate": 3.355867346938776e-05,
      "loss": 0.5983,
      "step": 1675700
    },
    {
      "epoch": 26.307692307692307,
      "grad_norm": 3.8746421337127686,
      "learning_rate": 3.3557692307692304e-05,
      "loss": 0.5819,
      "step": 1675800
    },
    {
      "epoch": 26.309262166405023,
      "grad_norm": 3.2611067295074463,
      "learning_rate": 3.355671114599686e-05,
      "loss": 0.59,
      "step": 1675900
    },
    {
      "epoch": 26.31083202511774,
      "grad_norm": 4.5664801597595215,
      "learning_rate": 3.355572998430141e-05,
      "loss": 0.6426,
      "step": 1676000
    },
    {
      "epoch": 26.312401883830454,
      "grad_norm": 3.6894593238830566,
      "learning_rate": 3.355474882260597e-05,
      "loss": 0.5946,
      "step": 1676100
    },
    {
      "epoch": 26.31397174254317,
      "grad_norm": 2.6418068408966064,
      "learning_rate": 3.3553767660910515e-05,
      "loss": 0.5573,
      "step": 1676200
    },
    {
      "epoch": 26.315541601255887,
      "grad_norm": 3.343916177749634,
      "learning_rate": 3.355278649921507e-05,
      "loss": 0.5808,
      "step": 1676300
    },
    {
      "epoch": 26.317111459968604,
      "grad_norm": 3.8631603717803955,
      "learning_rate": 3.3551805337519624e-05,
      "loss": 0.6288,
      "step": 1676400
    },
    {
      "epoch": 26.318681318681318,
      "grad_norm": 3.7759652137756348,
      "learning_rate": 3.3550824175824175e-05,
      "loss": 0.6129,
      "step": 1676500
    },
    {
      "epoch": 26.320251177394034,
      "grad_norm": 3.7944071292877197,
      "learning_rate": 3.3549843014128726e-05,
      "loss": 0.6186,
      "step": 1676600
    },
    {
      "epoch": 26.32182103610675,
      "grad_norm": 3.633106231689453,
      "learning_rate": 3.3548861852433284e-05,
      "loss": 0.5752,
      "step": 1676700
    },
    {
      "epoch": 26.323390894819465,
      "grad_norm": 3.398124933242798,
      "learning_rate": 3.3547880690737835e-05,
      "loss": 0.6358,
      "step": 1676800
    },
    {
      "epoch": 26.32496075353218,
      "grad_norm": 3.710714340209961,
      "learning_rate": 3.3546899529042386e-05,
      "loss": 0.6135,
      "step": 1676900
    },
    {
      "epoch": 26.3265306122449,
      "grad_norm": 2.2502810955047607,
      "learning_rate": 3.354591836734694e-05,
      "loss": 0.6245,
      "step": 1677000
    },
    {
      "epoch": 26.328100470957615,
      "grad_norm": 4.209794998168945,
      "learning_rate": 3.3544937205651494e-05,
      "loss": 0.5727,
      "step": 1677100
    },
    {
      "epoch": 26.32967032967033,
      "grad_norm": 2.03266978263855,
      "learning_rate": 3.3543956043956045e-05,
      "loss": 0.5853,
      "step": 1677200
    },
    {
      "epoch": 26.331240188383045,
      "grad_norm": 3.052258014678955,
      "learning_rate": 3.3542974882260596e-05,
      "loss": 0.5614,
      "step": 1677300
    },
    {
      "epoch": 26.332810047095762,
      "grad_norm": 3.1771047115325928,
      "learning_rate": 3.3541993720565154e-05,
      "loss": 0.6164,
      "step": 1677400
    },
    {
      "epoch": 26.334379905808476,
      "grad_norm": 3.8896987438201904,
      "learning_rate": 3.3541012558869705e-05,
      "loss": 0.5764,
      "step": 1677500
    },
    {
      "epoch": 26.335949764521192,
      "grad_norm": 3.7008814811706543,
      "learning_rate": 3.3540031397174256e-05,
      "loss": 0.5943,
      "step": 1677600
    },
    {
      "epoch": 26.33751962323391,
      "grad_norm": 3.6262855529785156,
      "learning_rate": 3.353905023547881e-05,
      "loss": 0.5717,
      "step": 1677700
    },
    {
      "epoch": 26.339089481946626,
      "grad_norm": 2.49556565284729,
      "learning_rate": 3.3538069073783365e-05,
      "loss": 0.596,
      "step": 1677800
    },
    {
      "epoch": 26.34065934065934,
      "grad_norm": 3.074382781982422,
      "learning_rate": 3.353708791208791e-05,
      "loss": 0.6032,
      "step": 1677900
    },
    {
      "epoch": 26.342229199372056,
      "grad_norm": 2.785968542098999,
      "learning_rate": 3.353610675039247e-05,
      "loss": 0.605,
      "step": 1678000
    },
    {
      "epoch": 26.343799058084773,
      "grad_norm": 3.2041447162628174,
      "learning_rate": 3.353512558869702e-05,
      "loss": 0.6009,
      "step": 1678100
    },
    {
      "epoch": 26.345368916797486,
      "grad_norm": 4.13893461227417,
      "learning_rate": 3.3534144427001575e-05,
      "loss": 0.5811,
      "step": 1678200
    },
    {
      "epoch": 26.346938775510203,
      "grad_norm": 5.149309158325195,
      "learning_rate": 3.353316326530612e-05,
      "loss": 0.5971,
      "step": 1678300
    },
    {
      "epoch": 26.34850863422292,
      "grad_norm": 2.5148091316223145,
      "learning_rate": 3.353218210361068e-05,
      "loss": 0.5929,
      "step": 1678400
    },
    {
      "epoch": 26.350078492935637,
      "grad_norm": 4.21067476272583,
      "learning_rate": 3.353120094191523e-05,
      "loss": 0.5692,
      "step": 1678500
    },
    {
      "epoch": 26.35164835164835,
      "grad_norm": 1.9022315740585327,
      "learning_rate": 3.353021978021978e-05,
      "loss": 0.5856,
      "step": 1678600
    },
    {
      "epoch": 26.353218210361067,
      "grad_norm": 4.158370494842529,
      "learning_rate": 3.352923861852433e-05,
      "loss": 0.5575,
      "step": 1678700
    },
    {
      "epoch": 26.354788069073784,
      "grad_norm": 3.206207036972046,
      "learning_rate": 3.352825745682889e-05,
      "loss": 0.5636,
      "step": 1678800
    },
    {
      "epoch": 26.356357927786497,
      "grad_norm": 3.768099784851074,
      "learning_rate": 3.352727629513344e-05,
      "loss": 0.6006,
      "step": 1678900
    },
    {
      "epoch": 26.357927786499214,
      "grad_norm": 3.5187935829162598,
      "learning_rate": 3.352629513343799e-05,
      "loss": 0.6148,
      "step": 1679000
    },
    {
      "epoch": 26.35949764521193,
      "grad_norm": 3.715285301208496,
      "learning_rate": 3.352531397174255e-05,
      "loss": 0.6164,
      "step": 1679100
    },
    {
      "epoch": 26.361067503924648,
      "grad_norm": 2.395879030227661,
      "learning_rate": 3.35243328100471e-05,
      "loss": 0.5754,
      "step": 1679200
    },
    {
      "epoch": 26.36263736263736,
      "grad_norm": 3.6064116954803467,
      "learning_rate": 3.352335164835165e-05,
      "loss": 0.5951,
      "step": 1679300
    },
    {
      "epoch": 26.364207221350078,
      "grad_norm": 4.251124858856201,
      "learning_rate": 3.35223704866562e-05,
      "loss": 0.5853,
      "step": 1679400
    },
    {
      "epoch": 26.365777080062795,
      "grad_norm": 3.683429718017578,
      "learning_rate": 3.352138932496076e-05,
      "loss": 0.5628,
      "step": 1679500
    },
    {
      "epoch": 26.367346938775512,
      "grad_norm": 4.657883167266846,
      "learning_rate": 3.352040816326531e-05,
      "loss": 0.5522,
      "step": 1679600
    },
    {
      "epoch": 26.368916797488225,
      "grad_norm": 3.7423601150512695,
      "learning_rate": 3.351942700156986e-05,
      "loss": 0.5836,
      "step": 1679700
    },
    {
      "epoch": 26.370486656200942,
      "grad_norm": 4.516942977905273,
      "learning_rate": 3.351844583987441e-05,
      "loss": 0.6343,
      "step": 1679800
    },
    {
      "epoch": 26.37205651491366,
      "grad_norm": 2.348008394241333,
      "learning_rate": 3.351746467817897e-05,
      "loss": 0.6098,
      "step": 1679900
    },
    {
      "epoch": 26.373626373626372,
      "grad_norm": 3.4533984661102295,
      "learning_rate": 3.3516483516483513e-05,
      "loss": 0.5632,
      "step": 1680000
    },
    {
      "epoch": 26.37519623233909,
      "grad_norm": 4.191750526428223,
      "learning_rate": 3.351550235478807e-05,
      "loss": 0.5705,
      "step": 1680100
    },
    {
      "epoch": 26.376766091051806,
      "grad_norm": 4.680202960968018,
      "learning_rate": 3.351452119309262e-05,
      "loss": 0.621,
      "step": 1680200
    },
    {
      "epoch": 26.378335949764523,
      "grad_norm": 4.446998119354248,
      "learning_rate": 3.351354003139718e-05,
      "loss": 0.6417,
      "step": 1680300
    },
    {
      "epoch": 26.379905808477236,
      "grad_norm": 4.445743083953857,
      "learning_rate": 3.3512558869701724e-05,
      "loss": 0.6114,
      "step": 1680400
    },
    {
      "epoch": 26.381475667189953,
      "grad_norm": 3.261889696121216,
      "learning_rate": 3.351157770800628e-05,
      "loss": 0.6159,
      "step": 1680500
    },
    {
      "epoch": 26.38304552590267,
      "grad_norm": 2.3282864093780518,
      "learning_rate": 3.351059654631083e-05,
      "loss": 0.6213,
      "step": 1680600
    },
    {
      "epoch": 26.384615384615383,
      "grad_norm": 4.204622745513916,
      "learning_rate": 3.3509615384615384e-05,
      "loss": 0.6242,
      "step": 1680700
    },
    {
      "epoch": 26.3861852433281,
      "grad_norm": 4.251330852508545,
      "learning_rate": 3.3508634222919935e-05,
      "loss": 0.6151,
      "step": 1680800
    },
    {
      "epoch": 26.387755102040817,
      "grad_norm": 3.3388218879699707,
      "learning_rate": 3.350765306122449e-05,
      "loss": 0.6154,
      "step": 1680900
    },
    {
      "epoch": 26.389324960753534,
      "grad_norm": 4.416794300079346,
      "learning_rate": 3.3506671899529044e-05,
      "loss": 0.5847,
      "step": 1681000
    },
    {
      "epoch": 26.390894819466247,
      "grad_norm": 4.105751991271973,
      "learning_rate": 3.3505690737833595e-05,
      "loss": 0.5779,
      "step": 1681100
    },
    {
      "epoch": 26.392464678178964,
      "grad_norm": 3.8704895973205566,
      "learning_rate": 3.350470957613815e-05,
      "loss": 0.5915,
      "step": 1681200
    },
    {
      "epoch": 26.39403453689168,
      "grad_norm": 3.8987810611724854,
      "learning_rate": 3.35037284144427e-05,
      "loss": 0.5703,
      "step": 1681300
    },
    {
      "epoch": 26.395604395604394,
      "grad_norm": 4.033543586730957,
      "learning_rate": 3.3502747252747254e-05,
      "loss": 0.6155,
      "step": 1681400
    },
    {
      "epoch": 26.39717425431711,
      "grad_norm": 4.6153035163879395,
      "learning_rate": 3.3501766091051805e-05,
      "loss": 0.6059,
      "step": 1681500
    },
    {
      "epoch": 26.398744113029828,
      "grad_norm": 3.2936365604400635,
      "learning_rate": 3.350078492935636e-05,
      "loss": 0.5815,
      "step": 1681600
    },
    {
      "epoch": 26.400313971742545,
      "grad_norm": 2.9885380268096924,
      "learning_rate": 3.3499803767660914e-05,
      "loss": 0.6056,
      "step": 1681700
    },
    {
      "epoch": 26.401883830455258,
      "grad_norm": 2.7449729442596436,
      "learning_rate": 3.3498822605965465e-05,
      "loss": 0.6012,
      "step": 1681800
    },
    {
      "epoch": 26.403453689167975,
      "grad_norm": 4.273767948150635,
      "learning_rate": 3.3497841444270016e-05,
      "loss": 0.6214,
      "step": 1681900
    },
    {
      "epoch": 26.405023547880692,
      "grad_norm": 3.813025712966919,
      "learning_rate": 3.3496860282574574e-05,
      "loss": 0.5859,
      "step": 1682000
    },
    {
      "epoch": 26.406593406593405,
      "grad_norm": 3.8618223667144775,
      "learning_rate": 3.349587912087912e-05,
      "loss": 0.6107,
      "step": 1682100
    },
    {
      "epoch": 26.408163265306122,
      "grad_norm": 3.609590768814087,
      "learning_rate": 3.3494897959183676e-05,
      "loss": 0.6061,
      "step": 1682200
    },
    {
      "epoch": 26.40973312401884,
      "grad_norm": 2.8409924507141113,
      "learning_rate": 3.349391679748823e-05,
      "loss": 0.6037,
      "step": 1682300
    },
    {
      "epoch": 26.411302982731556,
      "grad_norm": 4.147940158843994,
      "learning_rate": 3.3492935635792784e-05,
      "loss": 0.5528,
      "step": 1682400
    },
    {
      "epoch": 26.41287284144427,
      "grad_norm": 2.903398275375366,
      "learning_rate": 3.349195447409733e-05,
      "loss": 0.5744,
      "step": 1682500
    },
    {
      "epoch": 26.414442700156986,
      "grad_norm": 3.4480810165405273,
      "learning_rate": 3.3490973312401886e-05,
      "loss": 0.5809,
      "step": 1682600
    },
    {
      "epoch": 26.416012558869703,
      "grad_norm": 3.6788151264190674,
      "learning_rate": 3.348999215070644e-05,
      "loss": 0.5853,
      "step": 1682700
    },
    {
      "epoch": 26.417582417582416,
      "grad_norm": 3.1334238052368164,
      "learning_rate": 3.348901098901099e-05,
      "loss": 0.5677,
      "step": 1682800
    },
    {
      "epoch": 26.419152276295133,
      "grad_norm": 3.7121946811676025,
      "learning_rate": 3.348802982731554e-05,
      "loss": 0.5981,
      "step": 1682900
    },
    {
      "epoch": 26.42072213500785,
      "grad_norm": 4.033257484436035,
      "learning_rate": 3.34870486656201e-05,
      "loss": 0.5924,
      "step": 1683000
    },
    {
      "epoch": 26.422291993720567,
      "grad_norm": 4.941885471343994,
      "learning_rate": 3.348606750392465e-05,
      "loss": 0.5913,
      "step": 1683100
    },
    {
      "epoch": 26.42386185243328,
      "grad_norm": 3.376526355743408,
      "learning_rate": 3.34850863422292e-05,
      "loss": 0.5337,
      "step": 1683200
    },
    {
      "epoch": 26.425431711145997,
      "grad_norm": 4.271777153015137,
      "learning_rate": 3.348410518053376e-05,
      "loss": 0.5857,
      "step": 1683300
    },
    {
      "epoch": 26.427001569858714,
      "grad_norm": 4.575707912445068,
      "learning_rate": 3.348312401883831e-05,
      "loss": 0.6217,
      "step": 1683400
    },
    {
      "epoch": 26.428571428571427,
      "grad_norm": 4.052588939666748,
      "learning_rate": 3.348214285714286e-05,
      "loss": 0.5626,
      "step": 1683500
    },
    {
      "epoch": 26.430141287284144,
      "grad_norm": 4.385340690612793,
      "learning_rate": 3.348116169544741e-05,
      "loss": 0.587,
      "step": 1683600
    },
    {
      "epoch": 26.43171114599686,
      "grad_norm": 4.412353515625,
      "learning_rate": 3.348018053375197e-05,
      "loss": 0.6017,
      "step": 1683700
    },
    {
      "epoch": 26.433281004709578,
      "grad_norm": 3.4040658473968506,
      "learning_rate": 3.347919937205652e-05,
      "loss": 0.5718,
      "step": 1683800
    },
    {
      "epoch": 26.43485086342229,
      "grad_norm": 4.038066864013672,
      "learning_rate": 3.347821821036107e-05,
      "loss": 0.5953,
      "step": 1683900
    },
    {
      "epoch": 26.436420722135008,
      "grad_norm": 2.8761086463928223,
      "learning_rate": 3.347723704866562e-05,
      "loss": 0.631,
      "step": 1684000
    },
    {
      "epoch": 26.437990580847725,
      "grad_norm": 4.223499298095703,
      "learning_rate": 3.347625588697018e-05,
      "loss": 0.6191,
      "step": 1684100
    },
    {
      "epoch": 26.439560439560438,
      "grad_norm": 3.5951735973358154,
      "learning_rate": 3.347527472527472e-05,
      "loss": 0.5948,
      "step": 1684200
    },
    {
      "epoch": 26.441130298273155,
      "grad_norm": 4.597245216369629,
      "learning_rate": 3.347429356357928e-05,
      "loss": 0.6123,
      "step": 1684300
    },
    {
      "epoch": 26.44270015698587,
      "grad_norm": 3.667369842529297,
      "learning_rate": 3.347331240188383e-05,
      "loss": 0.6154,
      "step": 1684400
    },
    {
      "epoch": 26.44427001569859,
      "grad_norm": 3.4598944187164307,
      "learning_rate": 3.347233124018839e-05,
      "loss": 0.6007,
      "step": 1684500
    },
    {
      "epoch": 26.445839874411302,
      "grad_norm": 3.8770153522491455,
      "learning_rate": 3.347135007849293e-05,
      "loss": 0.6481,
      "step": 1684600
    },
    {
      "epoch": 26.44740973312402,
      "grad_norm": 3.9213802814483643,
      "learning_rate": 3.347036891679749e-05,
      "loss": 0.5987,
      "step": 1684700
    },
    {
      "epoch": 26.448979591836736,
      "grad_norm": 3.3743433952331543,
      "learning_rate": 3.346938775510204e-05,
      "loss": 0.5883,
      "step": 1684800
    },
    {
      "epoch": 26.45054945054945,
      "grad_norm": 2.8707737922668457,
      "learning_rate": 3.346840659340659e-05,
      "loss": 0.5785,
      "step": 1684900
    },
    {
      "epoch": 26.452119309262166,
      "grad_norm": 4.1002373695373535,
      "learning_rate": 3.3467425431711144e-05,
      "loss": 0.5838,
      "step": 1685000
    },
    {
      "epoch": 26.453689167974883,
      "grad_norm": 4.62177848815918,
      "learning_rate": 3.34664442700157e-05,
      "loss": 0.6162,
      "step": 1685100
    },
    {
      "epoch": 26.4552590266876,
      "grad_norm": 3.4606361389160156,
      "learning_rate": 3.346546310832025e-05,
      "loss": 0.5885,
      "step": 1685200
    },
    {
      "epoch": 26.456828885400313,
      "grad_norm": 3.6306073665618896,
      "learning_rate": 3.3464481946624804e-05,
      "loss": 0.6069,
      "step": 1685300
    },
    {
      "epoch": 26.45839874411303,
      "grad_norm": 2.6539578437805176,
      "learning_rate": 3.346350078492936e-05,
      "loss": 0.6027,
      "step": 1685400
    },
    {
      "epoch": 26.459968602825747,
      "grad_norm": 3.9160497188568115,
      "learning_rate": 3.346251962323391e-05,
      "loss": 0.5568,
      "step": 1685500
    },
    {
      "epoch": 26.46153846153846,
      "grad_norm": 4.074328899383545,
      "learning_rate": 3.346153846153846e-05,
      "loss": 0.5952,
      "step": 1685600
    },
    {
      "epoch": 26.463108320251177,
      "grad_norm": 3.1317298412323,
      "learning_rate": 3.3460557299843014e-05,
      "loss": 0.5633,
      "step": 1685700
    },
    {
      "epoch": 26.464678178963894,
      "grad_norm": 4.332109451293945,
      "learning_rate": 3.345957613814757e-05,
      "loss": 0.618,
      "step": 1685800
    },
    {
      "epoch": 26.46624803767661,
      "grad_norm": 3.3515846729278564,
      "learning_rate": 3.345859497645212e-05,
      "loss": 0.6059,
      "step": 1685900
    },
    {
      "epoch": 26.467817896389324,
      "grad_norm": 3.5699965953826904,
      "learning_rate": 3.3457613814756674e-05,
      "loss": 0.5703,
      "step": 1686000
    },
    {
      "epoch": 26.46938775510204,
      "grad_norm": 4.3355512619018555,
      "learning_rate": 3.3456632653061225e-05,
      "loss": 0.5713,
      "step": 1686100
    },
    {
      "epoch": 26.470957613814758,
      "grad_norm": 4.05672025680542,
      "learning_rate": 3.345565149136578e-05,
      "loss": 0.6252,
      "step": 1686200
    },
    {
      "epoch": 26.47252747252747,
      "grad_norm": 3.1894774436950684,
      "learning_rate": 3.345467032967033e-05,
      "loss": 0.5853,
      "step": 1686300
    },
    {
      "epoch": 26.474097331240188,
      "grad_norm": 4.0997233390808105,
      "learning_rate": 3.3453689167974885e-05,
      "loss": 0.603,
      "step": 1686400
    },
    {
      "epoch": 26.475667189952905,
      "grad_norm": 4.492012977600098,
      "learning_rate": 3.3452708006279436e-05,
      "loss": 0.5366,
      "step": 1686500
    },
    {
      "epoch": 26.47723704866562,
      "grad_norm": 4.527475357055664,
      "learning_rate": 3.3451726844583993e-05,
      "loss": 0.582,
      "step": 1686600
    },
    {
      "epoch": 26.478806907378335,
      "grad_norm": 4.029007434844971,
      "learning_rate": 3.345074568288854e-05,
      "loss": 0.6086,
      "step": 1686700
    },
    {
      "epoch": 26.48037676609105,
      "grad_norm": 4.351083755493164,
      "learning_rate": 3.3449764521193095e-05,
      "loss": 0.6154,
      "step": 1686800
    },
    {
      "epoch": 26.48194662480377,
      "grad_norm": 4.109248161315918,
      "learning_rate": 3.3448783359497646e-05,
      "loss": 0.6125,
      "step": 1686900
    },
    {
      "epoch": 26.483516483516482,
      "grad_norm": 3.722412347793579,
      "learning_rate": 3.34478021978022e-05,
      "loss": 0.5957,
      "step": 1687000
    },
    {
      "epoch": 26.4850863422292,
      "grad_norm": 3.857285499572754,
      "learning_rate": 3.344682103610675e-05,
      "loss": 0.6105,
      "step": 1687100
    },
    {
      "epoch": 26.486656200941916,
      "grad_norm": 3.3837733268737793,
      "learning_rate": 3.3445839874411306e-05,
      "loss": 0.5899,
      "step": 1687200
    },
    {
      "epoch": 26.488226059654632,
      "grad_norm": 4.28684663772583,
      "learning_rate": 3.344485871271586e-05,
      "loss": 0.6022,
      "step": 1687300
    },
    {
      "epoch": 26.489795918367346,
      "grad_norm": 3.8472740650177,
      "learning_rate": 3.344387755102041e-05,
      "loss": 0.6184,
      "step": 1687400
    },
    {
      "epoch": 26.491365777080063,
      "grad_norm": 2.667459487915039,
      "learning_rate": 3.3442896389324966e-05,
      "loss": 0.6287,
      "step": 1687500
    },
    {
      "epoch": 26.49293563579278,
      "grad_norm": 4.000290393829346,
      "learning_rate": 3.344191522762952e-05,
      "loss": 0.61,
      "step": 1687600
    },
    {
      "epoch": 26.494505494505496,
      "grad_norm": 3.2639737129211426,
      "learning_rate": 3.344093406593407e-05,
      "loss": 0.6048,
      "step": 1687700
    },
    {
      "epoch": 26.49607535321821,
      "grad_norm": 3.8612570762634277,
      "learning_rate": 3.343995290423862e-05,
      "loss": 0.57,
      "step": 1687800
    },
    {
      "epoch": 26.497645211930926,
      "grad_norm": 4.495486259460449,
      "learning_rate": 3.3438971742543177e-05,
      "loss": 0.6001,
      "step": 1687900
    },
    {
      "epoch": 26.499215070643643,
      "grad_norm": 4.299181938171387,
      "learning_rate": 3.343799058084773e-05,
      "loss": 0.5923,
      "step": 1688000
    },
    {
      "epoch": 26.500784929356357,
      "grad_norm": 2.5935328006744385,
      "learning_rate": 3.343700941915228e-05,
      "loss": 0.608,
      "step": 1688100
    },
    {
      "epoch": 26.502354788069074,
      "grad_norm": 4.108551502227783,
      "learning_rate": 3.343602825745683e-05,
      "loss": 0.5598,
      "step": 1688200
    },
    {
      "epoch": 26.50392464678179,
      "grad_norm": 4.783690452575684,
      "learning_rate": 3.343504709576139e-05,
      "loss": 0.6007,
      "step": 1688300
    },
    {
      "epoch": 26.505494505494504,
      "grad_norm": 2.70397686958313,
      "learning_rate": 3.343406593406593e-05,
      "loss": 0.5387,
      "step": 1688400
    },
    {
      "epoch": 26.50706436420722,
      "grad_norm": 4.091501712799072,
      "learning_rate": 3.343308477237049e-05,
      "loss": 0.6043,
      "step": 1688500
    },
    {
      "epoch": 26.508634222919937,
      "grad_norm": 4.000465393066406,
      "learning_rate": 3.343210361067504e-05,
      "loss": 0.6357,
      "step": 1688600
    },
    {
      "epoch": 26.510204081632654,
      "grad_norm": 4.330691337585449,
      "learning_rate": 3.34311224489796e-05,
      "loss": 0.611,
      "step": 1688700
    },
    {
      "epoch": 26.511773940345368,
      "grad_norm": 4.294583320617676,
      "learning_rate": 3.343014128728414e-05,
      "loss": 0.606,
      "step": 1688800
    },
    {
      "epoch": 26.513343799058084,
      "grad_norm": 3.6173322200775146,
      "learning_rate": 3.34291601255887e-05,
      "loss": 0.5977,
      "step": 1688900
    },
    {
      "epoch": 26.5149136577708,
      "grad_norm": 3.858954668045044,
      "learning_rate": 3.342817896389325e-05,
      "loss": 0.6254,
      "step": 1689000
    },
    {
      "epoch": 26.516483516483518,
      "grad_norm": 3.708756446838379,
      "learning_rate": 3.34271978021978e-05,
      "loss": 0.5892,
      "step": 1689100
    },
    {
      "epoch": 26.51805337519623,
      "grad_norm": 4.574827194213867,
      "learning_rate": 3.342621664050235e-05,
      "loss": 0.5904,
      "step": 1689200
    },
    {
      "epoch": 26.51962323390895,
      "grad_norm": 3.9615890979766846,
      "learning_rate": 3.342523547880691e-05,
      "loss": 0.596,
      "step": 1689300
    },
    {
      "epoch": 26.521193092621665,
      "grad_norm": 5.099892616271973,
      "learning_rate": 3.342425431711146e-05,
      "loss": 0.5797,
      "step": 1689400
    },
    {
      "epoch": 26.52276295133438,
      "grad_norm": 3.1957600116729736,
      "learning_rate": 3.342327315541601e-05,
      "loss": 0.5913,
      "step": 1689500
    },
    {
      "epoch": 26.524332810047095,
      "grad_norm": 3.7939257621765137,
      "learning_rate": 3.342229199372057e-05,
      "loss": 0.5792,
      "step": 1689600
    },
    {
      "epoch": 26.525902668759812,
      "grad_norm": 3.6398048400878906,
      "learning_rate": 3.342131083202512e-05,
      "loss": 0.6092,
      "step": 1689700
    },
    {
      "epoch": 26.52747252747253,
      "grad_norm": 3.0732691287994385,
      "learning_rate": 3.342032967032967e-05,
      "loss": 0.6003,
      "step": 1689800
    },
    {
      "epoch": 26.529042386185242,
      "grad_norm": 2.8291966915130615,
      "learning_rate": 3.341934850863422e-05,
      "loss": 0.563,
      "step": 1689900
    },
    {
      "epoch": 26.53061224489796,
      "grad_norm": 3.6710898876190186,
      "learning_rate": 3.341836734693878e-05,
      "loss": 0.6147,
      "step": 1690000
    },
    {
      "epoch": 26.532182103610676,
      "grad_norm": 4.029750823974609,
      "learning_rate": 3.341738618524333e-05,
      "loss": 0.5657,
      "step": 1690100
    },
    {
      "epoch": 26.53375196232339,
      "grad_norm": 4.02487850189209,
      "learning_rate": 3.341640502354788e-05,
      "loss": 0.6652,
      "step": 1690200
    },
    {
      "epoch": 26.535321821036106,
      "grad_norm": 3.501356840133667,
      "learning_rate": 3.3415423861852434e-05,
      "loss": 0.5883,
      "step": 1690300
    },
    {
      "epoch": 26.536891679748823,
      "grad_norm": 1.9796711206436157,
      "learning_rate": 3.341444270015699e-05,
      "loss": 0.5649,
      "step": 1690400
    },
    {
      "epoch": 26.53846153846154,
      "grad_norm": 5.0483245849609375,
      "learning_rate": 3.3413461538461536e-05,
      "loss": 0.6013,
      "step": 1690500
    },
    {
      "epoch": 26.540031397174253,
      "grad_norm": 4.453911781311035,
      "learning_rate": 3.3412480376766094e-05,
      "loss": 0.5736,
      "step": 1690600
    },
    {
      "epoch": 26.54160125588697,
      "grad_norm": 4.398169040679932,
      "learning_rate": 3.3411499215070645e-05,
      "loss": 0.6085,
      "step": 1690700
    },
    {
      "epoch": 26.543171114599687,
      "grad_norm": 4.179030418395996,
      "learning_rate": 3.34105180533752e-05,
      "loss": 0.5672,
      "step": 1690800
    },
    {
      "epoch": 26.5447409733124,
      "grad_norm": 3.9751861095428467,
      "learning_rate": 3.340953689167975e-05,
      "loss": 0.5875,
      "step": 1690900
    },
    {
      "epoch": 26.546310832025117,
      "grad_norm": 4.31654167175293,
      "learning_rate": 3.3408555729984304e-05,
      "loss": 0.5582,
      "step": 1691000
    },
    {
      "epoch": 26.547880690737834,
      "grad_norm": 2.991831064224243,
      "learning_rate": 3.3407574568288855e-05,
      "loss": 0.5777,
      "step": 1691100
    },
    {
      "epoch": 26.54945054945055,
      "grad_norm": 4.2382965087890625,
      "learning_rate": 3.3406593406593406e-05,
      "loss": 0.5708,
      "step": 1691200
    },
    {
      "epoch": 26.551020408163264,
      "grad_norm": 3.4481112957000732,
      "learning_rate": 3.340561224489796e-05,
      "loss": 0.6211,
      "step": 1691300
    },
    {
      "epoch": 26.55259026687598,
      "grad_norm": 4.5359954833984375,
      "learning_rate": 3.3404631083202515e-05,
      "loss": 0.613,
      "step": 1691400
    },
    {
      "epoch": 26.554160125588698,
      "grad_norm": 3.427009344100952,
      "learning_rate": 3.3403649921507066e-05,
      "loss": 0.5924,
      "step": 1691500
    },
    {
      "epoch": 26.55572998430141,
      "grad_norm": 3.7900502681732178,
      "learning_rate": 3.340266875981162e-05,
      "loss": 0.5896,
      "step": 1691600
    },
    {
      "epoch": 26.55729984301413,
      "grad_norm": 5.490347862243652,
      "learning_rate": 3.3401687598116175e-05,
      "loss": 0.6024,
      "step": 1691700
    },
    {
      "epoch": 26.558869701726845,
      "grad_norm": 3.985597848892212,
      "learning_rate": 3.3400706436420726e-05,
      "loss": 0.5766,
      "step": 1691800
    },
    {
      "epoch": 26.560439560439562,
      "grad_norm": 4.705826759338379,
      "learning_rate": 3.339972527472528e-05,
      "loss": 0.6,
      "step": 1691900
    },
    {
      "epoch": 26.562009419152275,
      "grad_norm": 3.6680519580841064,
      "learning_rate": 3.339874411302983e-05,
      "loss": 0.5874,
      "step": 1692000
    },
    {
      "epoch": 26.563579277864992,
      "grad_norm": 3.607327461242676,
      "learning_rate": 3.3397762951334386e-05,
      "loss": 0.6014,
      "step": 1692100
    },
    {
      "epoch": 26.56514913657771,
      "grad_norm": 3.5737578868865967,
      "learning_rate": 3.3396781789638936e-05,
      "loss": 0.5718,
      "step": 1692200
    },
    {
      "epoch": 26.566718995290422,
      "grad_norm": 4.080854892730713,
      "learning_rate": 3.339580062794349e-05,
      "loss": 0.5873,
      "step": 1692300
    },
    {
      "epoch": 26.56828885400314,
      "grad_norm": 4.04115629196167,
      "learning_rate": 3.339481946624804e-05,
      "loss": 0.5887,
      "step": 1692400
    },
    {
      "epoch": 26.569858712715856,
      "grad_norm": 3.5970091819763184,
      "learning_rate": 3.3393838304552596e-05,
      "loss": 0.5895,
      "step": 1692500
    },
    {
      "epoch": 26.571428571428573,
      "grad_norm": 3.316028118133545,
      "learning_rate": 3.339285714285714e-05,
      "loss": 0.6045,
      "step": 1692600
    },
    {
      "epoch": 26.572998430141286,
      "grad_norm": 4.4750657081604,
      "learning_rate": 3.33918759811617e-05,
      "loss": 0.5975,
      "step": 1692700
    },
    {
      "epoch": 26.574568288854003,
      "grad_norm": 3.2177557945251465,
      "learning_rate": 3.339089481946625e-05,
      "loss": 0.6225,
      "step": 1692800
    },
    {
      "epoch": 26.57613814756672,
      "grad_norm": 3.9123477935791016,
      "learning_rate": 3.338991365777081e-05,
      "loss": 0.5956,
      "step": 1692900
    },
    {
      "epoch": 26.577708006279433,
      "grad_norm": 2.9334053993225098,
      "learning_rate": 3.338893249607535e-05,
      "loss": 0.6017,
      "step": 1693000
    },
    {
      "epoch": 26.57927786499215,
      "grad_norm": 3.665128231048584,
      "learning_rate": 3.338795133437991e-05,
      "loss": 0.6024,
      "step": 1693100
    },
    {
      "epoch": 26.580847723704867,
      "grad_norm": 3.337968587875366,
      "learning_rate": 3.338697017268446e-05,
      "loss": 0.6318,
      "step": 1693200
    },
    {
      "epoch": 26.582417582417584,
      "grad_norm": 4.006324768066406,
      "learning_rate": 3.338598901098901e-05,
      "loss": 0.6306,
      "step": 1693300
    },
    {
      "epoch": 26.583987441130297,
      "grad_norm": 3.9159090518951416,
      "learning_rate": 3.338500784929356e-05,
      "loss": 0.5461,
      "step": 1693400
    },
    {
      "epoch": 26.585557299843014,
      "grad_norm": 3.9257569313049316,
      "learning_rate": 3.338402668759812e-05,
      "loss": 0.5814,
      "step": 1693500
    },
    {
      "epoch": 26.58712715855573,
      "grad_norm": 5.378490924835205,
      "learning_rate": 3.338304552590267e-05,
      "loss": 0.58,
      "step": 1693600
    },
    {
      "epoch": 26.588697017268444,
      "grad_norm": 4.13452672958374,
      "learning_rate": 3.338206436420722e-05,
      "loss": 0.6156,
      "step": 1693700
    },
    {
      "epoch": 26.59026687598116,
      "grad_norm": 3.976155996322632,
      "learning_rate": 3.338108320251177e-05,
      "loss": 0.5933,
      "step": 1693800
    },
    {
      "epoch": 26.591836734693878,
      "grad_norm": 4.07142448425293,
      "learning_rate": 3.338010204081633e-05,
      "loss": 0.6133,
      "step": 1693900
    },
    {
      "epoch": 26.593406593406595,
      "grad_norm": 3.641021490097046,
      "learning_rate": 3.337912087912088e-05,
      "loss": 0.554,
      "step": 1694000
    },
    {
      "epoch": 26.594976452119308,
      "grad_norm": 2.6133992671966553,
      "learning_rate": 3.337813971742543e-05,
      "loss": 0.6282,
      "step": 1694100
    },
    {
      "epoch": 26.596546310832025,
      "grad_norm": 2.454677104949951,
      "learning_rate": 3.337715855572999e-05,
      "loss": 0.5849,
      "step": 1694200
    },
    {
      "epoch": 26.598116169544742,
      "grad_norm": 3.227968692779541,
      "learning_rate": 3.337617739403454e-05,
      "loss": 0.5657,
      "step": 1694300
    },
    {
      "epoch": 26.599686028257455,
      "grad_norm": 3.2061116695404053,
      "learning_rate": 3.337519623233909e-05,
      "loss": 0.607,
      "step": 1694400
    },
    {
      "epoch": 26.601255886970172,
      "grad_norm": 3.709132194519043,
      "learning_rate": 3.337421507064364e-05,
      "loss": 0.6002,
      "step": 1694500
    },
    {
      "epoch": 26.60282574568289,
      "grad_norm": 4.576196670532227,
      "learning_rate": 3.33732339089482e-05,
      "loss": 0.557,
      "step": 1694600
    },
    {
      "epoch": 26.604395604395606,
      "grad_norm": 3.75748872756958,
      "learning_rate": 3.3372252747252745e-05,
      "loss": 0.565,
      "step": 1694700
    },
    {
      "epoch": 26.60596546310832,
      "grad_norm": 3.8324010372161865,
      "learning_rate": 3.33712715855573e-05,
      "loss": 0.6108,
      "step": 1694800
    },
    {
      "epoch": 26.607535321821036,
      "grad_norm": 4.183237075805664,
      "learning_rate": 3.3370290423861854e-05,
      "loss": 0.5773,
      "step": 1694900
    },
    {
      "epoch": 26.609105180533753,
      "grad_norm": 4.40443754196167,
      "learning_rate": 3.3369309262166405e-05,
      "loss": 0.6026,
      "step": 1695000
    },
    {
      "epoch": 26.610675039246466,
      "grad_norm": 3.759714365005493,
      "learning_rate": 3.3368328100470956e-05,
      "loss": 0.6026,
      "step": 1695100
    },
    {
      "epoch": 26.612244897959183,
      "grad_norm": 3.3718032836914062,
      "learning_rate": 3.336734693877551e-05,
      "loss": 0.5953,
      "step": 1695200
    },
    {
      "epoch": 26.6138147566719,
      "grad_norm": 5.032674789428711,
      "learning_rate": 3.3366365777080064e-05,
      "loss": 0.6144,
      "step": 1695300
    },
    {
      "epoch": 26.615384615384617,
      "grad_norm": 4.147924423217773,
      "learning_rate": 3.3365384615384615e-05,
      "loss": 0.5952,
      "step": 1695400
    },
    {
      "epoch": 26.61695447409733,
      "grad_norm": 3.7048208713531494,
      "learning_rate": 3.3364403453689166e-05,
      "loss": 0.5714,
      "step": 1695500
    },
    {
      "epoch": 26.618524332810047,
      "grad_norm": 2.7051236629486084,
      "learning_rate": 3.3363422291993724e-05,
      "loss": 0.5568,
      "step": 1695600
    },
    {
      "epoch": 26.620094191522764,
      "grad_norm": 3.501608371734619,
      "learning_rate": 3.3362441130298275e-05,
      "loss": 0.6081,
      "step": 1695700
    },
    {
      "epoch": 26.621664050235477,
      "grad_norm": 4.087924957275391,
      "learning_rate": 3.3361459968602826e-05,
      "loss": 0.6517,
      "step": 1695800
    },
    {
      "epoch": 26.623233908948194,
      "grad_norm": 4.312036037445068,
      "learning_rate": 3.336047880690738e-05,
      "loss": 0.6208,
      "step": 1695900
    },
    {
      "epoch": 26.62480376766091,
      "grad_norm": 3.6032376289367676,
      "learning_rate": 3.3359497645211935e-05,
      "loss": 0.5914,
      "step": 1696000
    },
    {
      "epoch": 26.626373626373628,
      "grad_norm": 2.049434185028076,
      "learning_rate": 3.3358516483516486e-05,
      "loss": 0.5758,
      "step": 1696100
    },
    {
      "epoch": 26.62794348508634,
      "grad_norm": 3.8750224113464355,
      "learning_rate": 3.335753532182104e-05,
      "loss": 0.621,
      "step": 1696200
    },
    {
      "epoch": 26.629513343799058,
      "grad_norm": 4.629457950592041,
      "learning_rate": 3.3356554160125594e-05,
      "loss": 0.577,
      "step": 1696300
    },
    {
      "epoch": 26.631083202511775,
      "grad_norm": 4.304316997528076,
      "learning_rate": 3.335557299843014e-05,
      "loss": 0.5832,
      "step": 1696400
    },
    {
      "epoch": 26.632653061224488,
      "grad_norm": 3.115349292755127,
      "learning_rate": 3.3354591836734696e-05,
      "loss": 0.6073,
      "step": 1696500
    },
    {
      "epoch": 26.634222919937205,
      "grad_norm": 4.021759033203125,
      "learning_rate": 3.335361067503925e-05,
      "loss": 0.5926,
      "step": 1696600
    },
    {
      "epoch": 26.635792778649922,
      "grad_norm": 3.4757721424102783,
      "learning_rate": 3.3352629513343805e-05,
      "loss": 0.5687,
      "step": 1696700
    },
    {
      "epoch": 26.63736263736264,
      "grad_norm": 3.7817351818084717,
      "learning_rate": 3.335164835164835e-05,
      "loss": 0.6233,
      "step": 1696800
    },
    {
      "epoch": 26.638932496075352,
      "grad_norm": 3.5608749389648438,
      "learning_rate": 3.335066718995291e-05,
      "loss": 0.612,
      "step": 1696900
    },
    {
      "epoch": 26.64050235478807,
      "grad_norm": 5.1172685623168945,
      "learning_rate": 3.334968602825746e-05,
      "loss": 0.6246,
      "step": 1697000
    },
    {
      "epoch": 26.642072213500786,
      "grad_norm": 3.4539475440979004,
      "learning_rate": 3.334870486656201e-05,
      "loss": 0.6395,
      "step": 1697100
    },
    {
      "epoch": 26.643642072213503,
      "grad_norm": 3.5047385692596436,
      "learning_rate": 3.334772370486656e-05,
      "loss": 0.6079,
      "step": 1697200
    },
    {
      "epoch": 26.645211930926216,
      "grad_norm": 2.5481886863708496,
      "learning_rate": 3.334674254317112e-05,
      "loss": 0.6039,
      "step": 1697300
    },
    {
      "epoch": 26.646781789638933,
      "grad_norm": 2.9343297481536865,
      "learning_rate": 3.334576138147567e-05,
      "loss": 0.5919,
      "step": 1697400
    },
    {
      "epoch": 26.64835164835165,
      "grad_norm": 3.900757312774658,
      "learning_rate": 3.334478021978022e-05,
      "loss": 0.5996,
      "step": 1697500
    },
    {
      "epoch": 26.649921507064363,
      "grad_norm": 4.910172462463379,
      "learning_rate": 3.334379905808477e-05,
      "loss": 0.5644,
      "step": 1697600
    },
    {
      "epoch": 26.65149136577708,
      "grad_norm": 4.167272567749023,
      "learning_rate": 3.334281789638933e-05,
      "loss": 0.6118,
      "step": 1697700
    },
    {
      "epoch": 26.653061224489797,
      "grad_norm": 3.382402181625366,
      "learning_rate": 3.334183673469388e-05,
      "loss": 0.5954,
      "step": 1697800
    },
    {
      "epoch": 26.65463108320251,
      "grad_norm": 4.30012845993042,
      "learning_rate": 3.334085557299843e-05,
      "loss": 0.559,
      "step": 1697900
    },
    {
      "epoch": 26.656200941915227,
      "grad_norm": 3.9779109954833984,
      "learning_rate": 3.333987441130298e-05,
      "loss": 0.5932,
      "step": 1698000
    },
    {
      "epoch": 26.657770800627944,
      "grad_norm": 4.477042198181152,
      "learning_rate": 3.333889324960754e-05,
      "loss": 0.5821,
      "step": 1698100
    },
    {
      "epoch": 26.65934065934066,
      "grad_norm": 4.361033916473389,
      "learning_rate": 3.333791208791209e-05,
      "loss": 0.6214,
      "step": 1698200
    },
    {
      "epoch": 26.660910518053374,
      "grad_norm": 2.5725138187408447,
      "learning_rate": 3.333693092621664e-05,
      "loss": 0.575,
      "step": 1698300
    },
    {
      "epoch": 26.66248037676609,
      "grad_norm": 2.645456552505493,
      "learning_rate": 3.33359497645212e-05,
      "loss": 0.5963,
      "step": 1698400
    },
    {
      "epoch": 26.664050235478808,
      "grad_norm": 3.9265944957733154,
      "learning_rate": 3.333496860282574e-05,
      "loss": 0.5715,
      "step": 1698500
    },
    {
      "epoch": 26.665620094191524,
      "grad_norm": 4.222862243652344,
      "learning_rate": 3.33339874411303e-05,
      "loss": 0.6162,
      "step": 1698600
    },
    {
      "epoch": 26.667189952904238,
      "grad_norm": 2.503990888595581,
      "learning_rate": 3.333300627943485e-05,
      "loss": 0.635,
      "step": 1698700
    },
    {
      "epoch": 26.668759811616955,
      "grad_norm": 3.220960855484009,
      "learning_rate": 3.333202511773941e-05,
      "loss": 0.593,
      "step": 1698800
    },
    {
      "epoch": 26.67032967032967,
      "grad_norm": 3.7401111125946045,
      "learning_rate": 3.3331043956043954e-05,
      "loss": 0.6162,
      "step": 1698900
    },
    {
      "epoch": 26.671899529042385,
      "grad_norm": 3.75860333442688,
      "learning_rate": 3.333006279434851e-05,
      "loss": 0.6129,
      "step": 1699000
    },
    {
      "epoch": 26.6734693877551,
      "grad_norm": 4.257467269897461,
      "learning_rate": 3.332908163265306e-05,
      "loss": 0.6159,
      "step": 1699100
    },
    {
      "epoch": 26.67503924646782,
      "grad_norm": 4.26479959487915,
      "learning_rate": 3.3328100470957614e-05,
      "loss": 0.5834,
      "step": 1699200
    },
    {
      "epoch": 26.676609105180535,
      "grad_norm": 4.342308044433594,
      "learning_rate": 3.3327119309262165e-05,
      "loss": 0.6405,
      "step": 1699300
    },
    {
      "epoch": 26.67817896389325,
      "grad_norm": 2.919367551803589,
      "learning_rate": 3.332613814756672e-05,
      "loss": 0.5761,
      "step": 1699400
    },
    {
      "epoch": 26.679748822605966,
      "grad_norm": 4.320276737213135,
      "learning_rate": 3.332515698587127e-05,
      "loss": 0.5731,
      "step": 1699500
    },
    {
      "epoch": 26.681318681318682,
      "grad_norm": 4.320587635040283,
      "learning_rate": 3.3324175824175824e-05,
      "loss": 0.6166,
      "step": 1699600
    },
    {
      "epoch": 26.682888540031396,
      "grad_norm": 3.4894983768463135,
      "learning_rate": 3.3323194662480375e-05,
      "loss": 0.5922,
      "step": 1699700
    },
    {
      "epoch": 26.684458398744113,
      "grad_norm": 4.01282262802124,
      "learning_rate": 3.332221350078493e-05,
      "loss": 0.5824,
      "step": 1699800
    },
    {
      "epoch": 26.68602825745683,
      "grad_norm": 4.198184967041016,
      "learning_rate": 3.3321232339089484e-05,
      "loss": 0.6062,
      "step": 1699900
    },
    {
      "epoch": 26.687598116169546,
      "grad_norm": 4.253525257110596,
      "learning_rate": 3.3320251177394035e-05,
      "loss": 0.5953,
      "step": 1700000
    },
    {
      "epoch": 26.68916797488226,
      "grad_norm": 3.6546201705932617,
      "learning_rate": 3.3319270015698586e-05,
      "loss": 0.6051,
      "step": 1700100
    },
    {
      "epoch": 26.690737833594977,
      "grad_norm": 4.203878402709961,
      "learning_rate": 3.3318288854003144e-05,
      "loss": 0.6529,
      "step": 1700200
    },
    {
      "epoch": 26.692307692307693,
      "grad_norm": 4.5605998039245605,
      "learning_rate": 3.3317307692307695e-05,
      "loss": 0.5812,
      "step": 1700300
    },
    {
      "epoch": 26.693877551020407,
      "grad_norm": 3.321246385574341,
      "learning_rate": 3.3316326530612246e-05,
      "loss": 0.5762,
      "step": 1700400
    },
    {
      "epoch": 26.695447409733124,
      "grad_norm": 5.258505344390869,
      "learning_rate": 3.3315345368916803e-05,
      "loss": 0.648,
      "step": 1700500
    },
    {
      "epoch": 26.69701726844584,
      "grad_norm": 3.892585039138794,
      "learning_rate": 3.331436420722135e-05,
      "loss": 0.6189,
      "step": 1700600
    },
    {
      "epoch": 26.698587127158557,
      "grad_norm": 4.211091041564941,
      "learning_rate": 3.3313383045525905e-05,
      "loss": 0.6138,
      "step": 1700700
    },
    {
      "epoch": 26.70015698587127,
      "grad_norm": 4.348194122314453,
      "learning_rate": 3.3312401883830456e-05,
      "loss": 0.5909,
      "step": 1700800
    },
    {
      "epoch": 26.701726844583987,
      "grad_norm": 4.889430522918701,
      "learning_rate": 3.3311420722135014e-05,
      "loss": 0.5551,
      "step": 1700900
    },
    {
      "epoch": 26.703296703296704,
      "grad_norm": 4.056696891784668,
      "learning_rate": 3.331043956043956e-05,
      "loss": 0.5623,
      "step": 1701000
    },
    {
      "epoch": 26.704866562009418,
      "grad_norm": 3.3103890419006348,
      "learning_rate": 3.3309458398744116e-05,
      "loss": 0.5891,
      "step": 1701100
    },
    {
      "epoch": 26.706436420722135,
      "grad_norm": 3.1826093196868896,
      "learning_rate": 3.330847723704867e-05,
      "loss": 0.5773,
      "step": 1701200
    },
    {
      "epoch": 26.70800627943485,
      "grad_norm": 3.283862352371216,
      "learning_rate": 3.330749607535322e-05,
      "loss": 0.6124,
      "step": 1701300
    },
    {
      "epoch": 26.70957613814757,
      "grad_norm": 3.941840171813965,
      "learning_rate": 3.330651491365777e-05,
      "loss": 0.6091,
      "step": 1701400
    },
    {
      "epoch": 26.71114599686028,
      "grad_norm": 3.117154121398926,
      "learning_rate": 3.330553375196233e-05,
      "loss": 0.609,
      "step": 1701500
    },
    {
      "epoch": 26.712715855573,
      "grad_norm": 3.1630687713623047,
      "learning_rate": 3.330455259026688e-05,
      "loss": 0.5767,
      "step": 1701600
    },
    {
      "epoch": 26.714285714285715,
      "grad_norm": 3.706671953201294,
      "learning_rate": 3.330357142857143e-05,
      "loss": 0.5714,
      "step": 1701700
    },
    {
      "epoch": 26.71585557299843,
      "grad_norm": 4.353928089141846,
      "learning_rate": 3.330259026687598e-05,
      "loss": 0.6127,
      "step": 1701800
    },
    {
      "epoch": 26.717425431711145,
      "grad_norm": 4.479794979095459,
      "learning_rate": 3.330160910518054e-05,
      "loss": 0.6149,
      "step": 1701900
    },
    {
      "epoch": 26.718995290423862,
      "grad_norm": 3.7731425762176514,
      "learning_rate": 3.330062794348509e-05,
      "loss": 0.5639,
      "step": 1702000
    },
    {
      "epoch": 26.72056514913658,
      "grad_norm": 3.5201256275177,
      "learning_rate": 3.329964678178964e-05,
      "loss": 0.5785,
      "step": 1702100
    },
    {
      "epoch": 26.722135007849293,
      "grad_norm": 3.7257096767425537,
      "learning_rate": 3.329866562009419e-05,
      "loss": 0.5989,
      "step": 1702200
    },
    {
      "epoch": 26.72370486656201,
      "grad_norm": 4.319020748138428,
      "learning_rate": 3.329768445839875e-05,
      "loss": 0.6109,
      "step": 1702300
    },
    {
      "epoch": 26.725274725274726,
      "grad_norm": 4.450418949127197,
      "learning_rate": 3.32967032967033e-05,
      "loss": 0.5964,
      "step": 1702400
    },
    {
      "epoch": 26.72684458398744,
      "grad_norm": 4.207792282104492,
      "learning_rate": 3.329572213500785e-05,
      "loss": 0.5577,
      "step": 1702500
    },
    {
      "epoch": 26.728414442700156,
      "grad_norm": 2.866008758544922,
      "learning_rate": 3.329474097331241e-05,
      "loss": 0.5546,
      "step": 1702600
    },
    {
      "epoch": 26.729984301412873,
      "grad_norm": 4.326688289642334,
      "learning_rate": 3.329375981161695e-05,
      "loss": 0.5962,
      "step": 1702700
    },
    {
      "epoch": 26.73155416012559,
      "grad_norm": 3.7066917419433594,
      "learning_rate": 3.329277864992151e-05,
      "loss": 0.5834,
      "step": 1702800
    },
    {
      "epoch": 26.733124018838303,
      "grad_norm": 5.428410530090332,
      "learning_rate": 3.329179748822606e-05,
      "loss": 0.5788,
      "step": 1702900
    },
    {
      "epoch": 26.73469387755102,
      "grad_norm": 3.763925075531006,
      "learning_rate": 3.329081632653062e-05,
      "loss": 0.5582,
      "step": 1703000
    },
    {
      "epoch": 26.736263736263737,
      "grad_norm": 3.8497304916381836,
      "learning_rate": 3.328983516483516e-05,
      "loss": 0.601,
      "step": 1703100
    },
    {
      "epoch": 26.73783359497645,
      "grad_norm": 4.390745639801025,
      "learning_rate": 3.328885400313972e-05,
      "loss": 0.5931,
      "step": 1703200
    },
    {
      "epoch": 26.739403453689167,
      "grad_norm": 4.960815906524658,
      "learning_rate": 3.328787284144427e-05,
      "loss": 0.6111,
      "step": 1703300
    },
    {
      "epoch": 26.740973312401884,
      "grad_norm": 2.1844568252563477,
      "learning_rate": 3.328689167974882e-05,
      "loss": 0.5884,
      "step": 1703400
    },
    {
      "epoch": 26.7425431711146,
      "grad_norm": 5.200032711029053,
      "learning_rate": 3.3285910518053374e-05,
      "loss": 0.6182,
      "step": 1703500
    },
    {
      "epoch": 26.744113029827314,
      "grad_norm": 4.079349040985107,
      "learning_rate": 3.328492935635793e-05,
      "loss": 0.5619,
      "step": 1703600
    },
    {
      "epoch": 26.74568288854003,
      "grad_norm": 3.283973217010498,
      "learning_rate": 3.328394819466248e-05,
      "loss": 0.5423,
      "step": 1703700
    },
    {
      "epoch": 26.747252747252748,
      "grad_norm": 4.425178050994873,
      "learning_rate": 3.328296703296703e-05,
      "loss": 0.6162,
      "step": 1703800
    },
    {
      "epoch": 26.74882260596546,
      "grad_norm": 2.559102773666382,
      "learning_rate": 3.3281985871271584e-05,
      "loss": 0.6027,
      "step": 1703900
    },
    {
      "epoch": 26.75039246467818,
      "grad_norm": 3.3653693199157715,
      "learning_rate": 3.328100470957614e-05,
      "loss": 0.5934,
      "step": 1704000
    },
    {
      "epoch": 26.751962323390895,
      "grad_norm": 3.454697847366333,
      "learning_rate": 3.328002354788069e-05,
      "loss": 0.5472,
      "step": 1704100
    },
    {
      "epoch": 26.753532182103612,
      "grad_norm": 4.210253715515137,
      "learning_rate": 3.3279042386185244e-05,
      "loss": 0.6102,
      "step": 1704200
    },
    {
      "epoch": 26.755102040816325,
      "grad_norm": 4.91710901260376,
      "learning_rate": 3.3278061224489795e-05,
      "loss": 0.5861,
      "step": 1704300
    },
    {
      "epoch": 26.756671899529042,
      "grad_norm": 3.515857458114624,
      "learning_rate": 3.327708006279435e-05,
      "loss": 0.6228,
      "step": 1704400
    },
    {
      "epoch": 26.75824175824176,
      "grad_norm": 3.687983751296997,
      "learning_rate": 3.3276098901098904e-05,
      "loss": 0.6185,
      "step": 1704500
    },
    {
      "epoch": 26.759811616954472,
      "grad_norm": 3.6930058002471924,
      "learning_rate": 3.3275117739403455e-05,
      "loss": 0.6273,
      "step": 1704600
    },
    {
      "epoch": 26.76138147566719,
      "grad_norm": 3.610502004623413,
      "learning_rate": 3.327413657770801e-05,
      "loss": 0.5827,
      "step": 1704700
    },
    {
      "epoch": 26.762951334379906,
      "grad_norm": 3.056741714477539,
      "learning_rate": 3.327315541601256e-05,
      "loss": 0.5979,
      "step": 1704800
    },
    {
      "epoch": 26.764521193092623,
      "grad_norm": 3.184673309326172,
      "learning_rate": 3.3272174254317114e-05,
      "loss": 0.5947,
      "step": 1704900
    },
    {
      "epoch": 26.766091051805336,
      "grad_norm": 4.9345598220825195,
      "learning_rate": 3.3271193092621665e-05,
      "loss": 0.591,
      "step": 1705000
    },
    {
      "epoch": 26.767660910518053,
      "grad_norm": 3.712779998779297,
      "learning_rate": 3.327021193092622e-05,
      "loss": 0.6453,
      "step": 1705100
    },
    {
      "epoch": 26.76923076923077,
      "grad_norm": 4.148812770843506,
      "learning_rate": 3.326923076923077e-05,
      "loss": 0.5789,
      "step": 1705200
    },
    {
      "epoch": 26.770800627943487,
      "grad_norm": 4.500370025634766,
      "learning_rate": 3.3268249607535325e-05,
      "loss": 0.5888,
      "step": 1705300
    },
    {
      "epoch": 26.7723704866562,
      "grad_norm": 4.059281349182129,
      "learning_rate": 3.3267268445839876e-05,
      "loss": 0.6296,
      "step": 1705400
    },
    {
      "epoch": 26.773940345368917,
      "grad_norm": 3.7750492095947266,
      "learning_rate": 3.326628728414443e-05,
      "loss": 0.6023,
      "step": 1705500
    },
    {
      "epoch": 26.775510204081634,
      "grad_norm": 4.174069404602051,
      "learning_rate": 3.326530612244898e-05,
      "loss": 0.5737,
      "step": 1705600
    },
    {
      "epoch": 26.777080062794347,
      "grad_norm": 3.923372983932495,
      "learning_rate": 3.3264324960753536e-05,
      "loss": 0.579,
      "step": 1705700
    },
    {
      "epoch": 26.778649921507064,
      "grad_norm": 5.797903537750244,
      "learning_rate": 3.326334379905809e-05,
      "loss": 0.6207,
      "step": 1705800
    },
    {
      "epoch": 26.78021978021978,
      "grad_norm": 3.664515256881714,
      "learning_rate": 3.326236263736264e-05,
      "loss": 0.5895,
      "step": 1705900
    },
    {
      "epoch": 26.781789638932494,
      "grad_norm": 2.8873393535614014,
      "learning_rate": 3.326138147566719e-05,
      "loss": 0.6078,
      "step": 1706000
    },
    {
      "epoch": 26.78335949764521,
      "grad_norm": 3.774035930633545,
      "learning_rate": 3.3260400313971747e-05,
      "loss": 0.6035,
      "step": 1706100
    },
    {
      "epoch": 26.784929356357928,
      "grad_norm": 4.132955551147461,
      "learning_rate": 3.32594191522763e-05,
      "loss": 0.581,
      "step": 1706200
    },
    {
      "epoch": 26.786499215070645,
      "grad_norm": 3.547605514526367,
      "learning_rate": 3.325843799058085e-05,
      "loss": 0.6499,
      "step": 1706300
    },
    {
      "epoch": 26.788069073783358,
      "grad_norm": 4.265026092529297,
      "learning_rate": 3.32574568288854e-05,
      "loss": 0.5874,
      "step": 1706400
    },
    {
      "epoch": 26.789638932496075,
      "grad_norm": 2.6355700492858887,
      "learning_rate": 3.325647566718996e-05,
      "loss": 0.61,
      "step": 1706500
    },
    {
      "epoch": 26.791208791208792,
      "grad_norm": 4.1162614822387695,
      "learning_rate": 3.325549450549451e-05,
      "loss": 0.6108,
      "step": 1706600
    },
    {
      "epoch": 26.79277864992151,
      "grad_norm": 4.105323314666748,
      "learning_rate": 3.325451334379906e-05,
      "loss": 0.6233,
      "step": 1706700
    },
    {
      "epoch": 26.794348508634222,
      "grad_norm": 4.20227575302124,
      "learning_rate": 3.325353218210362e-05,
      "loss": 0.5967,
      "step": 1706800
    },
    {
      "epoch": 26.79591836734694,
      "grad_norm": 5.043045520782471,
      "learning_rate": 3.325255102040816e-05,
      "loss": 0.5766,
      "step": 1706900
    },
    {
      "epoch": 26.797488226059656,
      "grad_norm": 4.372359275817871,
      "learning_rate": 3.325156985871272e-05,
      "loss": 0.5548,
      "step": 1707000
    },
    {
      "epoch": 26.79905808477237,
      "grad_norm": 3.387418270111084,
      "learning_rate": 3.325058869701727e-05,
      "loss": 0.6214,
      "step": 1707100
    },
    {
      "epoch": 26.800627943485086,
      "grad_norm": 4.022267818450928,
      "learning_rate": 3.324960753532183e-05,
      "loss": 0.5537,
      "step": 1707200
    },
    {
      "epoch": 26.802197802197803,
      "grad_norm": 3.123150110244751,
      "learning_rate": 3.324862637362637e-05,
      "loss": 0.605,
      "step": 1707300
    },
    {
      "epoch": 26.80376766091052,
      "grad_norm": 3.648007392883301,
      "learning_rate": 3.324764521193093e-05,
      "loss": 0.583,
      "step": 1707400
    },
    {
      "epoch": 26.805337519623233,
      "grad_norm": 3.9571378231048584,
      "learning_rate": 3.324666405023548e-05,
      "loss": 0.5725,
      "step": 1707500
    },
    {
      "epoch": 26.80690737833595,
      "grad_norm": 4.119597434997559,
      "learning_rate": 3.324568288854003e-05,
      "loss": 0.6039,
      "step": 1707600
    },
    {
      "epoch": 26.808477237048667,
      "grad_norm": 3.341822624206543,
      "learning_rate": 3.324470172684458e-05,
      "loss": 0.6065,
      "step": 1707700
    },
    {
      "epoch": 26.81004709576138,
      "grad_norm": 3.1032674312591553,
      "learning_rate": 3.324372056514914e-05,
      "loss": 0.5787,
      "step": 1707800
    },
    {
      "epoch": 26.811616954474097,
      "grad_norm": 4.320953845977783,
      "learning_rate": 3.324273940345369e-05,
      "loss": 0.5638,
      "step": 1707900
    },
    {
      "epoch": 26.813186813186814,
      "grad_norm": 4.0368971824646,
      "learning_rate": 3.324175824175824e-05,
      "loss": 0.58,
      "step": 1708000
    },
    {
      "epoch": 26.81475667189953,
      "grad_norm": 4.572658061981201,
      "learning_rate": 3.324077708006279e-05,
      "loss": 0.6004,
      "step": 1708100
    },
    {
      "epoch": 26.816326530612244,
      "grad_norm": 5.106945514678955,
      "learning_rate": 3.323979591836735e-05,
      "loss": 0.5888,
      "step": 1708200
    },
    {
      "epoch": 26.81789638932496,
      "grad_norm": 4.047232627868652,
      "learning_rate": 3.32388147566719e-05,
      "loss": 0.621,
      "step": 1708300
    },
    {
      "epoch": 26.819466248037678,
      "grad_norm": 3.769209861755371,
      "learning_rate": 3.323783359497645e-05,
      "loss": 0.5949,
      "step": 1708400
    },
    {
      "epoch": 26.82103610675039,
      "grad_norm": 3.99125075340271,
      "learning_rate": 3.3236852433281004e-05,
      "loss": 0.617,
      "step": 1708500
    },
    {
      "epoch": 26.822605965463108,
      "grad_norm": 3.4161970615386963,
      "learning_rate": 3.323587127158556e-05,
      "loss": 0.554,
      "step": 1708600
    },
    {
      "epoch": 26.824175824175825,
      "grad_norm": 4.552899360656738,
      "learning_rate": 3.323489010989011e-05,
      "loss": 0.6024,
      "step": 1708700
    },
    {
      "epoch": 26.82574568288854,
      "grad_norm": 3.5605249404907227,
      "learning_rate": 3.3233908948194664e-05,
      "loss": 0.5754,
      "step": 1708800
    },
    {
      "epoch": 26.827315541601255,
      "grad_norm": 2.873918294906616,
      "learning_rate": 3.323292778649922e-05,
      "loss": 0.5785,
      "step": 1708900
    },
    {
      "epoch": 26.828885400313972,
      "grad_norm": 4.4184956550598145,
      "learning_rate": 3.3231946624803766e-05,
      "loss": 0.5783,
      "step": 1709000
    },
    {
      "epoch": 26.83045525902669,
      "grad_norm": 3.126561403274536,
      "learning_rate": 3.3230965463108323e-05,
      "loss": 0.6336,
      "step": 1709100
    },
    {
      "epoch": 26.832025117739402,
      "grad_norm": 3.4834465980529785,
      "learning_rate": 3.3229984301412874e-05,
      "loss": 0.5984,
      "step": 1709200
    },
    {
      "epoch": 26.83359497645212,
      "grad_norm": 3.973924398422241,
      "learning_rate": 3.322900313971743e-05,
      "loss": 0.5841,
      "step": 1709300
    },
    {
      "epoch": 26.835164835164836,
      "grad_norm": 4.252645015716553,
      "learning_rate": 3.3228021978021976e-05,
      "loss": 0.5938,
      "step": 1709400
    },
    {
      "epoch": 26.836734693877553,
      "grad_norm": 3.1522154808044434,
      "learning_rate": 3.3227040816326534e-05,
      "loss": 0.6004,
      "step": 1709500
    },
    {
      "epoch": 26.838304552590266,
      "grad_norm": 4.65281343460083,
      "learning_rate": 3.3226059654631085e-05,
      "loss": 0.6002,
      "step": 1709600
    },
    {
      "epoch": 26.839874411302983,
      "grad_norm": 4.055651664733887,
      "learning_rate": 3.3225078492935636e-05,
      "loss": 0.588,
      "step": 1709700
    },
    {
      "epoch": 26.8414442700157,
      "grad_norm": 3.153080701828003,
      "learning_rate": 3.322409733124019e-05,
      "loss": 0.64,
      "step": 1709800
    },
    {
      "epoch": 26.843014128728413,
      "grad_norm": 4.516629695892334,
      "learning_rate": 3.3223116169544745e-05,
      "loss": 0.576,
      "step": 1709900
    },
    {
      "epoch": 26.84458398744113,
      "grad_norm": 3.9023454189300537,
      "learning_rate": 3.3222135007849296e-05,
      "loss": 0.579,
      "step": 1710000
    },
    {
      "epoch": 26.846153846153847,
      "grad_norm": 4.229574680328369,
      "learning_rate": 3.322115384615385e-05,
      "loss": 0.6437,
      "step": 1710100
    },
    {
      "epoch": 26.847723704866564,
      "grad_norm": 4.213686943054199,
      "learning_rate": 3.32201726844584e-05,
      "loss": 0.6423,
      "step": 1710200
    },
    {
      "epoch": 26.849293563579277,
      "grad_norm": 4.213935852050781,
      "learning_rate": 3.3219191522762956e-05,
      "loss": 0.6516,
      "step": 1710300
    },
    {
      "epoch": 26.850863422291994,
      "grad_norm": 3.3656647205352783,
      "learning_rate": 3.3218210361067506e-05,
      "loss": 0.5712,
      "step": 1710400
    },
    {
      "epoch": 26.85243328100471,
      "grad_norm": 4.520819664001465,
      "learning_rate": 3.321722919937206e-05,
      "loss": 0.5666,
      "step": 1710500
    },
    {
      "epoch": 26.854003139717424,
      "grad_norm": 4.331577777862549,
      "learning_rate": 3.321624803767661e-05,
      "loss": 0.6111,
      "step": 1710600
    },
    {
      "epoch": 26.85557299843014,
      "grad_norm": 3.7276792526245117,
      "learning_rate": 3.3215266875981166e-05,
      "loss": 0.5892,
      "step": 1710700
    },
    {
      "epoch": 26.857142857142858,
      "grad_norm": 3.069528818130493,
      "learning_rate": 3.321428571428572e-05,
      "loss": 0.6187,
      "step": 1710800
    },
    {
      "epoch": 26.858712715855575,
      "grad_norm": 3.4890873432159424,
      "learning_rate": 3.321330455259027e-05,
      "loss": 0.5561,
      "step": 1710900
    },
    {
      "epoch": 26.860282574568288,
      "grad_norm": 3.6980481147766113,
      "learning_rate": 3.3212323390894826e-05,
      "loss": 0.6226,
      "step": 1711000
    },
    {
      "epoch": 26.861852433281005,
      "grad_norm": 4.051182746887207,
      "learning_rate": 3.321134222919937e-05,
      "loss": 0.6131,
      "step": 1711100
    },
    {
      "epoch": 26.86342229199372,
      "grad_norm": 3.7176027297973633,
      "learning_rate": 3.321036106750393e-05,
      "loss": 0.6124,
      "step": 1711200
    },
    {
      "epoch": 26.864992150706435,
      "grad_norm": 3.8726019859313965,
      "learning_rate": 3.320937990580848e-05,
      "loss": 0.5305,
      "step": 1711300
    },
    {
      "epoch": 26.86656200941915,
      "grad_norm": 3.015836715698242,
      "learning_rate": 3.320839874411304e-05,
      "loss": 0.5456,
      "step": 1711400
    },
    {
      "epoch": 26.86813186813187,
      "grad_norm": 2.4008047580718994,
      "learning_rate": 3.320741758241758e-05,
      "loss": 0.6146,
      "step": 1711500
    },
    {
      "epoch": 26.869701726844585,
      "grad_norm": 4.107799530029297,
      "learning_rate": 3.320643642072214e-05,
      "loss": 0.604,
      "step": 1711600
    },
    {
      "epoch": 26.8712715855573,
      "grad_norm": 4.545321464538574,
      "learning_rate": 3.320545525902669e-05,
      "loss": 0.6201,
      "step": 1711700
    },
    {
      "epoch": 26.872841444270016,
      "grad_norm": 2.6427161693573,
      "learning_rate": 3.320447409733124e-05,
      "loss": 0.5942,
      "step": 1711800
    },
    {
      "epoch": 26.874411302982733,
      "grad_norm": 4.015557289123535,
      "learning_rate": 3.320349293563579e-05,
      "loss": 0.6088,
      "step": 1711900
    },
    {
      "epoch": 26.875981161695446,
      "grad_norm": 3.334981679916382,
      "learning_rate": 3.320251177394035e-05,
      "loss": 0.5454,
      "step": 1712000
    },
    {
      "epoch": 26.877551020408163,
      "grad_norm": 3.6244781017303467,
      "learning_rate": 3.32015306122449e-05,
      "loss": 0.5896,
      "step": 1712100
    },
    {
      "epoch": 26.87912087912088,
      "grad_norm": 2.4131674766540527,
      "learning_rate": 3.320054945054945e-05,
      "loss": 0.5802,
      "step": 1712200
    },
    {
      "epoch": 26.880690737833596,
      "grad_norm": 4.085526466369629,
      "learning_rate": 3.3199568288854e-05,
      "loss": 0.6159,
      "step": 1712300
    },
    {
      "epoch": 26.88226059654631,
      "grad_norm": 3.7438976764678955,
      "learning_rate": 3.319858712715856e-05,
      "loss": 0.6006,
      "step": 1712400
    },
    {
      "epoch": 26.883830455259027,
      "grad_norm": 3.2810685634613037,
      "learning_rate": 3.319760596546311e-05,
      "loss": 0.5971,
      "step": 1712500
    },
    {
      "epoch": 26.885400313971743,
      "grad_norm": 3.8779137134552,
      "learning_rate": 3.319662480376766e-05,
      "loss": 0.6048,
      "step": 1712600
    },
    {
      "epoch": 26.886970172684457,
      "grad_norm": 3.362243175506592,
      "learning_rate": 3.319564364207221e-05,
      "loss": 0.5737,
      "step": 1712700
    },
    {
      "epoch": 26.888540031397174,
      "grad_norm": 3.328371524810791,
      "learning_rate": 3.319466248037677e-05,
      "loss": 0.578,
      "step": 1712800
    },
    {
      "epoch": 26.89010989010989,
      "grad_norm": 4.070727348327637,
      "learning_rate": 3.319368131868132e-05,
      "loss": 0.5639,
      "step": 1712900
    },
    {
      "epoch": 26.891679748822607,
      "grad_norm": 4.4153666496276855,
      "learning_rate": 3.319270015698587e-05,
      "loss": 0.5998,
      "step": 1713000
    },
    {
      "epoch": 26.89324960753532,
      "grad_norm": 4.298240661621094,
      "learning_rate": 3.319171899529043e-05,
      "loss": 0.5476,
      "step": 1713100
    },
    {
      "epoch": 26.894819466248038,
      "grad_norm": 4.50227165222168,
      "learning_rate": 3.3190737833594975e-05,
      "loss": 0.6094,
      "step": 1713200
    },
    {
      "epoch": 26.896389324960754,
      "grad_norm": 4.410279750823975,
      "learning_rate": 3.318975667189953e-05,
      "loss": 0.5984,
      "step": 1713300
    },
    {
      "epoch": 26.897959183673468,
      "grad_norm": 4.68943452835083,
      "learning_rate": 3.318877551020408e-05,
      "loss": 0.5885,
      "step": 1713400
    },
    {
      "epoch": 26.899529042386185,
      "grad_norm": 3.884902000427246,
      "learning_rate": 3.318779434850864e-05,
      "loss": 0.585,
      "step": 1713500
    },
    {
      "epoch": 26.9010989010989,
      "grad_norm": 2.3390774726867676,
      "learning_rate": 3.3186813186813185e-05,
      "loss": 0.528,
      "step": 1713600
    },
    {
      "epoch": 26.90266875981162,
      "grad_norm": 4.66379451751709,
      "learning_rate": 3.318583202511774e-05,
      "loss": 0.5694,
      "step": 1713700
    },
    {
      "epoch": 26.90423861852433,
      "grad_norm": 3.1110479831695557,
      "learning_rate": 3.3184850863422294e-05,
      "loss": 0.6157,
      "step": 1713800
    },
    {
      "epoch": 26.90580847723705,
      "grad_norm": 3.741589069366455,
      "learning_rate": 3.3183869701726845e-05,
      "loss": 0.6164,
      "step": 1713900
    },
    {
      "epoch": 26.907378335949765,
      "grad_norm": 4.48466682434082,
      "learning_rate": 3.3182888540031396e-05,
      "loss": 0.5879,
      "step": 1714000
    },
    {
      "epoch": 26.90894819466248,
      "grad_norm": 3.4856600761413574,
      "learning_rate": 3.3181907378335954e-05,
      "loss": 0.5908,
      "step": 1714100
    },
    {
      "epoch": 26.910518053375196,
      "grad_norm": 4.3023247718811035,
      "learning_rate": 3.3180926216640505e-05,
      "loss": 0.5786,
      "step": 1714200
    },
    {
      "epoch": 26.912087912087912,
      "grad_norm": 4.089395523071289,
      "learning_rate": 3.3179945054945056e-05,
      "loss": 0.6113,
      "step": 1714300
    },
    {
      "epoch": 26.91365777080063,
      "grad_norm": 3.8186850547790527,
      "learning_rate": 3.317896389324961e-05,
      "loss": 0.5894,
      "step": 1714400
    },
    {
      "epoch": 26.915227629513343,
      "grad_norm": 3.1823973655700684,
      "learning_rate": 3.3177982731554164e-05,
      "loss": 0.5988,
      "step": 1714500
    },
    {
      "epoch": 26.91679748822606,
      "grad_norm": 4.514971733093262,
      "learning_rate": 3.3177001569858715e-05,
      "loss": 0.5838,
      "step": 1714600
    },
    {
      "epoch": 26.918367346938776,
      "grad_norm": 4.292608261108398,
      "learning_rate": 3.3176020408163266e-05,
      "loss": 0.5778,
      "step": 1714700
    },
    {
      "epoch": 26.919937205651493,
      "grad_norm": 4.100605010986328,
      "learning_rate": 3.317503924646782e-05,
      "loss": 0.5721,
      "step": 1714800
    },
    {
      "epoch": 26.921507064364206,
      "grad_norm": 3.605991840362549,
      "learning_rate": 3.3174058084772375e-05,
      "loss": 0.628,
      "step": 1714900
    },
    {
      "epoch": 26.923076923076923,
      "grad_norm": 4.409263610839844,
      "learning_rate": 3.3173076923076926e-05,
      "loss": 0.5752,
      "step": 1715000
    },
    {
      "epoch": 26.92464678178964,
      "grad_norm": 3.204721689224243,
      "learning_rate": 3.317209576138148e-05,
      "loss": 0.6131,
      "step": 1715100
    },
    {
      "epoch": 26.926216640502354,
      "grad_norm": 3.948596477508545,
      "learning_rate": 3.3171114599686035e-05,
      "loss": 0.6266,
      "step": 1715200
    },
    {
      "epoch": 26.92778649921507,
      "grad_norm": 2.684844970703125,
      "learning_rate": 3.317013343799058e-05,
      "loss": 0.6216,
      "step": 1715300
    },
    {
      "epoch": 26.929356357927787,
      "grad_norm": 4.478387832641602,
      "learning_rate": 3.316915227629514e-05,
      "loss": 0.6221,
      "step": 1715400
    },
    {
      "epoch": 26.9309262166405,
      "grad_norm": 3.9534664154052734,
      "learning_rate": 3.316817111459969e-05,
      "loss": 0.6018,
      "step": 1715500
    },
    {
      "epoch": 26.932496075353217,
      "grad_norm": 5.155334949493408,
      "learning_rate": 3.3167189952904246e-05,
      "loss": 0.5863,
      "step": 1715600
    },
    {
      "epoch": 26.934065934065934,
      "grad_norm": 4.221421718597412,
      "learning_rate": 3.316620879120879e-05,
      "loss": 0.614,
      "step": 1715700
    },
    {
      "epoch": 26.93563579277865,
      "grad_norm": 3.802088499069214,
      "learning_rate": 3.316522762951335e-05,
      "loss": 0.5875,
      "step": 1715800
    },
    {
      "epoch": 26.937205651491364,
      "grad_norm": 3.0286881923675537,
      "learning_rate": 3.31642464678179e-05,
      "loss": 0.6073,
      "step": 1715900
    },
    {
      "epoch": 26.93877551020408,
      "grad_norm": 3.861542224884033,
      "learning_rate": 3.316326530612245e-05,
      "loss": 0.6053,
      "step": 1716000
    },
    {
      "epoch": 26.940345368916798,
      "grad_norm": 3.797379493713379,
      "learning_rate": 3.3162284144427e-05,
      "loss": 0.5692,
      "step": 1716100
    },
    {
      "epoch": 26.941915227629515,
      "grad_norm": 3.953113317489624,
      "learning_rate": 3.316130298273156e-05,
      "loss": 0.594,
      "step": 1716200
    },
    {
      "epoch": 26.94348508634223,
      "grad_norm": 3.275885820388794,
      "learning_rate": 3.316032182103611e-05,
      "loss": 0.5758,
      "step": 1716300
    },
    {
      "epoch": 26.945054945054945,
      "grad_norm": 3.190612316131592,
      "learning_rate": 3.315934065934066e-05,
      "loss": 0.5861,
      "step": 1716400
    },
    {
      "epoch": 26.946624803767662,
      "grad_norm": 3.978346347808838,
      "learning_rate": 3.315835949764521e-05,
      "loss": 0.6092,
      "step": 1716500
    },
    {
      "epoch": 26.948194662480375,
      "grad_norm": 4.602112293243408,
      "learning_rate": 3.315737833594977e-05,
      "loss": 0.6358,
      "step": 1716600
    },
    {
      "epoch": 26.949764521193092,
      "grad_norm": 3.997403144836426,
      "learning_rate": 3.315639717425432e-05,
      "loss": 0.6067,
      "step": 1716700
    },
    {
      "epoch": 26.95133437990581,
      "grad_norm": 4.3413214683532715,
      "learning_rate": 3.315541601255887e-05,
      "loss": 0.6272,
      "step": 1716800
    },
    {
      "epoch": 26.952904238618526,
      "grad_norm": 4.25388765335083,
      "learning_rate": 3.315443485086342e-05,
      "loss": 0.6082,
      "step": 1716900
    },
    {
      "epoch": 26.95447409733124,
      "grad_norm": 3.6728012561798096,
      "learning_rate": 3.315345368916798e-05,
      "loss": 0.6132,
      "step": 1717000
    },
    {
      "epoch": 26.956043956043956,
      "grad_norm": 3.9388391971588135,
      "learning_rate": 3.315247252747253e-05,
      "loss": 0.6007,
      "step": 1717100
    },
    {
      "epoch": 26.957613814756673,
      "grad_norm": 3.92087721824646,
      "learning_rate": 3.315149136577708e-05,
      "loss": 0.611,
      "step": 1717200
    },
    {
      "epoch": 26.959183673469386,
      "grad_norm": 4.055788040161133,
      "learning_rate": 3.315051020408164e-05,
      "loss": 0.5892,
      "step": 1717300
    },
    {
      "epoch": 26.960753532182103,
      "grad_norm": 4.595353126525879,
      "learning_rate": 3.3149529042386184e-05,
      "loss": 0.5684,
      "step": 1717400
    },
    {
      "epoch": 26.96232339089482,
      "grad_norm": 3.376570701599121,
      "learning_rate": 3.314854788069074e-05,
      "loss": 0.6018,
      "step": 1717500
    },
    {
      "epoch": 26.963893249607537,
      "grad_norm": 4.092550754547119,
      "learning_rate": 3.314756671899529e-05,
      "loss": 0.5737,
      "step": 1717600
    },
    {
      "epoch": 26.96546310832025,
      "grad_norm": 3.6086997985839844,
      "learning_rate": 3.314658555729984e-05,
      "loss": 0.6061,
      "step": 1717700
    },
    {
      "epoch": 26.967032967032967,
      "grad_norm": 3.5586459636688232,
      "learning_rate": 3.3145604395604394e-05,
      "loss": 0.5674,
      "step": 1717800
    },
    {
      "epoch": 26.968602825745684,
      "grad_norm": 3.5288469791412354,
      "learning_rate": 3.314462323390895e-05,
      "loss": 0.5869,
      "step": 1717900
    },
    {
      "epoch": 26.970172684458397,
      "grad_norm": 3.7823486328125,
      "learning_rate": 3.31436420722135e-05,
      "loss": 0.604,
      "step": 1718000
    },
    {
      "epoch": 26.971742543171114,
      "grad_norm": 2.3080193996429443,
      "learning_rate": 3.3142660910518054e-05,
      "loss": 0.5948,
      "step": 1718100
    },
    {
      "epoch": 26.97331240188383,
      "grad_norm": 4.3218674659729,
      "learning_rate": 3.3141679748822605e-05,
      "loss": 0.6289,
      "step": 1718200
    },
    {
      "epoch": 26.974882260596548,
      "grad_norm": 4.387001991271973,
      "learning_rate": 3.314069858712716e-05,
      "loss": 0.6232,
      "step": 1718300
    },
    {
      "epoch": 26.97645211930926,
      "grad_norm": 4.424813747406006,
      "learning_rate": 3.313971742543171e-05,
      "loss": 0.585,
      "step": 1718400
    },
    {
      "epoch": 26.978021978021978,
      "grad_norm": 3.4757022857666016,
      "learning_rate": 3.3138736263736265e-05,
      "loss": 0.5936,
      "step": 1718500
    },
    {
      "epoch": 26.979591836734695,
      "grad_norm": 3.0847954750061035,
      "learning_rate": 3.3137755102040816e-05,
      "loss": 0.6029,
      "step": 1718600
    },
    {
      "epoch": 26.98116169544741,
      "grad_norm": 3.490067720413208,
      "learning_rate": 3.3136773940345373e-05,
      "loss": 0.5497,
      "step": 1718700
    },
    {
      "epoch": 26.982731554160125,
      "grad_norm": 3.370715379714966,
      "learning_rate": 3.3135792778649924e-05,
      "loss": 0.6065,
      "step": 1718800
    },
    {
      "epoch": 26.984301412872842,
      "grad_norm": 3.631256341934204,
      "learning_rate": 3.3134811616954475e-05,
      "loss": 0.6069,
      "step": 1718900
    },
    {
      "epoch": 26.98587127158556,
      "grad_norm": 3.025834798812866,
      "learning_rate": 3.3133830455259026e-05,
      "loss": 0.5949,
      "step": 1719000
    },
    {
      "epoch": 26.987441130298272,
      "grad_norm": 3.905313491821289,
      "learning_rate": 3.313284929356358e-05,
      "loss": 0.6245,
      "step": 1719100
    },
    {
      "epoch": 26.98901098901099,
      "grad_norm": 3.6004230976104736,
      "learning_rate": 3.3131868131868135e-05,
      "loss": 0.6074,
      "step": 1719200
    },
    {
      "epoch": 26.990580847723706,
      "grad_norm": 3.2252354621887207,
      "learning_rate": 3.3130886970172686e-05,
      "loss": 0.5835,
      "step": 1719300
    },
    {
      "epoch": 26.99215070643642,
      "grad_norm": 4.14991569519043,
      "learning_rate": 3.3129905808477244e-05,
      "loss": 0.5858,
      "step": 1719400
    },
    {
      "epoch": 26.993720565149136,
      "grad_norm": 2.594480514526367,
      "learning_rate": 3.312892464678179e-05,
      "loss": 0.5689,
      "step": 1719500
    },
    {
      "epoch": 26.995290423861853,
      "grad_norm": 4.426318645477295,
      "learning_rate": 3.3127943485086346e-05,
      "loss": 0.5493,
      "step": 1719600
    },
    {
      "epoch": 26.99686028257457,
      "grad_norm": 2.446702241897583,
      "learning_rate": 3.31269623233909e-05,
      "loss": 0.5922,
      "step": 1719700
    },
    {
      "epoch": 26.998430141287283,
      "grad_norm": 3.897296905517578,
      "learning_rate": 3.312598116169545e-05,
      "loss": 0.6036,
      "step": 1719800
    },
    {
      "epoch": 27.0,
      "grad_norm": 3.402951717376709,
      "learning_rate": 3.3125e-05,
      "loss": 0.5678,
      "step": 1719900
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.0238230228424072,
      "eval_runtime": 14.6124,
      "eval_samples_per_second": 229.462,
      "eval_steps_per_second": 229.462,
      "step": 1719900
    },
    {
      "epoch": 27.0,
      "eval_loss": 0.45736852288246155,
      "eval_runtime": 279.8174,
      "eval_samples_per_second": 227.648,
      "eval_steps_per_second": 227.648,
      "step": 1719900
    },
    {
      "epoch": 27.001569858712717,
      "grad_norm": 3.9251556396484375,
      "learning_rate": 3.3124018838304557e-05,
      "loss": 0.6005,
      "step": 1720000
    },
    {
      "epoch": 27.00313971742543,
      "grad_norm": 2.5411605834960938,
      "learning_rate": 3.312303767660911e-05,
      "loss": 0.5716,
      "step": 1720100
    },
    {
      "epoch": 27.004709576138147,
      "grad_norm": 4.193665504455566,
      "learning_rate": 3.312205651491366e-05,
      "loss": 0.5997,
      "step": 1720200
    },
    {
      "epoch": 27.006279434850864,
      "grad_norm": 4.690682888031006,
      "learning_rate": 3.312107535321821e-05,
      "loss": 0.5752,
      "step": 1720300
    },
    {
      "epoch": 27.00784929356358,
      "grad_norm": 3.9253060817718506,
      "learning_rate": 3.312009419152277e-05,
      "loss": 0.5712,
      "step": 1720400
    },
    {
      "epoch": 27.009419152276294,
      "grad_norm": 3.5169520378112793,
      "learning_rate": 3.311911302982731e-05,
      "loss": 0.6134,
      "step": 1720500
    },
    {
      "epoch": 27.01098901098901,
      "grad_norm": 3.438861846923828,
      "learning_rate": 3.311813186813187e-05,
      "loss": 0.5505,
      "step": 1720600
    },
    {
      "epoch": 27.012558869701728,
      "grad_norm": 2.005106210708618,
      "learning_rate": 3.311715070643642e-05,
      "loss": 0.6028,
      "step": 1720700
    },
    {
      "epoch": 27.01412872841444,
      "grad_norm": 5.359515190124512,
      "learning_rate": 3.311616954474098e-05,
      "loss": 0.588,
      "step": 1720800
    },
    {
      "epoch": 27.015698587127158,
      "grad_norm": 4.3328094482421875,
      "learning_rate": 3.311518838304553e-05,
      "loss": 0.5727,
      "step": 1720900
    },
    {
      "epoch": 27.017268445839875,
      "grad_norm": 3.3572027683258057,
      "learning_rate": 3.311420722135008e-05,
      "loss": 0.5738,
      "step": 1721000
    },
    {
      "epoch": 27.01883830455259,
      "grad_norm": 2.9190897941589355,
      "learning_rate": 3.311322605965463e-05,
      "loss": 0.5836,
      "step": 1721100
    },
    {
      "epoch": 27.020408163265305,
      "grad_norm": 3.4752962589263916,
      "learning_rate": 3.311224489795918e-05,
      "loss": 0.6283,
      "step": 1721200
    },
    {
      "epoch": 27.021978021978022,
      "grad_norm": 3.5007193088531494,
      "learning_rate": 3.311126373626374e-05,
      "loss": 0.599,
      "step": 1721300
    },
    {
      "epoch": 27.02354788069074,
      "grad_norm": 3.3883585929870605,
      "learning_rate": 3.311028257456829e-05,
      "loss": 0.6208,
      "step": 1721400
    },
    {
      "epoch": 27.025117739403452,
      "grad_norm": 3.8542487621307373,
      "learning_rate": 3.310930141287285e-05,
      "loss": 0.5492,
      "step": 1721500
    },
    {
      "epoch": 27.02668759811617,
      "grad_norm": 3.6912193298339844,
      "learning_rate": 3.310832025117739e-05,
      "loss": 0.5951,
      "step": 1721600
    },
    {
      "epoch": 27.028257456828886,
      "grad_norm": 4.265589714050293,
      "learning_rate": 3.310733908948195e-05,
      "loss": 0.6135,
      "step": 1721700
    },
    {
      "epoch": 27.029827315541603,
      "grad_norm": 2.83064866065979,
      "learning_rate": 3.31063579277865e-05,
      "loss": 0.622,
      "step": 1721800
    },
    {
      "epoch": 27.031397174254316,
      "grad_norm": 4.371159553527832,
      "learning_rate": 3.310537676609105e-05,
      "loss": 0.563,
      "step": 1721900
    },
    {
      "epoch": 27.032967032967033,
      "grad_norm": 4.577475070953369,
      "learning_rate": 3.31043956043956e-05,
      "loss": 0.5665,
      "step": 1722000
    },
    {
      "epoch": 27.03453689167975,
      "grad_norm": 3.599153757095337,
      "learning_rate": 3.310341444270016e-05,
      "loss": 0.5851,
      "step": 1722100
    },
    {
      "epoch": 27.036106750392463,
      "grad_norm": 3.262786626815796,
      "learning_rate": 3.310243328100471e-05,
      "loss": 0.5869,
      "step": 1722200
    },
    {
      "epoch": 27.03767660910518,
      "grad_norm": 3.343383312225342,
      "learning_rate": 3.310145211930926e-05,
      "loss": 0.5516,
      "step": 1722300
    },
    {
      "epoch": 27.039246467817897,
      "grad_norm": 4.620818614959717,
      "learning_rate": 3.3100470957613814e-05,
      "loss": 0.5801,
      "step": 1722400
    },
    {
      "epoch": 27.040816326530614,
      "grad_norm": 4.215632438659668,
      "learning_rate": 3.309948979591837e-05,
      "loss": 0.5944,
      "step": 1722500
    },
    {
      "epoch": 27.042386185243327,
      "grad_norm": 3.0685017108917236,
      "learning_rate": 3.3098508634222916e-05,
      "loss": 0.5627,
      "step": 1722600
    },
    {
      "epoch": 27.043956043956044,
      "grad_norm": 2.9911303520202637,
      "learning_rate": 3.3097527472527474e-05,
      "loss": 0.5891,
      "step": 1722700
    },
    {
      "epoch": 27.04552590266876,
      "grad_norm": 4.798209190368652,
      "learning_rate": 3.3096546310832025e-05,
      "loss": 0.6133,
      "step": 1722800
    },
    {
      "epoch": 27.047095761381474,
      "grad_norm": 3.002875566482544,
      "learning_rate": 3.309556514913658e-05,
      "loss": 0.6025,
      "step": 1722900
    },
    {
      "epoch": 27.04866562009419,
      "grad_norm": 4.2390546798706055,
      "learning_rate": 3.3094583987441133e-05,
      "loss": 0.5567,
      "step": 1723000
    },
    {
      "epoch": 27.050235478806908,
      "grad_norm": 3.3924977779388428,
      "learning_rate": 3.3093602825745684e-05,
      "loss": 0.5794,
      "step": 1723100
    },
    {
      "epoch": 27.051805337519625,
      "grad_norm": 3.563079595565796,
      "learning_rate": 3.3092621664050235e-05,
      "loss": 0.5764,
      "step": 1723200
    },
    {
      "epoch": 27.053375196232338,
      "grad_norm": 3.594907283782959,
      "learning_rate": 3.3091640502354786e-05,
      "loss": 0.5781,
      "step": 1723300
    },
    {
      "epoch": 27.054945054945055,
      "grad_norm": 3.3409507274627686,
      "learning_rate": 3.3090659340659344e-05,
      "loss": 0.5902,
      "step": 1723400
    },
    {
      "epoch": 27.05651491365777,
      "grad_norm": 4.297456741333008,
      "learning_rate": 3.3089678178963895e-05,
      "loss": 0.6088,
      "step": 1723500
    },
    {
      "epoch": 27.058084772370485,
      "grad_norm": 3.839958429336548,
      "learning_rate": 3.308869701726845e-05,
      "loss": 0.5811,
      "step": 1723600
    },
    {
      "epoch": 27.059654631083202,
      "grad_norm": 3.0052947998046875,
      "learning_rate": 3.3087715855573e-05,
      "loss": 0.6152,
      "step": 1723700
    },
    {
      "epoch": 27.06122448979592,
      "grad_norm": 3.700340986251831,
      "learning_rate": 3.3086734693877555e-05,
      "loss": 0.598,
      "step": 1723800
    },
    {
      "epoch": 27.062794348508636,
      "grad_norm": 2.9176676273345947,
      "learning_rate": 3.3085753532182106e-05,
      "loss": 0.558,
      "step": 1723900
    },
    {
      "epoch": 27.06436420722135,
      "grad_norm": 3.9569830894470215,
      "learning_rate": 3.308477237048666e-05,
      "loss": 0.5651,
      "step": 1724000
    },
    {
      "epoch": 27.065934065934066,
      "grad_norm": 4.129418849945068,
      "learning_rate": 3.308379120879121e-05,
      "loss": 0.5826,
      "step": 1724100
    },
    {
      "epoch": 27.067503924646783,
      "grad_norm": 4.107536792755127,
      "learning_rate": 3.3082810047095766e-05,
      "loss": 0.6154,
      "step": 1724200
    },
    {
      "epoch": 27.069073783359496,
      "grad_norm": 3.6765377521514893,
      "learning_rate": 3.3081828885400317e-05,
      "loss": 0.5693,
      "step": 1724300
    },
    {
      "epoch": 27.070643642072213,
      "grad_norm": 2.402724504470825,
      "learning_rate": 3.308084772370487e-05,
      "loss": 0.584,
      "step": 1724400
    },
    {
      "epoch": 27.07221350078493,
      "grad_norm": 5.224183082580566,
      "learning_rate": 3.307986656200942e-05,
      "loss": 0.5479,
      "step": 1724500
    },
    {
      "epoch": 27.073783359497646,
      "grad_norm": 3.8754189014434814,
      "learning_rate": 3.3078885400313976e-05,
      "loss": 0.6301,
      "step": 1724600
    },
    {
      "epoch": 27.07535321821036,
      "grad_norm": 3.032453775405884,
      "learning_rate": 3.307790423861852e-05,
      "loss": 0.5844,
      "step": 1724700
    },
    {
      "epoch": 27.076923076923077,
      "grad_norm": 3.7287893295288086,
      "learning_rate": 3.307692307692308e-05,
      "loss": 0.5822,
      "step": 1724800
    },
    {
      "epoch": 27.078492935635794,
      "grad_norm": 3.444333076477051,
      "learning_rate": 3.307594191522763e-05,
      "loss": 0.6077,
      "step": 1724900
    },
    {
      "epoch": 27.08006279434851,
      "grad_norm": 3.9075634479522705,
      "learning_rate": 3.307496075353219e-05,
      "loss": 0.5772,
      "step": 1725000
    },
    {
      "epoch": 27.081632653061224,
      "grad_norm": 3.4898204803466797,
      "learning_rate": 3.307397959183674e-05,
      "loss": 0.5768,
      "step": 1725100
    },
    {
      "epoch": 27.08320251177394,
      "grad_norm": 3.605043888092041,
      "learning_rate": 3.307299843014129e-05,
      "loss": 0.6388,
      "step": 1725200
    },
    {
      "epoch": 27.084772370486657,
      "grad_norm": 3.687232494354248,
      "learning_rate": 3.307201726844584e-05,
      "loss": 0.6161,
      "step": 1725300
    },
    {
      "epoch": 27.08634222919937,
      "grad_norm": 3.7111923694610596,
      "learning_rate": 3.307103610675039e-05,
      "loss": 0.5948,
      "step": 1725400
    },
    {
      "epoch": 27.087912087912088,
      "grad_norm": 3.895012617111206,
      "learning_rate": 3.307005494505495e-05,
      "loss": 0.5323,
      "step": 1725500
    },
    {
      "epoch": 27.089481946624804,
      "grad_norm": 4.588683128356934,
      "learning_rate": 3.30690737833595e-05,
      "loss": 0.6085,
      "step": 1725600
    },
    {
      "epoch": 27.09105180533752,
      "grad_norm": 4.690596580505371,
      "learning_rate": 3.306809262166406e-05,
      "loss": 0.5903,
      "step": 1725700
    },
    {
      "epoch": 27.092621664050235,
      "grad_norm": 4.958725929260254,
      "learning_rate": 3.30671114599686e-05,
      "loss": 0.6118,
      "step": 1725800
    },
    {
      "epoch": 27.09419152276295,
      "grad_norm": 4.052257537841797,
      "learning_rate": 3.306613029827316e-05,
      "loss": 0.5627,
      "step": 1725900
    },
    {
      "epoch": 27.09576138147567,
      "grad_norm": 3.405533790588379,
      "learning_rate": 3.306514913657771e-05,
      "loss": 0.5933,
      "step": 1726000
    },
    {
      "epoch": 27.09733124018838,
      "grad_norm": 2.708773612976074,
      "learning_rate": 3.306416797488226e-05,
      "loss": 0.5535,
      "step": 1726100
    },
    {
      "epoch": 27.0989010989011,
      "grad_norm": 3.9706990718841553,
      "learning_rate": 3.306318681318681e-05,
      "loss": 0.6133,
      "step": 1726200
    },
    {
      "epoch": 27.100470957613815,
      "grad_norm": 3.2864489555358887,
      "learning_rate": 3.306220565149137e-05,
      "loss": 0.5999,
      "step": 1726300
    },
    {
      "epoch": 27.102040816326532,
      "grad_norm": 3.837294340133667,
      "learning_rate": 3.306122448979592e-05,
      "loss": 0.5327,
      "step": 1726400
    },
    {
      "epoch": 27.103610675039246,
      "grad_norm": 5.033390522003174,
      "learning_rate": 3.306024332810047e-05,
      "loss": 0.5591,
      "step": 1726500
    },
    {
      "epoch": 27.105180533751962,
      "grad_norm": 3.983604907989502,
      "learning_rate": 3.305926216640502e-05,
      "loss": 0.5293,
      "step": 1726600
    },
    {
      "epoch": 27.10675039246468,
      "grad_norm": 3.626016855239868,
      "learning_rate": 3.305828100470958e-05,
      "loss": 0.5467,
      "step": 1726700
    },
    {
      "epoch": 27.108320251177393,
      "grad_norm": 4.333780765533447,
      "learning_rate": 3.3057299843014125e-05,
      "loss": 0.5989,
      "step": 1726800
    },
    {
      "epoch": 27.10989010989011,
      "grad_norm": 3.3661248683929443,
      "learning_rate": 3.305631868131868e-05,
      "loss": 0.5462,
      "step": 1726900
    },
    {
      "epoch": 27.111459968602826,
      "grad_norm": 4.222697734832764,
      "learning_rate": 3.3055337519623234e-05,
      "loss": 0.5766,
      "step": 1727000
    },
    {
      "epoch": 27.113029827315543,
      "grad_norm": 3.947950839996338,
      "learning_rate": 3.305435635792779e-05,
      "loss": 0.6128,
      "step": 1727100
    },
    {
      "epoch": 27.114599686028257,
      "grad_norm": 3.987415313720703,
      "learning_rate": 3.305337519623234e-05,
      "loss": 0.5964,
      "step": 1727200
    },
    {
      "epoch": 27.116169544740973,
      "grad_norm": 2.8020284175872803,
      "learning_rate": 3.305239403453689e-05,
      "loss": 0.6009,
      "step": 1727300
    },
    {
      "epoch": 27.11773940345369,
      "grad_norm": 3.294398546218872,
      "learning_rate": 3.3051412872841444e-05,
      "loss": 0.5658,
      "step": 1727400
    },
    {
      "epoch": 27.119309262166404,
      "grad_norm": 2.58137583732605,
      "learning_rate": 3.3050431711145995e-05,
      "loss": 0.583,
      "step": 1727500
    },
    {
      "epoch": 27.12087912087912,
      "grad_norm": 3.472792625427246,
      "learning_rate": 3.304945054945055e-05,
      "loss": 0.6022,
      "step": 1727600
    },
    {
      "epoch": 27.122448979591837,
      "grad_norm": 4.179670333862305,
      "learning_rate": 3.3048469387755104e-05,
      "loss": 0.6209,
      "step": 1727700
    },
    {
      "epoch": 27.124018838304554,
      "grad_norm": 3.751326322555542,
      "learning_rate": 3.304748822605966e-05,
      "loss": 0.6222,
      "step": 1727800
    },
    {
      "epoch": 27.125588697017267,
      "grad_norm": 3.5233688354492188,
      "learning_rate": 3.3046507064364206e-05,
      "loss": 0.5671,
      "step": 1727900
    },
    {
      "epoch": 27.127158555729984,
      "grad_norm": 3.714775800704956,
      "learning_rate": 3.3045525902668764e-05,
      "loss": 0.6112,
      "step": 1728000
    },
    {
      "epoch": 27.1287284144427,
      "grad_norm": 4.141468524932861,
      "learning_rate": 3.3044544740973315e-05,
      "loss": 0.6,
      "step": 1728100
    },
    {
      "epoch": 27.130298273155415,
      "grad_norm": 4.492346286773682,
      "learning_rate": 3.3043563579277866e-05,
      "loss": 0.6232,
      "step": 1728200
    },
    {
      "epoch": 27.13186813186813,
      "grad_norm": 3.91933274269104,
      "learning_rate": 3.304258241758242e-05,
      "loss": 0.5653,
      "step": 1728300
    },
    {
      "epoch": 27.13343799058085,
      "grad_norm": 5.085147380828857,
      "learning_rate": 3.3041601255886975e-05,
      "loss": 0.5862,
      "step": 1728400
    },
    {
      "epoch": 27.135007849293565,
      "grad_norm": 4.5562744140625,
      "learning_rate": 3.3040620094191525e-05,
      "loss": 0.5631,
      "step": 1728500
    },
    {
      "epoch": 27.13657770800628,
      "grad_norm": 4.23522424697876,
      "learning_rate": 3.3039638932496076e-05,
      "loss": 0.5815,
      "step": 1728600
    },
    {
      "epoch": 27.138147566718995,
      "grad_norm": 4.032148838043213,
      "learning_rate": 3.303865777080063e-05,
      "loss": 0.5879,
      "step": 1728700
    },
    {
      "epoch": 27.139717425431712,
      "grad_norm": 4.077276706695557,
      "learning_rate": 3.3037676609105185e-05,
      "loss": 0.5843,
      "step": 1728800
    },
    {
      "epoch": 27.141287284144425,
      "grad_norm": 3.9623539447784424,
      "learning_rate": 3.303669544740973e-05,
      "loss": 0.5902,
      "step": 1728900
    },
    {
      "epoch": 27.142857142857142,
      "grad_norm": 3.2160887718200684,
      "learning_rate": 3.303571428571429e-05,
      "loss": 0.6163,
      "step": 1729000
    },
    {
      "epoch": 27.14442700156986,
      "grad_norm": 4.039916515350342,
      "learning_rate": 3.303473312401884e-05,
      "loss": 0.6297,
      "step": 1729100
    },
    {
      "epoch": 27.145996860282576,
      "grad_norm": 3.5022265911102295,
      "learning_rate": 3.3033751962323396e-05,
      "loss": 0.5969,
      "step": 1729200
    },
    {
      "epoch": 27.14756671899529,
      "grad_norm": 3.760892152786255,
      "learning_rate": 3.303277080062794e-05,
      "loss": 0.5872,
      "step": 1729300
    },
    {
      "epoch": 27.149136577708006,
      "grad_norm": 3.556664228439331,
      "learning_rate": 3.30317896389325e-05,
      "loss": 0.6305,
      "step": 1729400
    },
    {
      "epoch": 27.150706436420723,
      "grad_norm": 4.45186710357666,
      "learning_rate": 3.303080847723705e-05,
      "loss": 0.5954,
      "step": 1729500
    },
    {
      "epoch": 27.152276295133436,
      "grad_norm": 5.766173362731934,
      "learning_rate": 3.30298273155416e-05,
      "loss": 0.5944,
      "step": 1729600
    },
    {
      "epoch": 27.153846153846153,
      "grad_norm": 3.6874752044677734,
      "learning_rate": 3.302884615384616e-05,
      "loss": 0.6177,
      "step": 1729700
    },
    {
      "epoch": 27.15541601255887,
      "grad_norm": 3.6342313289642334,
      "learning_rate": 3.302786499215071e-05,
      "loss": 0.5965,
      "step": 1729800
    },
    {
      "epoch": 27.156985871271587,
      "grad_norm": 4.031383991241455,
      "learning_rate": 3.3026883830455266e-05,
      "loss": 0.5999,
      "step": 1729900
    },
    {
      "epoch": 27.1585557299843,
      "grad_norm": 3.7819337844848633,
      "learning_rate": 3.302590266875981e-05,
      "loss": 0.6,
      "step": 1730000
    },
    {
      "epoch": 27.160125588697017,
      "grad_norm": 3.2504539489746094,
      "learning_rate": 3.302492150706437e-05,
      "loss": 0.5679,
      "step": 1730100
    },
    {
      "epoch": 27.161695447409734,
      "grad_norm": 2.7352590560913086,
      "learning_rate": 3.302394034536892e-05,
      "loss": 0.6187,
      "step": 1730200
    },
    {
      "epoch": 27.163265306122447,
      "grad_norm": 2.56526255607605,
      "learning_rate": 3.302295918367347e-05,
      "loss": 0.5972,
      "step": 1730300
    },
    {
      "epoch": 27.164835164835164,
      "grad_norm": 3.587207078933716,
      "learning_rate": 3.302197802197802e-05,
      "loss": 0.6003,
      "step": 1730400
    },
    {
      "epoch": 27.16640502354788,
      "grad_norm": 3.3077096939086914,
      "learning_rate": 3.302099686028258e-05,
      "loss": 0.6365,
      "step": 1730500
    },
    {
      "epoch": 27.167974882260598,
      "grad_norm": 3.550887107849121,
      "learning_rate": 3.302001569858713e-05,
      "loss": 0.609,
      "step": 1730600
    },
    {
      "epoch": 27.16954474097331,
      "grad_norm": 4.202388763427734,
      "learning_rate": 3.301903453689168e-05,
      "loss": 0.5787,
      "step": 1730700
    },
    {
      "epoch": 27.171114599686028,
      "grad_norm": 2.4918296337127686,
      "learning_rate": 3.301805337519623e-05,
      "loss": 0.6023,
      "step": 1730800
    },
    {
      "epoch": 27.172684458398745,
      "grad_norm": 4.598504543304443,
      "learning_rate": 3.301707221350079e-05,
      "loss": 0.5672,
      "step": 1730900
    },
    {
      "epoch": 27.17425431711146,
      "grad_norm": 4.9160237312316895,
      "learning_rate": 3.3016091051805334e-05,
      "loss": 0.5844,
      "step": 1731000
    },
    {
      "epoch": 27.175824175824175,
      "grad_norm": 4.198727607727051,
      "learning_rate": 3.301510989010989e-05,
      "loss": 0.5842,
      "step": 1731100
    },
    {
      "epoch": 27.177394034536892,
      "grad_norm": 3.6544697284698486,
      "learning_rate": 3.301412872841444e-05,
      "loss": 0.5861,
      "step": 1731200
    },
    {
      "epoch": 27.17896389324961,
      "grad_norm": 6.635985851287842,
      "learning_rate": 3.3013147566719e-05,
      "loss": 0.5941,
      "step": 1731300
    },
    {
      "epoch": 27.180533751962322,
      "grad_norm": 4.58676290512085,
      "learning_rate": 3.3012166405023545e-05,
      "loss": 0.6212,
      "step": 1731400
    },
    {
      "epoch": 27.18210361067504,
      "grad_norm": 4.29063606262207,
      "learning_rate": 3.30111852433281e-05,
      "loss": 0.5942,
      "step": 1731500
    },
    {
      "epoch": 27.183673469387756,
      "grad_norm": 3.639549970626831,
      "learning_rate": 3.301020408163265e-05,
      "loss": 0.6016,
      "step": 1731600
    },
    {
      "epoch": 27.18524332810047,
      "grad_norm": 2.4018516540527344,
      "learning_rate": 3.3009222919937204e-05,
      "loss": 0.5762,
      "step": 1731700
    },
    {
      "epoch": 27.186813186813186,
      "grad_norm": 3.970289945602417,
      "learning_rate": 3.300824175824176e-05,
      "loss": 0.6062,
      "step": 1731800
    },
    {
      "epoch": 27.188383045525903,
      "grad_norm": 3.85245943069458,
      "learning_rate": 3.300726059654631e-05,
      "loss": 0.6087,
      "step": 1731900
    },
    {
      "epoch": 27.18995290423862,
      "grad_norm": 3.181361675262451,
      "learning_rate": 3.300627943485087e-05,
      "loss": 0.6082,
      "step": 1732000
    },
    {
      "epoch": 27.191522762951333,
      "grad_norm": 4.5429463386535645,
      "learning_rate": 3.3005298273155415e-05,
      "loss": 0.6009,
      "step": 1732100
    },
    {
      "epoch": 27.19309262166405,
      "grad_norm": 4.5381903648376465,
      "learning_rate": 3.300431711145997e-05,
      "loss": 0.5888,
      "step": 1732200
    },
    {
      "epoch": 27.194662480376767,
      "grad_norm": 3.932878017425537,
      "learning_rate": 3.3003335949764524e-05,
      "loss": 0.5787,
      "step": 1732300
    },
    {
      "epoch": 27.19623233908948,
      "grad_norm": 4.127313137054443,
      "learning_rate": 3.3002354788069075e-05,
      "loss": 0.5758,
      "step": 1732400
    },
    {
      "epoch": 27.197802197802197,
      "grad_norm": 3.11114501953125,
      "learning_rate": 3.3001373626373626e-05,
      "loss": 0.6132,
      "step": 1732500
    },
    {
      "epoch": 27.199372056514914,
      "grad_norm": 4.349721431732178,
      "learning_rate": 3.3000392464678183e-05,
      "loss": 0.5928,
      "step": 1732600
    },
    {
      "epoch": 27.20094191522763,
      "grad_norm": 2.1942367553710938,
      "learning_rate": 3.2999411302982734e-05,
      "loss": 0.5719,
      "step": 1732700
    },
    {
      "epoch": 27.202511773940344,
      "grad_norm": 3.9399867057800293,
      "learning_rate": 3.2998430141287285e-05,
      "loss": 0.5761,
      "step": 1732800
    },
    {
      "epoch": 27.20408163265306,
      "grad_norm": 4.448546886444092,
      "learning_rate": 3.2997448979591836e-05,
      "loss": 0.6009,
      "step": 1732900
    },
    {
      "epoch": 27.205651491365778,
      "grad_norm": 4.371993064880371,
      "learning_rate": 3.2996467817896394e-05,
      "loss": 0.6181,
      "step": 1733000
    },
    {
      "epoch": 27.20722135007849,
      "grad_norm": 4.560823440551758,
      "learning_rate": 3.299548665620094e-05,
      "loss": 0.6164,
      "step": 1733100
    },
    {
      "epoch": 27.208791208791208,
      "grad_norm": 3.9709677696228027,
      "learning_rate": 3.2994505494505496e-05,
      "loss": 0.5799,
      "step": 1733200
    },
    {
      "epoch": 27.210361067503925,
      "grad_norm": 3.079771041870117,
      "learning_rate": 3.299352433281005e-05,
      "loss": 0.5946,
      "step": 1733300
    },
    {
      "epoch": 27.211930926216642,
      "grad_norm": 4.052165508270264,
      "learning_rate": 3.2992543171114605e-05,
      "loss": 0.6097,
      "step": 1733400
    },
    {
      "epoch": 27.213500784929355,
      "grad_norm": 3.8841359615325928,
      "learning_rate": 3.299156200941915e-05,
      "loss": 0.5938,
      "step": 1733500
    },
    {
      "epoch": 27.215070643642072,
      "grad_norm": 4.266352653503418,
      "learning_rate": 3.299058084772371e-05,
      "loss": 0.6029,
      "step": 1733600
    },
    {
      "epoch": 27.21664050235479,
      "grad_norm": 3.874068260192871,
      "learning_rate": 3.298959968602826e-05,
      "loss": 0.5859,
      "step": 1733700
    },
    {
      "epoch": 27.218210361067506,
      "grad_norm": 3.6277530193328857,
      "learning_rate": 3.298861852433281e-05,
      "loss": 0.5518,
      "step": 1733800
    },
    {
      "epoch": 27.21978021978022,
      "grad_norm": 3.5512535572052,
      "learning_rate": 3.2987637362637367e-05,
      "loss": 0.6143,
      "step": 1733900
    },
    {
      "epoch": 27.221350078492936,
      "grad_norm": 3.5449211597442627,
      "learning_rate": 3.298665620094192e-05,
      "loss": 0.6092,
      "step": 1734000
    },
    {
      "epoch": 27.222919937205653,
      "grad_norm": 3.9196157455444336,
      "learning_rate": 3.2985675039246475e-05,
      "loss": 0.5492,
      "step": 1734100
    },
    {
      "epoch": 27.224489795918366,
      "grad_norm": 3.8035738468170166,
      "learning_rate": 3.298469387755102e-05,
      "loss": 0.5902,
      "step": 1734200
    },
    {
      "epoch": 27.226059654631083,
      "grad_norm": 2.958444595336914,
      "learning_rate": 3.298371271585558e-05,
      "loss": 0.5977,
      "step": 1734300
    },
    {
      "epoch": 27.2276295133438,
      "grad_norm": 4.418500900268555,
      "learning_rate": 3.298273155416013e-05,
      "loss": 0.5972,
      "step": 1734400
    },
    {
      "epoch": 27.229199372056517,
      "grad_norm": 3.896671772003174,
      "learning_rate": 3.298175039246468e-05,
      "loss": 0.5632,
      "step": 1734500
    },
    {
      "epoch": 27.23076923076923,
      "grad_norm": 4.5895233154296875,
      "learning_rate": 3.298076923076923e-05,
      "loss": 0.6163,
      "step": 1734600
    },
    {
      "epoch": 27.232339089481947,
      "grad_norm": 3.7686314582824707,
      "learning_rate": 3.297978806907379e-05,
      "loss": 0.5818,
      "step": 1734700
    },
    {
      "epoch": 27.233908948194664,
      "grad_norm": 3.4963088035583496,
      "learning_rate": 3.297880690737834e-05,
      "loss": 0.5887,
      "step": 1734800
    },
    {
      "epoch": 27.235478806907377,
      "grad_norm": 4.4743547439575195,
      "learning_rate": 3.297782574568289e-05,
      "loss": 0.5706,
      "step": 1734900
    },
    {
      "epoch": 27.237048665620094,
      "grad_norm": 4.026907920837402,
      "learning_rate": 3.297684458398744e-05,
      "loss": 0.5169,
      "step": 1735000
    },
    {
      "epoch": 27.23861852433281,
      "grad_norm": 4.493170261383057,
      "learning_rate": 3.2975863422292e-05,
      "loss": 0.5746,
      "step": 1735100
    },
    {
      "epoch": 27.240188383045528,
      "grad_norm": 3.8812339305877686,
      "learning_rate": 3.297488226059654e-05,
      "loss": 0.6335,
      "step": 1735200
    },
    {
      "epoch": 27.24175824175824,
      "grad_norm": 3.911362886428833,
      "learning_rate": 3.29739010989011e-05,
      "loss": 0.5761,
      "step": 1735300
    },
    {
      "epoch": 27.243328100470958,
      "grad_norm": 3.8557486534118652,
      "learning_rate": 3.297291993720565e-05,
      "loss": 0.601,
      "step": 1735400
    },
    {
      "epoch": 27.244897959183675,
      "grad_norm": 4.086355209350586,
      "learning_rate": 3.297193877551021e-05,
      "loss": 0.5951,
      "step": 1735500
    },
    {
      "epoch": 27.246467817896388,
      "grad_norm": 3.7593040466308594,
      "learning_rate": 3.2970957613814754e-05,
      "loss": 0.5996,
      "step": 1735600
    },
    {
      "epoch": 27.248037676609105,
      "grad_norm": 3.4167516231536865,
      "learning_rate": 3.296997645211931e-05,
      "loss": 0.6269,
      "step": 1735700
    },
    {
      "epoch": 27.24960753532182,
      "grad_norm": 4.440598011016846,
      "learning_rate": 3.296899529042386e-05,
      "loss": 0.5485,
      "step": 1735800
    },
    {
      "epoch": 27.25117739403454,
      "grad_norm": 4.246664047241211,
      "learning_rate": 3.296801412872841e-05,
      "loss": 0.5765,
      "step": 1735900
    },
    {
      "epoch": 27.252747252747252,
      "grad_norm": 3.6575255393981934,
      "learning_rate": 3.296703296703297e-05,
      "loss": 0.6154,
      "step": 1736000
    },
    {
      "epoch": 27.25431711145997,
      "grad_norm": 2.4442782402038574,
      "learning_rate": 3.296605180533752e-05,
      "loss": 0.6035,
      "step": 1736100
    },
    {
      "epoch": 27.255886970172686,
      "grad_norm": 4.9032979011535645,
      "learning_rate": 3.296507064364208e-05,
      "loss": 0.5908,
      "step": 1736200
    },
    {
      "epoch": 27.2574568288854,
      "grad_norm": 3.219177722930908,
      "learning_rate": 3.2964089481946624e-05,
      "loss": 0.5933,
      "step": 1736300
    },
    {
      "epoch": 27.259026687598116,
      "grad_norm": 3.646963357925415,
      "learning_rate": 3.296310832025118e-05,
      "loss": 0.6092,
      "step": 1736400
    },
    {
      "epoch": 27.260596546310833,
      "grad_norm": 3.250474214553833,
      "learning_rate": 3.296212715855573e-05,
      "loss": 0.5867,
      "step": 1736500
    },
    {
      "epoch": 27.26216640502355,
      "grad_norm": 4.133598327636719,
      "learning_rate": 3.2961145996860284e-05,
      "loss": 0.5608,
      "step": 1736600
    },
    {
      "epoch": 27.263736263736263,
      "grad_norm": 3.797905445098877,
      "learning_rate": 3.2960164835164835e-05,
      "loss": 0.6219,
      "step": 1736700
    },
    {
      "epoch": 27.26530612244898,
      "grad_norm": 5.317196846008301,
      "learning_rate": 3.295918367346939e-05,
      "loss": 0.5913,
      "step": 1736800
    },
    {
      "epoch": 27.266875981161697,
      "grad_norm": 2.890270233154297,
      "learning_rate": 3.2958202511773943e-05,
      "loss": 0.6202,
      "step": 1736900
    },
    {
      "epoch": 27.26844583987441,
      "grad_norm": 4.708497524261475,
      "learning_rate": 3.2957221350078494e-05,
      "loss": 0.5988,
      "step": 1737000
    },
    {
      "epoch": 27.270015698587127,
      "grad_norm": 4.089850902557373,
      "learning_rate": 3.2956240188383045e-05,
      "loss": 0.5827,
      "step": 1737100
    },
    {
      "epoch": 27.271585557299844,
      "grad_norm": 3.342792272567749,
      "learning_rate": 3.29552590266876e-05,
      "loss": 0.554,
      "step": 1737200
    },
    {
      "epoch": 27.27315541601256,
      "grad_norm": 4.002642631530762,
      "learning_rate": 3.295427786499215e-05,
      "loss": 0.6022,
      "step": 1737300
    },
    {
      "epoch": 27.274725274725274,
      "grad_norm": 3.657918930053711,
      "learning_rate": 3.2953296703296705e-05,
      "loss": 0.5639,
      "step": 1737400
    },
    {
      "epoch": 27.27629513343799,
      "grad_norm": 3.433594226837158,
      "learning_rate": 3.2952315541601256e-05,
      "loss": 0.6344,
      "step": 1737500
    },
    {
      "epoch": 27.277864992150707,
      "grad_norm": 4.415144920349121,
      "learning_rate": 3.2951334379905814e-05,
      "loss": 0.5776,
      "step": 1737600
    },
    {
      "epoch": 27.27943485086342,
      "grad_norm": 3.4910006523132324,
      "learning_rate": 3.295035321821036e-05,
      "loss": 0.5671,
      "step": 1737700
    },
    {
      "epoch": 27.281004709576138,
      "grad_norm": 4.502474308013916,
      "learning_rate": 3.2949372056514916e-05,
      "loss": 0.5968,
      "step": 1737800
    },
    {
      "epoch": 27.282574568288855,
      "grad_norm": 3.7389886379241943,
      "learning_rate": 3.294839089481947e-05,
      "loss": 0.5715,
      "step": 1737900
    },
    {
      "epoch": 27.28414442700157,
      "grad_norm": 4.606727123260498,
      "learning_rate": 3.294740973312402e-05,
      "loss": 0.5957,
      "step": 1738000
    },
    {
      "epoch": 27.285714285714285,
      "grad_norm": 3.6941845417022705,
      "learning_rate": 3.2946428571428576e-05,
      "loss": 0.5974,
      "step": 1738100
    },
    {
      "epoch": 27.287284144427,
      "grad_norm": 2.148587703704834,
      "learning_rate": 3.2945447409733127e-05,
      "loss": 0.6084,
      "step": 1738200
    },
    {
      "epoch": 27.28885400313972,
      "grad_norm": 4.028417110443115,
      "learning_rate": 3.294446624803768e-05,
      "loss": 0.6059,
      "step": 1738300
    },
    {
      "epoch": 27.29042386185243,
      "grad_norm": 3.252931594848633,
      "learning_rate": 3.294348508634223e-05,
      "loss": 0.5991,
      "step": 1738400
    },
    {
      "epoch": 27.29199372056515,
      "grad_norm": 3.7258522510528564,
      "learning_rate": 3.2942503924646786e-05,
      "loss": 0.6171,
      "step": 1738500
    },
    {
      "epoch": 27.293563579277865,
      "grad_norm": 4.018866539001465,
      "learning_rate": 3.294152276295134e-05,
      "loss": 0.5919,
      "step": 1738600
    },
    {
      "epoch": 27.295133437990582,
      "grad_norm": 3.52848744392395,
      "learning_rate": 3.294054160125589e-05,
      "loss": 0.5963,
      "step": 1738700
    },
    {
      "epoch": 27.296703296703296,
      "grad_norm": 4.0199995040893555,
      "learning_rate": 3.293956043956044e-05,
      "loss": 0.5738,
      "step": 1738800
    },
    {
      "epoch": 27.298273155416013,
      "grad_norm": 4.0504961013793945,
      "learning_rate": 3.2938579277865e-05,
      "loss": 0.5941,
      "step": 1738900
    },
    {
      "epoch": 27.29984301412873,
      "grad_norm": 3.325662136077881,
      "learning_rate": 3.293759811616955e-05,
      "loss": 0.5853,
      "step": 1739000
    },
    {
      "epoch": 27.301412872841443,
      "grad_norm": 3.965946912765503,
      "learning_rate": 3.29366169544741e-05,
      "loss": 0.5846,
      "step": 1739100
    },
    {
      "epoch": 27.30298273155416,
      "grad_norm": 4.320865154266357,
      "learning_rate": 3.293563579277865e-05,
      "loss": 0.6178,
      "step": 1739200
    },
    {
      "epoch": 27.304552590266876,
      "grad_norm": 4.103635787963867,
      "learning_rate": 3.293465463108321e-05,
      "loss": 0.6326,
      "step": 1739300
    },
    {
      "epoch": 27.306122448979593,
      "grad_norm": 3.3550751209259033,
      "learning_rate": 3.293367346938775e-05,
      "loss": 0.5779,
      "step": 1739400
    },
    {
      "epoch": 27.307692307692307,
      "grad_norm": 3.2915871143341064,
      "learning_rate": 3.293269230769231e-05,
      "loss": 0.5993,
      "step": 1739500
    },
    {
      "epoch": 27.309262166405023,
      "grad_norm": 5.211217880249023,
      "learning_rate": 3.293171114599686e-05,
      "loss": 0.5603,
      "step": 1739600
    },
    {
      "epoch": 27.31083202511774,
      "grad_norm": 4.197728157043457,
      "learning_rate": 3.293072998430142e-05,
      "loss": 0.6221,
      "step": 1739700
    },
    {
      "epoch": 27.312401883830454,
      "grad_norm": 3.5610461235046387,
      "learning_rate": 3.292974882260596e-05,
      "loss": 0.6127,
      "step": 1739800
    },
    {
      "epoch": 27.31397174254317,
      "grad_norm": 4.262202739715576,
      "learning_rate": 3.292876766091052e-05,
      "loss": 0.6131,
      "step": 1739900
    },
    {
      "epoch": 27.315541601255887,
      "grad_norm": 3.9656589031219482,
      "learning_rate": 3.292778649921507e-05,
      "loss": 0.5546,
      "step": 1740000
    },
    {
      "epoch": 27.317111459968604,
      "grad_norm": 3.6842150688171387,
      "learning_rate": 3.292680533751962e-05,
      "loss": 0.6092,
      "step": 1740100
    },
    {
      "epoch": 27.318681318681318,
      "grad_norm": 4.827563762664795,
      "learning_rate": 3.292582417582418e-05,
      "loss": 0.6069,
      "step": 1740200
    },
    {
      "epoch": 27.320251177394034,
      "grad_norm": 3.803758144378662,
      "learning_rate": 3.292484301412873e-05,
      "loss": 0.5935,
      "step": 1740300
    },
    {
      "epoch": 27.32182103610675,
      "grad_norm": 3.268704891204834,
      "learning_rate": 3.292386185243328e-05,
      "loss": 0.5802,
      "step": 1740400
    },
    {
      "epoch": 27.323390894819465,
      "grad_norm": 4.147761344909668,
      "learning_rate": 3.292288069073783e-05,
      "loss": 0.6075,
      "step": 1740500
    },
    {
      "epoch": 27.32496075353218,
      "grad_norm": 3.4898464679718018,
      "learning_rate": 3.292189952904239e-05,
      "loss": 0.6174,
      "step": 1740600
    },
    {
      "epoch": 27.3265306122449,
      "grad_norm": 3.9686167240142822,
      "learning_rate": 3.292091836734694e-05,
      "loss": 0.5925,
      "step": 1740700
    },
    {
      "epoch": 27.328100470957615,
      "grad_norm": 3.8224332332611084,
      "learning_rate": 3.291993720565149e-05,
      "loss": 0.5957,
      "step": 1740800
    },
    {
      "epoch": 27.32967032967033,
      "grad_norm": 4.3439249992370605,
      "learning_rate": 3.2918956043956044e-05,
      "loss": 0.5973,
      "step": 1740900
    },
    {
      "epoch": 27.331240188383045,
      "grad_norm": 3.625175952911377,
      "learning_rate": 3.29179748822606e-05,
      "loss": 0.5839,
      "step": 1741000
    },
    {
      "epoch": 27.332810047095762,
      "grad_norm": 3.7163150310516357,
      "learning_rate": 3.2916993720565146e-05,
      "loss": 0.5684,
      "step": 1741100
    },
    {
      "epoch": 27.334379905808476,
      "grad_norm": 3.5249478816986084,
      "learning_rate": 3.2916012558869703e-05,
      "loss": 0.5839,
      "step": 1741200
    },
    {
      "epoch": 27.335949764521192,
      "grad_norm": 3.942408323287964,
      "learning_rate": 3.2915031397174254e-05,
      "loss": 0.5992,
      "step": 1741300
    },
    {
      "epoch": 27.33751962323391,
      "grad_norm": 3.7084999084472656,
      "learning_rate": 3.291405023547881e-05,
      "loss": 0.6014,
      "step": 1741400
    },
    {
      "epoch": 27.339089481946626,
      "grad_norm": 3.7043585777282715,
      "learning_rate": 3.2913069073783356e-05,
      "loss": 0.5917,
      "step": 1741500
    },
    {
      "epoch": 27.34065934065934,
      "grad_norm": 2.8052940368652344,
      "learning_rate": 3.2912087912087914e-05,
      "loss": 0.6108,
      "step": 1741600
    },
    {
      "epoch": 27.342229199372056,
      "grad_norm": 3.7754273414611816,
      "learning_rate": 3.2911106750392465e-05,
      "loss": 0.6173,
      "step": 1741700
    },
    {
      "epoch": 27.343799058084773,
      "grad_norm": 3.480545997619629,
      "learning_rate": 3.2910125588697016e-05,
      "loss": 0.5639,
      "step": 1741800
    },
    {
      "epoch": 27.345368916797486,
      "grad_norm": 3.6443886756896973,
      "learning_rate": 3.290914442700157e-05,
      "loss": 0.5867,
      "step": 1741900
    },
    {
      "epoch": 27.346938775510203,
      "grad_norm": 2.5286362171173096,
      "learning_rate": 3.2908163265306125e-05,
      "loss": 0.5498,
      "step": 1742000
    },
    {
      "epoch": 27.34850863422292,
      "grad_norm": 4.195262908935547,
      "learning_rate": 3.2907182103610676e-05,
      "loss": 0.5781,
      "step": 1742100
    },
    {
      "epoch": 27.350078492935637,
      "grad_norm": 2.7574634552001953,
      "learning_rate": 3.290620094191523e-05,
      "loss": 0.5919,
      "step": 1742200
    },
    {
      "epoch": 27.35164835164835,
      "grad_norm": 4.70635461807251,
      "learning_rate": 3.2905219780219785e-05,
      "loss": 0.5816,
      "step": 1742300
    },
    {
      "epoch": 27.353218210361067,
      "grad_norm": 3.62046217918396,
      "learning_rate": 3.2904238618524336e-05,
      "loss": 0.6008,
      "step": 1742400
    },
    {
      "epoch": 27.354788069073784,
      "grad_norm": 4.5179033279418945,
      "learning_rate": 3.2903257456828887e-05,
      "loss": 0.5685,
      "step": 1742500
    },
    {
      "epoch": 27.356357927786497,
      "grad_norm": 3.8303041458129883,
      "learning_rate": 3.290227629513344e-05,
      "loss": 0.5982,
      "step": 1742600
    },
    {
      "epoch": 27.357927786499214,
      "grad_norm": 3.6359100341796875,
      "learning_rate": 3.2901295133437995e-05,
      "loss": 0.6346,
      "step": 1742700
    },
    {
      "epoch": 27.35949764521193,
      "grad_norm": 4.431427001953125,
      "learning_rate": 3.2900313971742546e-05,
      "loss": 0.6117,
      "step": 1742800
    },
    {
      "epoch": 27.361067503924648,
      "grad_norm": 3.9007930755615234,
      "learning_rate": 3.28993328100471e-05,
      "loss": 0.6107,
      "step": 1742900
    },
    {
      "epoch": 27.36263736263736,
      "grad_norm": 3.2601821422576904,
      "learning_rate": 3.289835164835165e-05,
      "loss": 0.587,
      "step": 1743000
    },
    {
      "epoch": 27.364207221350078,
      "grad_norm": 3.160504102706909,
      "learning_rate": 3.2897370486656206e-05,
      "loss": 0.6292,
      "step": 1743100
    },
    {
      "epoch": 27.365777080062795,
      "grad_norm": 3.385277032852173,
      "learning_rate": 3.289638932496075e-05,
      "loss": 0.568,
      "step": 1743200
    },
    {
      "epoch": 27.367346938775512,
      "grad_norm": 4.226408958435059,
      "learning_rate": 3.289540816326531e-05,
      "loss": 0.5826,
      "step": 1743300
    },
    {
      "epoch": 27.368916797488225,
      "grad_norm": 3.310617685317993,
      "learning_rate": 3.289442700156986e-05,
      "loss": 0.6297,
      "step": 1743400
    },
    {
      "epoch": 27.370486656200942,
      "grad_norm": 3.2103748321533203,
      "learning_rate": 3.289344583987442e-05,
      "loss": 0.5745,
      "step": 1743500
    },
    {
      "epoch": 27.37205651491366,
      "grad_norm": 4.172363758087158,
      "learning_rate": 3.289246467817896e-05,
      "loss": 0.584,
      "step": 1743600
    },
    {
      "epoch": 27.373626373626372,
      "grad_norm": 3.73862361907959,
      "learning_rate": 3.289148351648352e-05,
      "loss": 0.5526,
      "step": 1743700
    },
    {
      "epoch": 27.37519623233909,
      "grad_norm": 4.448127746582031,
      "learning_rate": 3.289050235478807e-05,
      "loss": 0.5544,
      "step": 1743800
    },
    {
      "epoch": 27.376766091051806,
      "grad_norm": 4.023377418518066,
      "learning_rate": 3.288952119309262e-05,
      "loss": 0.6181,
      "step": 1743900
    },
    {
      "epoch": 27.378335949764523,
      "grad_norm": 2.703242540359497,
      "learning_rate": 3.288854003139717e-05,
      "loss": 0.6142,
      "step": 1744000
    },
    {
      "epoch": 27.379905808477236,
      "grad_norm": 2.074942111968994,
      "learning_rate": 3.288755886970173e-05,
      "loss": 0.5904,
      "step": 1744100
    },
    {
      "epoch": 27.381475667189953,
      "grad_norm": 3.570500373840332,
      "learning_rate": 3.288657770800628e-05,
      "loss": 0.6242,
      "step": 1744200
    },
    {
      "epoch": 27.38304552590267,
      "grad_norm": 3.8693697452545166,
      "learning_rate": 3.288559654631083e-05,
      "loss": 0.5741,
      "step": 1744300
    },
    {
      "epoch": 27.384615384615383,
      "grad_norm": 3.119840621948242,
      "learning_rate": 3.288461538461539e-05,
      "loss": 0.5821,
      "step": 1744400
    },
    {
      "epoch": 27.3861852433281,
      "grad_norm": 3.4579029083251953,
      "learning_rate": 3.288363422291994e-05,
      "loss": 0.5943,
      "step": 1744500
    },
    {
      "epoch": 27.387755102040817,
      "grad_norm": 4.176321983337402,
      "learning_rate": 3.288265306122449e-05,
      "loss": 0.5798,
      "step": 1744600
    },
    {
      "epoch": 27.389324960753534,
      "grad_norm": 4.004115581512451,
      "learning_rate": 3.288167189952904e-05,
      "loss": 0.587,
      "step": 1744700
    },
    {
      "epoch": 27.390894819466247,
      "grad_norm": 4.027466297149658,
      "learning_rate": 3.28806907378336e-05,
      "loss": 0.6073,
      "step": 1744800
    },
    {
      "epoch": 27.392464678178964,
      "grad_norm": 3.6190052032470703,
      "learning_rate": 3.287970957613815e-05,
      "loss": 0.5852,
      "step": 1744900
    },
    {
      "epoch": 27.39403453689168,
      "grad_norm": 4.251986980438232,
      "learning_rate": 3.28787284144427e-05,
      "loss": 0.5983,
      "step": 1745000
    },
    {
      "epoch": 27.395604395604394,
      "grad_norm": 5.273344993591309,
      "learning_rate": 3.287774725274725e-05,
      "loss": 0.5724,
      "step": 1745100
    },
    {
      "epoch": 27.39717425431711,
      "grad_norm": 3.6886415481567383,
      "learning_rate": 3.287676609105181e-05,
      "loss": 0.5564,
      "step": 1745200
    },
    {
      "epoch": 27.398744113029828,
      "grad_norm": 2.052414894104004,
      "learning_rate": 3.2875784929356355e-05,
      "loss": 0.5421,
      "step": 1745300
    },
    {
      "epoch": 27.400313971742545,
      "grad_norm": 4.12945032119751,
      "learning_rate": 3.287480376766091e-05,
      "loss": 0.5971,
      "step": 1745400
    },
    {
      "epoch": 27.401883830455258,
      "grad_norm": 3.25712513923645,
      "learning_rate": 3.287382260596546e-05,
      "loss": 0.5995,
      "step": 1745500
    },
    {
      "epoch": 27.403453689167975,
      "grad_norm": 2.7375810146331787,
      "learning_rate": 3.287284144427002e-05,
      "loss": 0.6104,
      "step": 1745600
    },
    {
      "epoch": 27.405023547880692,
      "grad_norm": 4.154027462005615,
      "learning_rate": 3.2871860282574565e-05,
      "loss": 0.6107,
      "step": 1745700
    },
    {
      "epoch": 27.406593406593405,
      "grad_norm": 4.212421894073486,
      "learning_rate": 3.287087912087912e-05,
      "loss": 0.5973,
      "step": 1745800
    },
    {
      "epoch": 27.408163265306122,
      "grad_norm": 4.212387561798096,
      "learning_rate": 3.2869897959183674e-05,
      "loss": 0.5913,
      "step": 1745900
    },
    {
      "epoch": 27.40973312401884,
      "grad_norm": 3.913348913192749,
      "learning_rate": 3.2868916797488225e-05,
      "loss": 0.613,
      "step": 1746000
    },
    {
      "epoch": 27.411302982731556,
      "grad_norm": 3.627689838409424,
      "learning_rate": 3.2867935635792776e-05,
      "loss": 0.5732,
      "step": 1746100
    },
    {
      "epoch": 27.41287284144427,
      "grad_norm": 4.504565238952637,
      "learning_rate": 3.2866954474097334e-05,
      "loss": 0.5779,
      "step": 1746200
    },
    {
      "epoch": 27.414442700156986,
      "grad_norm": 2.816535711288452,
      "learning_rate": 3.2865973312401885e-05,
      "loss": 0.5486,
      "step": 1746300
    },
    {
      "epoch": 27.416012558869703,
      "grad_norm": 4.5437331199646,
      "learning_rate": 3.2864992150706436e-05,
      "loss": 0.5984,
      "step": 1746400
    },
    {
      "epoch": 27.417582417582416,
      "grad_norm": 3.984434127807617,
      "learning_rate": 3.2864010989010994e-05,
      "loss": 0.6203,
      "step": 1746500
    },
    {
      "epoch": 27.419152276295133,
      "grad_norm": 3.365140199661255,
      "learning_rate": 3.2863029827315545e-05,
      "loss": 0.6027,
      "step": 1746600
    },
    {
      "epoch": 27.42072213500785,
      "grad_norm": 2.461743116378784,
      "learning_rate": 3.2862048665620095e-05,
      "loss": 0.5543,
      "step": 1746700
    },
    {
      "epoch": 27.422291993720567,
      "grad_norm": 4.710762023925781,
      "learning_rate": 3.2861067503924646e-05,
      "loss": 0.551,
      "step": 1746800
    },
    {
      "epoch": 27.42386185243328,
      "grad_norm": 2.8794655799865723,
      "learning_rate": 3.2860086342229204e-05,
      "loss": 0.5675,
      "step": 1746900
    },
    {
      "epoch": 27.425431711145997,
      "grad_norm": 2.8457822799682617,
      "learning_rate": 3.2859105180533755e-05,
      "loss": 0.5909,
      "step": 1747000
    },
    {
      "epoch": 27.427001569858714,
      "grad_norm": 3.933751106262207,
      "learning_rate": 3.2858124018838306e-05,
      "loss": 0.5957,
      "step": 1747100
    },
    {
      "epoch": 27.428571428571427,
      "grad_norm": 3.6452348232269287,
      "learning_rate": 3.285714285714286e-05,
      "loss": 0.5748,
      "step": 1747200
    },
    {
      "epoch": 27.430141287284144,
      "grad_norm": 4.981045246124268,
      "learning_rate": 3.2856161695447415e-05,
      "loss": 0.6103,
      "step": 1747300
    },
    {
      "epoch": 27.43171114599686,
      "grad_norm": 3.709332227706909,
      "learning_rate": 3.285518053375196e-05,
      "loss": 0.6417,
      "step": 1747400
    },
    {
      "epoch": 27.433281004709578,
      "grad_norm": 3.95296573638916,
      "learning_rate": 3.285419937205652e-05,
      "loss": 0.6386,
      "step": 1747500
    },
    {
      "epoch": 27.43485086342229,
      "grad_norm": 4.0473222732543945,
      "learning_rate": 3.285321821036107e-05,
      "loss": 0.6238,
      "step": 1747600
    },
    {
      "epoch": 27.436420722135008,
      "grad_norm": 5.0145158767700195,
      "learning_rate": 3.2852237048665626e-05,
      "loss": 0.5809,
      "step": 1747700
    },
    {
      "epoch": 27.437990580847725,
      "grad_norm": 3.5240347385406494,
      "learning_rate": 3.285125588697017e-05,
      "loss": 0.5851,
      "step": 1747800
    },
    {
      "epoch": 27.439560439560438,
      "grad_norm": 4.828019142150879,
      "learning_rate": 3.285027472527473e-05,
      "loss": 0.5996,
      "step": 1747900
    },
    {
      "epoch": 27.441130298273155,
      "grad_norm": 4.42434549331665,
      "learning_rate": 3.284929356357928e-05,
      "loss": 0.5858,
      "step": 1748000
    },
    {
      "epoch": 27.44270015698587,
      "grad_norm": 4.307474613189697,
      "learning_rate": 3.284831240188383e-05,
      "loss": 0.5646,
      "step": 1748100
    },
    {
      "epoch": 27.44427001569859,
      "grad_norm": 2.913092613220215,
      "learning_rate": 3.284733124018838e-05,
      "loss": 0.594,
      "step": 1748200
    },
    {
      "epoch": 27.445839874411302,
      "grad_norm": 3.6842358112335205,
      "learning_rate": 3.284635007849294e-05,
      "loss": 0.571,
      "step": 1748300
    },
    {
      "epoch": 27.44740973312402,
      "grad_norm": 3.4201242923736572,
      "learning_rate": 3.284536891679749e-05,
      "loss": 0.6083,
      "step": 1748400
    },
    {
      "epoch": 27.448979591836736,
      "grad_norm": 4.734166622161865,
      "learning_rate": 3.284438775510204e-05,
      "loss": 0.5932,
      "step": 1748500
    },
    {
      "epoch": 27.45054945054945,
      "grad_norm": 4.342308044433594,
      "learning_rate": 3.28434065934066e-05,
      "loss": 0.5596,
      "step": 1748600
    },
    {
      "epoch": 27.452119309262166,
      "grad_norm": 3.524531841278076,
      "learning_rate": 3.284242543171115e-05,
      "loss": 0.5625,
      "step": 1748700
    },
    {
      "epoch": 27.453689167974883,
      "grad_norm": 3.6687283515930176,
      "learning_rate": 3.28414442700157e-05,
      "loss": 0.5875,
      "step": 1748800
    },
    {
      "epoch": 27.4552590266876,
      "grad_norm": 4.21636962890625,
      "learning_rate": 3.284046310832025e-05,
      "loss": 0.5926,
      "step": 1748900
    },
    {
      "epoch": 27.456828885400313,
      "grad_norm": 3.502365827560425,
      "learning_rate": 3.283948194662481e-05,
      "loss": 0.6029,
      "step": 1749000
    },
    {
      "epoch": 27.45839874411303,
      "grad_norm": 3.570429563522339,
      "learning_rate": 3.283850078492936e-05,
      "loss": 0.5811,
      "step": 1749100
    },
    {
      "epoch": 27.459968602825747,
      "grad_norm": 3.27416729927063,
      "learning_rate": 3.283751962323391e-05,
      "loss": 0.5822,
      "step": 1749200
    },
    {
      "epoch": 27.46153846153846,
      "grad_norm": 3.2447493076324463,
      "learning_rate": 3.283653846153846e-05,
      "loss": 0.6156,
      "step": 1749300
    },
    {
      "epoch": 27.463108320251177,
      "grad_norm": 2.6322920322418213,
      "learning_rate": 3.283555729984302e-05,
      "loss": 0.566,
      "step": 1749400
    },
    {
      "epoch": 27.464678178963894,
      "grad_norm": 4.932531833648682,
      "learning_rate": 3.2834576138147564e-05,
      "loss": 0.5849,
      "step": 1749500
    },
    {
      "epoch": 27.46624803767661,
      "grad_norm": 4.091826915740967,
      "learning_rate": 3.283359497645212e-05,
      "loss": 0.604,
      "step": 1749600
    },
    {
      "epoch": 27.467817896389324,
      "grad_norm": 3.968702793121338,
      "learning_rate": 3.283261381475667e-05,
      "loss": 0.5847,
      "step": 1749700
    },
    {
      "epoch": 27.46938775510204,
      "grad_norm": 4.524481296539307,
      "learning_rate": 3.283163265306123e-05,
      "loss": 0.5836,
      "step": 1749800
    },
    {
      "epoch": 27.470957613814758,
      "grad_norm": 4.0344109535217285,
      "learning_rate": 3.2830651491365774e-05,
      "loss": 0.6103,
      "step": 1749900
    },
    {
      "epoch": 27.47252747252747,
      "grad_norm": 5.204557418823242,
      "learning_rate": 3.282967032967033e-05,
      "loss": 0.5689,
      "step": 1750000
    },
    {
      "epoch": 27.474097331240188,
      "grad_norm": 3.8331680297851562,
      "learning_rate": 3.282868916797488e-05,
      "loss": 0.5651,
      "step": 1750100
    },
    {
      "epoch": 27.475667189952905,
      "grad_norm": 3.0743236541748047,
      "learning_rate": 3.2827708006279434e-05,
      "loss": 0.6002,
      "step": 1750200
    },
    {
      "epoch": 27.47723704866562,
      "grad_norm": 3.923931360244751,
      "learning_rate": 3.2826726844583985e-05,
      "loss": 0.5982,
      "step": 1750300
    },
    {
      "epoch": 27.478806907378335,
      "grad_norm": 3.949674606323242,
      "learning_rate": 3.282574568288854e-05,
      "loss": 0.5589,
      "step": 1750400
    },
    {
      "epoch": 27.48037676609105,
      "grad_norm": 3.7781968116760254,
      "learning_rate": 3.2824764521193094e-05,
      "loss": 0.6273,
      "step": 1750500
    },
    {
      "epoch": 27.48194662480377,
      "grad_norm": 3.884465217590332,
      "learning_rate": 3.2823783359497645e-05,
      "loss": 0.5848,
      "step": 1750600
    },
    {
      "epoch": 27.483516483516482,
      "grad_norm": 2.8450191020965576,
      "learning_rate": 3.28228021978022e-05,
      "loss": 0.6004,
      "step": 1750700
    },
    {
      "epoch": 27.4850863422292,
      "grad_norm": 3.5468738079071045,
      "learning_rate": 3.2821821036106753e-05,
      "loss": 0.6273,
      "step": 1750800
    },
    {
      "epoch": 27.486656200941916,
      "grad_norm": 3.8896589279174805,
      "learning_rate": 3.2820839874411304e-05,
      "loss": 0.5877,
      "step": 1750900
    },
    {
      "epoch": 27.488226059654632,
      "grad_norm": 3.9514052867889404,
      "learning_rate": 3.2819858712715855e-05,
      "loss": 0.599,
      "step": 1751000
    },
    {
      "epoch": 27.489795918367346,
      "grad_norm": 3.6231250762939453,
      "learning_rate": 3.281887755102041e-05,
      "loss": 0.5729,
      "step": 1751100
    },
    {
      "epoch": 27.491365777080063,
      "grad_norm": 3.3102943897247314,
      "learning_rate": 3.2817896389324964e-05,
      "loss": 0.5844,
      "step": 1751200
    },
    {
      "epoch": 27.49293563579278,
      "grad_norm": 4.419039249420166,
      "learning_rate": 3.2816915227629515e-05,
      "loss": 0.5877,
      "step": 1751300
    },
    {
      "epoch": 27.494505494505496,
      "grad_norm": 3.8716278076171875,
      "learning_rate": 3.2815934065934066e-05,
      "loss": 0.5915,
      "step": 1751400
    },
    {
      "epoch": 27.49607535321821,
      "grad_norm": 3.7615115642547607,
      "learning_rate": 3.2814952904238624e-05,
      "loss": 0.5866,
      "step": 1751500
    },
    {
      "epoch": 27.497645211930926,
      "grad_norm": 3.535250425338745,
      "learning_rate": 3.281397174254317e-05,
      "loss": 0.5984,
      "step": 1751600
    },
    {
      "epoch": 27.499215070643643,
      "grad_norm": 3.008498191833496,
      "learning_rate": 3.2812990580847726e-05,
      "loss": 0.598,
      "step": 1751700
    },
    {
      "epoch": 27.500784929356357,
      "grad_norm": 3.8237476348876953,
      "learning_rate": 3.281200941915228e-05,
      "loss": 0.6266,
      "step": 1751800
    },
    {
      "epoch": 27.502354788069074,
      "grad_norm": 4.402682781219482,
      "learning_rate": 3.2811028257456835e-05,
      "loss": 0.6214,
      "step": 1751900
    },
    {
      "epoch": 27.50392464678179,
      "grad_norm": 4.754227161407471,
      "learning_rate": 3.281004709576138e-05,
      "loss": 0.6133,
      "step": 1752000
    },
    {
      "epoch": 27.505494505494504,
      "grad_norm": 4.087230682373047,
      "learning_rate": 3.2809065934065937e-05,
      "loss": 0.6076,
      "step": 1752100
    },
    {
      "epoch": 27.50706436420722,
      "grad_norm": 3.197345495223999,
      "learning_rate": 3.280808477237049e-05,
      "loss": 0.6312,
      "step": 1752200
    },
    {
      "epoch": 27.508634222919937,
      "grad_norm": 3.309981346130371,
      "learning_rate": 3.280710361067504e-05,
      "loss": 0.5501,
      "step": 1752300
    },
    {
      "epoch": 27.510204081632654,
      "grad_norm": 3.3882229328155518,
      "learning_rate": 3.280612244897959e-05,
      "loss": 0.6011,
      "step": 1752400
    },
    {
      "epoch": 27.511773940345368,
      "grad_norm": 4.249758720397949,
      "learning_rate": 3.280514128728415e-05,
      "loss": 0.6194,
      "step": 1752500
    },
    {
      "epoch": 27.513343799058084,
      "grad_norm": 3.5776705741882324,
      "learning_rate": 3.28041601255887e-05,
      "loss": 0.5725,
      "step": 1752600
    },
    {
      "epoch": 27.5149136577708,
      "grad_norm": 3.5818052291870117,
      "learning_rate": 3.280317896389325e-05,
      "loss": 0.6243,
      "step": 1752700
    },
    {
      "epoch": 27.516483516483518,
      "grad_norm": 3.688572883605957,
      "learning_rate": 3.280219780219781e-05,
      "loss": 0.589,
      "step": 1752800
    },
    {
      "epoch": 27.51805337519623,
      "grad_norm": 4.002696514129639,
      "learning_rate": 3.280121664050236e-05,
      "loss": 0.6211,
      "step": 1752900
    },
    {
      "epoch": 27.51962323390895,
      "grad_norm": 2.7412314414978027,
      "learning_rate": 3.280023547880691e-05,
      "loss": 0.5756,
      "step": 1753000
    },
    {
      "epoch": 27.521193092621665,
      "grad_norm": 3.386627435684204,
      "learning_rate": 3.279925431711146e-05,
      "loss": 0.6134,
      "step": 1753100
    },
    {
      "epoch": 27.52276295133438,
      "grad_norm": 3.910734176635742,
      "learning_rate": 3.279827315541602e-05,
      "loss": 0.6096,
      "step": 1753200
    },
    {
      "epoch": 27.524332810047095,
      "grad_norm": 4.304481029510498,
      "learning_rate": 3.279729199372057e-05,
      "loss": 0.5672,
      "step": 1753300
    },
    {
      "epoch": 27.525902668759812,
      "grad_norm": 4.927812099456787,
      "learning_rate": 3.279631083202512e-05,
      "loss": 0.5805,
      "step": 1753400
    },
    {
      "epoch": 27.52747252747253,
      "grad_norm": 4.007269859313965,
      "learning_rate": 3.279532967032967e-05,
      "loss": 0.6279,
      "step": 1753500
    },
    {
      "epoch": 27.529042386185242,
      "grad_norm": 3.253948211669922,
      "learning_rate": 3.279434850863423e-05,
      "loss": 0.5667,
      "step": 1753600
    },
    {
      "epoch": 27.53061224489796,
      "grad_norm": 2.5127477645874023,
      "learning_rate": 3.279336734693877e-05,
      "loss": 0.6087,
      "step": 1753700
    },
    {
      "epoch": 27.532182103610676,
      "grad_norm": 3.8499696254730225,
      "learning_rate": 3.279238618524333e-05,
      "loss": 0.5774,
      "step": 1753800
    },
    {
      "epoch": 27.53375196232339,
      "grad_norm": 2.0497384071350098,
      "learning_rate": 3.279140502354788e-05,
      "loss": 0.609,
      "step": 1753900
    },
    {
      "epoch": 27.535321821036106,
      "grad_norm": 5.728085041046143,
      "learning_rate": 3.279042386185244e-05,
      "loss": 0.6231,
      "step": 1754000
    },
    {
      "epoch": 27.536891679748823,
      "grad_norm": 3.827436685562134,
      "learning_rate": 3.278944270015698e-05,
      "loss": 0.6187,
      "step": 1754100
    },
    {
      "epoch": 27.53846153846154,
      "grad_norm": 3.8548343181610107,
      "learning_rate": 3.278846153846154e-05,
      "loss": 0.5835,
      "step": 1754200
    },
    {
      "epoch": 27.540031397174253,
      "grad_norm": 4.666727542877197,
      "learning_rate": 3.278748037676609e-05,
      "loss": 0.6009,
      "step": 1754300
    },
    {
      "epoch": 27.54160125588697,
      "grad_norm": 4.802985668182373,
      "learning_rate": 3.278649921507064e-05,
      "loss": 0.5976,
      "step": 1754400
    },
    {
      "epoch": 27.543171114599687,
      "grad_norm": 2.837587833404541,
      "learning_rate": 3.2785518053375194e-05,
      "loss": 0.5816,
      "step": 1754500
    },
    {
      "epoch": 27.5447409733124,
      "grad_norm": 4.268858909606934,
      "learning_rate": 3.278453689167975e-05,
      "loss": 0.579,
      "step": 1754600
    },
    {
      "epoch": 27.546310832025117,
      "grad_norm": 4.343692779541016,
      "learning_rate": 3.27835557299843e-05,
      "loss": 0.587,
      "step": 1754700
    },
    {
      "epoch": 27.547880690737834,
      "grad_norm": 3.6008119583129883,
      "learning_rate": 3.2782574568288854e-05,
      "loss": 0.6424,
      "step": 1754800
    },
    {
      "epoch": 27.54945054945055,
      "grad_norm": 4.813462257385254,
      "learning_rate": 3.278159340659341e-05,
      "loss": 0.5694,
      "step": 1754900
    },
    {
      "epoch": 27.551020408163264,
      "grad_norm": 3.6202425956726074,
      "learning_rate": 3.278061224489796e-05,
      "loss": 0.5921,
      "step": 1755000
    },
    {
      "epoch": 27.55259026687598,
      "grad_norm": 3.5784003734588623,
      "learning_rate": 3.2779631083202513e-05,
      "loss": 0.5677,
      "step": 1755100
    },
    {
      "epoch": 27.554160125588698,
      "grad_norm": 4.3313493728637695,
      "learning_rate": 3.2778649921507064e-05,
      "loss": 0.582,
      "step": 1755200
    },
    {
      "epoch": 27.55572998430141,
      "grad_norm": 3.164254665374756,
      "learning_rate": 3.277766875981162e-05,
      "loss": 0.5537,
      "step": 1755300
    },
    {
      "epoch": 27.55729984301413,
      "grad_norm": 2.5967094898223877,
      "learning_rate": 3.277668759811617e-05,
      "loss": 0.5743,
      "step": 1755400
    },
    {
      "epoch": 27.558869701726845,
      "grad_norm": 3.7599568367004395,
      "learning_rate": 3.2775706436420724e-05,
      "loss": 0.5985,
      "step": 1755500
    },
    {
      "epoch": 27.560439560439562,
      "grad_norm": 3.8491456508636475,
      "learning_rate": 3.2774725274725275e-05,
      "loss": 0.6344,
      "step": 1755600
    },
    {
      "epoch": 27.562009419152275,
      "grad_norm": 4.344656467437744,
      "learning_rate": 3.277374411302983e-05,
      "loss": 0.5764,
      "step": 1755700
    },
    {
      "epoch": 27.563579277864992,
      "grad_norm": 2.9045307636260986,
      "learning_rate": 3.277276295133438e-05,
      "loss": 0.6129,
      "step": 1755800
    },
    {
      "epoch": 27.56514913657771,
      "grad_norm": 1.9852036237716675,
      "learning_rate": 3.2771781789638935e-05,
      "loss": 0.6327,
      "step": 1755900
    },
    {
      "epoch": 27.566718995290422,
      "grad_norm": 4.7823166847229,
      "learning_rate": 3.2770800627943486e-05,
      "loss": 0.5889,
      "step": 1756000
    },
    {
      "epoch": 27.56828885400314,
      "grad_norm": 3.5467848777770996,
      "learning_rate": 3.2769819466248044e-05,
      "loss": 0.5936,
      "step": 1756100
    },
    {
      "epoch": 27.569858712715856,
      "grad_norm": 3.853776693344116,
      "learning_rate": 3.276883830455259e-05,
      "loss": 0.5725,
      "step": 1756200
    },
    {
      "epoch": 27.571428571428573,
      "grad_norm": 4.09593391418457,
      "learning_rate": 3.2767857142857146e-05,
      "loss": 0.6023,
      "step": 1756300
    },
    {
      "epoch": 27.572998430141286,
      "grad_norm": 3.5552899837493896,
      "learning_rate": 3.2766875981161697e-05,
      "loss": 0.5889,
      "step": 1756400
    },
    {
      "epoch": 27.574568288854003,
      "grad_norm": 3.1161701679229736,
      "learning_rate": 3.276589481946625e-05,
      "loss": 0.5807,
      "step": 1756500
    },
    {
      "epoch": 27.57613814756672,
      "grad_norm": 3.944262742996216,
      "learning_rate": 3.27649136577708e-05,
      "loss": 0.6167,
      "step": 1756600
    },
    {
      "epoch": 27.577708006279433,
      "grad_norm": 4.061236381530762,
      "learning_rate": 3.2763932496075356e-05,
      "loss": 0.6138,
      "step": 1756700
    },
    {
      "epoch": 27.57927786499215,
      "grad_norm": 2.7698934078216553,
      "learning_rate": 3.276295133437991e-05,
      "loss": 0.6111,
      "step": 1756800
    },
    {
      "epoch": 27.580847723704867,
      "grad_norm": 4.16096305847168,
      "learning_rate": 3.276197017268446e-05,
      "loss": 0.6289,
      "step": 1756900
    },
    {
      "epoch": 27.582417582417584,
      "grad_norm": 4.218036651611328,
      "learning_rate": 3.2760989010989016e-05,
      "loss": 0.5945,
      "step": 1757000
    },
    {
      "epoch": 27.583987441130297,
      "grad_norm": 2.0896904468536377,
      "learning_rate": 3.276000784929357e-05,
      "loss": 0.6199,
      "step": 1757100
    },
    {
      "epoch": 27.585557299843014,
      "grad_norm": 4.25499153137207,
      "learning_rate": 3.275902668759812e-05,
      "loss": 0.5794,
      "step": 1757200
    },
    {
      "epoch": 27.58712715855573,
      "grad_norm": 3.949934482574463,
      "learning_rate": 3.275804552590267e-05,
      "loss": 0.6355,
      "step": 1757300
    },
    {
      "epoch": 27.588697017268444,
      "grad_norm": 3.667952299118042,
      "learning_rate": 3.275706436420723e-05,
      "loss": 0.6153,
      "step": 1757400
    },
    {
      "epoch": 27.59026687598116,
      "grad_norm": 3.4259555339813232,
      "learning_rate": 3.275608320251178e-05,
      "loss": 0.5981,
      "step": 1757500
    },
    {
      "epoch": 27.591836734693878,
      "grad_norm": 4.450783729553223,
      "learning_rate": 3.275510204081633e-05,
      "loss": 0.5666,
      "step": 1757600
    },
    {
      "epoch": 27.593406593406595,
      "grad_norm": 4.228667736053467,
      "learning_rate": 3.275412087912088e-05,
      "loss": 0.6554,
      "step": 1757700
    },
    {
      "epoch": 27.594976452119308,
      "grad_norm": 4.175325393676758,
      "learning_rate": 3.275313971742544e-05,
      "loss": 0.5942,
      "step": 1757800
    },
    {
      "epoch": 27.596546310832025,
      "grad_norm": 4.572845458984375,
      "learning_rate": 3.275215855572998e-05,
      "loss": 0.5473,
      "step": 1757900
    },
    {
      "epoch": 27.598116169544742,
      "grad_norm": 3.5167248249053955,
      "learning_rate": 3.275117739403454e-05,
      "loss": 0.5744,
      "step": 1758000
    },
    {
      "epoch": 27.599686028257455,
      "grad_norm": 3.763763666152954,
      "learning_rate": 3.275019623233909e-05,
      "loss": 0.6024,
      "step": 1758100
    },
    {
      "epoch": 27.601255886970172,
      "grad_norm": 4.168612480163574,
      "learning_rate": 3.274921507064365e-05,
      "loss": 0.5899,
      "step": 1758200
    },
    {
      "epoch": 27.60282574568289,
      "grad_norm": 3.9401779174804688,
      "learning_rate": 3.274823390894819e-05,
      "loss": 0.626,
      "step": 1758300
    },
    {
      "epoch": 27.604395604395606,
      "grad_norm": 3.2799689769744873,
      "learning_rate": 3.274725274725275e-05,
      "loss": 0.6033,
      "step": 1758400
    },
    {
      "epoch": 27.60596546310832,
      "grad_norm": 4.726771354675293,
      "learning_rate": 3.27462715855573e-05,
      "loss": 0.5959,
      "step": 1758500
    },
    {
      "epoch": 27.607535321821036,
      "grad_norm": 3.5355217456817627,
      "learning_rate": 3.274529042386185e-05,
      "loss": 0.5709,
      "step": 1758600
    },
    {
      "epoch": 27.609105180533753,
      "grad_norm": 4.144258499145508,
      "learning_rate": 3.27443092621664e-05,
      "loss": 0.6172,
      "step": 1758700
    },
    {
      "epoch": 27.610675039246466,
      "grad_norm": 3.991560935974121,
      "learning_rate": 3.274332810047096e-05,
      "loss": 0.618,
      "step": 1758800
    },
    {
      "epoch": 27.612244897959183,
      "grad_norm": 3.8321900367736816,
      "learning_rate": 3.274234693877551e-05,
      "loss": 0.6027,
      "step": 1758900
    },
    {
      "epoch": 27.6138147566719,
      "grad_norm": 4.1846160888671875,
      "learning_rate": 3.274136577708006e-05,
      "loss": 0.6,
      "step": 1759000
    },
    {
      "epoch": 27.615384615384617,
      "grad_norm": 3.844008684158325,
      "learning_rate": 3.274038461538462e-05,
      "loss": 0.5853,
      "step": 1759100
    },
    {
      "epoch": 27.61695447409733,
      "grad_norm": 3.0991735458374023,
      "learning_rate": 3.273940345368917e-05,
      "loss": 0.625,
      "step": 1759200
    },
    {
      "epoch": 27.618524332810047,
      "grad_norm": 4.197501182556152,
      "learning_rate": 3.273842229199372e-05,
      "loss": 0.5532,
      "step": 1759300
    },
    {
      "epoch": 27.620094191522764,
      "grad_norm": 3.7247846126556396,
      "learning_rate": 3.2737441130298273e-05,
      "loss": 0.5839,
      "step": 1759400
    },
    {
      "epoch": 27.621664050235477,
      "grad_norm": 2.6359684467315674,
      "learning_rate": 3.273645996860283e-05,
      "loss": 0.6014,
      "step": 1759500
    },
    {
      "epoch": 27.623233908948194,
      "grad_norm": 3.2442190647125244,
      "learning_rate": 3.273547880690738e-05,
      "loss": 0.6208,
      "step": 1759600
    },
    {
      "epoch": 27.62480376766091,
      "grad_norm": 3.5003435611724854,
      "learning_rate": 3.273449764521193e-05,
      "loss": 0.5532,
      "step": 1759700
    },
    {
      "epoch": 27.626373626373628,
      "grad_norm": 3.4443657398223877,
      "learning_rate": 3.2733516483516484e-05,
      "loss": 0.5904,
      "step": 1759800
    },
    {
      "epoch": 27.62794348508634,
      "grad_norm": 3.7261667251586914,
      "learning_rate": 3.273253532182104e-05,
      "loss": 0.588,
      "step": 1759900
    },
    {
      "epoch": 27.629513343799058,
      "grad_norm": 3.799100875854492,
      "learning_rate": 3.2731554160125586e-05,
      "loss": 0.6098,
      "step": 1760000
    },
    {
      "epoch": 27.631083202511775,
      "grad_norm": 3.707310676574707,
      "learning_rate": 3.2730572998430144e-05,
      "loss": 0.5888,
      "step": 1760100
    },
    {
      "epoch": 27.632653061224488,
      "grad_norm": 3.448779821395874,
      "learning_rate": 3.2729591836734695e-05,
      "loss": 0.5873,
      "step": 1760200
    },
    {
      "epoch": 27.634222919937205,
      "grad_norm": 2.9747517108917236,
      "learning_rate": 3.272861067503925e-05,
      "loss": 0.5988,
      "step": 1760300
    },
    {
      "epoch": 27.635792778649922,
      "grad_norm": 3.4093635082244873,
      "learning_rate": 3.27276295133438e-05,
      "loss": 0.6052,
      "step": 1760400
    },
    {
      "epoch": 27.63736263736264,
      "grad_norm": 2.568082094192505,
      "learning_rate": 3.2726648351648355e-05,
      "loss": 0.5659,
      "step": 1760500
    },
    {
      "epoch": 27.638932496075352,
      "grad_norm": 3.168853282928467,
      "learning_rate": 3.2725667189952906e-05,
      "loss": 0.5774,
      "step": 1760600
    },
    {
      "epoch": 27.64050235478807,
      "grad_norm": 3.5122859477996826,
      "learning_rate": 3.2724686028257456e-05,
      "loss": 0.5828,
      "step": 1760700
    },
    {
      "epoch": 27.642072213500786,
      "grad_norm": 3.956737995147705,
      "learning_rate": 3.272370486656201e-05,
      "loss": 0.5673,
      "step": 1760800
    },
    {
      "epoch": 27.643642072213503,
      "grad_norm": 4.309309005737305,
      "learning_rate": 3.2722723704866565e-05,
      "loss": 0.5821,
      "step": 1760900
    },
    {
      "epoch": 27.645211930926216,
      "grad_norm": 3.988260507583618,
      "learning_rate": 3.2721742543171116e-05,
      "loss": 0.6045,
      "step": 1761000
    },
    {
      "epoch": 27.646781789638933,
      "grad_norm": 4.585896015167236,
      "learning_rate": 3.272076138147567e-05,
      "loss": 0.5958,
      "step": 1761100
    },
    {
      "epoch": 27.64835164835165,
      "grad_norm": 4.987491130828857,
      "learning_rate": 3.2719780219780225e-05,
      "loss": 0.5761,
      "step": 1761200
    },
    {
      "epoch": 27.649921507064363,
      "grad_norm": 4.261244297027588,
      "learning_rate": 3.2718799058084776e-05,
      "loss": 0.5377,
      "step": 1761300
    },
    {
      "epoch": 27.65149136577708,
      "grad_norm": 3.538511276245117,
      "learning_rate": 3.271781789638933e-05,
      "loss": 0.562,
      "step": 1761400
    },
    {
      "epoch": 27.653061224489797,
      "grad_norm": 5.784318923950195,
      "learning_rate": 3.271683673469388e-05,
      "loss": 0.5695,
      "step": 1761500
    },
    {
      "epoch": 27.65463108320251,
      "grad_norm": 3.726764678955078,
      "learning_rate": 3.2715855572998436e-05,
      "loss": 0.6184,
      "step": 1761600
    },
    {
      "epoch": 27.656200941915227,
      "grad_norm": 4.432475566864014,
      "learning_rate": 3.271487441130299e-05,
      "loss": 0.5721,
      "step": 1761700
    },
    {
      "epoch": 27.657770800627944,
      "grad_norm": 4.801031589508057,
      "learning_rate": 3.271389324960754e-05,
      "loss": 0.6119,
      "step": 1761800
    },
    {
      "epoch": 27.65934065934066,
      "grad_norm": 4.8444976806640625,
      "learning_rate": 3.271291208791209e-05,
      "loss": 0.6058,
      "step": 1761900
    },
    {
      "epoch": 27.660910518053374,
      "grad_norm": 3.0098154544830322,
      "learning_rate": 3.2711930926216646e-05,
      "loss": 0.5985,
      "step": 1762000
    },
    {
      "epoch": 27.66248037676609,
      "grad_norm": 2.257906675338745,
      "learning_rate": 3.271094976452119e-05,
      "loss": 0.626,
      "step": 1762100
    },
    {
      "epoch": 27.664050235478808,
      "grad_norm": 3.4510905742645264,
      "learning_rate": 3.270996860282575e-05,
      "loss": 0.6017,
      "step": 1762200
    },
    {
      "epoch": 27.665620094191524,
      "grad_norm": 4.107702732086182,
      "learning_rate": 3.27089874411303e-05,
      "loss": 0.6339,
      "step": 1762300
    },
    {
      "epoch": 27.667189952904238,
      "grad_norm": 3.79775071144104,
      "learning_rate": 3.270800627943486e-05,
      "loss": 0.5749,
      "step": 1762400
    },
    {
      "epoch": 27.668759811616955,
      "grad_norm": 4.335195541381836,
      "learning_rate": 3.27070251177394e-05,
      "loss": 0.6229,
      "step": 1762500
    },
    {
      "epoch": 27.67032967032967,
      "grad_norm": 4.086669921875,
      "learning_rate": 3.270604395604396e-05,
      "loss": 0.5961,
      "step": 1762600
    },
    {
      "epoch": 27.671899529042385,
      "grad_norm": 4.6539306640625,
      "learning_rate": 3.270506279434851e-05,
      "loss": 0.5947,
      "step": 1762700
    },
    {
      "epoch": 27.6734693877551,
      "grad_norm": 4.359930515289307,
      "learning_rate": 3.270408163265306e-05,
      "loss": 0.6274,
      "step": 1762800
    },
    {
      "epoch": 27.67503924646782,
      "grad_norm": 2.9204459190368652,
      "learning_rate": 3.270310047095761e-05,
      "loss": 0.5667,
      "step": 1762900
    },
    {
      "epoch": 27.676609105180535,
      "grad_norm": 3.034349203109741,
      "learning_rate": 3.270211930926217e-05,
      "loss": 0.5816,
      "step": 1763000
    },
    {
      "epoch": 27.67817896389325,
      "grad_norm": 5.0210113525390625,
      "learning_rate": 3.270113814756672e-05,
      "loss": 0.5806,
      "step": 1763100
    },
    {
      "epoch": 27.679748822605966,
      "grad_norm": 2.7228782176971436,
      "learning_rate": 3.270015698587127e-05,
      "loss": 0.6129,
      "step": 1763200
    },
    {
      "epoch": 27.681318681318682,
      "grad_norm": 4.478038787841797,
      "learning_rate": 3.269917582417583e-05,
      "loss": 0.5887,
      "step": 1763300
    },
    {
      "epoch": 27.682888540031396,
      "grad_norm": 3.643048048019409,
      "learning_rate": 3.269819466248038e-05,
      "loss": 0.5928,
      "step": 1763400
    },
    {
      "epoch": 27.684458398744113,
      "grad_norm": 3.735215902328491,
      "learning_rate": 3.269721350078493e-05,
      "loss": 0.5856,
      "step": 1763500
    },
    {
      "epoch": 27.68602825745683,
      "grad_norm": 2.0446441173553467,
      "learning_rate": 3.269623233908948e-05,
      "loss": 0.6098,
      "step": 1763600
    },
    {
      "epoch": 27.687598116169546,
      "grad_norm": 3.4911773204803467,
      "learning_rate": 3.269525117739404e-05,
      "loss": 0.5747,
      "step": 1763700
    },
    {
      "epoch": 27.68916797488226,
      "grad_norm": 2.3823275566101074,
      "learning_rate": 3.2694270015698584e-05,
      "loss": 0.5933,
      "step": 1763800
    },
    {
      "epoch": 27.690737833594977,
      "grad_norm": 4.262331962585449,
      "learning_rate": 3.269328885400314e-05,
      "loss": 0.5745,
      "step": 1763900
    },
    {
      "epoch": 27.692307692307693,
      "grad_norm": 4.303287982940674,
      "learning_rate": 3.269230769230769e-05,
      "loss": 0.5893,
      "step": 1764000
    },
    {
      "epoch": 27.693877551020407,
      "grad_norm": 4.7745771408081055,
      "learning_rate": 3.269132653061225e-05,
      "loss": 0.6194,
      "step": 1764100
    },
    {
      "epoch": 27.695447409733124,
      "grad_norm": 3.3436481952667236,
      "learning_rate": 3.2690345368916795e-05,
      "loss": 0.5603,
      "step": 1764200
    },
    {
      "epoch": 27.69701726844584,
      "grad_norm": 2.716759443283081,
      "learning_rate": 3.268936420722135e-05,
      "loss": 0.5806,
      "step": 1764300
    },
    {
      "epoch": 27.698587127158557,
      "grad_norm": 3.921581745147705,
      "learning_rate": 3.2688383045525904e-05,
      "loss": 0.5861,
      "step": 1764400
    },
    {
      "epoch": 27.70015698587127,
      "grad_norm": 3.586138963699341,
      "learning_rate": 3.2687401883830455e-05,
      "loss": 0.6112,
      "step": 1764500
    },
    {
      "epoch": 27.701726844583987,
      "grad_norm": 3.5354504585266113,
      "learning_rate": 3.2686420722135006e-05,
      "loss": 0.5739,
      "step": 1764600
    },
    {
      "epoch": 27.703296703296704,
      "grad_norm": 3.8394322395324707,
      "learning_rate": 3.2685439560439564e-05,
      "loss": 0.6052,
      "step": 1764700
    },
    {
      "epoch": 27.704866562009418,
      "grad_norm": 2.5138349533081055,
      "learning_rate": 3.2684458398744114e-05,
      "loss": 0.639,
      "step": 1764800
    },
    {
      "epoch": 27.706436420722135,
      "grad_norm": 4.365119934082031,
      "learning_rate": 3.2683477237048665e-05,
      "loss": 0.6048,
      "step": 1764900
    },
    {
      "epoch": 27.70800627943485,
      "grad_norm": 4.430635929107666,
      "learning_rate": 3.2682496075353216e-05,
      "loss": 0.5994,
      "step": 1765000
    },
    {
      "epoch": 27.70957613814757,
      "grad_norm": 3.5698814392089844,
      "learning_rate": 3.2681514913657774e-05,
      "loss": 0.6023,
      "step": 1765100
    },
    {
      "epoch": 27.71114599686028,
      "grad_norm": 3.108686923980713,
      "learning_rate": 3.2680533751962325e-05,
      "loss": 0.574,
      "step": 1765200
    },
    {
      "epoch": 27.712715855573,
      "grad_norm": 3.2863152027130127,
      "learning_rate": 3.2679552590266876e-05,
      "loss": 0.6013,
      "step": 1765300
    },
    {
      "epoch": 27.714285714285715,
      "grad_norm": 3.394498348236084,
      "learning_rate": 3.2678571428571434e-05,
      "loss": 0.5855,
      "step": 1765400
    },
    {
      "epoch": 27.71585557299843,
      "grad_norm": 3.8249106407165527,
      "learning_rate": 3.2677590266875985e-05,
      "loss": 0.6335,
      "step": 1765500
    },
    {
      "epoch": 27.717425431711145,
      "grad_norm": 3.3390109539031982,
      "learning_rate": 3.2676609105180536e-05,
      "loss": 0.5978,
      "step": 1765600
    },
    {
      "epoch": 27.718995290423862,
      "grad_norm": 2.5008490085601807,
      "learning_rate": 3.267562794348509e-05,
      "loss": 0.5777,
      "step": 1765700
    },
    {
      "epoch": 27.72056514913658,
      "grad_norm": 4.243076324462891,
      "learning_rate": 3.2674646781789645e-05,
      "loss": 0.6199,
      "step": 1765800
    },
    {
      "epoch": 27.722135007849293,
      "grad_norm": 2.992966890335083,
      "learning_rate": 3.267366562009419e-05,
      "loss": 0.5857,
      "step": 1765900
    },
    {
      "epoch": 27.72370486656201,
      "grad_norm": 3.789139747619629,
      "learning_rate": 3.2672684458398747e-05,
      "loss": 0.5933,
      "step": 1766000
    },
    {
      "epoch": 27.725274725274726,
      "grad_norm": 3.3132760524749756,
      "learning_rate": 3.26717032967033e-05,
      "loss": 0.5965,
      "step": 1766100
    },
    {
      "epoch": 27.72684458398744,
      "grad_norm": 3.786456823348999,
      "learning_rate": 3.2670722135007855e-05,
      "loss": 0.6032,
      "step": 1766200
    },
    {
      "epoch": 27.728414442700156,
      "grad_norm": 4.269438743591309,
      "learning_rate": 3.26697409733124e-05,
      "loss": 0.593,
      "step": 1766300
    },
    {
      "epoch": 27.729984301412873,
      "grad_norm": 4.138899803161621,
      "learning_rate": 3.266875981161696e-05,
      "loss": 0.5888,
      "step": 1766400
    },
    {
      "epoch": 27.73155416012559,
      "grad_norm": 3.511348009109497,
      "learning_rate": 3.266777864992151e-05,
      "loss": 0.6131,
      "step": 1766500
    },
    {
      "epoch": 27.733124018838303,
      "grad_norm": 3.8749117851257324,
      "learning_rate": 3.266679748822606e-05,
      "loss": 0.5545,
      "step": 1766600
    },
    {
      "epoch": 27.73469387755102,
      "grad_norm": 3.6027092933654785,
      "learning_rate": 3.266581632653061e-05,
      "loss": 0.6373,
      "step": 1766700
    },
    {
      "epoch": 27.736263736263737,
      "grad_norm": 3.429410457611084,
      "learning_rate": 3.266483516483517e-05,
      "loss": 0.599,
      "step": 1766800
    },
    {
      "epoch": 27.73783359497645,
      "grad_norm": 4.356709957122803,
      "learning_rate": 3.266385400313972e-05,
      "loss": 0.6338,
      "step": 1766900
    },
    {
      "epoch": 27.739403453689167,
      "grad_norm": 5.09491491317749,
      "learning_rate": 3.266287284144427e-05,
      "loss": 0.5798,
      "step": 1767000
    },
    {
      "epoch": 27.740973312401884,
      "grad_norm": 3.167032241821289,
      "learning_rate": 3.266189167974882e-05,
      "loss": 0.5848,
      "step": 1767100
    },
    {
      "epoch": 27.7425431711146,
      "grad_norm": 3.6512343883514404,
      "learning_rate": 3.266091051805338e-05,
      "loss": 0.589,
      "step": 1767200
    },
    {
      "epoch": 27.744113029827314,
      "grad_norm": 4.0934343338012695,
      "learning_rate": 3.265992935635793e-05,
      "loss": 0.5706,
      "step": 1767300
    },
    {
      "epoch": 27.74568288854003,
      "grad_norm": 3.090592384338379,
      "learning_rate": 3.265894819466248e-05,
      "loss": 0.5819,
      "step": 1767400
    },
    {
      "epoch": 27.747252747252748,
      "grad_norm": 4.160398006439209,
      "learning_rate": 3.265796703296704e-05,
      "loss": 0.605,
      "step": 1767500
    },
    {
      "epoch": 27.74882260596546,
      "grad_norm": 3.9828600883483887,
      "learning_rate": 3.265698587127159e-05,
      "loss": 0.5828,
      "step": 1767600
    },
    {
      "epoch": 27.75039246467818,
      "grad_norm": 4.696651458740234,
      "learning_rate": 3.265600470957614e-05,
      "loss": 0.5669,
      "step": 1767700
    },
    {
      "epoch": 27.751962323390895,
      "grad_norm": 4.131775856018066,
      "learning_rate": 3.265502354788069e-05,
      "loss": 0.567,
      "step": 1767800
    },
    {
      "epoch": 27.753532182103612,
      "grad_norm": 3.6237494945526123,
      "learning_rate": 3.265404238618525e-05,
      "loss": 0.5904,
      "step": 1767900
    },
    {
      "epoch": 27.755102040816325,
      "grad_norm": 4.172484874725342,
      "learning_rate": 3.265306122448979e-05,
      "loss": 0.5979,
      "step": 1768000
    },
    {
      "epoch": 27.756671899529042,
      "grad_norm": 4.30341100692749,
      "learning_rate": 3.265208006279435e-05,
      "loss": 0.5848,
      "step": 1768100
    },
    {
      "epoch": 27.75824175824176,
      "grad_norm": 3.21280837059021,
      "learning_rate": 3.26510989010989e-05,
      "loss": 0.5769,
      "step": 1768200
    },
    {
      "epoch": 27.759811616954472,
      "grad_norm": 3.046614646911621,
      "learning_rate": 3.265011773940346e-05,
      "loss": 0.5936,
      "step": 1768300
    },
    {
      "epoch": 27.76138147566719,
      "grad_norm": 4.945691108703613,
      "learning_rate": 3.2649136577708004e-05,
      "loss": 0.58,
      "step": 1768400
    },
    {
      "epoch": 27.762951334379906,
      "grad_norm": 3.6725926399230957,
      "learning_rate": 3.264815541601256e-05,
      "loss": 0.6314,
      "step": 1768500
    },
    {
      "epoch": 27.764521193092623,
      "grad_norm": 3.8295042514801025,
      "learning_rate": 3.264717425431711e-05,
      "loss": 0.6049,
      "step": 1768600
    },
    {
      "epoch": 27.766091051805336,
      "grad_norm": 3.9227700233459473,
      "learning_rate": 3.2646193092621664e-05,
      "loss": 0.6032,
      "step": 1768700
    },
    {
      "epoch": 27.767660910518053,
      "grad_norm": 4.439122676849365,
      "learning_rate": 3.2645211930926215e-05,
      "loss": 0.6226,
      "step": 1768800
    },
    {
      "epoch": 27.76923076923077,
      "grad_norm": 4.062114715576172,
      "learning_rate": 3.264423076923077e-05,
      "loss": 0.5993,
      "step": 1768900
    },
    {
      "epoch": 27.770800627943487,
      "grad_norm": 4.520025730133057,
      "learning_rate": 3.2643249607535323e-05,
      "loss": 0.6048,
      "step": 1769000
    },
    {
      "epoch": 27.7723704866562,
      "grad_norm": 3.820122718811035,
      "learning_rate": 3.2642268445839874e-05,
      "loss": 0.6329,
      "step": 1769100
    },
    {
      "epoch": 27.773940345368917,
      "grad_norm": 4.286804676055908,
      "learning_rate": 3.2641287284144425e-05,
      "loss": 0.6197,
      "step": 1769200
    },
    {
      "epoch": 27.775510204081634,
      "grad_norm": 4.438541889190674,
      "learning_rate": 3.264030612244898e-05,
      "loss": 0.589,
      "step": 1769300
    },
    {
      "epoch": 27.777080062794347,
      "grad_norm": 3.4534239768981934,
      "learning_rate": 3.2639324960753534e-05,
      "loss": 0.5646,
      "step": 1769400
    },
    {
      "epoch": 27.778649921507064,
      "grad_norm": 5.226616859436035,
      "learning_rate": 3.2638343799058085e-05,
      "loss": 0.605,
      "step": 1769500
    },
    {
      "epoch": 27.78021978021978,
      "grad_norm": 4.163625240325928,
      "learning_rate": 3.263736263736264e-05,
      "loss": 0.5643,
      "step": 1769600
    },
    {
      "epoch": 27.781789638932494,
      "grad_norm": 3.214890241622925,
      "learning_rate": 3.2636381475667194e-05,
      "loss": 0.6119,
      "step": 1769700
    },
    {
      "epoch": 27.78335949764521,
      "grad_norm": 3.4627976417541504,
      "learning_rate": 3.2635400313971745e-05,
      "loss": 0.606,
      "step": 1769800
    },
    {
      "epoch": 27.784929356357928,
      "grad_norm": 3.655927896499634,
      "learning_rate": 3.2634419152276296e-05,
      "loss": 0.5941,
      "step": 1769900
    },
    {
      "epoch": 27.786499215070645,
      "grad_norm": 3.957446575164795,
      "learning_rate": 3.2633437990580854e-05,
      "loss": 0.5845,
      "step": 1770000
    },
    {
      "epoch": 27.788069073783358,
      "grad_norm": 3.630470037460327,
      "learning_rate": 3.26324568288854e-05,
      "loss": 0.5781,
      "step": 1770100
    },
    {
      "epoch": 27.789638932496075,
      "grad_norm": 4.394188404083252,
      "learning_rate": 3.2631475667189956e-05,
      "loss": 0.6055,
      "step": 1770200
    },
    {
      "epoch": 27.791208791208792,
      "grad_norm": 3.935169219970703,
      "learning_rate": 3.2630494505494507e-05,
      "loss": 0.6369,
      "step": 1770300
    },
    {
      "epoch": 27.79277864992151,
      "grad_norm": 4.483129501342773,
      "learning_rate": 3.2629513343799064e-05,
      "loss": 0.6133,
      "step": 1770400
    },
    {
      "epoch": 27.794348508634222,
      "grad_norm": 4.24164342880249,
      "learning_rate": 3.262853218210361e-05,
      "loss": 0.6057,
      "step": 1770500
    },
    {
      "epoch": 27.79591836734694,
      "grad_norm": 4.550233840942383,
      "learning_rate": 3.2627551020408166e-05,
      "loss": 0.5668,
      "step": 1770600
    },
    {
      "epoch": 27.797488226059656,
      "grad_norm": 5.282561302185059,
      "learning_rate": 3.262656985871272e-05,
      "loss": 0.5941,
      "step": 1770700
    },
    {
      "epoch": 27.79905808477237,
      "grad_norm": 4.191629409790039,
      "learning_rate": 3.262558869701727e-05,
      "loss": 0.5678,
      "step": 1770800
    },
    {
      "epoch": 27.800627943485086,
      "grad_norm": 5.022356986999512,
      "learning_rate": 3.262460753532182e-05,
      "loss": 0.6152,
      "step": 1770900
    },
    {
      "epoch": 27.802197802197803,
      "grad_norm": 4.680658340454102,
      "learning_rate": 3.262362637362638e-05,
      "loss": 0.6217,
      "step": 1771000
    },
    {
      "epoch": 27.80376766091052,
      "grad_norm": 3.1684350967407227,
      "learning_rate": 3.262264521193093e-05,
      "loss": 0.6065,
      "step": 1771100
    },
    {
      "epoch": 27.805337519623233,
      "grad_norm": 4.4574456214904785,
      "learning_rate": 3.262166405023548e-05,
      "loss": 0.5678,
      "step": 1771200
    },
    {
      "epoch": 27.80690737833595,
      "grad_norm": 4.22532844543457,
      "learning_rate": 3.262068288854003e-05,
      "loss": 0.6075,
      "step": 1771300
    },
    {
      "epoch": 27.808477237048667,
      "grad_norm": 3.7125332355499268,
      "learning_rate": 3.261970172684459e-05,
      "loss": 0.5647,
      "step": 1771400
    },
    {
      "epoch": 27.81004709576138,
      "grad_norm": 4.471550941467285,
      "learning_rate": 3.261872056514914e-05,
      "loss": 0.5865,
      "step": 1771500
    },
    {
      "epoch": 27.811616954474097,
      "grad_norm": 3.5655879974365234,
      "learning_rate": 3.261773940345369e-05,
      "loss": 0.58,
      "step": 1771600
    },
    {
      "epoch": 27.813186813186814,
      "grad_norm": 3.496870756149292,
      "learning_rate": 3.261675824175824e-05,
      "loss": 0.5992,
      "step": 1771700
    },
    {
      "epoch": 27.81475667189953,
      "grad_norm": 4.120661735534668,
      "learning_rate": 3.26157770800628e-05,
      "loss": 0.6088,
      "step": 1771800
    },
    {
      "epoch": 27.816326530612244,
      "grad_norm": 3.6459028720855713,
      "learning_rate": 3.261479591836735e-05,
      "loss": 0.5906,
      "step": 1771900
    },
    {
      "epoch": 27.81789638932496,
      "grad_norm": 3.0198745727539062,
      "learning_rate": 3.26138147566719e-05,
      "loss": 0.5655,
      "step": 1772000
    },
    {
      "epoch": 27.819466248037678,
      "grad_norm": 3.2471635341644287,
      "learning_rate": 3.261283359497646e-05,
      "loss": 0.6038,
      "step": 1772100
    },
    {
      "epoch": 27.82103610675039,
      "grad_norm": 4.1376872062683105,
      "learning_rate": 3.2611852433281e-05,
      "loss": 0.6261,
      "step": 1772200
    },
    {
      "epoch": 27.822605965463108,
      "grad_norm": 4.638736248016357,
      "learning_rate": 3.261087127158556e-05,
      "loss": 0.6281,
      "step": 1772300
    },
    {
      "epoch": 27.824175824175825,
      "grad_norm": 3.6385080814361572,
      "learning_rate": 3.260989010989011e-05,
      "loss": 0.599,
      "step": 1772400
    },
    {
      "epoch": 27.82574568288854,
      "grad_norm": 4.5476861000061035,
      "learning_rate": 3.260890894819467e-05,
      "loss": 0.6189,
      "step": 1772500
    },
    {
      "epoch": 27.827315541601255,
      "grad_norm": 3.964184045791626,
      "learning_rate": 3.260792778649921e-05,
      "loss": 0.5803,
      "step": 1772600
    },
    {
      "epoch": 27.828885400313972,
      "grad_norm": 4.236832141876221,
      "learning_rate": 3.260694662480377e-05,
      "loss": 0.6048,
      "step": 1772700
    },
    {
      "epoch": 27.83045525902669,
      "grad_norm": 4.202783107757568,
      "learning_rate": 3.260596546310832e-05,
      "loss": 0.639,
      "step": 1772800
    },
    {
      "epoch": 27.832025117739402,
      "grad_norm": 3.900581121444702,
      "learning_rate": 3.260498430141287e-05,
      "loss": 0.5468,
      "step": 1772900
    },
    {
      "epoch": 27.83359497645212,
      "grad_norm": 3.953181266784668,
      "learning_rate": 3.2604003139717424e-05,
      "loss": 0.5874,
      "step": 1773000
    },
    {
      "epoch": 27.835164835164836,
      "grad_norm": 4.187631607055664,
      "learning_rate": 3.260302197802198e-05,
      "loss": 0.5975,
      "step": 1773100
    },
    {
      "epoch": 27.836734693877553,
      "grad_norm": 3.44697642326355,
      "learning_rate": 3.260204081632653e-05,
      "loss": 0.578,
      "step": 1773200
    },
    {
      "epoch": 27.838304552590266,
      "grad_norm": 4.254178524017334,
      "learning_rate": 3.2601059654631083e-05,
      "loss": 0.5956,
      "step": 1773300
    },
    {
      "epoch": 27.839874411302983,
      "grad_norm": 2.67598032951355,
      "learning_rate": 3.2600078492935634e-05,
      "loss": 0.5723,
      "step": 1773400
    },
    {
      "epoch": 27.8414442700157,
      "grad_norm": 4.057034492492676,
      "learning_rate": 3.259909733124019e-05,
      "loss": 0.578,
      "step": 1773500
    },
    {
      "epoch": 27.843014128728413,
      "grad_norm": 4.482053279876709,
      "learning_rate": 3.259811616954474e-05,
      "loss": 0.6118,
      "step": 1773600
    },
    {
      "epoch": 27.84458398744113,
      "grad_norm": 4.203266143798828,
      "learning_rate": 3.2597135007849294e-05,
      "loss": 0.6043,
      "step": 1773700
    },
    {
      "epoch": 27.846153846153847,
      "grad_norm": 3.945507764816284,
      "learning_rate": 3.2596153846153845e-05,
      "loss": 0.6012,
      "step": 1773800
    },
    {
      "epoch": 27.847723704866564,
      "grad_norm": 3.181325674057007,
      "learning_rate": 3.25951726844584e-05,
      "loss": 0.5809,
      "step": 1773900
    },
    {
      "epoch": 27.849293563579277,
      "grad_norm": 4.480607986450195,
      "learning_rate": 3.2594191522762954e-05,
      "loss": 0.5644,
      "step": 1774000
    },
    {
      "epoch": 27.850863422291994,
      "grad_norm": 3.440335750579834,
      "learning_rate": 3.2593210361067505e-05,
      "loss": 0.6065,
      "step": 1774100
    },
    {
      "epoch": 27.85243328100471,
      "grad_norm": 3.1391208171844482,
      "learning_rate": 3.259222919937206e-05,
      "loss": 0.5847,
      "step": 1774200
    },
    {
      "epoch": 27.854003139717424,
      "grad_norm": 4.10597038269043,
      "learning_rate": 3.259124803767661e-05,
      "loss": 0.5954,
      "step": 1774300
    },
    {
      "epoch": 27.85557299843014,
      "grad_norm": 2.926549196243286,
      "learning_rate": 3.2590266875981165e-05,
      "loss": 0.6108,
      "step": 1774400
    },
    {
      "epoch": 27.857142857142858,
      "grad_norm": 3.536223888397217,
      "learning_rate": 3.2589285714285716e-05,
      "loss": 0.625,
      "step": 1774500
    },
    {
      "epoch": 27.858712715855575,
      "grad_norm": 3.6946115493774414,
      "learning_rate": 3.258830455259027e-05,
      "loss": 0.5717,
      "step": 1774600
    },
    {
      "epoch": 27.860282574568288,
      "grad_norm": 3.662701368331909,
      "learning_rate": 3.258732339089482e-05,
      "loss": 0.6152,
      "step": 1774700
    },
    {
      "epoch": 27.861852433281005,
      "grad_norm": 3.6792409420013428,
      "learning_rate": 3.2586342229199375e-05,
      "loss": 0.6181,
      "step": 1774800
    },
    {
      "epoch": 27.86342229199372,
      "grad_norm": 3.893672466278076,
      "learning_rate": 3.2585361067503926e-05,
      "loss": 0.5887,
      "step": 1774900
    },
    {
      "epoch": 27.864992150706435,
      "grad_norm": 3.9769375324249268,
      "learning_rate": 3.258437990580848e-05,
      "loss": 0.5807,
      "step": 1775000
    },
    {
      "epoch": 27.86656200941915,
      "grad_norm": 4.79705810546875,
      "learning_rate": 3.258339874411303e-05,
      "loss": 0.6239,
      "step": 1775100
    },
    {
      "epoch": 27.86813186813187,
      "grad_norm": 3.52331805229187,
      "learning_rate": 3.2582417582417586e-05,
      "loss": 0.6015,
      "step": 1775200
    },
    {
      "epoch": 27.869701726844585,
      "grad_norm": 3.24406361579895,
      "learning_rate": 3.258143642072214e-05,
      "loss": 0.6226,
      "step": 1775300
    },
    {
      "epoch": 27.8712715855573,
      "grad_norm": 2.3761823177337646,
      "learning_rate": 3.258045525902669e-05,
      "loss": 0.6031,
      "step": 1775400
    },
    {
      "epoch": 27.872841444270016,
      "grad_norm": 2.2033514976501465,
      "learning_rate": 3.257947409733124e-05,
      "loss": 0.6087,
      "step": 1775500
    },
    {
      "epoch": 27.874411302982733,
      "grad_norm": 4.041942119598389,
      "learning_rate": 3.25784929356358e-05,
      "loss": 0.5702,
      "step": 1775600
    },
    {
      "epoch": 27.875981161695446,
      "grad_norm": 3.7035233974456787,
      "learning_rate": 3.257751177394035e-05,
      "loss": 0.6169,
      "step": 1775700
    },
    {
      "epoch": 27.877551020408163,
      "grad_norm": 3.7778639793395996,
      "learning_rate": 3.25765306122449e-05,
      "loss": 0.6472,
      "step": 1775800
    },
    {
      "epoch": 27.87912087912088,
      "grad_norm": 3.7469563484191895,
      "learning_rate": 3.257554945054945e-05,
      "loss": 0.6275,
      "step": 1775900
    },
    {
      "epoch": 27.880690737833596,
      "grad_norm": 3.6623380184173584,
      "learning_rate": 3.257456828885401e-05,
      "loss": 0.5921,
      "step": 1776000
    },
    {
      "epoch": 27.88226059654631,
      "grad_norm": 3.9266481399536133,
      "learning_rate": 3.257358712715856e-05,
      "loss": 0.5528,
      "step": 1776100
    },
    {
      "epoch": 27.883830455259027,
      "grad_norm": 3.9595131874084473,
      "learning_rate": 3.257260596546311e-05,
      "loss": 0.5853,
      "step": 1776200
    },
    {
      "epoch": 27.885400313971743,
      "grad_norm": 3.6886518001556396,
      "learning_rate": 3.257162480376767e-05,
      "loss": 0.6218,
      "step": 1776300
    },
    {
      "epoch": 27.886970172684457,
      "grad_norm": 3.3763198852539062,
      "learning_rate": 3.257064364207221e-05,
      "loss": 0.5774,
      "step": 1776400
    },
    {
      "epoch": 27.888540031397174,
      "grad_norm": 3.9817521572113037,
      "learning_rate": 3.256966248037677e-05,
      "loss": 0.6182,
      "step": 1776500
    },
    {
      "epoch": 27.89010989010989,
      "grad_norm": 3.912043809890747,
      "learning_rate": 3.256868131868132e-05,
      "loss": 0.6002,
      "step": 1776600
    },
    {
      "epoch": 27.891679748822607,
      "grad_norm": 3.3605151176452637,
      "learning_rate": 3.256770015698588e-05,
      "loss": 0.5751,
      "step": 1776700
    },
    {
      "epoch": 27.89324960753532,
      "grad_norm": 3.0316200256347656,
      "learning_rate": 3.256671899529042e-05,
      "loss": 0.6027,
      "step": 1776800
    },
    {
      "epoch": 27.894819466248038,
      "grad_norm": 4.405632019042969,
      "learning_rate": 3.256573783359498e-05,
      "loss": 0.63,
      "step": 1776900
    },
    {
      "epoch": 27.896389324960754,
      "grad_norm": 3.9868576526641846,
      "learning_rate": 3.256475667189953e-05,
      "loss": 0.5885,
      "step": 1777000
    },
    {
      "epoch": 27.897959183673468,
      "grad_norm": 3.9013843536376953,
      "learning_rate": 3.256377551020408e-05,
      "loss": 0.5748,
      "step": 1777100
    },
    {
      "epoch": 27.899529042386185,
      "grad_norm": 4.4717254638671875,
      "learning_rate": 3.256279434850863e-05,
      "loss": 0.5918,
      "step": 1777200
    },
    {
      "epoch": 27.9010989010989,
      "grad_norm": 3.990919828414917,
      "learning_rate": 3.256181318681319e-05,
      "loss": 0.5988,
      "step": 1777300
    },
    {
      "epoch": 27.90266875981162,
      "grad_norm": 2.9152779579162598,
      "learning_rate": 3.256083202511774e-05,
      "loss": 0.6056,
      "step": 1777400
    },
    {
      "epoch": 27.90423861852433,
      "grad_norm": 3.6236066818237305,
      "learning_rate": 3.255985086342229e-05,
      "loss": 0.5827,
      "step": 1777500
    },
    {
      "epoch": 27.90580847723705,
      "grad_norm": 4.409853458404541,
      "learning_rate": 3.2558869701726843e-05,
      "loss": 0.6057,
      "step": 1777600
    },
    {
      "epoch": 27.907378335949765,
      "grad_norm": 3.8465869426727295,
      "learning_rate": 3.25578885400314e-05,
      "loss": 0.6168,
      "step": 1777700
    },
    {
      "epoch": 27.90894819466248,
      "grad_norm": 3.3062644004821777,
      "learning_rate": 3.255690737833595e-05,
      "loss": 0.5815,
      "step": 1777800
    },
    {
      "epoch": 27.910518053375196,
      "grad_norm": 2.316556930541992,
      "learning_rate": 3.25559262166405e-05,
      "loss": 0.5737,
      "step": 1777900
    },
    {
      "epoch": 27.912087912087912,
      "grad_norm": 3.97833514213562,
      "learning_rate": 3.2554945054945054e-05,
      "loss": 0.5788,
      "step": 1778000
    },
    {
      "epoch": 27.91365777080063,
      "grad_norm": 3.7691569328308105,
      "learning_rate": 3.255396389324961e-05,
      "loss": 0.5704,
      "step": 1778100
    },
    {
      "epoch": 27.915227629513343,
      "grad_norm": 3.28656268119812,
      "learning_rate": 3.255298273155416e-05,
      "loss": 0.5641,
      "step": 1778200
    },
    {
      "epoch": 27.91679748822606,
      "grad_norm": 3.1910767555236816,
      "learning_rate": 3.2552001569858714e-05,
      "loss": 0.6306,
      "step": 1778300
    },
    {
      "epoch": 27.918367346938776,
      "grad_norm": 3.234650135040283,
      "learning_rate": 3.255102040816327e-05,
      "loss": 0.5887,
      "step": 1778400
    },
    {
      "epoch": 27.919937205651493,
      "grad_norm": 3.617283821105957,
      "learning_rate": 3.2550039246467816e-05,
      "loss": 0.597,
      "step": 1778500
    },
    {
      "epoch": 27.921507064364206,
      "grad_norm": 3.808948516845703,
      "learning_rate": 3.2549058084772374e-05,
      "loss": 0.5856,
      "step": 1778600
    },
    {
      "epoch": 27.923076923076923,
      "grad_norm": 4.331645488739014,
      "learning_rate": 3.2548076923076925e-05,
      "loss": 0.5925,
      "step": 1778700
    },
    {
      "epoch": 27.92464678178964,
      "grad_norm": 4.43372106552124,
      "learning_rate": 3.254709576138148e-05,
      "loss": 0.6548,
      "step": 1778800
    },
    {
      "epoch": 27.926216640502354,
      "grad_norm": 3.897289752960205,
      "learning_rate": 3.2546114599686026e-05,
      "loss": 0.6294,
      "step": 1778900
    },
    {
      "epoch": 27.92778649921507,
      "grad_norm": 4.660130500793457,
      "learning_rate": 3.2545133437990584e-05,
      "loss": 0.574,
      "step": 1779000
    },
    {
      "epoch": 27.929356357927787,
      "grad_norm": 3.5445845127105713,
      "learning_rate": 3.2544152276295135e-05,
      "loss": 0.5756,
      "step": 1779100
    },
    {
      "epoch": 27.9309262166405,
      "grad_norm": 4.2782392501831055,
      "learning_rate": 3.2543171114599686e-05,
      "loss": 0.581,
      "step": 1779200
    },
    {
      "epoch": 27.932496075353217,
      "grad_norm": 2.862893581390381,
      "learning_rate": 3.254218995290424e-05,
      "loss": 0.5852,
      "step": 1779300
    },
    {
      "epoch": 27.934065934065934,
      "grad_norm": 3.8119704723358154,
      "learning_rate": 3.2541208791208795e-05,
      "loss": 0.6262,
      "step": 1779400
    },
    {
      "epoch": 27.93563579277865,
      "grad_norm": 3.2147793769836426,
      "learning_rate": 3.2540227629513346e-05,
      "loss": 0.5612,
      "step": 1779500
    },
    {
      "epoch": 27.937205651491364,
      "grad_norm": 4.134511947631836,
      "learning_rate": 3.25392464678179e-05,
      "loss": 0.6025,
      "step": 1779600
    },
    {
      "epoch": 27.93877551020408,
      "grad_norm": 4.664165019989014,
      "learning_rate": 3.253826530612245e-05,
      "loss": 0.5731,
      "step": 1779700
    },
    {
      "epoch": 27.940345368916798,
      "grad_norm": 4.042933940887451,
      "learning_rate": 3.2537284144427006e-05,
      "loss": 0.5978,
      "step": 1779800
    },
    {
      "epoch": 27.941915227629515,
      "grad_norm": 3.6888675689697266,
      "learning_rate": 3.253630298273156e-05,
      "loss": 0.5564,
      "step": 1779900
    },
    {
      "epoch": 27.94348508634223,
      "grad_norm": 2.905897617340088,
      "learning_rate": 3.253532182103611e-05,
      "loss": 0.6099,
      "step": 1780000
    },
    {
      "epoch": 27.945054945054945,
      "grad_norm": 3.2361836433410645,
      "learning_rate": 3.253434065934066e-05,
      "loss": 0.6045,
      "step": 1780100
    },
    {
      "epoch": 27.946624803767662,
      "grad_norm": 4.290334701538086,
      "learning_rate": 3.2533359497645216e-05,
      "loss": 0.5698,
      "step": 1780200
    },
    {
      "epoch": 27.948194662480375,
      "grad_norm": 3.686685800552368,
      "learning_rate": 3.253237833594977e-05,
      "loss": 0.6158,
      "step": 1780300
    },
    {
      "epoch": 27.949764521193092,
      "grad_norm": 4.502987384796143,
      "learning_rate": 3.253139717425432e-05,
      "loss": 0.5913,
      "step": 1780400
    },
    {
      "epoch": 27.95133437990581,
      "grad_norm": 2.337632179260254,
      "learning_rate": 3.2530416012558876e-05,
      "loss": 0.6151,
      "step": 1780500
    },
    {
      "epoch": 27.952904238618526,
      "grad_norm": 3.7087786197662354,
      "learning_rate": 3.252943485086342e-05,
      "loss": 0.6339,
      "step": 1780600
    },
    {
      "epoch": 27.95447409733124,
      "grad_norm": 3.6481971740722656,
      "learning_rate": 3.252845368916798e-05,
      "loss": 0.605,
      "step": 1780700
    },
    {
      "epoch": 27.956043956043956,
      "grad_norm": 2.3794803619384766,
      "learning_rate": 3.252747252747253e-05,
      "loss": 0.5826,
      "step": 1780800
    },
    {
      "epoch": 27.957613814756673,
      "grad_norm": 3.7089898586273193,
      "learning_rate": 3.252649136577709e-05,
      "loss": 0.6291,
      "step": 1780900
    },
    {
      "epoch": 27.959183673469386,
      "grad_norm": 3.4371588230133057,
      "learning_rate": 3.252551020408163e-05,
      "loss": 0.6445,
      "step": 1781000
    },
    {
      "epoch": 27.960753532182103,
      "grad_norm": 3.587888479232788,
      "learning_rate": 3.252452904238619e-05,
      "loss": 0.6299,
      "step": 1781100
    },
    {
      "epoch": 27.96232339089482,
      "grad_norm": 3.544034004211426,
      "learning_rate": 3.252354788069074e-05,
      "loss": 0.5143,
      "step": 1781200
    },
    {
      "epoch": 27.963893249607537,
      "grad_norm": 3.1787025928497314,
      "learning_rate": 3.252256671899529e-05,
      "loss": 0.6095,
      "step": 1781300
    },
    {
      "epoch": 27.96546310832025,
      "grad_norm": 4.623300075531006,
      "learning_rate": 3.252158555729984e-05,
      "loss": 0.5905,
      "step": 1781400
    },
    {
      "epoch": 27.967032967032967,
      "grad_norm": 4.40675687789917,
      "learning_rate": 3.25206043956044e-05,
      "loss": 0.5559,
      "step": 1781500
    },
    {
      "epoch": 27.968602825745684,
      "grad_norm": 3.45936918258667,
      "learning_rate": 3.251962323390895e-05,
      "loss": 0.5506,
      "step": 1781600
    },
    {
      "epoch": 27.970172684458397,
      "grad_norm": 4.393157482147217,
      "learning_rate": 3.25186420722135e-05,
      "loss": 0.5821,
      "step": 1781700
    },
    {
      "epoch": 27.971742543171114,
      "grad_norm": 3.6818044185638428,
      "learning_rate": 3.251766091051805e-05,
      "loss": 0.5573,
      "step": 1781800
    },
    {
      "epoch": 27.97331240188383,
      "grad_norm": 3.7767598628997803,
      "learning_rate": 3.251667974882261e-05,
      "loss": 0.61,
      "step": 1781900
    },
    {
      "epoch": 27.974882260596548,
      "grad_norm": 1.803137183189392,
      "learning_rate": 3.251569858712716e-05,
      "loss": 0.5741,
      "step": 1782000
    },
    {
      "epoch": 27.97645211930926,
      "grad_norm": 3.2524290084838867,
      "learning_rate": 3.251471742543171e-05,
      "loss": 0.5852,
      "step": 1782100
    },
    {
      "epoch": 27.978021978021978,
      "grad_norm": 4.912754058837891,
      "learning_rate": 3.251373626373626e-05,
      "loss": 0.6315,
      "step": 1782200
    },
    {
      "epoch": 27.979591836734695,
      "grad_norm": 3.844792604446411,
      "learning_rate": 3.251275510204082e-05,
      "loss": 0.5933,
      "step": 1782300
    },
    {
      "epoch": 27.98116169544741,
      "grad_norm": 4.274234771728516,
      "learning_rate": 3.251177394034537e-05,
      "loss": 0.6119,
      "step": 1782400
    },
    {
      "epoch": 27.982731554160125,
      "grad_norm": 4.515601634979248,
      "learning_rate": 3.251079277864992e-05,
      "loss": 0.5802,
      "step": 1782500
    },
    {
      "epoch": 27.984301412872842,
      "grad_norm": 2.7558958530426025,
      "learning_rate": 3.250981161695448e-05,
      "loss": 0.594,
      "step": 1782600
    },
    {
      "epoch": 27.98587127158556,
      "grad_norm": 3.565887212753296,
      "learning_rate": 3.2508830455259025e-05,
      "loss": 0.6118,
      "step": 1782700
    },
    {
      "epoch": 27.987441130298272,
      "grad_norm": 3.9740140438079834,
      "learning_rate": 3.250784929356358e-05,
      "loss": 0.5776,
      "step": 1782800
    },
    {
      "epoch": 27.98901098901099,
      "grad_norm": 3.9193837642669678,
      "learning_rate": 3.2506868131868134e-05,
      "loss": 0.5833,
      "step": 1782900
    },
    {
      "epoch": 27.990580847723706,
      "grad_norm": 3.5698087215423584,
      "learning_rate": 3.250588697017269e-05,
      "loss": 0.5879,
      "step": 1783000
    },
    {
      "epoch": 27.99215070643642,
      "grad_norm": 4.5029425621032715,
      "learning_rate": 3.2504905808477235e-05,
      "loss": 0.5894,
      "step": 1783100
    },
    {
      "epoch": 27.993720565149136,
      "grad_norm": 3.379657745361328,
      "learning_rate": 3.250392464678179e-05,
      "loss": 0.6282,
      "step": 1783200
    },
    {
      "epoch": 27.995290423861853,
      "grad_norm": 4.406442642211914,
      "learning_rate": 3.2502943485086344e-05,
      "loss": 0.6324,
      "step": 1783300
    },
    {
      "epoch": 27.99686028257457,
      "grad_norm": 4.088151931762695,
      "learning_rate": 3.2501962323390895e-05,
      "loss": 0.5785,
      "step": 1783400
    },
    {
      "epoch": 27.998430141287283,
      "grad_norm": 3.566389560699463,
      "learning_rate": 3.2500981161695446e-05,
      "loss": 0.5663,
      "step": 1783500
    },
    {
      "epoch": 28.0,
      "grad_norm": 4.409987926483154,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.6044,
      "step": 1783600
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.034179449081421,
      "eval_runtime": 14.7879,
      "eval_samples_per_second": 226.739,
      "eval_steps_per_second": 226.739,
      "step": 1783600
    },
    {
      "epoch": 28.0,
      "eval_loss": 0.4573740065097809,
      "eval_runtime": 280.1112,
      "eval_samples_per_second": 227.41,
      "eval_steps_per_second": 227.41,
      "step": 1783600
    },
    {
      "epoch": 28.001569858712717,
      "grad_norm": 3.6356494426727295,
      "learning_rate": 3.2499018838304555e-05,
      "loss": 0.5642,
      "step": 1783700
    },
    {
      "epoch": 28.00313971742543,
      "grad_norm": 3.7324905395507812,
      "learning_rate": 3.2498037676609106e-05,
      "loss": 0.6049,
      "step": 1783800
    },
    {
      "epoch": 28.004709576138147,
      "grad_norm": 4.317914009094238,
      "learning_rate": 3.249705651491366e-05,
      "loss": 0.5738,
      "step": 1783900
    },
    {
      "epoch": 28.006279434850864,
      "grad_norm": 4.3524489402771,
      "learning_rate": 3.2496075353218215e-05,
      "loss": 0.6103,
      "step": 1784000
    },
    {
      "epoch": 28.00784929356358,
      "grad_norm": 4.271137714385986,
      "learning_rate": 3.2495094191522766e-05,
      "loss": 0.6073,
      "step": 1784100
    },
    {
      "epoch": 28.009419152276294,
      "grad_norm": 3.771278142929077,
      "learning_rate": 3.2494113029827317e-05,
      "loss": 0.6085,
      "step": 1784200
    },
    {
      "epoch": 28.01098901098901,
      "grad_norm": 4.632118225097656,
      "learning_rate": 3.249313186813187e-05,
      "loss": 0.564,
      "step": 1784300
    },
    {
      "epoch": 28.012558869701728,
      "grad_norm": 3.3246233463287354,
      "learning_rate": 3.2492150706436425e-05,
      "loss": 0.5882,
      "step": 1784400
    },
    {
      "epoch": 28.01412872841444,
      "grad_norm": 2.5087950229644775,
      "learning_rate": 3.2491169544740976e-05,
      "loss": 0.5944,
      "step": 1784500
    },
    {
      "epoch": 28.015698587127158,
      "grad_norm": 3.8915767669677734,
      "learning_rate": 3.249018838304553e-05,
      "loss": 0.5774,
      "step": 1784600
    },
    {
      "epoch": 28.017268445839875,
      "grad_norm": 3.2848856449127197,
      "learning_rate": 3.2489207221350085e-05,
      "loss": 0.6113,
      "step": 1784700
    },
    {
      "epoch": 28.01883830455259,
      "grad_norm": 4.586867809295654,
      "learning_rate": 3.248822605965463e-05,
      "loss": 0.5961,
      "step": 1784800
    },
    {
      "epoch": 28.020408163265305,
      "grad_norm": 3.8049581050872803,
      "learning_rate": 3.248724489795919e-05,
      "loss": 0.6005,
      "step": 1784900
    },
    {
      "epoch": 28.021978021978022,
      "grad_norm": 4.629854679107666,
      "learning_rate": 3.248626373626374e-05,
      "loss": 0.5873,
      "step": 1785000
    },
    {
      "epoch": 28.02354788069074,
      "grad_norm": 3.2717444896698,
      "learning_rate": 3.2485282574568296e-05,
      "loss": 0.6034,
      "step": 1785100
    },
    {
      "epoch": 28.025117739403452,
      "grad_norm": 3.277076244354248,
      "learning_rate": 3.248430141287284e-05,
      "loss": 0.5724,
      "step": 1785200
    },
    {
      "epoch": 28.02668759811617,
      "grad_norm": 3.3054544925689697,
      "learning_rate": 3.24833202511774e-05,
      "loss": 0.563,
      "step": 1785300
    },
    {
      "epoch": 28.028257456828886,
      "grad_norm": 4.081332206726074,
      "learning_rate": 3.248233908948195e-05,
      "loss": 0.6029,
      "step": 1785400
    },
    {
      "epoch": 28.029827315541603,
      "grad_norm": 3.515031576156616,
      "learning_rate": 3.24813579277865e-05,
      "loss": 0.5628,
      "step": 1785500
    },
    {
      "epoch": 28.031397174254316,
      "grad_norm": 4.68601131439209,
      "learning_rate": 3.248037676609105e-05,
      "loss": 0.5948,
      "step": 1785600
    },
    {
      "epoch": 28.032967032967033,
      "grad_norm": 4.303976058959961,
      "learning_rate": 3.247939560439561e-05,
      "loss": 0.6037,
      "step": 1785700
    },
    {
      "epoch": 28.03453689167975,
      "grad_norm": 3.5388481616973877,
      "learning_rate": 3.247841444270016e-05,
      "loss": 0.5811,
      "step": 1785800
    },
    {
      "epoch": 28.036106750392463,
      "grad_norm": 3.369926929473877,
      "learning_rate": 3.247743328100471e-05,
      "loss": 0.5573,
      "step": 1785900
    },
    {
      "epoch": 28.03767660910518,
      "grad_norm": 3.3833937644958496,
      "learning_rate": 3.247645211930926e-05,
      "loss": 0.5458,
      "step": 1786000
    },
    {
      "epoch": 28.039246467817897,
      "grad_norm": 3.565286159515381,
      "learning_rate": 3.247547095761382e-05,
      "loss": 0.5699,
      "step": 1786100
    },
    {
      "epoch": 28.040816326530614,
      "grad_norm": 3.930379867553711,
      "learning_rate": 3.247448979591837e-05,
      "loss": 0.5652,
      "step": 1786200
    },
    {
      "epoch": 28.042386185243327,
      "grad_norm": 4.355714797973633,
      "learning_rate": 3.247350863422292e-05,
      "loss": 0.5702,
      "step": 1786300
    },
    {
      "epoch": 28.043956043956044,
      "grad_norm": 2.87691330909729,
      "learning_rate": 3.247252747252747e-05,
      "loss": 0.5886,
      "step": 1786400
    },
    {
      "epoch": 28.04552590266876,
      "grad_norm": 3.7695729732513428,
      "learning_rate": 3.247154631083202e-05,
      "loss": 0.6197,
      "step": 1786500
    },
    {
      "epoch": 28.047095761381474,
      "grad_norm": 4.439157962799072,
      "learning_rate": 3.247056514913658e-05,
      "loss": 0.6139,
      "step": 1786600
    },
    {
      "epoch": 28.04866562009419,
      "grad_norm": 4.188358783721924,
      "learning_rate": 3.246958398744113e-05,
      "loss": 0.5872,
      "step": 1786700
    },
    {
      "epoch": 28.050235478806908,
      "grad_norm": 4.083838939666748,
      "learning_rate": 3.246860282574569e-05,
      "loss": 0.6086,
      "step": 1786800
    },
    {
      "epoch": 28.051805337519625,
      "grad_norm": 3.4082705974578857,
      "learning_rate": 3.2467621664050234e-05,
      "loss": 0.5716,
      "step": 1786900
    },
    {
      "epoch": 28.053375196232338,
      "grad_norm": 3.2207014560699463,
      "learning_rate": 3.246664050235479e-05,
      "loss": 0.5536,
      "step": 1787000
    },
    {
      "epoch": 28.054945054945055,
      "grad_norm": 4.368997573852539,
      "learning_rate": 3.246565934065934e-05,
      "loss": 0.5906,
      "step": 1787100
    },
    {
      "epoch": 28.05651491365777,
      "grad_norm": 3.08384370803833,
      "learning_rate": 3.2464678178963893e-05,
      "loss": 0.5677,
      "step": 1787200
    },
    {
      "epoch": 28.058084772370485,
      "grad_norm": 2.742276430130005,
      "learning_rate": 3.2463697017268444e-05,
      "loss": 0.6074,
      "step": 1787300
    },
    {
      "epoch": 28.059654631083202,
      "grad_norm": 4.0809831619262695,
      "learning_rate": 3.2462715855573e-05,
      "loss": 0.5991,
      "step": 1787400
    },
    {
      "epoch": 28.06122448979592,
      "grad_norm": 4.083714962005615,
      "learning_rate": 3.246173469387755e-05,
      "loss": 0.5815,
      "step": 1787500
    },
    {
      "epoch": 28.062794348508636,
      "grad_norm": 3.2701075077056885,
      "learning_rate": 3.2460753532182104e-05,
      "loss": 0.5931,
      "step": 1787600
    },
    {
      "epoch": 28.06436420722135,
      "grad_norm": 4.034063339233398,
      "learning_rate": 3.2459772370486655e-05,
      "loss": 0.6019,
      "step": 1787700
    },
    {
      "epoch": 28.065934065934066,
      "grad_norm": 3.4096109867095947,
      "learning_rate": 3.245879120879121e-05,
      "loss": 0.5831,
      "step": 1787800
    },
    {
      "epoch": 28.067503924646783,
      "grad_norm": 4.054178714752197,
      "learning_rate": 3.245781004709576e-05,
      "loss": 0.5901,
      "step": 1787900
    },
    {
      "epoch": 28.069073783359496,
      "grad_norm": 2.647993803024292,
      "learning_rate": 3.2456828885400315e-05,
      "loss": 0.5727,
      "step": 1788000
    },
    {
      "epoch": 28.070643642072213,
      "grad_norm": 3.8222761154174805,
      "learning_rate": 3.2455847723704866e-05,
      "loss": 0.5672,
      "step": 1788100
    },
    {
      "epoch": 28.07221350078493,
      "grad_norm": 2.9844377040863037,
      "learning_rate": 3.2454866562009424e-05,
      "loss": 0.5634,
      "step": 1788200
    },
    {
      "epoch": 28.073783359497646,
      "grad_norm": 3.6144211292266846,
      "learning_rate": 3.2453885400313975e-05,
      "loss": 0.5814,
      "step": 1788300
    },
    {
      "epoch": 28.07535321821036,
      "grad_norm": 3.6010611057281494,
      "learning_rate": 3.2452904238618526e-05,
      "loss": 0.5835,
      "step": 1788400
    },
    {
      "epoch": 28.076923076923077,
      "grad_norm": 4.899683952331543,
      "learning_rate": 3.2451923076923077e-05,
      "loss": 0.6085,
      "step": 1788500
    },
    {
      "epoch": 28.078492935635794,
      "grad_norm": 3.4600536823272705,
      "learning_rate": 3.245094191522763e-05,
      "loss": 0.5557,
      "step": 1788600
    },
    {
      "epoch": 28.08006279434851,
      "grad_norm": 4.807553768157959,
      "learning_rate": 3.2449960753532185e-05,
      "loss": 0.6251,
      "step": 1788700
    },
    {
      "epoch": 28.081632653061224,
      "grad_norm": 3.4201033115386963,
      "learning_rate": 3.2448979591836736e-05,
      "loss": 0.6109,
      "step": 1788800
    },
    {
      "epoch": 28.08320251177394,
      "grad_norm": 3.624589681625366,
      "learning_rate": 3.2447998430141294e-05,
      "loss": 0.5706,
      "step": 1788900
    },
    {
      "epoch": 28.084772370486657,
      "grad_norm": 3.8440330028533936,
      "learning_rate": 3.244701726844584e-05,
      "loss": 0.5907,
      "step": 1789000
    },
    {
      "epoch": 28.08634222919937,
      "grad_norm": 4.465712070465088,
      "learning_rate": 3.2446036106750396e-05,
      "loss": 0.5879,
      "step": 1789100
    },
    {
      "epoch": 28.087912087912088,
      "grad_norm": 3.3686165809631348,
      "learning_rate": 3.244505494505495e-05,
      "loss": 0.6211,
      "step": 1789200
    },
    {
      "epoch": 28.089481946624804,
      "grad_norm": 2.05901837348938,
      "learning_rate": 3.24440737833595e-05,
      "loss": 0.6103,
      "step": 1789300
    },
    {
      "epoch": 28.09105180533752,
      "grad_norm": 3.2686755657196045,
      "learning_rate": 3.244309262166405e-05,
      "loss": 0.5731,
      "step": 1789400
    },
    {
      "epoch": 28.092621664050235,
      "grad_norm": 5.029044151306152,
      "learning_rate": 3.244211145996861e-05,
      "loss": 0.5725,
      "step": 1789500
    },
    {
      "epoch": 28.09419152276295,
      "grad_norm": 2.786921262741089,
      "learning_rate": 3.244113029827316e-05,
      "loss": 0.5946,
      "step": 1789600
    },
    {
      "epoch": 28.09576138147567,
      "grad_norm": 4.524559497833252,
      "learning_rate": 3.244014913657771e-05,
      "loss": 0.582,
      "step": 1789700
    },
    {
      "epoch": 28.09733124018838,
      "grad_norm": 4.161696910858154,
      "learning_rate": 3.243916797488226e-05,
      "loss": 0.6048,
      "step": 1789800
    },
    {
      "epoch": 28.0989010989011,
      "grad_norm": 2.7133820056915283,
      "learning_rate": 3.243818681318682e-05,
      "loss": 0.6011,
      "step": 1789900
    },
    {
      "epoch": 28.100470957613815,
      "grad_norm": 4.1059675216674805,
      "learning_rate": 3.243720565149136e-05,
      "loss": 0.5972,
      "step": 1790000
    },
    {
      "epoch": 28.102040816326532,
      "grad_norm": 2.953204870223999,
      "learning_rate": 3.243622448979592e-05,
      "loss": 0.6131,
      "step": 1790100
    },
    {
      "epoch": 28.103610675039246,
      "grad_norm": 4.126589775085449,
      "learning_rate": 3.243524332810047e-05,
      "loss": 0.5826,
      "step": 1790200
    },
    {
      "epoch": 28.105180533751962,
      "grad_norm": 3.116986036300659,
      "learning_rate": 3.243426216640503e-05,
      "loss": 0.5847,
      "step": 1790300
    },
    {
      "epoch": 28.10675039246468,
      "grad_norm": 4.266510486602783,
      "learning_rate": 3.243328100470958e-05,
      "loss": 0.5946,
      "step": 1790400
    },
    {
      "epoch": 28.108320251177393,
      "grad_norm": 4.108893871307373,
      "learning_rate": 3.243229984301413e-05,
      "loss": 0.5946,
      "step": 1790500
    },
    {
      "epoch": 28.10989010989011,
      "grad_norm": 2.64837646484375,
      "learning_rate": 3.243131868131868e-05,
      "loss": 0.5541,
      "step": 1790600
    },
    {
      "epoch": 28.111459968602826,
      "grad_norm": 3.2569198608398438,
      "learning_rate": 3.243033751962323e-05,
      "loss": 0.5636,
      "step": 1790700
    },
    {
      "epoch": 28.113029827315543,
      "grad_norm": 3.463514804840088,
      "learning_rate": 3.242935635792779e-05,
      "loss": 0.5806,
      "step": 1790800
    },
    {
      "epoch": 28.114599686028257,
      "grad_norm": 4.278754234313965,
      "learning_rate": 3.242837519623234e-05,
      "loss": 0.5625,
      "step": 1790900
    },
    {
      "epoch": 28.116169544740973,
      "grad_norm": 4.277642250061035,
      "learning_rate": 3.24273940345369e-05,
      "loss": 0.5825,
      "step": 1791000
    },
    {
      "epoch": 28.11773940345369,
      "grad_norm": 3.647867202758789,
      "learning_rate": 3.242641287284144e-05,
      "loss": 0.5771,
      "step": 1791100
    },
    {
      "epoch": 28.119309262166404,
      "grad_norm": 4.7611589431762695,
      "learning_rate": 3.2425431711146e-05,
      "loss": 0.5924,
      "step": 1791200
    },
    {
      "epoch": 28.12087912087912,
      "grad_norm": 3.5799636840820312,
      "learning_rate": 3.242445054945055e-05,
      "loss": 0.5977,
      "step": 1791300
    },
    {
      "epoch": 28.122448979591837,
      "grad_norm": 4.506038665771484,
      "learning_rate": 3.24234693877551e-05,
      "loss": 0.6362,
      "step": 1791400
    },
    {
      "epoch": 28.124018838304554,
      "grad_norm": 4.138121604919434,
      "learning_rate": 3.2422488226059653e-05,
      "loss": 0.5893,
      "step": 1791500
    },
    {
      "epoch": 28.125588697017267,
      "grad_norm": 3.506531238555908,
      "learning_rate": 3.242150706436421e-05,
      "loss": 0.5975,
      "step": 1791600
    },
    {
      "epoch": 28.127158555729984,
      "grad_norm": 1.985177755355835,
      "learning_rate": 3.242052590266876e-05,
      "loss": 0.5946,
      "step": 1791700
    },
    {
      "epoch": 28.1287284144427,
      "grad_norm": 4.38767671585083,
      "learning_rate": 3.241954474097331e-05,
      "loss": 0.5955,
      "step": 1791800
    },
    {
      "epoch": 28.130298273155415,
      "grad_norm": 3.07438588142395,
      "learning_rate": 3.2418563579277864e-05,
      "loss": 0.6212,
      "step": 1791900
    },
    {
      "epoch": 28.13186813186813,
      "grad_norm": 3.336956024169922,
      "learning_rate": 3.241758241758242e-05,
      "loss": 0.5559,
      "step": 1792000
    },
    {
      "epoch": 28.13343799058085,
      "grad_norm": 5.140725612640381,
      "learning_rate": 3.2416601255886966e-05,
      "loss": 0.5876,
      "step": 1792100
    },
    {
      "epoch": 28.135007849293565,
      "grad_norm": 3.483003854751587,
      "learning_rate": 3.2415620094191524e-05,
      "loss": 0.5839,
      "step": 1792200
    },
    {
      "epoch": 28.13657770800628,
      "grad_norm": 4.343088626861572,
      "learning_rate": 3.2414638932496075e-05,
      "loss": 0.565,
      "step": 1792300
    },
    {
      "epoch": 28.138147566718995,
      "grad_norm": 3.7160415649414062,
      "learning_rate": 3.241365777080063e-05,
      "loss": 0.6149,
      "step": 1792400
    },
    {
      "epoch": 28.139717425431712,
      "grad_norm": 3.7560505867004395,
      "learning_rate": 3.2412676609105184e-05,
      "loss": 0.5856,
      "step": 1792500
    },
    {
      "epoch": 28.141287284144425,
      "grad_norm": 4.3147993087768555,
      "learning_rate": 3.2411695447409735e-05,
      "loss": 0.558,
      "step": 1792600
    },
    {
      "epoch": 28.142857142857142,
      "grad_norm": 3.2380921840667725,
      "learning_rate": 3.2410714285714286e-05,
      "loss": 0.5869,
      "step": 1792700
    },
    {
      "epoch": 28.14442700156986,
      "grad_norm": 3.985267162322998,
      "learning_rate": 3.2409733124018837e-05,
      "loss": 0.5766,
      "step": 1792800
    },
    {
      "epoch": 28.145996860282576,
      "grad_norm": 4.557409763336182,
      "learning_rate": 3.2408751962323394e-05,
      "loss": 0.5825,
      "step": 1792900
    },
    {
      "epoch": 28.14756671899529,
      "grad_norm": 4.201638221740723,
      "learning_rate": 3.2407770800627945e-05,
      "loss": 0.6018,
      "step": 1793000
    },
    {
      "epoch": 28.149136577708006,
      "grad_norm": 2.9464874267578125,
      "learning_rate": 3.24067896389325e-05,
      "loss": 0.5871,
      "step": 1793100
    },
    {
      "epoch": 28.150706436420723,
      "grad_norm": 4.046304225921631,
      "learning_rate": 3.240580847723705e-05,
      "loss": 0.5981,
      "step": 1793200
    },
    {
      "epoch": 28.152276295133436,
      "grad_norm": 3.7886104583740234,
      "learning_rate": 3.2404827315541605e-05,
      "loss": 0.5895,
      "step": 1793300
    },
    {
      "epoch": 28.153846153846153,
      "grad_norm": 2.6602859497070312,
      "learning_rate": 3.2403846153846156e-05,
      "loss": 0.6015,
      "step": 1793400
    },
    {
      "epoch": 28.15541601255887,
      "grad_norm": 4.170382976531982,
      "learning_rate": 3.240286499215071e-05,
      "loss": 0.5311,
      "step": 1793500
    },
    {
      "epoch": 28.156985871271587,
      "grad_norm": 2.4131603240966797,
      "learning_rate": 3.240188383045526e-05,
      "loss": 0.5753,
      "step": 1793600
    },
    {
      "epoch": 28.1585557299843,
      "grad_norm": 3.101973295211792,
      "learning_rate": 3.2400902668759816e-05,
      "loss": 0.5697,
      "step": 1793700
    },
    {
      "epoch": 28.160125588697017,
      "grad_norm": 3.7080655097961426,
      "learning_rate": 3.239992150706437e-05,
      "loss": 0.5779,
      "step": 1793800
    },
    {
      "epoch": 28.161695447409734,
      "grad_norm": 4.561207294464111,
      "learning_rate": 3.239894034536892e-05,
      "loss": 0.5687,
      "step": 1793900
    },
    {
      "epoch": 28.163265306122447,
      "grad_norm": 3.877918243408203,
      "learning_rate": 3.239795918367347e-05,
      "loss": 0.5906,
      "step": 1794000
    },
    {
      "epoch": 28.164835164835164,
      "grad_norm": 4.112221717834473,
      "learning_rate": 3.2396978021978026e-05,
      "loss": 0.6404,
      "step": 1794100
    },
    {
      "epoch": 28.16640502354788,
      "grad_norm": 2.721818685531616,
      "learning_rate": 3.239599686028257e-05,
      "loss": 0.5634,
      "step": 1794200
    },
    {
      "epoch": 28.167974882260598,
      "grad_norm": 4.357783794403076,
      "learning_rate": 3.239501569858713e-05,
      "loss": 0.6093,
      "step": 1794300
    },
    {
      "epoch": 28.16954474097331,
      "grad_norm": 3.1337332725524902,
      "learning_rate": 3.239403453689168e-05,
      "loss": 0.5848,
      "step": 1794400
    },
    {
      "epoch": 28.171114599686028,
      "grad_norm": 4.54861307144165,
      "learning_rate": 3.239305337519624e-05,
      "loss": 0.5716,
      "step": 1794500
    },
    {
      "epoch": 28.172684458398745,
      "grad_norm": 3.895559072494507,
      "learning_rate": 3.239207221350079e-05,
      "loss": 0.5925,
      "step": 1794600
    },
    {
      "epoch": 28.17425431711146,
      "grad_norm": 4.119795322418213,
      "learning_rate": 3.239109105180534e-05,
      "loss": 0.5789,
      "step": 1794700
    },
    {
      "epoch": 28.175824175824175,
      "grad_norm": 4.6072773933410645,
      "learning_rate": 3.239010989010989e-05,
      "loss": 0.5941,
      "step": 1794800
    },
    {
      "epoch": 28.177394034536892,
      "grad_norm": 3.6204137802124023,
      "learning_rate": 3.238912872841444e-05,
      "loss": 0.5959,
      "step": 1794900
    },
    {
      "epoch": 28.17896389324961,
      "grad_norm": 3.7325899600982666,
      "learning_rate": 3.2388147566719e-05,
      "loss": 0.6054,
      "step": 1795000
    },
    {
      "epoch": 28.180533751962322,
      "grad_norm": 3.970163106918335,
      "learning_rate": 3.238716640502355e-05,
      "loss": 0.6094,
      "step": 1795100
    },
    {
      "epoch": 28.18210361067504,
      "grad_norm": 4.362546443939209,
      "learning_rate": 3.238618524332811e-05,
      "loss": 0.6176,
      "step": 1795200
    },
    {
      "epoch": 28.183673469387756,
      "grad_norm": 4.4046807289123535,
      "learning_rate": 3.238520408163265e-05,
      "loss": 0.5778,
      "step": 1795300
    },
    {
      "epoch": 28.18524332810047,
      "grad_norm": 2.580451250076294,
      "learning_rate": 3.238422291993721e-05,
      "loss": 0.5262,
      "step": 1795400
    },
    {
      "epoch": 28.186813186813186,
      "grad_norm": 3.8837718963623047,
      "learning_rate": 3.238324175824176e-05,
      "loss": 0.558,
      "step": 1795500
    },
    {
      "epoch": 28.188383045525903,
      "grad_norm": 2.0375025272369385,
      "learning_rate": 3.238226059654631e-05,
      "loss": 0.5973,
      "step": 1795600
    },
    {
      "epoch": 28.18995290423862,
      "grad_norm": 2.669703245162964,
      "learning_rate": 3.238127943485086e-05,
      "loss": 0.6275,
      "step": 1795700
    },
    {
      "epoch": 28.191522762951333,
      "grad_norm": 3.7755300998687744,
      "learning_rate": 3.238029827315542e-05,
      "loss": 0.602,
      "step": 1795800
    },
    {
      "epoch": 28.19309262166405,
      "grad_norm": 4.035808086395264,
      "learning_rate": 3.237931711145997e-05,
      "loss": 0.6407,
      "step": 1795900
    },
    {
      "epoch": 28.194662480376767,
      "grad_norm": 4.488411903381348,
      "learning_rate": 3.237833594976452e-05,
      "loss": 0.6048,
      "step": 1796000
    },
    {
      "epoch": 28.19623233908948,
      "grad_norm": 3.60056209564209,
      "learning_rate": 3.237735478806907e-05,
      "loss": 0.5893,
      "step": 1796100
    },
    {
      "epoch": 28.197802197802197,
      "grad_norm": 4.163902282714844,
      "learning_rate": 3.237637362637363e-05,
      "loss": 0.6049,
      "step": 1796200
    },
    {
      "epoch": 28.199372056514914,
      "grad_norm": 3.982584238052368,
      "learning_rate": 3.2375392464678175e-05,
      "loss": 0.6061,
      "step": 1796300
    },
    {
      "epoch": 28.20094191522763,
      "grad_norm": 4.029566287994385,
      "learning_rate": 3.237441130298273e-05,
      "loss": 0.637,
      "step": 1796400
    },
    {
      "epoch": 28.202511773940344,
      "grad_norm": 4.053407192230225,
      "learning_rate": 3.2373430141287284e-05,
      "loss": 0.6078,
      "step": 1796500
    },
    {
      "epoch": 28.20408163265306,
      "grad_norm": 3.6132607460021973,
      "learning_rate": 3.237244897959184e-05,
      "loss": 0.6154,
      "step": 1796600
    },
    {
      "epoch": 28.205651491365778,
      "grad_norm": 4.341475963592529,
      "learning_rate": 3.237146781789639e-05,
      "loss": 0.6157,
      "step": 1796700
    },
    {
      "epoch": 28.20722135007849,
      "grad_norm": 5.245952129364014,
      "learning_rate": 3.2370486656200944e-05,
      "loss": 0.5827,
      "step": 1796800
    },
    {
      "epoch": 28.208791208791208,
      "grad_norm": 4.281078338623047,
      "learning_rate": 3.2369505494505495e-05,
      "loss": 0.6257,
      "step": 1796900
    },
    {
      "epoch": 28.210361067503925,
      "grad_norm": 4.409888744354248,
      "learning_rate": 3.2368524332810045e-05,
      "loss": 0.6082,
      "step": 1797000
    },
    {
      "epoch": 28.211930926216642,
      "grad_norm": 4.256036281585693,
      "learning_rate": 3.23675431711146e-05,
      "loss": 0.5314,
      "step": 1797100
    },
    {
      "epoch": 28.213500784929355,
      "grad_norm": 2.97298002243042,
      "learning_rate": 3.2366562009419154e-05,
      "loss": 0.5629,
      "step": 1797200
    },
    {
      "epoch": 28.215070643642072,
      "grad_norm": 3.432945728302002,
      "learning_rate": 3.236558084772371e-05,
      "loss": 0.5934,
      "step": 1797300
    },
    {
      "epoch": 28.21664050235479,
      "grad_norm": 3.583857297897339,
      "learning_rate": 3.2364599686028256e-05,
      "loss": 0.6105,
      "step": 1797400
    },
    {
      "epoch": 28.218210361067506,
      "grad_norm": 3.2097818851470947,
      "learning_rate": 3.2363618524332814e-05,
      "loss": 0.5889,
      "step": 1797500
    },
    {
      "epoch": 28.21978021978022,
      "grad_norm": 4.252836227416992,
      "learning_rate": 3.2362637362637365e-05,
      "loss": 0.5709,
      "step": 1797600
    },
    {
      "epoch": 28.221350078492936,
      "grad_norm": 4.026204586029053,
      "learning_rate": 3.2361656200941916e-05,
      "loss": 0.6013,
      "step": 1797700
    },
    {
      "epoch": 28.222919937205653,
      "grad_norm": 4.300069332122803,
      "learning_rate": 3.236067503924647e-05,
      "loss": 0.6214,
      "step": 1797800
    },
    {
      "epoch": 28.224489795918366,
      "grad_norm": 3.38057804107666,
      "learning_rate": 3.2359693877551025e-05,
      "loss": 0.6353,
      "step": 1797900
    },
    {
      "epoch": 28.226059654631083,
      "grad_norm": 4.071794509887695,
      "learning_rate": 3.2358712715855576e-05,
      "loss": 0.5983,
      "step": 1798000
    },
    {
      "epoch": 28.2276295133438,
      "grad_norm": 4.036471843719482,
      "learning_rate": 3.235773155416013e-05,
      "loss": 0.6138,
      "step": 1798100
    },
    {
      "epoch": 28.229199372056517,
      "grad_norm": 3.6693031787872314,
      "learning_rate": 3.235675039246468e-05,
      "loss": 0.5615,
      "step": 1798200
    },
    {
      "epoch": 28.23076923076923,
      "grad_norm": 4.085666179656982,
      "learning_rate": 3.2355769230769235e-05,
      "loss": 0.6017,
      "step": 1798300
    },
    {
      "epoch": 28.232339089481947,
      "grad_norm": 4.132620334625244,
      "learning_rate": 3.235478806907378e-05,
      "loss": 0.6047,
      "step": 1798400
    },
    {
      "epoch": 28.233908948194664,
      "grad_norm": 3.578404426574707,
      "learning_rate": 3.235380690737834e-05,
      "loss": 0.5851,
      "step": 1798500
    },
    {
      "epoch": 28.235478806907377,
      "grad_norm": 3.8629424571990967,
      "learning_rate": 3.235282574568289e-05,
      "loss": 0.5955,
      "step": 1798600
    },
    {
      "epoch": 28.237048665620094,
      "grad_norm": 3.9003379344940186,
      "learning_rate": 3.2351844583987446e-05,
      "loss": 0.6008,
      "step": 1798700
    },
    {
      "epoch": 28.23861852433281,
      "grad_norm": 4.056725978851318,
      "learning_rate": 3.2350863422292e-05,
      "loss": 0.6237,
      "step": 1798800
    },
    {
      "epoch": 28.240188383045528,
      "grad_norm": 3.977846622467041,
      "learning_rate": 3.234988226059655e-05,
      "loss": 0.6152,
      "step": 1798900
    },
    {
      "epoch": 28.24175824175824,
      "grad_norm": 4.049553394317627,
      "learning_rate": 3.23489010989011e-05,
      "loss": 0.5942,
      "step": 1799000
    },
    {
      "epoch": 28.243328100470958,
      "grad_norm": 3.6055495738983154,
      "learning_rate": 3.234791993720565e-05,
      "loss": 0.5793,
      "step": 1799100
    },
    {
      "epoch": 28.244897959183675,
      "grad_norm": 3.8745839595794678,
      "learning_rate": 3.234693877551021e-05,
      "loss": 0.5949,
      "step": 1799200
    },
    {
      "epoch": 28.246467817896388,
      "grad_norm": 3.451674222946167,
      "learning_rate": 3.234595761381476e-05,
      "loss": 0.6029,
      "step": 1799300
    },
    {
      "epoch": 28.248037676609105,
      "grad_norm": 4.37290096282959,
      "learning_rate": 3.2344976452119316e-05,
      "loss": 0.6397,
      "step": 1799400
    },
    {
      "epoch": 28.24960753532182,
      "grad_norm": 3.23372483253479,
      "learning_rate": 3.234399529042386e-05,
      "loss": 0.5664,
      "step": 1799500
    },
    {
      "epoch": 28.25117739403454,
      "grad_norm": 3.8257896900177,
      "learning_rate": 3.234301412872842e-05,
      "loss": 0.569,
      "step": 1799600
    },
    {
      "epoch": 28.252747252747252,
      "grad_norm": 4.061821460723877,
      "learning_rate": 3.234203296703297e-05,
      "loss": 0.6213,
      "step": 1799700
    },
    {
      "epoch": 28.25431711145997,
      "grad_norm": 2.3118085861206055,
      "learning_rate": 3.234105180533752e-05,
      "loss": 0.6056,
      "step": 1799800
    },
    {
      "epoch": 28.255886970172686,
      "grad_norm": 2.863701820373535,
      "learning_rate": 3.234007064364207e-05,
      "loss": 0.5703,
      "step": 1799900
    },
    {
      "epoch": 28.2574568288854,
      "grad_norm": 4.429659843444824,
      "learning_rate": 3.233908948194663e-05,
      "loss": 0.5571,
      "step": 1800000
    },
    {
      "epoch": 28.259026687598116,
      "grad_norm": 2.6825497150421143,
      "learning_rate": 3.233810832025118e-05,
      "loss": 0.6061,
      "step": 1800100
    },
    {
      "epoch": 28.260596546310833,
      "grad_norm": 3.8210244178771973,
      "learning_rate": 3.233712715855573e-05,
      "loss": 0.5955,
      "step": 1800200
    },
    {
      "epoch": 28.26216640502355,
      "grad_norm": 4.055502891540527,
      "learning_rate": 3.233614599686028e-05,
      "loss": 0.5658,
      "step": 1800300
    },
    {
      "epoch": 28.263736263736263,
      "grad_norm": 3.9254074096679688,
      "learning_rate": 3.233516483516484e-05,
      "loss": 0.6321,
      "step": 1800400
    },
    {
      "epoch": 28.26530612244898,
      "grad_norm": 4.097155570983887,
      "learning_rate": 3.2334183673469384e-05,
      "loss": 0.6007,
      "step": 1800500
    },
    {
      "epoch": 28.266875981161697,
      "grad_norm": 5.752748012542725,
      "learning_rate": 3.233320251177394e-05,
      "loss": 0.5824,
      "step": 1800600
    },
    {
      "epoch": 28.26844583987441,
      "grad_norm": 4.5333943367004395,
      "learning_rate": 3.233222135007849e-05,
      "loss": 0.5784,
      "step": 1800700
    },
    {
      "epoch": 28.270015698587127,
      "grad_norm": 4.811989784240723,
      "learning_rate": 3.233124018838305e-05,
      "loss": 0.5522,
      "step": 1800800
    },
    {
      "epoch": 28.271585557299844,
      "grad_norm": 4.682261943817139,
      "learning_rate": 3.23302590266876e-05,
      "loss": 0.5597,
      "step": 1800900
    },
    {
      "epoch": 28.27315541601256,
      "grad_norm": 4.188197135925293,
      "learning_rate": 3.232927786499215e-05,
      "loss": 0.5893,
      "step": 1801000
    },
    {
      "epoch": 28.274725274725274,
      "grad_norm": 4.405327320098877,
      "learning_rate": 3.2328296703296703e-05,
      "loss": 0.5809,
      "step": 1801100
    },
    {
      "epoch": 28.27629513343799,
      "grad_norm": 4.741198539733887,
      "learning_rate": 3.2327315541601254e-05,
      "loss": 0.5877,
      "step": 1801200
    },
    {
      "epoch": 28.277864992150707,
      "grad_norm": 4.167950630187988,
      "learning_rate": 3.232633437990581e-05,
      "loss": 0.6083,
      "step": 1801300
    },
    {
      "epoch": 28.27943485086342,
      "grad_norm": 3.4953720569610596,
      "learning_rate": 3.232535321821036e-05,
      "loss": 0.577,
      "step": 1801400
    },
    {
      "epoch": 28.281004709576138,
      "grad_norm": 2.9543817043304443,
      "learning_rate": 3.232437205651492e-05,
      "loss": 0.61,
      "step": 1801500
    },
    {
      "epoch": 28.282574568288855,
      "grad_norm": 3.390582323074341,
      "learning_rate": 3.2323390894819465e-05,
      "loss": 0.5798,
      "step": 1801600
    },
    {
      "epoch": 28.28414442700157,
      "grad_norm": 3.8405046463012695,
      "learning_rate": 3.232240973312402e-05,
      "loss": 0.6217,
      "step": 1801700
    },
    {
      "epoch": 28.285714285714285,
      "grad_norm": 2.494936466217041,
      "learning_rate": 3.2321428571428574e-05,
      "loss": 0.5993,
      "step": 1801800
    },
    {
      "epoch": 28.287284144427,
      "grad_norm": 4.214117050170898,
      "learning_rate": 3.2320447409733125e-05,
      "loss": 0.5978,
      "step": 1801900
    },
    {
      "epoch": 28.28885400313972,
      "grad_norm": 3.3489129543304443,
      "learning_rate": 3.2319466248037676e-05,
      "loss": 0.5677,
      "step": 1802000
    },
    {
      "epoch": 28.29042386185243,
      "grad_norm": 3.6263136863708496,
      "learning_rate": 3.2318485086342234e-05,
      "loss": 0.5695,
      "step": 1802100
    },
    {
      "epoch": 28.29199372056515,
      "grad_norm": 3.7632389068603516,
      "learning_rate": 3.2317503924646785e-05,
      "loss": 0.5911,
      "step": 1802200
    },
    {
      "epoch": 28.293563579277865,
      "grad_norm": 3.776829242706299,
      "learning_rate": 3.2316522762951336e-05,
      "loss": 0.5826,
      "step": 1802300
    },
    {
      "epoch": 28.295133437990582,
      "grad_norm": 5.282301425933838,
      "learning_rate": 3.2315541601255887e-05,
      "loss": 0.5549,
      "step": 1802400
    },
    {
      "epoch": 28.296703296703296,
      "grad_norm": 4.378260612487793,
      "learning_rate": 3.2314560439560444e-05,
      "loss": 0.6079,
      "step": 1802500
    },
    {
      "epoch": 28.298273155416013,
      "grad_norm": 3.5601887702941895,
      "learning_rate": 3.231357927786499e-05,
      "loss": 0.5608,
      "step": 1802600
    },
    {
      "epoch": 28.29984301412873,
      "grad_norm": 3.7880725860595703,
      "learning_rate": 3.2312598116169546e-05,
      "loss": 0.5969,
      "step": 1802700
    },
    {
      "epoch": 28.301412872841443,
      "grad_norm": 5.025303363800049,
      "learning_rate": 3.23116169544741e-05,
      "loss": 0.6286,
      "step": 1802800
    },
    {
      "epoch": 28.30298273155416,
      "grad_norm": 3.68698787689209,
      "learning_rate": 3.2310635792778655e-05,
      "loss": 0.6095,
      "step": 1802900
    },
    {
      "epoch": 28.304552590266876,
      "grad_norm": 3.7527313232421875,
      "learning_rate": 3.2309654631083206e-05,
      "loss": 0.5879,
      "step": 1803000
    },
    {
      "epoch": 28.306122448979593,
      "grad_norm": 3.6559336185455322,
      "learning_rate": 3.230867346938776e-05,
      "loss": 0.5946,
      "step": 1803100
    },
    {
      "epoch": 28.307692307692307,
      "grad_norm": 4.243443012237549,
      "learning_rate": 3.230769230769231e-05,
      "loss": 0.6143,
      "step": 1803200
    },
    {
      "epoch": 28.309262166405023,
      "grad_norm": 4.066782474517822,
      "learning_rate": 3.230671114599686e-05,
      "loss": 0.5697,
      "step": 1803300
    },
    {
      "epoch": 28.31083202511774,
      "grad_norm": 4.240553379058838,
      "learning_rate": 3.230572998430142e-05,
      "loss": 0.5381,
      "step": 1803400
    },
    {
      "epoch": 28.312401883830454,
      "grad_norm": 4.298270225524902,
      "learning_rate": 3.230474882260597e-05,
      "loss": 0.5976,
      "step": 1803500
    },
    {
      "epoch": 28.31397174254317,
      "grad_norm": 3.898205041885376,
      "learning_rate": 3.2303767660910525e-05,
      "loss": 0.5991,
      "step": 1803600
    },
    {
      "epoch": 28.315541601255887,
      "grad_norm": 2.8130970001220703,
      "learning_rate": 3.230278649921507e-05,
      "loss": 0.6152,
      "step": 1803700
    },
    {
      "epoch": 28.317111459968604,
      "grad_norm": 5.238708972930908,
      "learning_rate": 3.230180533751963e-05,
      "loss": 0.6264,
      "step": 1803800
    },
    {
      "epoch": 28.318681318681318,
      "grad_norm": 3.5788979530334473,
      "learning_rate": 3.230082417582418e-05,
      "loss": 0.562,
      "step": 1803900
    },
    {
      "epoch": 28.320251177394034,
      "grad_norm": 3.3991379737854004,
      "learning_rate": 3.229984301412873e-05,
      "loss": 0.5573,
      "step": 1804000
    },
    {
      "epoch": 28.32182103610675,
      "grad_norm": 2.6913647651672363,
      "learning_rate": 3.229886185243328e-05,
      "loss": 0.5888,
      "step": 1804100
    },
    {
      "epoch": 28.323390894819465,
      "grad_norm": 4.015300273895264,
      "learning_rate": 3.229788069073784e-05,
      "loss": 0.5622,
      "step": 1804200
    },
    {
      "epoch": 28.32496075353218,
      "grad_norm": 4.102301120758057,
      "learning_rate": 3.229689952904239e-05,
      "loss": 0.5711,
      "step": 1804300
    },
    {
      "epoch": 28.3265306122449,
      "grad_norm": 3.836991548538208,
      "learning_rate": 3.229591836734694e-05,
      "loss": 0.5738,
      "step": 1804400
    },
    {
      "epoch": 28.328100470957615,
      "grad_norm": 2.9081122875213623,
      "learning_rate": 3.229493720565149e-05,
      "loss": 0.5897,
      "step": 1804500
    },
    {
      "epoch": 28.32967032967033,
      "grad_norm": 4.611802101135254,
      "learning_rate": 3.229395604395605e-05,
      "loss": 0.5764,
      "step": 1804600
    },
    {
      "epoch": 28.331240188383045,
      "grad_norm": 2.709047794342041,
      "learning_rate": 3.229297488226059e-05,
      "loss": 0.5894,
      "step": 1804700
    },
    {
      "epoch": 28.332810047095762,
      "grad_norm": 4.4861063957214355,
      "learning_rate": 3.229199372056515e-05,
      "loss": 0.5866,
      "step": 1804800
    },
    {
      "epoch": 28.334379905808476,
      "grad_norm": 2.986851692199707,
      "learning_rate": 3.22910125588697e-05,
      "loss": 0.5777,
      "step": 1804900
    },
    {
      "epoch": 28.335949764521192,
      "grad_norm": 4.26432991027832,
      "learning_rate": 3.229003139717426e-05,
      "loss": 0.6065,
      "step": 1805000
    },
    {
      "epoch": 28.33751962323391,
      "grad_norm": 3.735626459121704,
      "learning_rate": 3.228905023547881e-05,
      "loss": 0.616,
      "step": 1805100
    },
    {
      "epoch": 28.339089481946626,
      "grad_norm": 3.749319314956665,
      "learning_rate": 3.228806907378336e-05,
      "loss": 0.5782,
      "step": 1805200
    },
    {
      "epoch": 28.34065934065934,
      "grad_norm": 4.212026119232178,
      "learning_rate": 3.228708791208791e-05,
      "loss": 0.621,
      "step": 1805300
    },
    {
      "epoch": 28.342229199372056,
      "grad_norm": 3.387862205505371,
      "learning_rate": 3.2286106750392463e-05,
      "loss": 0.5761,
      "step": 1805400
    },
    {
      "epoch": 28.343799058084773,
      "grad_norm": 3.592235565185547,
      "learning_rate": 3.228512558869702e-05,
      "loss": 0.588,
      "step": 1805500
    },
    {
      "epoch": 28.345368916797486,
      "grad_norm": 5.187151908874512,
      "learning_rate": 3.228414442700157e-05,
      "loss": 0.5948,
      "step": 1805600
    },
    {
      "epoch": 28.346938775510203,
      "grad_norm": 4.357149600982666,
      "learning_rate": 3.228316326530613e-05,
      "loss": 0.5827,
      "step": 1805700
    },
    {
      "epoch": 28.34850863422292,
      "grad_norm": 2.7336597442626953,
      "learning_rate": 3.2282182103610674e-05,
      "loss": 0.593,
      "step": 1805800
    },
    {
      "epoch": 28.350078492935637,
      "grad_norm": 4.894258499145508,
      "learning_rate": 3.228120094191523e-05,
      "loss": 0.6045,
      "step": 1805900
    },
    {
      "epoch": 28.35164835164835,
      "grad_norm": 3.5528202056884766,
      "learning_rate": 3.228021978021978e-05,
      "loss": 0.5554,
      "step": 1806000
    },
    {
      "epoch": 28.353218210361067,
      "grad_norm": 4.453514575958252,
      "learning_rate": 3.2279238618524334e-05,
      "loss": 0.5859,
      "step": 1806100
    },
    {
      "epoch": 28.354788069073784,
      "grad_norm": 4.398625373840332,
      "learning_rate": 3.2278257456828885e-05,
      "loss": 0.5359,
      "step": 1806200
    },
    {
      "epoch": 28.356357927786497,
      "grad_norm": 4.191447734832764,
      "learning_rate": 3.227727629513344e-05,
      "loss": 0.5871,
      "step": 1806300
    },
    {
      "epoch": 28.357927786499214,
      "grad_norm": 3.279022216796875,
      "learning_rate": 3.2276295133437994e-05,
      "loss": 0.5517,
      "step": 1806400
    },
    {
      "epoch": 28.35949764521193,
      "grad_norm": 3.2727701663970947,
      "learning_rate": 3.2275313971742545e-05,
      "loss": 0.5902,
      "step": 1806500
    },
    {
      "epoch": 28.361067503924648,
      "grad_norm": 3.925570249557495,
      "learning_rate": 3.2274332810047096e-05,
      "loss": 0.6211,
      "step": 1806600
    },
    {
      "epoch": 28.36263736263736,
      "grad_norm": 3.9338693618774414,
      "learning_rate": 3.227335164835165e-05,
      "loss": 0.5882,
      "step": 1806700
    },
    {
      "epoch": 28.364207221350078,
      "grad_norm": 4.250832557678223,
      "learning_rate": 3.22723704866562e-05,
      "loss": 0.6113,
      "step": 1806800
    },
    {
      "epoch": 28.365777080062795,
      "grad_norm": 3.530284881591797,
      "learning_rate": 3.2271389324960755e-05,
      "loss": 0.6198,
      "step": 1806900
    },
    {
      "epoch": 28.367346938775512,
      "grad_norm": 3.90366792678833,
      "learning_rate": 3.2270408163265306e-05,
      "loss": 0.5636,
      "step": 1807000
    },
    {
      "epoch": 28.368916797488225,
      "grad_norm": 2.60534405708313,
      "learning_rate": 3.2269427001569864e-05,
      "loss": 0.5935,
      "step": 1807100
    },
    {
      "epoch": 28.370486656200942,
      "grad_norm": 4.003037929534912,
      "learning_rate": 3.226844583987441e-05,
      "loss": 0.6083,
      "step": 1807200
    },
    {
      "epoch": 28.37205651491366,
      "grad_norm": 3.61605167388916,
      "learning_rate": 3.2267464678178966e-05,
      "loss": 0.6388,
      "step": 1807300
    },
    {
      "epoch": 28.373626373626372,
      "grad_norm": 3.156756639480591,
      "learning_rate": 3.226648351648352e-05,
      "loss": 0.5477,
      "step": 1807400
    },
    {
      "epoch": 28.37519623233909,
      "grad_norm": 4.12558126449585,
      "learning_rate": 3.226550235478807e-05,
      "loss": 0.5844,
      "step": 1807500
    },
    {
      "epoch": 28.376766091051806,
      "grad_norm": 4.416105270385742,
      "learning_rate": 3.2264521193092626e-05,
      "loss": 0.6111,
      "step": 1807600
    },
    {
      "epoch": 28.378335949764523,
      "grad_norm": 3.444765090942383,
      "learning_rate": 3.226354003139718e-05,
      "loss": 0.6006,
      "step": 1807700
    },
    {
      "epoch": 28.379905808477236,
      "grad_norm": 4.021749973297119,
      "learning_rate": 3.2262558869701734e-05,
      "loss": 0.6019,
      "step": 1807800
    },
    {
      "epoch": 28.381475667189953,
      "grad_norm": 4.549412727355957,
      "learning_rate": 3.226157770800628e-05,
      "loss": 0.5496,
      "step": 1807900
    },
    {
      "epoch": 28.38304552590267,
      "grad_norm": 4.25787353515625,
      "learning_rate": 3.2260596546310836e-05,
      "loss": 0.6288,
      "step": 1808000
    },
    {
      "epoch": 28.384615384615383,
      "grad_norm": 4.4019670486450195,
      "learning_rate": 3.225961538461539e-05,
      "loss": 0.6083,
      "step": 1808100
    },
    {
      "epoch": 28.3861852433281,
      "grad_norm": 3.2698092460632324,
      "learning_rate": 3.225863422291994e-05,
      "loss": 0.6036,
      "step": 1808200
    },
    {
      "epoch": 28.387755102040817,
      "grad_norm": 4.831794261932373,
      "learning_rate": 3.225765306122449e-05,
      "loss": 0.6042,
      "step": 1808300
    },
    {
      "epoch": 28.389324960753534,
      "grad_norm": 3.629534959793091,
      "learning_rate": 3.225667189952905e-05,
      "loss": 0.6028,
      "step": 1808400
    },
    {
      "epoch": 28.390894819466247,
      "grad_norm": 4.623168468475342,
      "learning_rate": 3.22556907378336e-05,
      "loss": 0.5825,
      "step": 1808500
    },
    {
      "epoch": 28.392464678178964,
      "grad_norm": 3.8348166942596436,
      "learning_rate": 3.225470957613815e-05,
      "loss": 0.5754,
      "step": 1808600
    },
    {
      "epoch": 28.39403453689168,
      "grad_norm": 4.028964519500732,
      "learning_rate": 3.22537284144427e-05,
      "loss": 0.5999,
      "step": 1808700
    },
    {
      "epoch": 28.395604395604394,
      "grad_norm": 3.479497194290161,
      "learning_rate": 3.225274725274726e-05,
      "loss": 0.5472,
      "step": 1808800
    },
    {
      "epoch": 28.39717425431711,
      "grad_norm": 3.812939167022705,
      "learning_rate": 3.22517660910518e-05,
      "loss": 0.5849,
      "step": 1808900
    },
    {
      "epoch": 28.398744113029828,
      "grad_norm": 3.6596691608428955,
      "learning_rate": 3.225078492935636e-05,
      "loss": 0.6304,
      "step": 1809000
    },
    {
      "epoch": 28.400313971742545,
      "grad_norm": 4.398794651031494,
      "learning_rate": 3.224980376766091e-05,
      "loss": 0.5766,
      "step": 1809100
    },
    {
      "epoch": 28.401883830455258,
      "grad_norm": 3.857461452484131,
      "learning_rate": 3.224882260596546e-05,
      "loss": 0.6051,
      "step": 1809200
    },
    {
      "epoch": 28.403453689167975,
      "grad_norm": 3.2986979484558105,
      "learning_rate": 3.224784144427001e-05,
      "loss": 0.586,
      "step": 1809300
    },
    {
      "epoch": 28.405023547880692,
      "grad_norm": 3.3800034523010254,
      "learning_rate": 3.224686028257457e-05,
      "loss": 0.5839,
      "step": 1809400
    },
    {
      "epoch": 28.406593406593405,
      "grad_norm": 3.541464328765869,
      "learning_rate": 3.224587912087912e-05,
      "loss": 0.5816,
      "step": 1809500
    },
    {
      "epoch": 28.408163265306122,
      "grad_norm": 3.636011838912964,
      "learning_rate": 3.224489795918367e-05,
      "loss": 0.5799,
      "step": 1809600
    },
    {
      "epoch": 28.40973312401884,
      "grad_norm": 3.3832943439483643,
      "learning_rate": 3.224391679748823e-05,
      "loss": 0.5659,
      "step": 1809700
    },
    {
      "epoch": 28.411302982731556,
      "grad_norm": 4.020110130310059,
      "learning_rate": 3.224293563579278e-05,
      "loss": 0.6123,
      "step": 1809800
    },
    {
      "epoch": 28.41287284144427,
      "grad_norm": 3.6471974849700928,
      "learning_rate": 3.224195447409733e-05,
      "loss": 0.5679,
      "step": 1809900
    },
    {
      "epoch": 28.414442700156986,
      "grad_norm": 4.462000370025635,
      "learning_rate": 3.224097331240188e-05,
      "loss": 0.5739,
      "step": 1810000
    },
    {
      "epoch": 28.416012558869703,
      "grad_norm": 3.7673089504241943,
      "learning_rate": 3.223999215070644e-05,
      "loss": 0.5937,
      "step": 1810100
    },
    {
      "epoch": 28.417582417582416,
      "grad_norm": 4.107931137084961,
      "learning_rate": 3.223901098901099e-05,
      "loss": 0.5866,
      "step": 1810200
    },
    {
      "epoch": 28.419152276295133,
      "grad_norm": 3.223268985748291,
      "learning_rate": 3.223802982731554e-05,
      "loss": 0.5741,
      "step": 1810300
    },
    {
      "epoch": 28.42072213500785,
      "grad_norm": 4.169380187988281,
      "learning_rate": 3.2237048665620094e-05,
      "loss": 0.5937,
      "step": 1810400
    },
    {
      "epoch": 28.422291993720567,
      "grad_norm": 3.832566022872925,
      "learning_rate": 3.223606750392465e-05,
      "loss": 0.5996,
      "step": 1810500
    },
    {
      "epoch": 28.42386185243328,
      "grad_norm": 2.0098185539245605,
      "learning_rate": 3.2235086342229196e-05,
      "loss": 0.607,
      "step": 1810600
    },
    {
      "epoch": 28.425431711145997,
      "grad_norm": 4.8634467124938965,
      "learning_rate": 3.2234105180533754e-05,
      "loss": 0.6129,
      "step": 1810700
    },
    {
      "epoch": 28.427001569858714,
      "grad_norm": 3.7943429946899414,
      "learning_rate": 3.2233124018838305e-05,
      "loss": 0.6335,
      "step": 1810800
    },
    {
      "epoch": 28.428571428571427,
      "grad_norm": 4.229469299316406,
      "learning_rate": 3.223214285714286e-05,
      "loss": 0.6151,
      "step": 1810900
    },
    {
      "epoch": 28.430141287284144,
      "grad_norm": 2.791825294494629,
      "learning_rate": 3.2231161695447407e-05,
      "loss": 0.5698,
      "step": 1811000
    },
    {
      "epoch": 28.43171114599686,
      "grad_norm": 2.4840126037597656,
      "learning_rate": 3.2230180533751964e-05,
      "loss": 0.5717,
      "step": 1811100
    },
    {
      "epoch": 28.433281004709578,
      "grad_norm": 5.281643390655518,
      "learning_rate": 3.2229199372056515e-05,
      "loss": 0.5832,
      "step": 1811200
    },
    {
      "epoch": 28.43485086342229,
      "grad_norm": 3.0261778831481934,
      "learning_rate": 3.2228218210361066e-05,
      "loss": 0.5794,
      "step": 1811300
    },
    {
      "epoch": 28.436420722135008,
      "grad_norm": 3.895580291748047,
      "learning_rate": 3.222723704866562e-05,
      "loss": 0.5756,
      "step": 1811400
    },
    {
      "epoch": 28.437990580847725,
      "grad_norm": 4.160940170288086,
      "learning_rate": 3.2226255886970175e-05,
      "loss": 0.5955,
      "step": 1811500
    },
    {
      "epoch": 28.439560439560438,
      "grad_norm": 3.3479301929473877,
      "learning_rate": 3.2225274725274726e-05,
      "loss": 0.6216,
      "step": 1811600
    },
    {
      "epoch": 28.441130298273155,
      "grad_norm": 4.449650764465332,
      "learning_rate": 3.222429356357928e-05,
      "loss": 0.6001,
      "step": 1811700
    },
    {
      "epoch": 28.44270015698587,
      "grad_norm": 3.3500359058380127,
      "learning_rate": 3.2223312401883835e-05,
      "loss": 0.6078,
      "step": 1811800
    },
    {
      "epoch": 28.44427001569859,
      "grad_norm": 3.316389799118042,
      "learning_rate": 3.2222331240188386e-05,
      "loss": 0.5936,
      "step": 1811900
    },
    {
      "epoch": 28.445839874411302,
      "grad_norm": 3.11596417427063,
      "learning_rate": 3.222135007849294e-05,
      "loss": 0.5855,
      "step": 1812000
    },
    {
      "epoch": 28.44740973312402,
      "grad_norm": 3.985361337661743,
      "learning_rate": 3.222036891679749e-05,
      "loss": 0.6059,
      "step": 1812100
    },
    {
      "epoch": 28.448979591836736,
      "grad_norm": 4.140103816986084,
      "learning_rate": 3.2219387755102045e-05,
      "loss": 0.6051,
      "step": 1812200
    },
    {
      "epoch": 28.45054945054945,
      "grad_norm": 4.405035972595215,
      "learning_rate": 3.2218406593406596e-05,
      "loss": 0.5962,
      "step": 1812300
    },
    {
      "epoch": 28.452119309262166,
      "grad_norm": 3.4505600929260254,
      "learning_rate": 3.221742543171115e-05,
      "loss": 0.5614,
      "step": 1812400
    },
    {
      "epoch": 28.453689167974883,
      "grad_norm": 4.742351055145264,
      "learning_rate": 3.22164442700157e-05,
      "loss": 0.6187,
      "step": 1812500
    },
    {
      "epoch": 28.4552590266876,
      "grad_norm": 3.940196990966797,
      "learning_rate": 3.2215463108320256e-05,
      "loss": 0.5778,
      "step": 1812600
    },
    {
      "epoch": 28.456828885400313,
      "grad_norm": 4.359220027923584,
      "learning_rate": 3.22144819466248e-05,
      "loss": 0.5515,
      "step": 1812700
    },
    {
      "epoch": 28.45839874411303,
      "grad_norm": 3.1644601821899414,
      "learning_rate": 3.221350078492936e-05,
      "loss": 0.5745,
      "step": 1812800
    },
    {
      "epoch": 28.459968602825747,
      "grad_norm": 3.4775874614715576,
      "learning_rate": 3.221251962323391e-05,
      "loss": 0.591,
      "step": 1812900
    },
    {
      "epoch": 28.46153846153846,
      "grad_norm": 3.8050642013549805,
      "learning_rate": 3.221153846153847e-05,
      "loss": 0.5823,
      "step": 1813000
    },
    {
      "epoch": 28.463108320251177,
      "grad_norm": 4.034950256347656,
      "learning_rate": 3.221055729984301e-05,
      "loss": 0.6325,
      "step": 1813100
    },
    {
      "epoch": 28.464678178963894,
      "grad_norm": 3.6674442291259766,
      "learning_rate": 3.220957613814757e-05,
      "loss": 0.6145,
      "step": 1813200
    },
    {
      "epoch": 28.46624803767661,
      "grad_norm": 4.3831000328063965,
      "learning_rate": 3.220859497645212e-05,
      "loss": 0.5953,
      "step": 1813300
    },
    {
      "epoch": 28.467817896389324,
      "grad_norm": 2.3880279064178467,
      "learning_rate": 3.220761381475667e-05,
      "loss": 0.5733,
      "step": 1813400
    },
    {
      "epoch": 28.46938775510204,
      "grad_norm": 2.8223788738250732,
      "learning_rate": 3.220663265306122e-05,
      "loss": 0.5999,
      "step": 1813500
    },
    {
      "epoch": 28.470957613814758,
      "grad_norm": 3.183223009109497,
      "learning_rate": 3.220565149136578e-05,
      "loss": 0.5679,
      "step": 1813600
    },
    {
      "epoch": 28.47252747252747,
      "grad_norm": 3.6007063388824463,
      "learning_rate": 3.220467032967033e-05,
      "loss": 0.5494,
      "step": 1813700
    },
    {
      "epoch": 28.474097331240188,
      "grad_norm": 4.630945682525635,
      "learning_rate": 3.220368916797488e-05,
      "loss": 0.6146,
      "step": 1813800
    },
    {
      "epoch": 28.475667189952905,
      "grad_norm": 3.46766996383667,
      "learning_rate": 3.220270800627944e-05,
      "loss": 0.5866,
      "step": 1813900
    },
    {
      "epoch": 28.47723704866562,
      "grad_norm": 3.9392731189727783,
      "learning_rate": 3.220172684458399e-05,
      "loss": 0.6191,
      "step": 1814000
    },
    {
      "epoch": 28.478806907378335,
      "grad_norm": 4.552670001983643,
      "learning_rate": 3.220074568288854e-05,
      "loss": 0.5636,
      "step": 1814100
    },
    {
      "epoch": 28.48037676609105,
      "grad_norm": 2.8357415199279785,
      "learning_rate": 3.219976452119309e-05,
      "loss": 0.5963,
      "step": 1814200
    },
    {
      "epoch": 28.48194662480377,
      "grad_norm": 3.923964262008667,
      "learning_rate": 3.219878335949765e-05,
      "loss": 0.5573,
      "step": 1814300
    },
    {
      "epoch": 28.483516483516482,
      "grad_norm": 2.288069009780884,
      "learning_rate": 3.21978021978022e-05,
      "loss": 0.6056,
      "step": 1814400
    },
    {
      "epoch": 28.4850863422292,
      "grad_norm": 4.547790050506592,
      "learning_rate": 3.219682103610675e-05,
      "loss": 0.5556,
      "step": 1814500
    },
    {
      "epoch": 28.486656200941916,
      "grad_norm": 4.801211357116699,
      "learning_rate": 3.21958398744113e-05,
      "loss": 0.6029,
      "step": 1814600
    },
    {
      "epoch": 28.488226059654632,
      "grad_norm": 3.785536527633667,
      "learning_rate": 3.219485871271586e-05,
      "loss": 0.6155,
      "step": 1814700
    },
    {
      "epoch": 28.489795918367346,
      "grad_norm": 2.314358949661255,
      "learning_rate": 3.2193877551020405e-05,
      "loss": 0.6185,
      "step": 1814800
    },
    {
      "epoch": 28.491365777080063,
      "grad_norm": 2.9366321563720703,
      "learning_rate": 3.219289638932496e-05,
      "loss": 0.5504,
      "step": 1814900
    },
    {
      "epoch": 28.49293563579278,
      "grad_norm": 4.116641998291016,
      "learning_rate": 3.2191915227629514e-05,
      "loss": 0.5579,
      "step": 1815000
    },
    {
      "epoch": 28.494505494505496,
      "grad_norm": 4.131062030792236,
      "learning_rate": 3.219093406593407e-05,
      "loss": 0.5972,
      "step": 1815100
    },
    {
      "epoch": 28.49607535321821,
      "grad_norm": 3.7588725090026855,
      "learning_rate": 3.2189952904238615e-05,
      "loss": 0.5707,
      "step": 1815200
    },
    {
      "epoch": 28.497645211930926,
      "grad_norm": 3.875317335128784,
      "learning_rate": 3.218897174254317e-05,
      "loss": 0.6131,
      "step": 1815300
    },
    {
      "epoch": 28.499215070643643,
      "grad_norm": 4.400192737579346,
      "learning_rate": 3.2187990580847724e-05,
      "loss": 0.5941,
      "step": 1815400
    },
    {
      "epoch": 28.500784929356357,
      "grad_norm": 4.190979957580566,
      "learning_rate": 3.2187009419152275e-05,
      "loss": 0.5784,
      "step": 1815500
    },
    {
      "epoch": 28.502354788069074,
      "grad_norm": 1.5805696249008179,
      "learning_rate": 3.2186028257456826e-05,
      "loss": 0.5891,
      "step": 1815600
    },
    {
      "epoch": 28.50392464678179,
      "grad_norm": 3.6130919456481934,
      "learning_rate": 3.2185047095761384e-05,
      "loss": 0.6024,
      "step": 1815700
    },
    {
      "epoch": 28.505494505494504,
      "grad_norm": 5.027434349060059,
      "learning_rate": 3.2184065934065935e-05,
      "loss": 0.6307,
      "step": 1815800
    },
    {
      "epoch": 28.50706436420722,
      "grad_norm": 3.1485466957092285,
      "learning_rate": 3.2183084772370486e-05,
      "loss": 0.5593,
      "step": 1815900
    },
    {
      "epoch": 28.508634222919937,
      "grad_norm": 3.9185268878936768,
      "learning_rate": 3.2182103610675044e-05,
      "loss": 0.5964,
      "step": 1816000
    },
    {
      "epoch": 28.510204081632654,
      "grad_norm": 3.7718734741210938,
      "learning_rate": 3.2181122448979595e-05,
      "loss": 0.5682,
      "step": 1816100
    },
    {
      "epoch": 28.511773940345368,
      "grad_norm": 4.370476722717285,
      "learning_rate": 3.2180141287284146e-05,
      "loss": 0.5779,
      "step": 1816200
    },
    {
      "epoch": 28.513343799058084,
      "grad_norm": 4.0588765144348145,
      "learning_rate": 3.21791601255887e-05,
      "loss": 0.5766,
      "step": 1816300
    },
    {
      "epoch": 28.5149136577708,
      "grad_norm": 4.4978346824646,
      "learning_rate": 3.2178178963893254e-05,
      "loss": 0.5946,
      "step": 1816400
    },
    {
      "epoch": 28.516483516483518,
      "grad_norm": 4.151033401489258,
      "learning_rate": 3.2177197802197805e-05,
      "loss": 0.6195,
      "step": 1816500
    },
    {
      "epoch": 28.51805337519623,
      "grad_norm": 3.8443245887756348,
      "learning_rate": 3.2176216640502356e-05,
      "loss": 0.5834,
      "step": 1816600
    },
    {
      "epoch": 28.51962323390895,
      "grad_norm": 3.0961906909942627,
      "learning_rate": 3.217523547880691e-05,
      "loss": 0.5718,
      "step": 1816700
    },
    {
      "epoch": 28.521193092621665,
      "grad_norm": 3.681640863418579,
      "learning_rate": 3.2174254317111465e-05,
      "loss": 0.5902,
      "step": 1816800
    },
    {
      "epoch": 28.52276295133438,
      "grad_norm": 2.904078245162964,
      "learning_rate": 3.217327315541601e-05,
      "loss": 0.582,
      "step": 1816900
    },
    {
      "epoch": 28.524332810047095,
      "grad_norm": 2.7796919345855713,
      "learning_rate": 3.217229199372057e-05,
      "loss": 0.6109,
      "step": 1817000
    },
    {
      "epoch": 28.525902668759812,
      "grad_norm": 3.509925603866577,
      "learning_rate": 3.217131083202512e-05,
      "loss": 0.5848,
      "step": 1817100
    },
    {
      "epoch": 28.52747252747253,
      "grad_norm": 4.530335903167725,
      "learning_rate": 3.2170329670329676e-05,
      "loss": 0.5603,
      "step": 1817200
    },
    {
      "epoch": 28.529042386185242,
      "grad_norm": 4.883801460266113,
      "learning_rate": 3.216934850863422e-05,
      "loss": 0.5678,
      "step": 1817300
    },
    {
      "epoch": 28.53061224489796,
      "grad_norm": 4.442076683044434,
      "learning_rate": 3.216836734693878e-05,
      "loss": 0.6081,
      "step": 1817400
    },
    {
      "epoch": 28.532182103610676,
      "grad_norm": 4.034846782684326,
      "learning_rate": 3.216738618524333e-05,
      "loss": 0.5982,
      "step": 1817500
    },
    {
      "epoch": 28.53375196232339,
      "grad_norm": 4.018677234649658,
      "learning_rate": 3.216640502354788e-05,
      "loss": 0.5796,
      "step": 1817600
    },
    {
      "epoch": 28.535321821036106,
      "grad_norm": 3.1324496269226074,
      "learning_rate": 3.216542386185243e-05,
      "loss": 0.6173,
      "step": 1817700
    },
    {
      "epoch": 28.536891679748823,
      "grad_norm": 3.467957019805908,
      "learning_rate": 3.216444270015699e-05,
      "loss": 0.5839,
      "step": 1817800
    },
    {
      "epoch": 28.53846153846154,
      "grad_norm": 4.719643592834473,
      "learning_rate": 3.216346153846154e-05,
      "loss": 0.6139,
      "step": 1817900
    },
    {
      "epoch": 28.540031397174253,
      "grad_norm": 4.372041702270508,
      "learning_rate": 3.216248037676609e-05,
      "loss": 0.5869,
      "step": 1818000
    },
    {
      "epoch": 28.54160125588697,
      "grad_norm": 4.308023452758789,
      "learning_rate": 3.216149921507065e-05,
      "loss": 0.5979,
      "step": 1818100
    },
    {
      "epoch": 28.543171114599687,
      "grad_norm": 4.839271545410156,
      "learning_rate": 3.21605180533752e-05,
      "loss": 0.5798,
      "step": 1818200
    },
    {
      "epoch": 28.5447409733124,
      "grad_norm": 4.404433250427246,
      "learning_rate": 3.215953689167975e-05,
      "loss": 0.6004,
      "step": 1818300
    },
    {
      "epoch": 28.546310832025117,
      "grad_norm": 3.675607919692993,
      "learning_rate": 3.21585557299843e-05,
      "loss": 0.5989,
      "step": 1818400
    },
    {
      "epoch": 28.547880690737834,
      "grad_norm": 6.268198490142822,
      "learning_rate": 3.215757456828886e-05,
      "loss": 0.5559,
      "step": 1818500
    },
    {
      "epoch": 28.54945054945055,
      "grad_norm": 5.2895426750183105,
      "learning_rate": 3.215659340659341e-05,
      "loss": 0.6056,
      "step": 1818600
    },
    {
      "epoch": 28.551020408163264,
      "grad_norm": 2.9375956058502197,
      "learning_rate": 3.215561224489796e-05,
      "loss": 0.6343,
      "step": 1818700
    },
    {
      "epoch": 28.55259026687598,
      "grad_norm": 1.9456844329833984,
      "learning_rate": 3.215463108320251e-05,
      "loss": 0.5996,
      "step": 1818800
    },
    {
      "epoch": 28.554160125588698,
      "grad_norm": 3.5183980464935303,
      "learning_rate": 3.215364992150707e-05,
      "loss": 0.5861,
      "step": 1818900
    },
    {
      "epoch": 28.55572998430141,
      "grad_norm": 3.1818244457244873,
      "learning_rate": 3.2152668759811614e-05,
      "loss": 0.5645,
      "step": 1819000
    },
    {
      "epoch": 28.55729984301413,
      "grad_norm": 3.3712282180786133,
      "learning_rate": 3.215168759811617e-05,
      "loss": 0.5885,
      "step": 1819100
    },
    {
      "epoch": 28.558869701726845,
      "grad_norm": 3.2074191570281982,
      "learning_rate": 3.215070643642072e-05,
      "loss": 0.6175,
      "step": 1819200
    },
    {
      "epoch": 28.560439560439562,
      "grad_norm": 5.0922675132751465,
      "learning_rate": 3.214972527472528e-05,
      "loss": 0.5529,
      "step": 1819300
    },
    {
      "epoch": 28.562009419152275,
      "grad_norm": 3.7471163272857666,
      "learning_rate": 3.2148744113029824e-05,
      "loss": 0.6176,
      "step": 1819400
    },
    {
      "epoch": 28.563579277864992,
      "grad_norm": 3.006333589553833,
      "learning_rate": 3.214776295133438e-05,
      "loss": 0.6213,
      "step": 1819500
    },
    {
      "epoch": 28.56514913657771,
      "grad_norm": 3.6923623085021973,
      "learning_rate": 3.214678178963893e-05,
      "loss": 0.5808,
      "step": 1819600
    },
    {
      "epoch": 28.566718995290422,
      "grad_norm": 3.6722521781921387,
      "learning_rate": 3.2145800627943484e-05,
      "loss": 0.6171,
      "step": 1819700
    },
    {
      "epoch": 28.56828885400314,
      "grad_norm": 3.557340621948242,
      "learning_rate": 3.2144819466248035e-05,
      "loss": 0.5677,
      "step": 1819800
    },
    {
      "epoch": 28.569858712715856,
      "grad_norm": 4.028498649597168,
      "learning_rate": 3.214383830455259e-05,
      "loss": 0.5689,
      "step": 1819900
    },
    {
      "epoch": 28.571428571428573,
      "grad_norm": 4.82095193862915,
      "learning_rate": 3.2142857142857144e-05,
      "loss": 0.6098,
      "step": 1820000
    },
    {
      "epoch": 28.572998430141286,
      "grad_norm": 3.262822389602661,
      "learning_rate": 3.2141875981161695e-05,
      "loss": 0.6011,
      "step": 1820100
    },
    {
      "epoch": 28.574568288854003,
      "grad_norm": 4.05822229385376,
      "learning_rate": 3.214089481946625e-05,
      "loss": 0.6042,
      "step": 1820200
    },
    {
      "epoch": 28.57613814756672,
      "grad_norm": 3.7510969638824463,
      "learning_rate": 3.2139913657770804e-05,
      "loss": 0.6475,
      "step": 1820300
    },
    {
      "epoch": 28.577708006279433,
      "grad_norm": 3.5464248657226562,
      "learning_rate": 3.2138932496075355e-05,
      "loss": 0.6216,
      "step": 1820400
    },
    {
      "epoch": 28.57927786499215,
      "grad_norm": 3.0403623580932617,
      "learning_rate": 3.2137951334379906e-05,
      "loss": 0.5681,
      "step": 1820500
    },
    {
      "epoch": 28.580847723704867,
      "grad_norm": 3.3569955825805664,
      "learning_rate": 3.213697017268446e-05,
      "loss": 0.6105,
      "step": 1820600
    },
    {
      "epoch": 28.582417582417584,
      "grad_norm": 3.430058717727661,
      "learning_rate": 3.2135989010989014e-05,
      "loss": 0.5918,
      "step": 1820700
    },
    {
      "epoch": 28.583987441130297,
      "grad_norm": 2.486937999725342,
      "learning_rate": 3.2135007849293565e-05,
      "loss": 0.5776,
      "step": 1820800
    },
    {
      "epoch": 28.585557299843014,
      "grad_norm": 4.334521293640137,
      "learning_rate": 3.2134026687598116e-05,
      "loss": 0.5765,
      "step": 1820900
    },
    {
      "epoch": 28.58712715855573,
      "grad_norm": 3.391266107559204,
      "learning_rate": 3.2133045525902674e-05,
      "loss": 0.6229,
      "step": 1821000
    },
    {
      "epoch": 28.588697017268444,
      "grad_norm": 5.222743511199951,
      "learning_rate": 3.213206436420722e-05,
      "loss": 0.5965,
      "step": 1821100
    },
    {
      "epoch": 28.59026687598116,
      "grad_norm": 3.8705244064331055,
      "learning_rate": 3.2131083202511776e-05,
      "loss": 0.5547,
      "step": 1821200
    },
    {
      "epoch": 28.591836734693878,
      "grad_norm": 3.0570180416107178,
      "learning_rate": 3.213010204081633e-05,
      "loss": 0.6498,
      "step": 1821300
    },
    {
      "epoch": 28.593406593406595,
      "grad_norm": 3.1614127159118652,
      "learning_rate": 3.2129120879120885e-05,
      "loss": 0.5863,
      "step": 1821400
    },
    {
      "epoch": 28.594976452119308,
      "grad_norm": 4.432478427886963,
      "learning_rate": 3.212813971742543e-05,
      "loss": 0.5584,
      "step": 1821500
    },
    {
      "epoch": 28.596546310832025,
      "grad_norm": 2.8314478397369385,
      "learning_rate": 3.212715855572999e-05,
      "loss": 0.637,
      "step": 1821600
    },
    {
      "epoch": 28.598116169544742,
      "grad_norm": 3.031675338745117,
      "learning_rate": 3.212617739403454e-05,
      "loss": 0.6284,
      "step": 1821700
    },
    {
      "epoch": 28.599686028257455,
      "grad_norm": 4.625585556030273,
      "learning_rate": 3.212519623233909e-05,
      "loss": 0.5776,
      "step": 1821800
    },
    {
      "epoch": 28.601255886970172,
      "grad_norm": 3.6144182682037354,
      "learning_rate": 3.212421507064364e-05,
      "loss": 0.6262,
      "step": 1821900
    },
    {
      "epoch": 28.60282574568289,
      "grad_norm": 3.102978467941284,
      "learning_rate": 3.21232339089482e-05,
      "loss": 0.6082,
      "step": 1822000
    },
    {
      "epoch": 28.604395604395606,
      "grad_norm": 2.1467015743255615,
      "learning_rate": 3.212225274725275e-05,
      "loss": 0.6076,
      "step": 1822100
    },
    {
      "epoch": 28.60596546310832,
      "grad_norm": 4.192603588104248,
      "learning_rate": 3.21212715855573e-05,
      "loss": 0.6359,
      "step": 1822200
    },
    {
      "epoch": 28.607535321821036,
      "grad_norm": 4.189162254333496,
      "learning_rate": 3.212029042386186e-05,
      "loss": 0.6275,
      "step": 1822300
    },
    {
      "epoch": 28.609105180533753,
      "grad_norm": 3.496786594390869,
      "learning_rate": 3.211930926216641e-05,
      "loss": 0.5842,
      "step": 1822400
    },
    {
      "epoch": 28.610675039246466,
      "grad_norm": 4.582755088806152,
      "learning_rate": 3.211832810047096e-05,
      "loss": 0.6159,
      "step": 1822500
    },
    {
      "epoch": 28.612244897959183,
      "grad_norm": 3.746417284011841,
      "learning_rate": 3.211734693877551e-05,
      "loss": 0.5596,
      "step": 1822600
    },
    {
      "epoch": 28.6138147566719,
      "grad_norm": 3.4532251358032227,
      "learning_rate": 3.211636577708007e-05,
      "loss": 0.5721,
      "step": 1822700
    },
    {
      "epoch": 28.615384615384617,
      "grad_norm": 2.67604923248291,
      "learning_rate": 3.211538461538462e-05,
      "loss": 0.6088,
      "step": 1822800
    },
    {
      "epoch": 28.61695447409733,
      "grad_norm": 3.260237693786621,
      "learning_rate": 3.211440345368917e-05,
      "loss": 0.5794,
      "step": 1822900
    },
    {
      "epoch": 28.618524332810047,
      "grad_norm": 3.9104599952697754,
      "learning_rate": 3.211342229199372e-05,
      "loss": 0.5737,
      "step": 1823000
    },
    {
      "epoch": 28.620094191522764,
      "grad_norm": 3.3282835483551025,
      "learning_rate": 3.211244113029828e-05,
      "loss": 0.6081,
      "step": 1823100
    },
    {
      "epoch": 28.621664050235477,
      "grad_norm": 3.0837621688842773,
      "learning_rate": 3.211145996860282e-05,
      "loss": 0.6052,
      "step": 1823200
    },
    {
      "epoch": 28.623233908948194,
      "grad_norm": 3.2229301929473877,
      "learning_rate": 3.211047880690738e-05,
      "loss": 0.6071,
      "step": 1823300
    },
    {
      "epoch": 28.62480376766091,
      "grad_norm": 3.9891507625579834,
      "learning_rate": 3.210949764521193e-05,
      "loss": 0.5571,
      "step": 1823400
    },
    {
      "epoch": 28.626373626373628,
      "grad_norm": 4.126087665557861,
      "learning_rate": 3.210851648351649e-05,
      "loss": 0.5812,
      "step": 1823500
    },
    {
      "epoch": 28.62794348508634,
      "grad_norm": 3.130628824234009,
      "learning_rate": 3.2107535321821033e-05,
      "loss": 0.6065,
      "step": 1823600
    },
    {
      "epoch": 28.629513343799058,
      "grad_norm": 3.915724039077759,
      "learning_rate": 3.210655416012559e-05,
      "loss": 0.5841,
      "step": 1823700
    },
    {
      "epoch": 28.631083202511775,
      "grad_norm": 3.1575989723205566,
      "learning_rate": 3.210557299843014e-05,
      "loss": 0.5947,
      "step": 1823800
    },
    {
      "epoch": 28.632653061224488,
      "grad_norm": 4.201272964477539,
      "learning_rate": 3.210459183673469e-05,
      "loss": 0.5965,
      "step": 1823900
    },
    {
      "epoch": 28.634222919937205,
      "grad_norm": 4.226655006408691,
      "learning_rate": 3.2103610675039244e-05,
      "loss": 0.6288,
      "step": 1824000
    },
    {
      "epoch": 28.635792778649922,
      "grad_norm": 3.7416796684265137,
      "learning_rate": 3.21026295133438e-05,
      "loss": 0.5787,
      "step": 1824100
    },
    {
      "epoch": 28.63736263736264,
      "grad_norm": 3.798438787460327,
      "learning_rate": 3.210164835164835e-05,
      "loss": 0.6073,
      "step": 1824200
    },
    {
      "epoch": 28.638932496075352,
      "grad_norm": 3.2370846271514893,
      "learning_rate": 3.2100667189952904e-05,
      "loss": 0.561,
      "step": 1824300
    },
    {
      "epoch": 28.64050235478807,
      "grad_norm": 3.6600136756896973,
      "learning_rate": 3.209968602825746e-05,
      "loss": 0.5776,
      "step": 1824400
    },
    {
      "epoch": 28.642072213500786,
      "grad_norm": 3.6749720573425293,
      "learning_rate": 3.209870486656201e-05,
      "loss": 0.6153,
      "step": 1824500
    },
    {
      "epoch": 28.643642072213503,
      "grad_norm": 4.736438274383545,
      "learning_rate": 3.2097723704866564e-05,
      "loss": 0.5746,
      "step": 1824600
    },
    {
      "epoch": 28.645211930926216,
      "grad_norm": 3.213993787765503,
      "learning_rate": 3.2096742543171115e-05,
      "loss": 0.5773,
      "step": 1824700
    },
    {
      "epoch": 28.646781789638933,
      "grad_norm": 4.432462692260742,
      "learning_rate": 3.209576138147567e-05,
      "loss": 0.5537,
      "step": 1824800
    },
    {
      "epoch": 28.64835164835165,
      "grad_norm": 4.203008651733398,
      "learning_rate": 3.209478021978022e-05,
      "loss": 0.5371,
      "step": 1824900
    },
    {
      "epoch": 28.649921507064363,
      "grad_norm": 3.1176021099090576,
      "learning_rate": 3.2093799058084774e-05,
      "loss": 0.5678,
      "step": 1825000
    },
    {
      "epoch": 28.65149136577708,
      "grad_norm": 3.4167237281799316,
      "learning_rate": 3.2092817896389325e-05,
      "loss": 0.5634,
      "step": 1825100
    },
    {
      "epoch": 28.653061224489797,
      "grad_norm": 3.2439124584198,
      "learning_rate": 3.209183673469388e-05,
      "loss": 0.5982,
      "step": 1825200
    },
    {
      "epoch": 28.65463108320251,
      "grad_norm": 3.0097453594207764,
      "learning_rate": 3.209085557299843e-05,
      "loss": 0.5741,
      "step": 1825300
    },
    {
      "epoch": 28.656200941915227,
      "grad_norm": 4.037818431854248,
      "learning_rate": 3.2089874411302985e-05,
      "loss": 0.5655,
      "step": 1825400
    },
    {
      "epoch": 28.657770800627944,
      "grad_norm": 3.5924627780914307,
      "learning_rate": 3.2088893249607536e-05,
      "loss": 0.5613,
      "step": 1825500
    },
    {
      "epoch": 28.65934065934066,
      "grad_norm": 4.618031024932861,
      "learning_rate": 3.2087912087912094e-05,
      "loss": 0.5573,
      "step": 1825600
    },
    {
      "epoch": 28.660910518053374,
      "grad_norm": 3.459618091583252,
      "learning_rate": 3.208693092621664e-05,
      "loss": 0.5712,
      "step": 1825700
    },
    {
      "epoch": 28.66248037676609,
      "grad_norm": 4.959903717041016,
      "learning_rate": 3.2085949764521196e-05,
      "loss": 0.6036,
      "step": 1825800
    },
    {
      "epoch": 28.664050235478808,
      "grad_norm": 4.24155855178833,
      "learning_rate": 3.208496860282575e-05,
      "loss": 0.6152,
      "step": 1825900
    },
    {
      "epoch": 28.665620094191524,
      "grad_norm": 3.4669227600097656,
      "learning_rate": 3.20839874411303e-05,
      "loss": 0.6095,
      "step": 1826000
    },
    {
      "epoch": 28.667189952904238,
      "grad_norm": 2.6375372409820557,
      "learning_rate": 3.208300627943485e-05,
      "loss": 0.5764,
      "step": 1826100
    },
    {
      "epoch": 28.668759811616955,
      "grad_norm": 5.0385918617248535,
      "learning_rate": 3.2082025117739406e-05,
      "loss": 0.5651,
      "step": 1826200
    },
    {
      "epoch": 28.67032967032967,
      "grad_norm": 3.9060635566711426,
      "learning_rate": 3.208104395604396e-05,
      "loss": 0.6216,
      "step": 1826300
    },
    {
      "epoch": 28.671899529042385,
      "grad_norm": 3.6349103450775146,
      "learning_rate": 3.208006279434851e-05,
      "loss": 0.592,
      "step": 1826400
    },
    {
      "epoch": 28.6734693877551,
      "grad_norm": 3.506335973739624,
      "learning_rate": 3.2079081632653066e-05,
      "loss": 0.5783,
      "step": 1826500
    },
    {
      "epoch": 28.67503924646782,
      "grad_norm": 4.991050720214844,
      "learning_rate": 3.207810047095762e-05,
      "loss": 0.5966,
      "step": 1826600
    },
    {
      "epoch": 28.676609105180535,
      "grad_norm": 2.7876667976379395,
      "learning_rate": 3.207711930926217e-05,
      "loss": 0.5875,
      "step": 1826700
    },
    {
      "epoch": 28.67817896389325,
      "grad_norm": 3.1460423469543457,
      "learning_rate": 3.207613814756672e-05,
      "loss": 0.5886,
      "step": 1826800
    },
    {
      "epoch": 28.679748822605966,
      "grad_norm": 3.710597038269043,
      "learning_rate": 3.207515698587128e-05,
      "loss": 0.6484,
      "step": 1826900
    },
    {
      "epoch": 28.681318681318682,
      "grad_norm": 3.8712620735168457,
      "learning_rate": 3.207417582417583e-05,
      "loss": 0.5975,
      "step": 1827000
    },
    {
      "epoch": 28.682888540031396,
      "grad_norm": 4.367341995239258,
      "learning_rate": 3.207319466248038e-05,
      "loss": 0.613,
      "step": 1827100
    },
    {
      "epoch": 28.684458398744113,
      "grad_norm": 4.505285739898682,
      "learning_rate": 3.207221350078493e-05,
      "loss": 0.5759,
      "step": 1827200
    },
    {
      "epoch": 28.68602825745683,
      "grad_norm": 4.027815818786621,
      "learning_rate": 3.207123233908949e-05,
      "loss": 0.6037,
      "step": 1827300
    },
    {
      "epoch": 28.687598116169546,
      "grad_norm": 4.165520191192627,
      "learning_rate": 3.207025117739403e-05,
      "loss": 0.5877,
      "step": 1827400
    },
    {
      "epoch": 28.68916797488226,
      "grad_norm": 3.017136812210083,
      "learning_rate": 3.206927001569859e-05,
      "loss": 0.6154,
      "step": 1827500
    },
    {
      "epoch": 28.690737833594977,
      "grad_norm": 4.397994518280029,
      "learning_rate": 3.206828885400314e-05,
      "loss": 0.6018,
      "step": 1827600
    },
    {
      "epoch": 28.692307692307693,
      "grad_norm": 4.140347957611084,
      "learning_rate": 3.20673076923077e-05,
      "loss": 0.6131,
      "step": 1827700
    },
    {
      "epoch": 28.693877551020407,
      "grad_norm": 3.563833475112915,
      "learning_rate": 3.206632653061224e-05,
      "loss": 0.5623,
      "step": 1827800
    },
    {
      "epoch": 28.695447409733124,
      "grad_norm": 3.983086347579956,
      "learning_rate": 3.20653453689168e-05,
      "loss": 0.5874,
      "step": 1827900
    },
    {
      "epoch": 28.69701726844584,
      "grad_norm": 4.028322219848633,
      "learning_rate": 3.206436420722135e-05,
      "loss": 0.5965,
      "step": 1828000
    },
    {
      "epoch": 28.698587127158557,
      "grad_norm": 4.22267484664917,
      "learning_rate": 3.20633830455259e-05,
      "loss": 0.5909,
      "step": 1828100
    },
    {
      "epoch": 28.70015698587127,
      "grad_norm": 2.527559280395508,
      "learning_rate": 3.206240188383045e-05,
      "loss": 0.5926,
      "step": 1828200
    },
    {
      "epoch": 28.701726844583987,
      "grad_norm": 4.852296352386475,
      "learning_rate": 3.206142072213501e-05,
      "loss": 0.5933,
      "step": 1828300
    },
    {
      "epoch": 28.703296703296704,
      "grad_norm": 3.756514072418213,
      "learning_rate": 3.206043956043956e-05,
      "loss": 0.6169,
      "step": 1828400
    },
    {
      "epoch": 28.704866562009418,
      "grad_norm": 3.7489192485809326,
      "learning_rate": 3.205945839874411e-05,
      "loss": 0.597,
      "step": 1828500
    },
    {
      "epoch": 28.706436420722135,
      "grad_norm": 4.242902755737305,
      "learning_rate": 3.205847723704867e-05,
      "loss": 0.6407,
      "step": 1828600
    },
    {
      "epoch": 28.70800627943485,
      "grad_norm": 3.5439131259918213,
      "learning_rate": 3.205749607535322e-05,
      "loss": 0.5804,
      "step": 1828700
    },
    {
      "epoch": 28.70957613814757,
      "grad_norm": 3.824228286743164,
      "learning_rate": 3.205651491365777e-05,
      "loss": 0.579,
      "step": 1828800
    },
    {
      "epoch": 28.71114599686028,
      "grad_norm": 4.472926616668701,
      "learning_rate": 3.2055533751962324e-05,
      "loss": 0.6032,
      "step": 1828900
    },
    {
      "epoch": 28.712715855573,
      "grad_norm": 3.3585243225097656,
      "learning_rate": 3.205455259026688e-05,
      "loss": 0.6032,
      "step": 1829000
    },
    {
      "epoch": 28.714285714285715,
      "grad_norm": 3.2924540042877197,
      "learning_rate": 3.205357142857143e-05,
      "loss": 0.5921,
      "step": 1829100
    },
    {
      "epoch": 28.71585557299843,
      "grad_norm": 2.7952754497528076,
      "learning_rate": 3.205259026687598e-05,
      "loss": 0.6058,
      "step": 1829200
    },
    {
      "epoch": 28.717425431711145,
      "grad_norm": 5.112781047821045,
      "learning_rate": 3.2051609105180534e-05,
      "loss": 0.5971,
      "step": 1829300
    },
    {
      "epoch": 28.718995290423862,
      "grad_norm": 4.092004299163818,
      "learning_rate": 3.205062794348509e-05,
      "loss": 0.5989,
      "step": 1829400
    },
    {
      "epoch": 28.72056514913658,
      "grad_norm": 4.275511741638184,
      "learning_rate": 3.2049646781789636e-05,
      "loss": 0.5503,
      "step": 1829500
    },
    {
      "epoch": 28.722135007849293,
      "grad_norm": 3.416654109954834,
      "learning_rate": 3.2048665620094194e-05,
      "loss": 0.5868,
      "step": 1829600
    },
    {
      "epoch": 28.72370486656201,
      "grad_norm": 4.219162940979004,
      "learning_rate": 3.2047684458398745e-05,
      "loss": 0.5627,
      "step": 1829700
    },
    {
      "epoch": 28.725274725274726,
      "grad_norm": 3.1664648056030273,
      "learning_rate": 3.20467032967033e-05,
      "loss": 0.5678,
      "step": 1829800
    },
    {
      "epoch": 28.72684458398744,
      "grad_norm": 4.126694202423096,
      "learning_rate": 3.204572213500785e-05,
      "loss": 0.5799,
      "step": 1829900
    },
    {
      "epoch": 28.728414442700156,
      "grad_norm": 3.2587575912475586,
      "learning_rate": 3.2044740973312405e-05,
      "loss": 0.5284,
      "step": 1830000
    },
    {
      "epoch": 28.729984301412873,
      "grad_norm": 4.5062031745910645,
      "learning_rate": 3.2043759811616956e-05,
      "loss": 0.6078,
      "step": 1830100
    },
    {
      "epoch": 28.73155416012559,
      "grad_norm": 3.7040958404541016,
      "learning_rate": 3.204277864992151e-05,
      "loss": 0.5869,
      "step": 1830200
    },
    {
      "epoch": 28.733124018838303,
      "grad_norm": 4.413400650024414,
      "learning_rate": 3.204179748822606e-05,
      "loss": 0.6001,
      "step": 1830300
    },
    {
      "epoch": 28.73469387755102,
      "grad_norm": 4.105203151702881,
      "learning_rate": 3.2040816326530615e-05,
      "loss": 0.5884,
      "step": 1830400
    },
    {
      "epoch": 28.736263736263737,
      "grad_norm": 3.5608487129211426,
      "learning_rate": 3.2039835164835166e-05,
      "loss": 0.6079,
      "step": 1830500
    },
    {
      "epoch": 28.73783359497645,
      "grad_norm": 2.949007272720337,
      "learning_rate": 3.203885400313972e-05,
      "loss": 0.5853,
      "step": 1830600
    },
    {
      "epoch": 28.739403453689167,
      "grad_norm": 4.218767166137695,
      "learning_rate": 3.2037872841444275e-05,
      "loss": 0.6021,
      "step": 1830700
    },
    {
      "epoch": 28.740973312401884,
      "grad_norm": 5.9284515380859375,
      "learning_rate": 3.2036891679748826e-05,
      "loss": 0.5933,
      "step": 1830800
    },
    {
      "epoch": 28.7425431711146,
      "grad_norm": 3.35176157951355,
      "learning_rate": 3.203591051805338e-05,
      "loss": 0.6044,
      "step": 1830900
    },
    {
      "epoch": 28.744113029827314,
      "grad_norm": 2.9146530628204346,
      "learning_rate": 3.203492935635793e-05,
      "loss": 0.552,
      "step": 1831000
    },
    {
      "epoch": 28.74568288854003,
      "grad_norm": 3.374462127685547,
      "learning_rate": 3.2033948194662486e-05,
      "loss": 0.5793,
      "step": 1831100
    },
    {
      "epoch": 28.747252747252748,
      "grad_norm": 2.7502965927124023,
      "learning_rate": 3.203296703296704e-05,
      "loss": 0.6131,
      "step": 1831200
    },
    {
      "epoch": 28.74882260596546,
      "grad_norm": 3.62990403175354,
      "learning_rate": 3.203198587127159e-05,
      "loss": 0.5981,
      "step": 1831300
    },
    {
      "epoch": 28.75039246467818,
      "grad_norm": 4.045933723449707,
      "learning_rate": 3.203100470957614e-05,
      "loss": 0.587,
      "step": 1831400
    },
    {
      "epoch": 28.751962323390895,
      "grad_norm": 2.940674304962158,
      "learning_rate": 3.2030023547880697e-05,
      "loss": 0.5833,
      "step": 1831500
    },
    {
      "epoch": 28.753532182103612,
      "grad_norm": 2.5526092052459717,
      "learning_rate": 3.202904238618524e-05,
      "loss": 0.6218,
      "step": 1831600
    },
    {
      "epoch": 28.755102040816325,
      "grad_norm": 3.0107908248901367,
      "learning_rate": 3.20280612244898e-05,
      "loss": 0.5854,
      "step": 1831700
    },
    {
      "epoch": 28.756671899529042,
      "grad_norm": 2.430772066116333,
      "learning_rate": 3.202708006279435e-05,
      "loss": 0.5793,
      "step": 1831800
    },
    {
      "epoch": 28.75824175824176,
      "grad_norm": 3.3049874305725098,
      "learning_rate": 3.20260989010989e-05,
      "loss": 0.5645,
      "step": 1831900
    },
    {
      "epoch": 28.759811616954472,
      "grad_norm": 2.593853235244751,
      "learning_rate": 3.202511773940345e-05,
      "loss": 0.5748,
      "step": 1832000
    },
    {
      "epoch": 28.76138147566719,
      "grad_norm": 4.1291985511779785,
      "learning_rate": 3.202413657770801e-05,
      "loss": 0.6024,
      "step": 1832100
    },
    {
      "epoch": 28.762951334379906,
      "grad_norm": 4.4590301513671875,
      "learning_rate": 3.202315541601256e-05,
      "loss": 0.6222,
      "step": 1832200
    },
    {
      "epoch": 28.764521193092623,
      "grad_norm": 4.40862512588501,
      "learning_rate": 3.202217425431711e-05,
      "loss": 0.6126,
      "step": 1832300
    },
    {
      "epoch": 28.766091051805336,
      "grad_norm": 4.154551982879639,
      "learning_rate": 3.202119309262166e-05,
      "loss": 0.6311,
      "step": 1832400
    },
    {
      "epoch": 28.767660910518053,
      "grad_norm": 4.37943696975708,
      "learning_rate": 3.202021193092622e-05,
      "loss": 0.5691,
      "step": 1832500
    },
    {
      "epoch": 28.76923076923077,
      "grad_norm": 3.043003559112549,
      "learning_rate": 3.201923076923077e-05,
      "loss": 0.598,
      "step": 1832600
    },
    {
      "epoch": 28.770800627943487,
      "grad_norm": 3.1056747436523438,
      "learning_rate": 3.201824960753532e-05,
      "loss": 0.5745,
      "step": 1832700
    },
    {
      "epoch": 28.7723704866562,
      "grad_norm": 3.6121370792388916,
      "learning_rate": 3.201726844583988e-05,
      "loss": 0.6099,
      "step": 1832800
    },
    {
      "epoch": 28.773940345368917,
      "grad_norm": 4.395668029785156,
      "learning_rate": 3.201628728414443e-05,
      "loss": 0.6701,
      "step": 1832900
    },
    {
      "epoch": 28.775510204081634,
      "grad_norm": 3.1409993171691895,
      "learning_rate": 3.201530612244898e-05,
      "loss": 0.5842,
      "step": 1833000
    },
    {
      "epoch": 28.777080062794347,
      "grad_norm": 3.81670880317688,
      "learning_rate": 3.201432496075353e-05,
      "loss": 0.6191,
      "step": 1833100
    },
    {
      "epoch": 28.778649921507064,
      "grad_norm": 5.040846824645996,
      "learning_rate": 3.201334379905809e-05,
      "loss": 0.5993,
      "step": 1833200
    },
    {
      "epoch": 28.78021978021978,
      "grad_norm": 3.7381935119628906,
      "learning_rate": 3.2012362637362635e-05,
      "loss": 0.5822,
      "step": 1833300
    },
    {
      "epoch": 28.781789638932494,
      "grad_norm": 4.01150369644165,
      "learning_rate": 3.201138147566719e-05,
      "loss": 0.5839,
      "step": 1833400
    },
    {
      "epoch": 28.78335949764521,
      "grad_norm": 3.2235262393951416,
      "learning_rate": 3.201040031397174e-05,
      "loss": 0.6051,
      "step": 1833500
    },
    {
      "epoch": 28.784929356357928,
      "grad_norm": 4.083746910095215,
      "learning_rate": 3.20094191522763e-05,
      "loss": 0.6031,
      "step": 1833600
    },
    {
      "epoch": 28.786499215070645,
      "grad_norm": 3.6860976219177246,
      "learning_rate": 3.2008437990580845e-05,
      "loss": 0.5779,
      "step": 1833700
    },
    {
      "epoch": 28.788069073783358,
      "grad_norm": 4.345290660858154,
      "learning_rate": 3.20074568288854e-05,
      "loss": 0.5941,
      "step": 1833800
    },
    {
      "epoch": 28.789638932496075,
      "grad_norm": 2.837583065032959,
      "learning_rate": 3.2006475667189954e-05,
      "loss": 0.6126,
      "step": 1833900
    },
    {
      "epoch": 28.791208791208792,
      "grad_norm": 2.7036983966827393,
      "learning_rate": 3.2005494505494505e-05,
      "loss": 0.5946,
      "step": 1834000
    },
    {
      "epoch": 28.79277864992151,
      "grad_norm": 3.2820792198181152,
      "learning_rate": 3.2004513343799056e-05,
      "loss": 0.5956,
      "step": 1834100
    },
    {
      "epoch": 28.794348508634222,
      "grad_norm": 3.51061749458313,
      "learning_rate": 3.2003532182103614e-05,
      "loss": 0.5997,
      "step": 1834200
    },
    {
      "epoch": 28.79591836734694,
      "grad_norm": 4.174989223480225,
      "learning_rate": 3.2002551020408165e-05,
      "loss": 0.5749,
      "step": 1834300
    },
    {
      "epoch": 28.797488226059656,
      "grad_norm": 3.9515087604522705,
      "learning_rate": 3.2001569858712716e-05,
      "loss": 0.5835,
      "step": 1834400
    },
    {
      "epoch": 28.79905808477237,
      "grad_norm": 3.7013609409332275,
      "learning_rate": 3.2000588697017267e-05,
      "loss": 0.5698,
      "step": 1834500
    },
    {
      "epoch": 28.800627943485086,
      "grad_norm": 3.5336174964904785,
      "learning_rate": 3.1999607535321824e-05,
      "loss": 0.5951,
      "step": 1834600
    },
    {
      "epoch": 28.802197802197803,
      "grad_norm": 4.1844258308410645,
      "learning_rate": 3.1998626373626375e-05,
      "loss": 0.568,
      "step": 1834700
    },
    {
      "epoch": 28.80376766091052,
      "grad_norm": 4.514759540557861,
      "learning_rate": 3.1997645211930926e-05,
      "loss": 0.5863,
      "step": 1834800
    },
    {
      "epoch": 28.805337519623233,
      "grad_norm": 4.035957336425781,
      "learning_rate": 3.1996664050235484e-05,
      "loss": 0.6142,
      "step": 1834900
    },
    {
      "epoch": 28.80690737833595,
      "grad_norm": 3.9756054878234863,
      "learning_rate": 3.1995682888540035e-05,
      "loss": 0.548,
      "step": 1835000
    },
    {
      "epoch": 28.808477237048667,
      "grad_norm": 4.354780197143555,
      "learning_rate": 3.1994701726844586e-05,
      "loss": 0.5963,
      "step": 1835100
    },
    {
      "epoch": 28.81004709576138,
      "grad_norm": 4.778981685638428,
      "learning_rate": 3.199372056514914e-05,
      "loss": 0.6347,
      "step": 1835200
    },
    {
      "epoch": 28.811616954474097,
      "grad_norm": 4.353818893432617,
      "learning_rate": 3.1992739403453695e-05,
      "loss": 0.591,
      "step": 1835300
    },
    {
      "epoch": 28.813186813186814,
      "grad_norm": 3.7698750495910645,
      "learning_rate": 3.199175824175824e-05,
      "loss": 0.6029,
      "step": 1835400
    },
    {
      "epoch": 28.81475667189953,
      "grad_norm": 4.3904194831848145,
      "learning_rate": 3.19907770800628e-05,
      "loss": 0.5752,
      "step": 1835500
    },
    {
      "epoch": 28.816326530612244,
      "grad_norm": 3.5004618167877197,
      "learning_rate": 3.198979591836735e-05,
      "loss": 0.6083,
      "step": 1835600
    },
    {
      "epoch": 28.81789638932496,
      "grad_norm": 3.7268786430358887,
      "learning_rate": 3.1988814756671905e-05,
      "loss": 0.585,
      "step": 1835700
    },
    {
      "epoch": 28.819466248037678,
      "grad_norm": 3.9728786945343018,
      "learning_rate": 3.198783359497645e-05,
      "loss": 0.6061,
      "step": 1835800
    },
    {
      "epoch": 28.82103610675039,
      "grad_norm": 3.3396034240722656,
      "learning_rate": 3.198685243328101e-05,
      "loss": 0.5646,
      "step": 1835900
    },
    {
      "epoch": 28.822605965463108,
      "grad_norm": 3.4095778465270996,
      "learning_rate": 3.198587127158556e-05,
      "loss": 0.6009,
      "step": 1836000
    },
    {
      "epoch": 28.824175824175825,
      "grad_norm": 4.673145771026611,
      "learning_rate": 3.198489010989011e-05,
      "loss": 0.6085,
      "step": 1836100
    },
    {
      "epoch": 28.82574568288854,
      "grad_norm": 4.299363613128662,
      "learning_rate": 3.198390894819466e-05,
      "loss": 0.6053,
      "step": 1836200
    },
    {
      "epoch": 28.827315541601255,
      "grad_norm": 2.915404796600342,
      "learning_rate": 3.198292778649922e-05,
      "loss": 0.5981,
      "step": 1836300
    },
    {
      "epoch": 28.828885400313972,
      "grad_norm": 4.039735317230225,
      "learning_rate": 3.198194662480377e-05,
      "loss": 0.5682,
      "step": 1836400
    },
    {
      "epoch": 28.83045525902669,
      "grad_norm": 5.712957859039307,
      "learning_rate": 3.198096546310832e-05,
      "loss": 0.5833,
      "step": 1836500
    },
    {
      "epoch": 28.832025117739402,
      "grad_norm": 3.8634870052337646,
      "learning_rate": 3.197998430141287e-05,
      "loss": 0.6049,
      "step": 1836600
    },
    {
      "epoch": 28.83359497645212,
      "grad_norm": 4.003025531768799,
      "learning_rate": 3.197900313971743e-05,
      "loss": 0.6158,
      "step": 1836700
    },
    {
      "epoch": 28.835164835164836,
      "grad_norm": 4.196132183074951,
      "learning_rate": 3.197802197802198e-05,
      "loss": 0.5793,
      "step": 1836800
    },
    {
      "epoch": 28.836734693877553,
      "grad_norm": 3.5959763526916504,
      "learning_rate": 3.197704081632653e-05,
      "loss": 0.5895,
      "step": 1836900
    },
    {
      "epoch": 28.838304552590266,
      "grad_norm": 4.087028980255127,
      "learning_rate": 3.197605965463109e-05,
      "loss": 0.5956,
      "step": 1837000
    },
    {
      "epoch": 28.839874411302983,
      "grad_norm": 2.474985122680664,
      "learning_rate": 3.197507849293564e-05,
      "loss": 0.6161,
      "step": 1837100
    },
    {
      "epoch": 28.8414442700157,
      "grad_norm": 4.800826549530029,
      "learning_rate": 3.197409733124019e-05,
      "loss": 0.5939,
      "step": 1837200
    },
    {
      "epoch": 28.843014128728413,
      "grad_norm": 2.210669755935669,
      "learning_rate": 3.197311616954474e-05,
      "loss": 0.5953,
      "step": 1837300
    },
    {
      "epoch": 28.84458398744113,
      "grad_norm": 3.530791997909546,
      "learning_rate": 3.19721350078493e-05,
      "loss": 0.5705,
      "step": 1837400
    },
    {
      "epoch": 28.846153846153847,
      "grad_norm": 4.706782817840576,
      "learning_rate": 3.1971153846153843e-05,
      "loss": 0.6122,
      "step": 1837500
    },
    {
      "epoch": 28.847723704866564,
      "grad_norm": 3.5868561267852783,
      "learning_rate": 3.19701726844584e-05,
      "loss": 0.5962,
      "step": 1837600
    },
    {
      "epoch": 28.849293563579277,
      "grad_norm": 4.090631008148193,
      "learning_rate": 3.196919152276295e-05,
      "loss": 0.5969,
      "step": 1837700
    },
    {
      "epoch": 28.850863422291994,
      "grad_norm": 3.119978427886963,
      "learning_rate": 3.196821036106751e-05,
      "loss": 0.5701,
      "step": 1837800
    },
    {
      "epoch": 28.85243328100471,
      "grad_norm": 2.676142692565918,
      "learning_rate": 3.1967229199372054e-05,
      "loss": 0.5883,
      "step": 1837900
    },
    {
      "epoch": 28.854003139717424,
      "grad_norm": 4.406471252441406,
      "learning_rate": 3.196624803767661e-05,
      "loss": 0.58,
      "step": 1838000
    },
    {
      "epoch": 28.85557299843014,
      "grad_norm": 3.9425673484802246,
      "learning_rate": 3.196526687598116e-05,
      "loss": 0.5578,
      "step": 1838100
    },
    {
      "epoch": 28.857142857142858,
      "grad_norm": 3.9357657432556152,
      "learning_rate": 3.1964285714285714e-05,
      "loss": 0.5831,
      "step": 1838200
    },
    {
      "epoch": 28.858712715855575,
      "grad_norm": 3.7224533557891846,
      "learning_rate": 3.1963304552590265e-05,
      "loss": 0.6068,
      "step": 1838300
    },
    {
      "epoch": 28.860282574568288,
      "grad_norm": 4.291721343994141,
      "learning_rate": 3.196232339089482e-05,
      "loss": 0.6144,
      "step": 1838400
    },
    {
      "epoch": 28.861852433281005,
      "grad_norm": 3.312872886657715,
      "learning_rate": 3.1961342229199374e-05,
      "loss": 0.606,
      "step": 1838500
    },
    {
      "epoch": 28.86342229199372,
      "grad_norm": 3.938458204269409,
      "learning_rate": 3.1960361067503925e-05,
      "loss": 0.6056,
      "step": 1838600
    },
    {
      "epoch": 28.864992150706435,
      "grad_norm": 3.1477062702178955,
      "learning_rate": 3.1959379905808476e-05,
      "loss": 0.5767,
      "step": 1838700
    },
    {
      "epoch": 28.86656200941915,
      "grad_norm": 3.9859001636505127,
      "learning_rate": 3.195839874411303e-05,
      "loss": 0.584,
      "step": 1838800
    },
    {
      "epoch": 28.86813186813187,
      "grad_norm": 3.9185431003570557,
      "learning_rate": 3.1957417582417584e-05,
      "loss": 0.587,
      "step": 1838900
    },
    {
      "epoch": 28.869701726844585,
      "grad_norm": 3.551457643508911,
      "learning_rate": 3.1956436420722135e-05,
      "loss": 0.5906,
      "step": 1839000
    },
    {
      "epoch": 28.8712715855573,
      "grad_norm": 3.3061838150024414,
      "learning_rate": 3.195545525902669e-05,
      "loss": 0.5914,
      "step": 1839100
    },
    {
      "epoch": 28.872841444270016,
      "grad_norm": 3.9917726516723633,
      "learning_rate": 3.1954474097331244e-05,
      "loss": 0.588,
      "step": 1839200
    },
    {
      "epoch": 28.874411302982733,
      "grad_norm": 4.29754114151001,
      "learning_rate": 3.1953492935635795e-05,
      "loss": 0.6109,
      "step": 1839300
    },
    {
      "epoch": 28.875981161695446,
      "grad_norm": 3.074185848236084,
      "learning_rate": 3.1952511773940346e-05,
      "loss": 0.5546,
      "step": 1839400
    },
    {
      "epoch": 28.877551020408163,
      "grad_norm": 4.187894344329834,
      "learning_rate": 3.1951530612244904e-05,
      "loss": 0.5911,
      "step": 1839500
    },
    {
      "epoch": 28.87912087912088,
      "grad_norm": 4.180296421051025,
      "learning_rate": 3.195054945054945e-05,
      "loss": 0.5924,
      "step": 1839600
    },
    {
      "epoch": 28.880690737833596,
      "grad_norm": 4.003579139709473,
      "learning_rate": 3.1949568288854006e-05,
      "loss": 0.6146,
      "step": 1839700
    },
    {
      "epoch": 28.88226059654631,
      "grad_norm": 3.927764892578125,
      "learning_rate": 3.194858712715856e-05,
      "loss": 0.5704,
      "step": 1839800
    },
    {
      "epoch": 28.883830455259027,
      "grad_norm": 3.211216688156128,
      "learning_rate": 3.1947605965463114e-05,
      "loss": 0.6089,
      "step": 1839900
    },
    {
      "epoch": 28.885400313971743,
      "grad_norm": 3.8561651706695557,
      "learning_rate": 3.194662480376766e-05,
      "loss": 0.5764,
      "step": 1840000
    },
    {
      "epoch": 28.886970172684457,
      "grad_norm": 2.2088987827301025,
      "learning_rate": 3.1945643642072216e-05,
      "loss": 0.6211,
      "step": 1840100
    },
    {
      "epoch": 28.888540031397174,
      "grad_norm": 4.222872734069824,
      "learning_rate": 3.194466248037677e-05,
      "loss": 0.5726,
      "step": 1840200
    },
    {
      "epoch": 28.89010989010989,
      "grad_norm": 6.026890754699707,
      "learning_rate": 3.194368131868132e-05,
      "loss": 0.5078,
      "step": 1840300
    },
    {
      "epoch": 28.891679748822607,
      "grad_norm": 2.9288787841796875,
      "learning_rate": 3.194270015698587e-05,
      "loss": 0.5769,
      "step": 1840400
    },
    {
      "epoch": 28.89324960753532,
      "grad_norm": 3.954366683959961,
      "learning_rate": 3.194171899529043e-05,
      "loss": 0.632,
      "step": 1840500
    },
    {
      "epoch": 28.894819466248038,
      "grad_norm": 3.8206489086151123,
      "learning_rate": 3.194073783359498e-05,
      "loss": 0.5809,
      "step": 1840600
    },
    {
      "epoch": 28.896389324960754,
      "grad_norm": 4.306427955627441,
      "learning_rate": 3.193975667189953e-05,
      "loss": 0.5974,
      "step": 1840700
    },
    {
      "epoch": 28.897959183673468,
      "grad_norm": 4.488718509674072,
      "learning_rate": 3.193877551020408e-05,
      "loss": 0.6136,
      "step": 1840800
    },
    {
      "epoch": 28.899529042386185,
      "grad_norm": 3.5418951511383057,
      "learning_rate": 3.193779434850864e-05,
      "loss": 0.5846,
      "step": 1840900
    },
    {
      "epoch": 28.9010989010989,
      "grad_norm": 3.6983485221862793,
      "learning_rate": 3.193681318681319e-05,
      "loss": 0.5818,
      "step": 1841000
    },
    {
      "epoch": 28.90266875981162,
      "grad_norm": 4.4700927734375,
      "learning_rate": 3.193583202511774e-05,
      "loss": 0.6413,
      "step": 1841100
    },
    {
      "epoch": 28.90423861852433,
      "grad_norm": 5.674374103546143,
      "learning_rate": 3.19348508634223e-05,
      "loss": 0.5935,
      "step": 1841200
    },
    {
      "epoch": 28.90580847723705,
      "grad_norm": 3.5925393104553223,
      "learning_rate": 3.193386970172685e-05,
      "loss": 0.5468,
      "step": 1841300
    },
    {
      "epoch": 28.907378335949765,
      "grad_norm": 3.6957454681396484,
      "learning_rate": 3.19328885400314e-05,
      "loss": 0.623,
      "step": 1841400
    },
    {
      "epoch": 28.90894819466248,
      "grad_norm": 3.929263114929199,
      "learning_rate": 3.193190737833595e-05,
      "loss": 0.5952,
      "step": 1841500
    },
    {
      "epoch": 28.910518053375196,
      "grad_norm": 3.7181620597839355,
      "learning_rate": 3.193092621664051e-05,
      "loss": 0.5956,
      "step": 1841600
    },
    {
      "epoch": 28.912087912087912,
      "grad_norm": 4.235859394073486,
      "learning_rate": 3.192994505494505e-05,
      "loss": 0.6007,
      "step": 1841700
    },
    {
      "epoch": 28.91365777080063,
      "grad_norm": 4.775994777679443,
      "learning_rate": 3.192896389324961e-05,
      "loss": 0.5916,
      "step": 1841800
    },
    {
      "epoch": 28.915227629513343,
      "grad_norm": 3.2843430042266846,
      "learning_rate": 3.192798273155416e-05,
      "loss": 0.5608,
      "step": 1841900
    },
    {
      "epoch": 28.91679748822606,
      "grad_norm": 3.752021551132202,
      "learning_rate": 3.192700156985872e-05,
      "loss": 0.6413,
      "step": 1842000
    },
    {
      "epoch": 28.918367346938776,
      "grad_norm": 2.9672865867614746,
      "learning_rate": 3.192602040816326e-05,
      "loss": 0.5769,
      "step": 1842100
    },
    {
      "epoch": 28.919937205651493,
      "grad_norm": 2.886733055114746,
      "learning_rate": 3.192503924646782e-05,
      "loss": 0.5865,
      "step": 1842200
    },
    {
      "epoch": 28.921507064364206,
      "grad_norm": 4.872402191162109,
      "learning_rate": 3.192405808477237e-05,
      "loss": 0.5582,
      "step": 1842300
    },
    {
      "epoch": 28.923076923076923,
      "grad_norm": 3.525625705718994,
      "learning_rate": 3.192307692307692e-05,
      "loss": 0.5987,
      "step": 1842400
    },
    {
      "epoch": 28.92464678178964,
      "grad_norm": 3.5426905155181885,
      "learning_rate": 3.1922095761381474e-05,
      "loss": 0.5798,
      "step": 1842500
    },
    {
      "epoch": 28.926216640502354,
      "grad_norm": 4.658824443817139,
      "learning_rate": 3.192111459968603e-05,
      "loss": 0.5856,
      "step": 1842600
    },
    {
      "epoch": 28.92778649921507,
      "grad_norm": 3.998133897781372,
      "learning_rate": 3.192013343799058e-05,
      "loss": 0.6055,
      "step": 1842700
    },
    {
      "epoch": 28.929356357927787,
      "grad_norm": 3.814444065093994,
      "learning_rate": 3.1919152276295134e-05,
      "loss": 0.6052,
      "step": 1842800
    },
    {
      "epoch": 28.9309262166405,
      "grad_norm": 4.011667728424072,
      "learning_rate": 3.1918171114599685e-05,
      "loss": 0.5674,
      "step": 1842900
    },
    {
      "epoch": 28.932496075353217,
      "grad_norm": 4.24435567855835,
      "learning_rate": 3.191718995290424e-05,
      "loss": 0.6334,
      "step": 1843000
    },
    {
      "epoch": 28.934065934065934,
      "grad_norm": 3.8291282653808594,
      "learning_rate": 3.191620879120879e-05,
      "loss": 0.5995,
      "step": 1843100
    },
    {
      "epoch": 28.93563579277865,
      "grad_norm": 4.3719377517700195,
      "learning_rate": 3.1915227629513344e-05,
      "loss": 0.5943,
      "step": 1843200
    },
    {
      "epoch": 28.937205651491364,
      "grad_norm": 4.08776330947876,
      "learning_rate": 3.19142464678179e-05,
      "loss": 0.6371,
      "step": 1843300
    },
    {
      "epoch": 28.93877551020408,
      "grad_norm": 4.444706439971924,
      "learning_rate": 3.191326530612245e-05,
      "loss": 0.599,
      "step": 1843400
    },
    {
      "epoch": 28.940345368916798,
      "grad_norm": 5.2210001945495605,
      "learning_rate": 3.1912284144427004e-05,
      "loss": 0.6059,
      "step": 1843500
    },
    {
      "epoch": 28.941915227629515,
      "grad_norm": 4.974320411682129,
      "learning_rate": 3.1911302982731555e-05,
      "loss": 0.6236,
      "step": 1843600
    },
    {
      "epoch": 28.94348508634223,
      "grad_norm": 3.3076257705688477,
      "learning_rate": 3.191032182103611e-05,
      "loss": 0.5889,
      "step": 1843700
    },
    {
      "epoch": 28.945054945054945,
      "grad_norm": 3.764148235321045,
      "learning_rate": 3.190934065934066e-05,
      "loss": 0.5658,
      "step": 1843800
    },
    {
      "epoch": 28.946624803767662,
      "grad_norm": 4.146386623382568,
      "learning_rate": 3.1908359497645215e-05,
      "loss": 0.5576,
      "step": 1843900
    },
    {
      "epoch": 28.948194662480375,
      "grad_norm": 3.7548131942749023,
      "learning_rate": 3.1907378335949766e-05,
      "loss": 0.5996,
      "step": 1844000
    },
    {
      "epoch": 28.949764521193092,
      "grad_norm": 5.163817405700684,
      "learning_rate": 3.1906397174254323e-05,
      "loss": 0.6039,
      "step": 1844100
    },
    {
      "epoch": 28.95133437990581,
      "grad_norm": 4.152823448181152,
      "learning_rate": 3.190541601255887e-05,
      "loss": 0.6247,
      "step": 1844200
    },
    {
      "epoch": 28.952904238618526,
      "grad_norm": 3.598109006881714,
      "learning_rate": 3.1904434850863425e-05,
      "loss": 0.6021,
      "step": 1844300
    },
    {
      "epoch": 28.95447409733124,
      "grad_norm": 3.7545905113220215,
      "learning_rate": 3.1903453689167976e-05,
      "loss": 0.6598,
      "step": 1844400
    },
    {
      "epoch": 28.956043956043956,
      "grad_norm": 3.9716460704803467,
      "learning_rate": 3.190247252747253e-05,
      "loss": 0.6089,
      "step": 1844500
    },
    {
      "epoch": 28.957613814756673,
      "grad_norm": 3.872857093811035,
      "learning_rate": 3.190149136577708e-05,
      "loss": 0.616,
      "step": 1844600
    },
    {
      "epoch": 28.959183673469386,
      "grad_norm": 3.8248021602630615,
      "learning_rate": 3.1900510204081636e-05,
      "loss": 0.5703,
      "step": 1844700
    },
    {
      "epoch": 28.960753532182103,
      "grad_norm": 4.47209358215332,
      "learning_rate": 3.189952904238619e-05,
      "loss": 0.6041,
      "step": 1844800
    },
    {
      "epoch": 28.96232339089482,
      "grad_norm": 3.814924478530884,
      "learning_rate": 3.189854788069074e-05,
      "loss": 0.6088,
      "step": 1844900
    },
    {
      "epoch": 28.963893249607537,
      "grad_norm": 2.9213268756866455,
      "learning_rate": 3.189756671899529e-05,
      "loss": 0.5477,
      "step": 1845000
    },
    {
      "epoch": 28.96546310832025,
      "grad_norm": 2.832821846008301,
      "learning_rate": 3.189658555729985e-05,
      "loss": 0.6057,
      "step": 1845100
    },
    {
      "epoch": 28.967032967032967,
      "grad_norm": 4.430343151092529,
      "learning_rate": 3.18956043956044e-05,
      "loss": 0.5651,
      "step": 1845200
    },
    {
      "epoch": 28.968602825745684,
      "grad_norm": 2.704805850982666,
      "learning_rate": 3.189462323390895e-05,
      "loss": 0.6276,
      "step": 1845300
    },
    {
      "epoch": 28.970172684458397,
      "grad_norm": 3.5502476692199707,
      "learning_rate": 3.1893642072213507e-05,
      "loss": 0.6118,
      "step": 1845400
    },
    {
      "epoch": 28.971742543171114,
      "grad_norm": 3.4942402839660645,
      "learning_rate": 3.189266091051806e-05,
      "loss": 0.5963,
      "step": 1845500
    },
    {
      "epoch": 28.97331240188383,
      "grad_norm": 4.081743240356445,
      "learning_rate": 3.189167974882261e-05,
      "loss": 0.5815,
      "step": 1845600
    },
    {
      "epoch": 28.974882260596548,
      "grad_norm": 4.14132833480835,
      "learning_rate": 3.189069858712716e-05,
      "loss": 0.5887,
      "step": 1845700
    },
    {
      "epoch": 28.97645211930926,
      "grad_norm": 4.130044460296631,
      "learning_rate": 3.188971742543172e-05,
      "loss": 0.5573,
      "step": 1845800
    },
    {
      "epoch": 28.978021978021978,
      "grad_norm": 3.596571683883667,
      "learning_rate": 3.188873626373626e-05,
      "loss": 0.597,
      "step": 1845900
    },
    {
      "epoch": 28.979591836734695,
      "grad_norm": 4.605310440063477,
      "learning_rate": 3.188775510204082e-05,
      "loss": 0.5944,
      "step": 1846000
    },
    {
      "epoch": 28.98116169544741,
      "grad_norm": 4.000056266784668,
      "learning_rate": 3.188677394034537e-05,
      "loss": 0.6113,
      "step": 1846100
    },
    {
      "epoch": 28.982731554160125,
      "grad_norm": 5.044538497924805,
      "learning_rate": 3.188579277864993e-05,
      "loss": 0.6166,
      "step": 1846200
    },
    {
      "epoch": 28.984301412872842,
      "grad_norm": 3.7479662895202637,
      "learning_rate": 3.188481161695447e-05,
      "loss": 0.5765,
      "step": 1846300
    },
    {
      "epoch": 28.98587127158556,
      "grad_norm": 3.4646544456481934,
      "learning_rate": 3.188383045525903e-05,
      "loss": 0.5682,
      "step": 1846400
    },
    {
      "epoch": 28.987441130298272,
      "grad_norm": 3.1708290576934814,
      "learning_rate": 3.188284929356358e-05,
      "loss": 0.5708,
      "step": 1846500
    },
    {
      "epoch": 28.98901098901099,
      "grad_norm": 3.6834404468536377,
      "learning_rate": 3.188186813186813e-05,
      "loss": 0.5942,
      "step": 1846600
    },
    {
      "epoch": 28.990580847723706,
      "grad_norm": 3.502142906188965,
      "learning_rate": 3.188088697017268e-05,
      "loss": 0.5837,
      "step": 1846700
    },
    {
      "epoch": 28.99215070643642,
      "grad_norm": 3.173794984817505,
      "learning_rate": 3.187990580847724e-05,
      "loss": 0.5765,
      "step": 1846800
    },
    {
      "epoch": 28.993720565149136,
      "grad_norm": 2.9682278633117676,
      "learning_rate": 3.187892464678179e-05,
      "loss": 0.6069,
      "step": 1846900
    },
    {
      "epoch": 28.995290423861853,
      "grad_norm": 4.462544918060303,
      "learning_rate": 3.187794348508634e-05,
      "loss": 0.5656,
      "step": 1847000
    },
    {
      "epoch": 28.99686028257457,
      "grad_norm": 3.7201383113861084,
      "learning_rate": 3.1876962323390894e-05,
      "loss": 0.6032,
      "step": 1847100
    },
    {
      "epoch": 28.998430141287283,
      "grad_norm": 6.121790409088135,
      "learning_rate": 3.187598116169545e-05,
      "loss": 0.5972,
      "step": 1847200
    },
    {
      "epoch": 29.0,
      "grad_norm": 4.179945468902588,
      "learning_rate": 3.1875e-05,
      "loss": 0.5999,
      "step": 1847300
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.026092290878296,
      "eval_runtime": 14.7085,
      "eval_samples_per_second": 227.964,
      "eval_steps_per_second": 227.964,
      "step": 1847300
    },
    {
      "epoch": 29.0,
      "eval_loss": 0.4546871483325958,
      "eval_runtime": 280.7483,
      "eval_samples_per_second": 226.894,
      "eval_steps_per_second": 226.894,
      "step": 1847300
    },
    {
      "epoch": 29.001569858712717,
      "grad_norm": 3.5030369758605957,
      "learning_rate": 3.187401883830455e-05,
      "loss": 0.585,
      "step": 1847400
    },
    {
      "epoch": 29.00313971742543,
      "grad_norm": 4.017791271209717,
      "learning_rate": 3.187303767660911e-05,
      "loss": 0.559,
      "step": 1847500
    },
    {
      "epoch": 29.004709576138147,
      "grad_norm": 4.280678749084473,
      "learning_rate": 3.187205651491366e-05,
      "loss": 0.6173,
      "step": 1847600
    },
    {
      "epoch": 29.006279434850864,
      "grad_norm": 4.523481369018555,
      "learning_rate": 3.187107535321821e-05,
      "loss": 0.574,
      "step": 1847700
    },
    {
      "epoch": 29.00784929356358,
      "grad_norm": 4.04743766784668,
      "learning_rate": 3.1870094191522764e-05,
      "loss": 0.5812,
      "step": 1847800
    },
    {
      "epoch": 29.009419152276294,
      "grad_norm": 4.716669082641602,
      "learning_rate": 3.186911302982732e-05,
      "loss": 0.5716,
      "step": 1847900
    },
    {
      "epoch": 29.01098901098901,
      "grad_norm": 3.5966196060180664,
      "learning_rate": 3.1868131868131866e-05,
      "loss": 0.588,
      "step": 1848000
    },
    {
      "epoch": 29.012558869701728,
      "grad_norm": 3.6863455772399902,
      "learning_rate": 3.1867150706436424e-05,
      "loss": 0.5332,
      "step": 1848100
    },
    {
      "epoch": 29.01412872841444,
      "grad_norm": 4.41538143157959,
      "learning_rate": 3.1866169544740975e-05,
      "loss": 0.6031,
      "step": 1848200
    },
    {
      "epoch": 29.015698587127158,
      "grad_norm": 3.4042892456054688,
      "learning_rate": 3.186518838304553e-05,
      "loss": 0.6141,
      "step": 1848300
    },
    {
      "epoch": 29.017268445839875,
      "grad_norm": 4.204775333404541,
      "learning_rate": 3.186420722135008e-05,
      "loss": 0.5877,
      "step": 1848400
    },
    {
      "epoch": 29.01883830455259,
      "grad_norm": 4.082296371459961,
      "learning_rate": 3.1863226059654634e-05,
      "loss": 0.5471,
      "step": 1848500
    },
    {
      "epoch": 29.020408163265305,
      "grad_norm": 4.199235916137695,
      "learning_rate": 3.1862244897959185e-05,
      "loss": 0.5958,
      "step": 1848600
    },
    {
      "epoch": 29.021978021978022,
      "grad_norm": 2.7852611541748047,
      "learning_rate": 3.1861263736263736e-05,
      "loss": 0.5564,
      "step": 1848700
    },
    {
      "epoch": 29.02354788069074,
      "grad_norm": 4.096183776855469,
      "learning_rate": 3.186028257456829e-05,
      "loss": 0.5818,
      "step": 1848800
    },
    {
      "epoch": 29.025117739403452,
      "grad_norm": 4.4856276512146,
      "learning_rate": 3.1859301412872845e-05,
      "loss": 0.5733,
      "step": 1848900
    },
    {
      "epoch": 29.02668759811617,
      "grad_norm": 3.8402507305145264,
      "learning_rate": 3.1858320251177396e-05,
      "loss": 0.6133,
      "step": 1849000
    },
    {
      "epoch": 29.028257456828886,
      "grad_norm": 4.3306121826171875,
      "learning_rate": 3.185733908948195e-05,
      "loss": 0.5773,
      "step": 1849100
    },
    {
      "epoch": 29.029827315541603,
      "grad_norm": 3.3004648685455322,
      "learning_rate": 3.18563579277865e-05,
      "loss": 0.6205,
      "step": 1849200
    },
    {
      "epoch": 29.031397174254316,
      "grad_norm": 4.261889934539795,
      "learning_rate": 3.1855376766091056e-05,
      "loss": 0.5909,
      "step": 1849300
    },
    {
      "epoch": 29.032967032967033,
      "grad_norm": 3.2584922313690186,
      "learning_rate": 3.185439560439561e-05,
      "loss": 0.595,
      "step": 1849400
    },
    {
      "epoch": 29.03453689167975,
      "grad_norm": 3.720769166946411,
      "learning_rate": 3.185341444270016e-05,
      "loss": 0.5881,
      "step": 1849500
    },
    {
      "epoch": 29.036106750392463,
      "grad_norm": 3.785898208618164,
      "learning_rate": 3.1852433281004716e-05,
      "loss": 0.5874,
      "step": 1849600
    },
    {
      "epoch": 29.03767660910518,
      "grad_norm": 4.68532657623291,
      "learning_rate": 3.1851452119309267e-05,
      "loss": 0.6093,
      "step": 1849700
    },
    {
      "epoch": 29.039246467817897,
      "grad_norm": 4.295747756958008,
      "learning_rate": 3.185047095761382e-05,
      "loss": 0.5753,
      "step": 1849800
    },
    {
      "epoch": 29.040816326530614,
      "grad_norm": 3.842310667037964,
      "learning_rate": 3.184948979591837e-05,
      "loss": 0.5896,
      "step": 1849900
    },
    {
      "epoch": 29.042386185243327,
      "grad_norm": 2.5573246479034424,
      "learning_rate": 3.1848508634222926e-05,
      "loss": 0.5952,
      "step": 1850000
    },
    {
      "epoch": 29.043956043956044,
      "grad_norm": 6.507669448852539,
      "learning_rate": 3.184752747252747e-05,
      "loss": 0.6313,
      "step": 1850100
    },
    {
      "epoch": 29.04552590266876,
      "grad_norm": 3.509876251220703,
      "learning_rate": 3.184654631083203e-05,
      "loss": 0.5849,
      "step": 1850200
    },
    {
      "epoch": 29.047095761381474,
      "grad_norm": 3.8700833320617676,
      "learning_rate": 3.184556514913658e-05,
      "loss": 0.5716,
      "step": 1850300
    },
    {
      "epoch": 29.04866562009419,
      "grad_norm": 3.9426352977752686,
      "learning_rate": 3.184458398744114e-05,
      "loss": 0.579,
      "step": 1850400
    },
    {
      "epoch": 29.050235478806908,
      "grad_norm": 4.05382776260376,
      "learning_rate": 3.184360282574568e-05,
      "loss": 0.6081,
      "step": 1850500
    },
    {
      "epoch": 29.051805337519625,
      "grad_norm": 3.3857998847961426,
      "learning_rate": 3.184262166405024e-05,
      "loss": 0.5577,
      "step": 1850600
    },
    {
      "epoch": 29.053375196232338,
      "grad_norm": 5.400122165679932,
      "learning_rate": 3.184164050235479e-05,
      "loss": 0.6204,
      "step": 1850700
    },
    {
      "epoch": 29.054945054945055,
      "grad_norm": 2.612089157104492,
      "learning_rate": 3.184065934065934e-05,
      "loss": 0.5909,
      "step": 1850800
    },
    {
      "epoch": 29.05651491365777,
      "grad_norm": 3.663325548171997,
      "learning_rate": 3.183967817896389e-05,
      "loss": 0.6007,
      "step": 1850900
    },
    {
      "epoch": 29.058084772370485,
      "grad_norm": 3.9966542720794678,
      "learning_rate": 3.183869701726845e-05,
      "loss": 0.6171,
      "step": 1851000
    },
    {
      "epoch": 29.059654631083202,
      "grad_norm": 3.738530158996582,
      "learning_rate": 3.1837715855573e-05,
      "loss": 0.5976,
      "step": 1851100
    },
    {
      "epoch": 29.06122448979592,
      "grad_norm": 3.1618738174438477,
      "learning_rate": 3.183673469387755e-05,
      "loss": 0.6123,
      "step": 1851200
    },
    {
      "epoch": 29.062794348508636,
      "grad_norm": 4.4935302734375,
      "learning_rate": 3.18357535321821e-05,
      "loss": 0.613,
      "step": 1851300
    },
    {
      "epoch": 29.06436420722135,
      "grad_norm": 3.340803384780884,
      "learning_rate": 3.183477237048666e-05,
      "loss": 0.5853,
      "step": 1851400
    },
    {
      "epoch": 29.065934065934066,
      "grad_norm": 2.0865280628204346,
      "learning_rate": 3.183379120879121e-05,
      "loss": 0.5777,
      "step": 1851500
    },
    {
      "epoch": 29.067503924646783,
      "grad_norm": 4.059988021850586,
      "learning_rate": 3.183281004709576e-05,
      "loss": 0.5654,
      "step": 1851600
    },
    {
      "epoch": 29.069073783359496,
      "grad_norm": 3.204077959060669,
      "learning_rate": 3.183182888540031e-05,
      "loss": 0.5749,
      "step": 1851700
    },
    {
      "epoch": 29.070643642072213,
      "grad_norm": 4.431251049041748,
      "learning_rate": 3.183084772370487e-05,
      "loss": 0.5864,
      "step": 1851800
    },
    {
      "epoch": 29.07221350078493,
      "grad_norm": 3.8367862701416016,
      "learning_rate": 3.182986656200942e-05,
      "loss": 0.5832,
      "step": 1851900
    },
    {
      "epoch": 29.073783359497646,
      "grad_norm": 3.335850477218628,
      "learning_rate": 3.182888540031397e-05,
      "loss": 0.5942,
      "step": 1852000
    },
    {
      "epoch": 29.07535321821036,
      "grad_norm": 4.521709442138672,
      "learning_rate": 3.182790423861853e-05,
      "loss": 0.6205,
      "step": 1852100
    },
    {
      "epoch": 29.076923076923077,
      "grad_norm": 4.687849521636963,
      "learning_rate": 3.1826923076923075e-05,
      "loss": 0.6319,
      "step": 1852200
    },
    {
      "epoch": 29.078492935635794,
      "grad_norm": 3.5922904014587402,
      "learning_rate": 3.182594191522763e-05,
      "loss": 0.6156,
      "step": 1852300
    },
    {
      "epoch": 29.08006279434851,
      "grad_norm": 5.0333099365234375,
      "learning_rate": 3.1824960753532184e-05,
      "loss": 0.5625,
      "step": 1852400
    },
    {
      "epoch": 29.081632653061224,
      "grad_norm": 4.056035995483398,
      "learning_rate": 3.182397959183674e-05,
      "loss": 0.6147,
      "step": 1852500
    },
    {
      "epoch": 29.08320251177394,
      "grad_norm": 4.97104024887085,
      "learning_rate": 3.1822998430141286e-05,
      "loss": 0.5477,
      "step": 1852600
    },
    {
      "epoch": 29.084772370486657,
      "grad_norm": 3.3688442707061768,
      "learning_rate": 3.182201726844584e-05,
      "loss": 0.6249,
      "step": 1852700
    },
    {
      "epoch": 29.08634222919937,
      "grad_norm": 4.51302433013916,
      "learning_rate": 3.1821036106750394e-05,
      "loss": 0.6042,
      "step": 1852800
    },
    {
      "epoch": 29.087912087912088,
      "grad_norm": 3.536492347717285,
      "learning_rate": 3.1820054945054945e-05,
      "loss": 0.5814,
      "step": 1852900
    },
    {
      "epoch": 29.089481946624804,
      "grad_norm": 3.9287307262420654,
      "learning_rate": 3.1819073783359496e-05,
      "loss": 0.5573,
      "step": 1853000
    },
    {
      "epoch": 29.09105180533752,
      "grad_norm": 3.131808042526245,
      "learning_rate": 3.1818092621664054e-05,
      "loss": 0.5964,
      "step": 1853100
    },
    {
      "epoch": 29.092621664050235,
      "grad_norm": 3.265878438949585,
      "learning_rate": 3.1817111459968605e-05,
      "loss": 0.5634,
      "step": 1853200
    },
    {
      "epoch": 29.09419152276295,
      "grad_norm": 3.9283924102783203,
      "learning_rate": 3.1816130298273156e-05,
      "loss": 0.5781,
      "step": 1853300
    },
    {
      "epoch": 29.09576138147567,
      "grad_norm": 3.4259119033813477,
      "learning_rate": 3.181514913657771e-05,
      "loss": 0.5735,
      "step": 1853400
    },
    {
      "epoch": 29.09733124018838,
      "grad_norm": 2.486783742904663,
      "learning_rate": 3.1814167974882265e-05,
      "loss": 0.5581,
      "step": 1853500
    },
    {
      "epoch": 29.0989010989011,
      "grad_norm": 4.632901668548584,
      "learning_rate": 3.1813186813186816e-05,
      "loss": 0.5681,
      "step": 1853600
    },
    {
      "epoch": 29.100470957613815,
      "grad_norm": 3.9643616676330566,
      "learning_rate": 3.181220565149137e-05,
      "loss": 0.5948,
      "step": 1853700
    },
    {
      "epoch": 29.102040816326532,
      "grad_norm": 5.037049770355225,
      "learning_rate": 3.181122448979592e-05,
      "loss": 0.5967,
      "step": 1853800
    },
    {
      "epoch": 29.103610675039246,
      "grad_norm": 4.950552940368652,
      "learning_rate": 3.1810243328100475e-05,
      "loss": 0.5589,
      "step": 1853900
    },
    {
      "epoch": 29.105180533751962,
      "grad_norm": 4.152597427368164,
      "learning_rate": 3.1809262166405026e-05,
      "loss": 0.5424,
      "step": 1854000
    },
    {
      "epoch": 29.10675039246468,
      "grad_norm": 3.4597363471984863,
      "learning_rate": 3.180828100470958e-05,
      "loss": 0.5802,
      "step": 1854100
    },
    {
      "epoch": 29.108320251177393,
      "grad_norm": 3.483452558517456,
      "learning_rate": 3.1807299843014135e-05,
      "loss": 0.6188,
      "step": 1854200
    },
    {
      "epoch": 29.10989010989011,
      "grad_norm": 2.7318646907806396,
      "learning_rate": 3.180631868131868e-05,
      "loss": 0.6033,
      "step": 1854300
    },
    {
      "epoch": 29.111459968602826,
      "grad_norm": 3.667682647705078,
      "learning_rate": 3.180533751962324e-05,
      "loss": 0.5718,
      "step": 1854400
    },
    {
      "epoch": 29.113029827315543,
      "grad_norm": 2.8882954120635986,
      "learning_rate": 3.180435635792779e-05,
      "loss": 0.5583,
      "step": 1854500
    },
    {
      "epoch": 29.114599686028257,
      "grad_norm": 2.8270161151885986,
      "learning_rate": 3.180337519623234e-05,
      "loss": 0.5971,
      "step": 1854600
    },
    {
      "epoch": 29.116169544740973,
      "grad_norm": 4.549374580383301,
      "learning_rate": 3.180239403453689e-05,
      "loss": 0.5454,
      "step": 1854700
    },
    {
      "epoch": 29.11773940345369,
      "grad_norm": 3.7931642532348633,
      "learning_rate": 3.180141287284145e-05,
      "loss": 0.5971,
      "step": 1854800
    },
    {
      "epoch": 29.119309262166404,
      "grad_norm": 4.163913726806641,
      "learning_rate": 3.1800431711146e-05,
      "loss": 0.5848,
      "step": 1854900
    },
    {
      "epoch": 29.12087912087912,
      "grad_norm": 4.298576831817627,
      "learning_rate": 3.179945054945055e-05,
      "loss": 0.5676,
      "step": 1855000
    },
    {
      "epoch": 29.122448979591837,
      "grad_norm": 3.6029906272888184,
      "learning_rate": 3.17984693877551e-05,
      "loss": 0.6145,
      "step": 1855100
    },
    {
      "epoch": 29.124018838304554,
      "grad_norm": 4.199278831481934,
      "learning_rate": 3.179748822605966e-05,
      "loss": 0.5803,
      "step": 1855200
    },
    {
      "epoch": 29.125588697017267,
      "grad_norm": 4.459397792816162,
      "learning_rate": 3.17965070643642e-05,
      "loss": 0.6053,
      "step": 1855300
    },
    {
      "epoch": 29.127158555729984,
      "grad_norm": 4.433432102203369,
      "learning_rate": 3.179552590266876e-05,
      "loss": 0.5866,
      "step": 1855400
    },
    {
      "epoch": 29.1287284144427,
      "grad_norm": 2.6711409091949463,
      "learning_rate": 3.179454474097331e-05,
      "loss": 0.5923,
      "step": 1855500
    },
    {
      "epoch": 29.130298273155415,
      "grad_norm": 3.5178983211517334,
      "learning_rate": 3.179356357927787e-05,
      "loss": 0.6122,
      "step": 1855600
    },
    {
      "epoch": 29.13186813186813,
      "grad_norm": 3.935274839401245,
      "learning_rate": 3.179258241758242e-05,
      "loss": 0.5792,
      "step": 1855700
    },
    {
      "epoch": 29.13343799058085,
      "grad_norm": 3.9634761810302734,
      "learning_rate": 3.179160125588697e-05,
      "loss": 0.56,
      "step": 1855800
    },
    {
      "epoch": 29.135007849293565,
      "grad_norm": 3.5917985439300537,
      "learning_rate": 3.179062009419152e-05,
      "loss": 0.5974,
      "step": 1855900
    },
    {
      "epoch": 29.13657770800628,
      "grad_norm": 3.8871254920959473,
      "learning_rate": 3.178963893249607e-05,
      "loss": 0.5918,
      "step": 1856000
    },
    {
      "epoch": 29.138147566718995,
      "grad_norm": 4.676827430725098,
      "learning_rate": 3.178865777080063e-05,
      "loss": 0.6004,
      "step": 1856100
    },
    {
      "epoch": 29.139717425431712,
      "grad_norm": 3.5165276527404785,
      "learning_rate": 3.178767660910518e-05,
      "loss": 0.5758,
      "step": 1856200
    },
    {
      "epoch": 29.141287284144425,
      "grad_norm": 5.041108131408691,
      "learning_rate": 3.178669544740974e-05,
      "loss": 0.605,
      "step": 1856300
    },
    {
      "epoch": 29.142857142857142,
      "grad_norm": 4.290491104125977,
      "learning_rate": 3.1785714285714284e-05,
      "loss": 0.6018,
      "step": 1856400
    },
    {
      "epoch": 29.14442700156986,
      "grad_norm": 3.715707540512085,
      "learning_rate": 3.178473312401884e-05,
      "loss": 0.5837,
      "step": 1856500
    },
    {
      "epoch": 29.145996860282576,
      "grad_norm": 1.9548825025558472,
      "learning_rate": 3.178375196232339e-05,
      "loss": 0.5445,
      "step": 1856600
    },
    {
      "epoch": 29.14756671899529,
      "grad_norm": 3.1849076747894287,
      "learning_rate": 3.1782770800627944e-05,
      "loss": 0.5997,
      "step": 1856700
    },
    {
      "epoch": 29.149136577708006,
      "grad_norm": 6.241024017333984,
      "learning_rate": 3.1781789638932495e-05,
      "loss": 0.5662,
      "step": 1856800
    },
    {
      "epoch": 29.150706436420723,
      "grad_norm": 3.8153235912323,
      "learning_rate": 3.178080847723705e-05,
      "loss": 0.6078,
      "step": 1856900
    },
    {
      "epoch": 29.152276295133436,
      "grad_norm": 3.700448513031006,
      "learning_rate": 3.17798273155416e-05,
      "loss": 0.6054,
      "step": 1857000
    },
    {
      "epoch": 29.153846153846153,
      "grad_norm": 2.9911515712738037,
      "learning_rate": 3.1778846153846154e-05,
      "loss": 0.6203,
      "step": 1857100
    },
    {
      "epoch": 29.15541601255887,
      "grad_norm": 3.46061372756958,
      "learning_rate": 3.1777864992150705e-05,
      "loss": 0.5804,
      "step": 1857200
    },
    {
      "epoch": 29.156985871271587,
      "grad_norm": 3.816132068634033,
      "learning_rate": 3.177688383045526e-05,
      "loss": 0.6214,
      "step": 1857300
    },
    {
      "epoch": 29.1585557299843,
      "grad_norm": 3.329293966293335,
      "learning_rate": 3.177590266875981e-05,
      "loss": 0.5832,
      "step": 1857400
    },
    {
      "epoch": 29.160125588697017,
      "grad_norm": 4.441804885864258,
      "learning_rate": 3.1774921507064365e-05,
      "loss": 0.6129,
      "step": 1857500
    },
    {
      "epoch": 29.161695447409734,
      "grad_norm": 3.5318007469177246,
      "learning_rate": 3.1773940345368916e-05,
      "loss": 0.5629,
      "step": 1857600
    },
    {
      "epoch": 29.163265306122447,
      "grad_norm": 3.1427218914031982,
      "learning_rate": 3.1772959183673474e-05,
      "loss": 0.5969,
      "step": 1857700
    },
    {
      "epoch": 29.164835164835164,
      "grad_norm": 3.95662784576416,
      "learning_rate": 3.1771978021978025e-05,
      "loss": 0.595,
      "step": 1857800
    },
    {
      "epoch": 29.16640502354788,
      "grad_norm": 3.2857089042663574,
      "learning_rate": 3.1770996860282576e-05,
      "loss": 0.5768,
      "step": 1857900
    },
    {
      "epoch": 29.167974882260598,
      "grad_norm": 4.558342933654785,
      "learning_rate": 3.177001569858713e-05,
      "loss": 0.5865,
      "step": 1858000
    },
    {
      "epoch": 29.16954474097331,
      "grad_norm": 3.4883501529693604,
      "learning_rate": 3.176903453689168e-05,
      "loss": 0.6096,
      "step": 1858100
    },
    {
      "epoch": 29.171114599686028,
      "grad_norm": 2.7320284843444824,
      "learning_rate": 3.1768053375196235e-05,
      "loss": 0.5886,
      "step": 1858200
    },
    {
      "epoch": 29.172684458398745,
      "grad_norm": 2.461580514907837,
      "learning_rate": 3.1767072213500786e-05,
      "loss": 0.5694,
      "step": 1858300
    },
    {
      "epoch": 29.17425431711146,
      "grad_norm": 3.9512717723846436,
      "learning_rate": 3.1766091051805344e-05,
      "loss": 0.6034,
      "step": 1858400
    },
    {
      "epoch": 29.175824175824175,
      "grad_norm": 3.981703042984009,
      "learning_rate": 3.176510989010989e-05,
      "loss": 0.5694,
      "step": 1858500
    },
    {
      "epoch": 29.177394034536892,
      "grad_norm": 3.291213035583496,
      "learning_rate": 3.1764128728414446e-05,
      "loss": 0.5896,
      "step": 1858600
    },
    {
      "epoch": 29.17896389324961,
      "grad_norm": 4.963615894317627,
      "learning_rate": 3.1763147566719e-05,
      "loss": 0.5809,
      "step": 1858700
    },
    {
      "epoch": 29.180533751962322,
      "grad_norm": 2.8054864406585693,
      "learning_rate": 3.176216640502355e-05,
      "loss": 0.5757,
      "step": 1858800
    },
    {
      "epoch": 29.18210361067504,
      "grad_norm": 3.3219480514526367,
      "learning_rate": 3.17611852433281e-05,
      "loss": 0.5907,
      "step": 1858900
    },
    {
      "epoch": 29.183673469387756,
      "grad_norm": 4.581153392791748,
      "learning_rate": 3.176020408163266e-05,
      "loss": 0.602,
      "step": 1859000
    },
    {
      "epoch": 29.18524332810047,
      "grad_norm": 2.1036036014556885,
      "learning_rate": 3.175922291993721e-05,
      "loss": 0.6005,
      "step": 1859100
    },
    {
      "epoch": 29.186813186813186,
      "grad_norm": 3.923269271850586,
      "learning_rate": 3.175824175824176e-05,
      "loss": 0.5871,
      "step": 1859200
    },
    {
      "epoch": 29.188383045525903,
      "grad_norm": 4.192721366882324,
      "learning_rate": 3.175726059654631e-05,
      "loss": 0.5961,
      "step": 1859300
    },
    {
      "epoch": 29.18995290423862,
      "grad_norm": 4.857607364654541,
      "learning_rate": 3.175627943485087e-05,
      "loss": 0.6139,
      "step": 1859400
    },
    {
      "epoch": 29.191522762951333,
      "grad_norm": 4.688917636871338,
      "learning_rate": 3.175529827315541e-05,
      "loss": 0.5747,
      "step": 1859500
    },
    {
      "epoch": 29.19309262166405,
      "grad_norm": 3.620753049850464,
      "learning_rate": 3.175431711145997e-05,
      "loss": 0.6076,
      "step": 1859600
    },
    {
      "epoch": 29.194662480376767,
      "grad_norm": 4.907175064086914,
      "learning_rate": 3.175333594976452e-05,
      "loss": 0.6375,
      "step": 1859700
    },
    {
      "epoch": 29.19623233908948,
      "grad_norm": 3.6503255367279053,
      "learning_rate": 3.175235478806908e-05,
      "loss": 0.5472,
      "step": 1859800
    },
    {
      "epoch": 29.197802197802197,
      "grad_norm": 3.820988178253174,
      "learning_rate": 3.175137362637363e-05,
      "loss": 0.6062,
      "step": 1859900
    },
    {
      "epoch": 29.199372056514914,
      "grad_norm": 3.8498988151550293,
      "learning_rate": 3.175039246467818e-05,
      "loss": 0.6214,
      "step": 1860000
    },
    {
      "epoch": 29.20094191522763,
      "grad_norm": 4.7905473709106445,
      "learning_rate": 3.174941130298273e-05,
      "loss": 0.5959,
      "step": 1860100
    },
    {
      "epoch": 29.202511773940344,
      "grad_norm": 7.070477485656738,
      "learning_rate": 3.174843014128728e-05,
      "loss": 0.5904,
      "step": 1860200
    },
    {
      "epoch": 29.20408163265306,
      "grad_norm": 1.8786879777908325,
      "learning_rate": 3.174744897959184e-05,
      "loss": 0.5908,
      "step": 1860300
    },
    {
      "epoch": 29.205651491365778,
      "grad_norm": 3.5690581798553467,
      "learning_rate": 3.174646781789639e-05,
      "loss": 0.5714,
      "step": 1860400
    },
    {
      "epoch": 29.20722135007849,
      "grad_norm": 4.706639766693115,
      "learning_rate": 3.174548665620095e-05,
      "loss": 0.6067,
      "step": 1860500
    },
    {
      "epoch": 29.208791208791208,
      "grad_norm": 4.206488132476807,
      "learning_rate": 3.174450549450549e-05,
      "loss": 0.5974,
      "step": 1860600
    },
    {
      "epoch": 29.210361067503925,
      "grad_norm": 3.2825098037719727,
      "learning_rate": 3.174352433281005e-05,
      "loss": 0.5713,
      "step": 1860700
    },
    {
      "epoch": 29.211930926216642,
      "grad_norm": 3.5509226322174072,
      "learning_rate": 3.17425431711146e-05,
      "loss": 0.6089,
      "step": 1860800
    },
    {
      "epoch": 29.213500784929355,
      "grad_norm": 4.290117263793945,
      "learning_rate": 3.174156200941915e-05,
      "loss": 0.6329,
      "step": 1860900
    },
    {
      "epoch": 29.215070643642072,
      "grad_norm": 3.8417038917541504,
      "learning_rate": 3.1740580847723704e-05,
      "loss": 0.5994,
      "step": 1861000
    },
    {
      "epoch": 29.21664050235479,
      "grad_norm": 3.511605739593506,
      "learning_rate": 3.173959968602826e-05,
      "loss": 0.5644,
      "step": 1861100
    },
    {
      "epoch": 29.218210361067506,
      "grad_norm": 3.5344278812408447,
      "learning_rate": 3.173861852433281e-05,
      "loss": 0.6022,
      "step": 1861200
    },
    {
      "epoch": 29.21978021978022,
      "grad_norm": 3.743429183959961,
      "learning_rate": 3.173763736263736e-05,
      "loss": 0.636,
      "step": 1861300
    },
    {
      "epoch": 29.221350078492936,
      "grad_norm": 3.8236029148101807,
      "learning_rate": 3.1736656200941914e-05,
      "loss": 0.6121,
      "step": 1861400
    },
    {
      "epoch": 29.222919937205653,
      "grad_norm": 4.455836296081543,
      "learning_rate": 3.173567503924647e-05,
      "loss": 0.6181,
      "step": 1861500
    },
    {
      "epoch": 29.224489795918366,
      "grad_norm": 4.091716766357422,
      "learning_rate": 3.1734693877551016e-05,
      "loss": 0.6194,
      "step": 1861600
    },
    {
      "epoch": 29.226059654631083,
      "grad_norm": 4.201257705688477,
      "learning_rate": 3.1733712715855574e-05,
      "loss": 0.6008,
      "step": 1861700
    },
    {
      "epoch": 29.2276295133438,
      "grad_norm": 5.101346969604492,
      "learning_rate": 3.1732731554160125e-05,
      "loss": 0.5948,
      "step": 1861800
    },
    {
      "epoch": 29.229199372056517,
      "grad_norm": 4.128133773803711,
      "learning_rate": 3.173175039246468e-05,
      "loss": 0.569,
      "step": 1861900
    },
    {
      "epoch": 29.23076923076923,
      "grad_norm": 4.393538951873779,
      "learning_rate": 3.1730769230769234e-05,
      "loss": 0.6036,
      "step": 1862000
    },
    {
      "epoch": 29.232339089481947,
      "grad_norm": 3.8238141536712646,
      "learning_rate": 3.1729788069073785e-05,
      "loss": 0.5834,
      "step": 1862100
    },
    {
      "epoch": 29.233908948194664,
      "grad_norm": 3.690995693206787,
      "learning_rate": 3.1728806907378336e-05,
      "loss": 0.6002,
      "step": 1862200
    },
    {
      "epoch": 29.235478806907377,
      "grad_norm": 3.0692906379699707,
      "learning_rate": 3.172782574568289e-05,
      "loss": 0.5755,
      "step": 1862300
    },
    {
      "epoch": 29.237048665620094,
      "grad_norm": 4.152098655700684,
      "learning_rate": 3.1726844583987444e-05,
      "loss": 0.6463,
      "step": 1862400
    },
    {
      "epoch": 29.23861852433281,
      "grad_norm": 4.088183879852295,
      "learning_rate": 3.1725863422291995e-05,
      "loss": 0.5962,
      "step": 1862500
    },
    {
      "epoch": 29.240188383045528,
      "grad_norm": 2.6390960216522217,
      "learning_rate": 3.172488226059655e-05,
      "loss": 0.5361,
      "step": 1862600
    },
    {
      "epoch": 29.24175824175824,
      "grad_norm": 3.0902392864227295,
      "learning_rate": 3.17239010989011e-05,
      "loss": 0.5466,
      "step": 1862700
    },
    {
      "epoch": 29.243328100470958,
      "grad_norm": 4.293541431427002,
      "learning_rate": 3.1722919937205655e-05,
      "loss": 0.5831,
      "step": 1862800
    },
    {
      "epoch": 29.244897959183675,
      "grad_norm": 3.054828405380249,
      "learning_rate": 3.1721938775510206e-05,
      "loss": 0.5659,
      "step": 1862900
    },
    {
      "epoch": 29.246467817896388,
      "grad_norm": 3.1671712398529053,
      "learning_rate": 3.172095761381476e-05,
      "loss": 0.5972,
      "step": 1863000
    },
    {
      "epoch": 29.248037676609105,
      "grad_norm": 4.23480749130249,
      "learning_rate": 3.171997645211931e-05,
      "loss": 0.5631,
      "step": 1863100
    },
    {
      "epoch": 29.24960753532182,
      "grad_norm": 3.8309590816497803,
      "learning_rate": 3.1718995290423866e-05,
      "loss": 0.6024,
      "step": 1863200
    },
    {
      "epoch": 29.25117739403454,
      "grad_norm": 3.1293182373046875,
      "learning_rate": 3.171801412872842e-05,
      "loss": 0.6141,
      "step": 1863300
    },
    {
      "epoch": 29.252747252747252,
      "grad_norm": 4.224893569946289,
      "learning_rate": 3.171703296703297e-05,
      "loss": 0.6002,
      "step": 1863400
    },
    {
      "epoch": 29.25431711145997,
      "grad_norm": 5.282728672027588,
      "learning_rate": 3.171605180533752e-05,
      "loss": 0.5934,
      "step": 1863500
    },
    {
      "epoch": 29.255886970172686,
      "grad_norm": 3.7534778118133545,
      "learning_rate": 3.1715070643642077e-05,
      "loss": 0.5725,
      "step": 1863600
    },
    {
      "epoch": 29.2574568288854,
      "grad_norm": 5.49497652053833,
      "learning_rate": 3.171408948194662e-05,
      "loss": 0.5885,
      "step": 1863700
    },
    {
      "epoch": 29.259026687598116,
      "grad_norm": 3.7055318355560303,
      "learning_rate": 3.171310832025118e-05,
      "loss": 0.586,
      "step": 1863800
    },
    {
      "epoch": 29.260596546310833,
      "grad_norm": 3.6566379070281982,
      "learning_rate": 3.171212715855573e-05,
      "loss": 0.5803,
      "step": 1863900
    },
    {
      "epoch": 29.26216640502355,
      "grad_norm": 2.9697115421295166,
      "learning_rate": 3.171114599686029e-05,
      "loss": 0.5745,
      "step": 1864000
    },
    {
      "epoch": 29.263736263736263,
      "grad_norm": 3.6956164836883545,
      "learning_rate": 3.171016483516484e-05,
      "loss": 0.5797,
      "step": 1864100
    },
    {
      "epoch": 29.26530612244898,
      "grad_norm": 3.9858803749084473,
      "learning_rate": 3.170918367346939e-05,
      "loss": 0.5599,
      "step": 1864200
    },
    {
      "epoch": 29.266875981161697,
      "grad_norm": 4.322256565093994,
      "learning_rate": 3.170820251177394e-05,
      "loss": 0.5497,
      "step": 1864300
    },
    {
      "epoch": 29.26844583987441,
      "grad_norm": 3.4532413482666016,
      "learning_rate": 3.170722135007849e-05,
      "loss": 0.5806,
      "step": 1864400
    },
    {
      "epoch": 29.270015698587127,
      "grad_norm": 4.791813850402832,
      "learning_rate": 3.170624018838305e-05,
      "loss": 0.5609,
      "step": 1864500
    },
    {
      "epoch": 29.271585557299844,
      "grad_norm": 4.369513988494873,
      "learning_rate": 3.17052590266876e-05,
      "loss": 0.5686,
      "step": 1864600
    },
    {
      "epoch": 29.27315541601256,
      "grad_norm": 5.15263557434082,
      "learning_rate": 3.170427786499216e-05,
      "loss": 0.5873,
      "step": 1864700
    },
    {
      "epoch": 29.274725274725274,
      "grad_norm": 3.8676629066467285,
      "learning_rate": 3.17032967032967e-05,
      "loss": 0.5335,
      "step": 1864800
    },
    {
      "epoch": 29.27629513343799,
      "grad_norm": 3.747727632522583,
      "learning_rate": 3.170231554160126e-05,
      "loss": 0.6426,
      "step": 1864900
    },
    {
      "epoch": 29.277864992150707,
      "grad_norm": 5.079808712005615,
      "learning_rate": 3.170133437990581e-05,
      "loss": 0.602,
      "step": 1865000
    },
    {
      "epoch": 29.27943485086342,
      "grad_norm": 3.991643190383911,
      "learning_rate": 3.170035321821036e-05,
      "loss": 0.6426,
      "step": 1865100
    },
    {
      "epoch": 29.281004709576138,
      "grad_norm": 3.23395037651062,
      "learning_rate": 3.169937205651491e-05,
      "loss": 0.615,
      "step": 1865200
    },
    {
      "epoch": 29.282574568288855,
      "grad_norm": 3.1835765838623047,
      "learning_rate": 3.169839089481947e-05,
      "loss": 0.5615,
      "step": 1865300
    },
    {
      "epoch": 29.28414442700157,
      "grad_norm": 4.264037132263184,
      "learning_rate": 3.169740973312402e-05,
      "loss": 0.5674,
      "step": 1865400
    },
    {
      "epoch": 29.285714285714285,
      "grad_norm": 3.9468414783477783,
      "learning_rate": 3.169642857142857e-05,
      "loss": 0.6119,
      "step": 1865500
    },
    {
      "epoch": 29.287284144427,
      "grad_norm": 3.924072742462158,
      "learning_rate": 3.169544740973312e-05,
      "loss": 0.632,
      "step": 1865600
    },
    {
      "epoch": 29.28885400313972,
      "grad_norm": 4.413390636444092,
      "learning_rate": 3.169446624803768e-05,
      "loss": 0.6011,
      "step": 1865700
    },
    {
      "epoch": 29.29042386185243,
      "grad_norm": 4.697136402130127,
      "learning_rate": 3.1693485086342225e-05,
      "loss": 0.6355,
      "step": 1865800
    },
    {
      "epoch": 29.29199372056515,
      "grad_norm": 3.7795443534851074,
      "learning_rate": 3.169250392464678e-05,
      "loss": 0.615,
      "step": 1865900
    },
    {
      "epoch": 29.293563579277865,
      "grad_norm": 3.924487829208374,
      "learning_rate": 3.1691522762951334e-05,
      "loss": 0.5977,
      "step": 1866000
    },
    {
      "epoch": 29.295133437990582,
      "grad_norm": 3.15313982963562,
      "learning_rate": 3.169054160125589e-05,
      "loss": 0.5743,
      "step": 1866100
    },
    {
      "epoch": 29.296703296703296,
      "grad_norm": 3.8328733444213867,
      "learning_rate": 3.168956043956044e-05,
      "loss": 0.6088,
      "step": 1866200
    },
    {
      "epoch": 29.298273155416013,
      "grad_norm": 4.110700607299805,
      "learning_rate": 3.1688579277864994e-05,
      "loss": 0.6161,
      "step": 1866300
    },
    {
      "epoch": 29.29984301412873,
      "grad_norm": 3.5691044330596924,
      "learning_rate": 3.1687598116169545e-05,
      "loss": 0.5814,
      "step": 1866400
    },
    {
      "epoch": 29.301412872841443,
      "grad_norm": 2.9332497119903564,
      "learning_rate": 3.1686616954474096e-05,
      "loss": 0.6268,
      "step": 1866500
    },
    {
      "epoch": 29.30298273155416,
      "grad_norm": 3.9141759872436523,
      "learning_rate": 3.1685635792778653e-05,
      "loss": 0.5732,
      "step": 1866600
    },
    {
      "epoch": 29.304552590266876,
      "grad_norm": 2.679065704345703,
      "learning_rate": 3.1684654631083204e-05,
      "loss": 0.6036,
      "step": 1866700
    },
    {
      "epoch": 29.306122448979593,
      "grad_norm": 4.092737674713135,
      "learning_rate": 3.168367346938776e-05,
      "loss": 0.5988,
      "step": 1866800
    },
    {
      "epoch": 29.307692307692307,
      "grad_norm": 4.809489727020264,
      "learning_rate": 3.1682692307692306e-05,
      "loss": 0.5687,
      "step": 1866900
    },
    {
      "epoch": 29.309262166405023,
      "grad_norm": 4.4724040031433105,
      "learning_rate": 3.1681711145996864e-05,
      "loss": 0.6021,
      "step": 1867000
    },
    {
      "epoch": 29.31083202511774,
      "grad_norm": 3.2562620639801025,
      "learning_rate": 3.1680729984301415e-05,
      "loss": 0.614,
      "step": 1867100
    },
    {
      "epoch": 29.312401883830454,
      "grad_norm": 3.169154405593872,
      "learning_rate": 3.1679748822605966e-05,
      "loss": 0.6018,
      "step": 1867200
    },
    {
      "epoch": 29.31397174254317,
      "grad_norm": 3.3267691135406494,
      "learning_rate": 3.167876766091052e-05,
      "loss": 0.5756,
      "step": 1867300
    },
    {
      "epoch": 29.315541601255887,
      "grad_norm": 3.6887896060943604,
      "learning_rate": 3.1677786499215075e-05,
      "loss": 0.6196,
      "step": 1867400
    },
    {
      "epoch": 29.317111459968604,
      "grad_norm": 3.478832483291626,
      "learning_rate": 3.1676805337519626e-05,
      "loss": 0.6188,
      "step": 1867500
    },
    {
      "epoch": 29.318681318681318,
      "grad_norm": 2.9249894618988037,
      "learning_rate": 3.167582417582418e-05,
      "loss": 0.5756,
      "step": 1867600
    },
    {
      "epoch": 29.320251177394034,
      "grad_norm": 4.366734504699707,
      "learning_rate": 3.167484301412873e-05,
      "loss": 0.5771,
      "step": 1867700
    },
    {
      "epoch": 29.32182103610675,
      "grad_norm": 3.1667635440826416,
      "learning_rate": 3.1673861852433286e-05,
      "loss": 0.5595,
      "step": 1867800
    },
    {
      "epoch": 29.323390894819465,
      "grad_norm": 3.582972526550293,
      "learning_rate": 3.167288069073783e-05,
      "loss": 0.5964,
      "step": 1867900
    },
    {
      "epoch": 29.32496075353218,
      "grad_norm": 2.8367927074432373,
      "learning_rate": 3.167189952904239e-05,
      "loss": 0.5989,
      "step": 1868000
    },
    {
      "epoch": 29.3265306122449,
      "grad_norm": 4.427872180938721,
      "learning_rate": 3.167091836734694e-05,
      "loss": 0.5623,
      "step": 1868100
    },
    {
      "epoch": 29.328100470957615,
      "grad_norm": 4.43759298324585,
      "learning_rate": 3.1669937205651496e-05,
      "loss": 0.5678,
      "step": 1868200
    },
    {
      "epoch": 29.32967032967033,
      "grad_norm": 4.163214683532715,
      "learning_rate": 3.166895604395605e-05,
      "loss": 0.566,
      "step": 1868300
    },
    {
      "epoch": 29.331240188383045,
      "grad_norm": 3.639106512069702,
      "learning_rate": 3.16679748822606e-05,
      "loss": 0.5654,
      "step": 1868400
    },
    {
      "epoch": 29.332810047095762,
      "grad_norm": 4.648726463317871,
      "learning_rate": 3.166699372056515e-05,
      "loss": 0.5742,
      "step": 1868500
    },
    {
      "epoch": 29.334379905808476,
      "grad_norm": 3.9595413208007812,
      "learning_rate": 3.16660125588697e-05,
      "loss": 0.5599,
      "step": 1868600
    },
    {
      "epoch": 29.335949764521192,
      "grad_norm": 4.963411331176758,
      "learning_rate": 3.166503139717426e-05,
      "loss": 0.6163,
      "step": 1868700
    },
    {
      "epoch": 29.33751962323391,
      "grad_norm": 4.620631694793701,
      "learning_rate": 3.166405023547881e-05,
      "loss": 0.5969,
      "step": 1868800
    },
    {
      "epoch": 29.339089481946626,
      "grad_norm": 3.4874205589294434,
      "learning_rate": 3.166306907378337e-05,
      "loss": 0.5699,
      "step": 1868900
    },
    {
      "epoch": 29.34065934065934,
      "grad_norm": 2.912018060684204,
      "learning_rate": 3.166208791208791e-05,
      "loss": 0.5744,
      "step": 1869000
    },
    {
      "epoch": 29.342229199372056,
      "grad_norm": 4.211901664733887,
      "learning_rate": 3.166110675039247e-05,
      "loss": 0.598,
      "step": 1869100
    },
    {
      "epoch": 29.343799058084773,
      "grad_norm": 3.2428455352783203,
      "learning_rate": 3.166012558869702e-05,
      "loss": 0.5993,
      "step": 1869200
    },
    {
      "epoch": 29.345368916797486,
      "grad_norm": 3.07438063621521,
      "learning_rate": 3.165914442700157e-05,
      "loss": 0.599,
      "step": 1869300
    },
    {
      "epoch": 29.346938775510203,
      "grad_norm": 4.090540885925293,
      "learning_rate": 3.165816326530612e-05,
      "loss": 0.5627,
      "step": 1869400
    },
    {
      "epoch": 29.34850863422292,
      "grad_norm": 4.1646904945373535,
      "learning_rate": 3.165718210361068e-05,
      "loss": 0.5929,
      "step": 1869500
    },
    {
      "epoch": 29.350078492935637,
      "grad_norm": 3.8419032096862793,
      "learning_rate": 3.165620094191523e-05,
      "loss": 0.6089,
      "step": 1869600
    },
    {
      "epoch": 29.35164835164835,
      "grad_norm": 3.7612318992614746,
      "learning_rate": 3.165521978021978e-05,
      "loss": 0.5856,
      "step": 1869700
    },
    {
      "epoch": 29.353218210361067,
      "grad_norm": 4.2725114822387695,
      "learning_rate": 3.165423861852433e-05,
      "loss": 0.6013,
      "step": 1869800
    },
    {
      "epoch": 29.354788069073784,
      "grad_norm": 3.536644458770752,
      "learning_rate": 3.165325745682889e-05,
      "loss": 0.5883,
      "step": 1869900
    },
    {
      "epoch": 29.356357927786497,
      "grad_norm": 4.230154991149902,
      "learning_rate": 3.1652276295133434e-05,
      "loss": 0.5816,
      "step": 1870000
    },
    {
      "epoch": 29.357927786499214,
      "grad_norm": 3.7508790493011475,
      "learning_rate": 3.165129513343799e-05,
      "loss": 0.5685,
      "step": 1870100
    },
    {
      "epoch": 29.35949764521193,
      "grad_norm": 3.325137138366699,
      "learning_rate": 3.165031397174254e-05,
      "loss": 0.6182,
      "step": 1870200
    },
    {
      "epoch": 29.361067503924648,
      "grad_norm": 4.4294209480285645,
      "learning_rate": 3.16493328100471e-05,
      "loss": 0.5764,
      "step": 1870300
    },
    {
      "epoch": 29.36263736263736,
      "grad_norm": 3.580949544906616,
      "learning_rate": 3.164835164835165e-05,
      "loss": 0.5895,
      "step": 1870400
    },
    {
      "epoch": 29.364207221350078,
      "grad_norm": 4.138782501220703,
      "learning_rate": 3.16473704866562e-05,
      "loss": 0.6048,
      "step": 1870500
    },
    {
      "epoch": 29.365777080062795,
      "grad_norm": 3.8409922122955322,
      "learning_rate": 3.1646389324960754e-05,
      "loss": 0.6447,
      "step": 1870600
    },
    {
      "epoch": 29.367346938775512,
      "grad_norm": 4.242597579956055,
      "learning_rate": 3.1645408163265305e-05,
      "loss": 0.5807,
      "step": 1870700
    },
    {
      "epoch": 29.368916797488225,
      "grad_norm": 3.313502788543701,
      "learning_rate": 3.164442700156986e-05,
      "loss": 0.5971,
      "step": 1870800
    },
    {
      "epoch": 29.370486656200942,
      "grad_norm": 3.723820447921753,
      "learning_rate": 3.164344583987441e-05,
      "loss": 0.6306,
      "step": 1870900
    },
    {
      "epoch": 29.37205651491366,
      "grad_norm": 5.104321479797363,
      "learning_rate": 3.164246467817897e-05,
      "loss": 0.6385,
      "step": 1871000
    },
    {
      "epoch": 29.373626373626372,
      "grad_norm": 3.035245895385742,
      "learning_rate": 3.1641483516483515e-05,
      "loss": 0.6074,
      "step": 1871100
    },
    {
      "epoch": 29.37519623233909,
      "grad_norm": 2.9286928176879883,
      "learning_rate": 3.164050235478807e-05,
      "loss": 0.6099,
      "step": 1871200
    },
    {
      "epoch": 29.376766091051806,
      "grad_norm": 3.4960944652557373,
      "learning_rate": 3.1639521193092624e-05,
      "loss": 0.599,
      "step": 1871300
    },
    {
      "epoch": 29.378335949764523,
      "grad_norm": 3.1950128078460693,
      "learning_rate": 3.1638540031397175e-05,
      "loss": 0.5507,
      "step": 1871400
    },
    {
      "epoch": 29.379905808477236,
      "grad_norm": 4.759150505065918,
      "learning_rate": 3.1637558869701726e-05,
      "loss": 0.5509,
      "step": 1871500
    },
    {
      "epoch": 29.381475667189953,
      "grad_norm": 3.3047983646392822,
      "learning_rate": 3.1636577708006284e-05,
      "loss": 0.5641,
      "step": 1871600
    },
    {
      "epoch": 29.38304552590267,
      "grad_norm": 4.700138568878174,
      "learning_rate": 3.1635596546310835e-05,
      "loss": 0.5907,
      "step": 1871700
    },
    {
      "epoch": 29.384615384615383,
      "grad_norm": 3.62768816947937,
      "learning_rate": 3.1634615384615386e-05,
      "loss": 0.5885,
      "step": 1871800
    },
    {
      "epoch": 29.3861852433281,
      "grad_norm": 4.551136016845703,
      "learning_rate": 3.163363422291994e-05,
      "loss": 0.5887,
      "step": 1871900
    },
    {
      "epoch": 29.387755102040817,
      "grad_norm": 3.49916672706604,
      "learning_rate": 3.1632653061224494e-05,
      "loss": 0.6058,
      "step": 1872000
    },
    {
      "epoch": 29.389324960753534,
      "grad_norm": 4.257496356964111,
      "learning_rate": 3.163167189952904e-05,
      "loss": 0.5923,
      "step": 1872100
    },
    {
      "epoch": 29.390894819466247,
      "grad_norm": 5.156051158905029,
      "learning_rate": 3.1630690737833596e-05,
      "loss": 0.6106,
      "step": 1872200
    },
    {
      "epoch": 29.392464678178964,
      "grad_norm": 3.8517091274261475,
      "learning_rate": 3.162970957613815e-05,
      "loss": 0.5811,
      "step": 1872300
    },
    {
      "epoch": 29.39403453689168,
      "grad_norm": 4.686392307281494,
      "learning_rate": 3.1628728414442705e-05,
      "loss": 0.6342,
      "step": 1872400
    },
    {
      "epoch": 29.395604395604394,
      "grad_norm": 3.4039909839630127,
      "learning_rate": 3.1627747252747256e-05,
      "loss": 0.5955,
      "step": 1872500
    },
    {
      "epoch": 29.39717425431711,
      "grad_norm": 2.7302286624908447,
      "learning_rate": 3.162676609105181e-05,
      "loss": 0.6115,
      "step": 1872600
    },
    {
      "epoch": 29.398744113029828,
      "grad_norm": 4.010315895080566,
      "learning_rate": 3.162578492935636e-05,
      "loss": 0.5898,
      "step": 1872700
    },
    {
      "epoch": 29.400313971742545,
      "grad_norm": 3.262869119644165,
      "learning_rate": 3.162480376766091e-05,
      "loss": 0.5735,
      "step": 1872800
    },
    {
      "epoch": 29.401883830455258,
      "grad_norm": 4.150051116943359,
      "learning_rate": 3.162382260596547e-05,
      "loss": 0.5854,
      "step": 1872900
    },
    {
      "epoch": 29.403453689167975,
      "grad_norm": 4.480041027069092,
      "learning_rate": 3.162284144427002e-05,
      "loss": 0.5848,
      "step": 1873000
    },
    {
      "epoch": 29.405023547880692,
      "grad_norm": 3.347017288208008,
      "learning_rate": 3.1621860282574576e-05,
      "loss": 0.5643,
      "step": 1873100
    },
    {
      "epoch": 29.406593406593405,
      "grad_norm": 4.066739559173584,
      "learning_rate": 3.162087912087912e-05,
      "loss": 0.5851,
      "step": 1873200
    },
    {
      "epoch": 29.408163265306122,
      "grad_norm": 4.090149879455566,
      "learning_rate": 3.161989795918368e-05,
      "loss": 0.6471,
      "step": 1873300
    },
    {
      "epoch": 29.40973312401884,
      "grad_norm": 4.436805725097656,
      "learning_rate": 3.161891679748823e-05,
      "loss": 0.5644,
      "step": 1873400
    },
    {
      "epoch": 29.411302982731556,
      "grad_norm": 3.933455467224121,
      "learning_rate": 3.161793563579278e-05,
      "loss": 0.5772,
      "step": 1873500
    },
    {
      "epoch": 29.41287284144427,
      "grad_norm": 3.488154888153076,
      "learning_rate": 3.161695447409733e-05,
      "loss": 0.5703,
      "step": 1873600
    },
    {
      "epoch": 29.414442700156986,
      "grad_norm": 4.392553329467773,
      "learning_rate": 3.161597331240189e-05,
      "loss": 0.5708,
      "step": 1873700
    },
    {
      "epoch": 29.416012558869703,
      "grad_norm": 3.5585358142852783,
      "learning_rate": 3.161499215070644e-05,
      "loss": 0.5917,
      "step": 1873800
    },
    {
      "epoch": 29.417582417582416,
      "grad_norm": 4.156641006469727,
      "learning_rate": 3.161401098901099e-05,
      "loss": 0.5889,
      "step": 1873900
    },
    {
      "epoch": 29.419152276295133,
      "grad_norm": 2.7453689575195312,
      "learning_rate": 3.161302982731554e-05,
      "loss": 0.5745,
      "step": 1874000
    },
    {
      "epoch": 29.42072213500785,
      "grad_norm": 3.4624269008636475,
      "learning_rate": 3.16120486656201e-05,
      "loss": 0.6131,
      "step": 1874100
    },
    {
      "epoch": 29.422291993720567,
      "grad_norm": 3.133155584335327,
      "learning_rate": 3.161106750392464e-05,
      "loss": 0.6048,
      "step": 1874200
    },
    {
      "epoch": 29.42386185243328,
      "grad_norm": 3.3767004013061523,
      "learning_rate": 3.16100863422292e-05,
      "loss": 0.5983,
      "step": 1874300
    },
    {
      "epoch": 29.425431711145997,
      "grad_norm": 2.5354254245758057,
      "learning_rate": 3.160910518053375e-05,
      "loss": 0.5756,
      "step": 1874400
    },
    {
      "epoch": 29.427001569858714,
      "grad_norm": 2.0777087211608887,
      "learning_rate": 3.160812401883831e-05,
      "loss": 0.5554,
      "step": 1874500
    },
    {
      "epoch": 29.428571428571427,
      "grad_norm": 3.847604513168335,
      "learning_rate": 3.160714285714286e-05,
      "loss": 0.5812,
      "step": 1874600
    },
    {
      "epoch": 29.430141287284144,
      "grad_norm": 4.283019065856934,
      "learning_rate": 3.160616169544741e-05,
      "loss": 0.6389,
      "step": 1874700
    },
    {
      "epoch": 29.43171114599686,
      "grad_norm": 3.1976711750030518,
      "learning_rate": 3.160518053375196e-05,
      "loss": 0.6019,
      "step": 1874800
    },
    {
      "epoch": 29.433281004709578,
      "grad_norm": 4.281558513641357,
      "learning_rate": 3.1604199372056514e-05,
      "loss": 0.582,
      "step": 1874900
    },
    {
      "epoch": 29.43485086342229,
      "grad_norm": 3.678288459777832,
      "learning_rate": 3.160321821036107e-05,
      "loss": 0.6153,
      "step": 1875000
    },
    {
      "epoch": 29.436420722135008,
      "grad_norm": 3.3110523223876953,
      "learning_rate": 3.160223704866562e-05,
      "loss": 0.5855,
      "step": 1875100
    },
    {
      "epoch": 29.437990580847725,
      "grad_norm": 3.535432815551758,
      "learning_rate": 3.160125588697018e-05,
      "loss": 0.5776,
      "step": 1875200
    },
    {
      "epoch": 29.439560439560438,
      "grad_norm": 2.487347364425659,
      "learning_rate": 3.1600274725274724e-05,
      "loss": 0.6114,
      "step": 1875300
    },
    {
      "epoch": 29.441130298273155,
      "grad_norm": 3.213686227798462,
      "learning_rate": 3.159929356357928e-05,
      "loss": 0.5771,
      "step": 1875400
    },
    {
      "epoch": 29.44270015698587,
      "grad_norm": 3.0651886463165283,
      "learning_rate": 3.159831240188383e-05,
      "loss": 0.5356,
      "step": 1875500
    },
    {
      "epoch": 29.44427001569859,
      "grad_norm": 4.020420074462891,
      "learning_rate": 3.1597331240188384e-05,
      "loss": 0.5575,
      "step": 1875600
    },
    {
      "epoch": 29.445839874411302,
      "grad_norm": 3.9615588188171387,
      "learning_rate": 3.1596350078492935e-05,
      "loss": 0.5472,
      "step": 1875700
    },
    {
      "epoch": 29.44740973312402,
      "grad_norm": 2.2533068656921387,
      "learning_rate": 3.159536891679749e-05,
      "loss": 0.6007,
      "step": 1875800
    },
    {
      "epoch": 29.448979591836736,
      "grad_norm": 4.396877765655518,
      "learning_rate": 3.1594387755102044e-05,
      "loss": 0.5722,
      "step": 1875900
    },
    {
      "epoch": 29.45054945054945,
      "grad_norm": 3.341609001159668,
      "learning_rate": 3.1593406593406595e-05,
      "loss": 0.583,
      "step": 1876000
    },
    {
      "epoch": 29.452119309262166,
      "grad_norm": 3.4151453971862793,
      "learning_rate": 3.1592425431711146e-05,
      "loss": 0.5601,
      "step": 1876100
    },
    {
      "epoch": 29.453689167974883,
      "grad_norm": 4.476903438568115,
      "learning_rate": 3.1591444270015703e-05,
      "loss": 0.6025,
      "step": 1876200
    },
    {
      "epoch": 29.4552590266876,
      "grad_norm": 4.164022922515869,
      "learning_rate": 3.159046310832025e-05,
      "loss": 0.5813,
      "step": 1876300
    },
    {
      "epoch": 29.456828885400313,
      "grad_norm": 4.515214443206787,
      "learning_rate": 3.1589481946624805e-05,
      "loss": 0.5693,
      "step": 1876400
    },
    {
      "epoch": 29.45839874411303,
      "grad_norm": 4.348659992218018,
      "learning_rate": 3.1588500784929356e-05,
      "loss": 0.5794,
      "step": 1876500
    },
    {
      "epoch": 29.459968602825747,
      "grad_norm": 3.3367362022399902,
      "learning_rate": 3.1587519623233914e-05,
      "loss": 0.5833,
      "step": 1876600
    },
    {
      "epoch": 29.46153846153846,
      "grad_norm": 3.4844658374786377,
      "learning_rate": 3.1586538461538465e-05,
      "loss": 0.6222,
      "step": 1876700
    },
    {
      "epoch": 29.463108320251177,
      "grad_norm": 3.702821969985962,
      "learning_rate": 3.1585557299843016e-05,
      "loss": 0.6004,
      "step": 1876800
    },
    {
      "epoch": 29.464678178963894,
      "grad_norm": 3.0317606925964355,
      "learning_rate": 3.158457613814757e-05,
      "loss": 0.6017,
      "step": 1876900
    },
    {
      "epoch": 29.46624803767661,
      "grad_norm": 3.759173631668091,
      "learning_rate": 3.158359497645212e-05,
      "loss": 0.5654,
      "step": 1877000
    },
    {
      "epoch": 29.467817896389324,
      "grad_norm": 4.487543106079102,
      "learning_rate": 3.1582613814756676e-05,
      "loss": 0.6022,
      "step": 1877100
    },
    {
      "epoch": 29.46938775510204,
      "grad_norm": 3.9033405780792236,
      "learning_rate": 3.158163265306123e-05,
      "loss": 0.5597,
      "step": 1877200
    },
    {
      "epoch": 29.470957613814758,
      "grad_norm": 3.60587739944458,
      "learning_rate": 3.158065149136578e-05,
      "loss": 0.5776,
      "step": 1877300
    },
    {
      "epoch": 29.47252747252747,
      "grad_norm": 4.201676845550537,
      "learning_rate": 3.157967032967033e-05,
      "loss": 0.5597,
      "step": 1877400
    },
    {
      "epoch": 29.474097331240188,
      "grad_norm": 3.0456044673919678,
      "learning_rate": 3.1578689167974887e-05,
      "loss": 0.6024,
      "step": 1877500
    },
    {
      "epoch": 29.475667189952905,
      "grad_norm": 4.111869812011719,
      "learning_rate": 3.157770800627944e-05,
      "loss": 0.5815,
      "step": 1877600
    },
    {
      "epoch": 29.47723704866562,
      "grad_norm": 4.091215133666992,
      "learning_rate": 3.157672684458399e-05,
      "loss": 0.5921,
      "step": 1877700
    },
    {
      "epoch": 29.478806907378335,
      "grad_norm": 3.8082680702209473,
      "learning_rate": 3.157574568288854e-05,
      "loss": 0.5793,
      "step": 1877800
    },
    {
      "epoch": 29.48037676609105,
      "grad_norm": 3.2831764221191406,
      "learning_rate": 3.15747645211931e-05,
      "loss": 0.5972,
      "step": 1877900
    },
    {
      "epoch": 29.48194662480377,
      "grad_norm": 3.4959797859191895,
      "learning_rate": 3.157378335949764e-05,
      "loss": 0.5653,
      "step": 1878000
    },
    {
      "epoch": 29.483516483516482,
      "grad_norm": 3.5636420249938965,
      "learning_rate": 3.15728021978022e-05,
      "loss": 0.597,
      "step": 1878100
    },
    {
      "epoch": 29.4850863422292,
      "grad_norm": 3.4752187728881836,
      "learning_rate": 3.157182103610675e-05,
      "loss": 0.5897,
      "step": 1878200
    },
    {
      "epoch": 29.486656200941916,
      "grad_norm": 4.245685577392578,
      "learning_rate": 3.157083987441131e-05,
      "loss": 0.594,
      "step": 1878300
    },
    {
      "epoch": 29.488226059654632,
      "grad_norm": 4.0153727531433105,
      "learning_rate": 3.156985871271585e-05,
      "loss": 0.6027,
      "step": 1878400
    },
    {
      "epoch": 29.489795918367346,
      "grad_norm": 3.8820393085479736,
      "learning_rate": 3.156887755102041e-05,
      "loss": 0.5992,
      "step": 1878500
    },
    {
      "epoch": 29.491365777080063,
      "grad_norm": 4.3991780281066895,
      "learning_rate": 3.156789638932496e-05,
      "loss": 0.5824,
      "step": 1878600
    },
    {
      "epoch": 29.49293563579278,
      "grad_norm": 3.438446283340454,
      "learning_rate": 3.156691522762951e-05,
      "loss": 0.5511,
      "step": 1878700
    },
    {
      "epoch": 29.494505494505496,
      "grad_norm": 3.71484375,
      "learning_rate": 3.156593406593407e-05,
      "loss": 0.5984,
      "step": 1878800
    },
    {
      "epoch": 29.49607535321821,
      "grad_norm": 3.7868614196777344,
      "learning_rate": 3.156495290423862e-05,
      "loss": 0.5713,
      "step": 1878900
    },
    {
      "epoch": 29.497645211930926,
      "grad_norm": 4.141982078552246,
      "learning_rate": 3.156397174254317e-05,
      "loss": 0.5724,
      "step": 1879000
    },
    {
      "epoch": 29.499215070643643,
      "grad_norm": 3.5067098140716553,
      "learning_rate": 3.156299058084772e-05,
      "loss": 0.5751,
      "step": 1879100
    },
    {
      "epoch": 29.500784929356357,
      "grad_norm": 4.091602802276611,
      "learning_rate": 3.156200941915228e-05,
      "loss": 0.5864,
      "step": 1879200
    },
    {
      "epoch": 29.502354788069074,
      "grad_norm": 4.365627288818359,
      "learning_rate": 3.156102825745683e-05,
      "loss": 0.5826,
      "step": 1879300
    },
    {
      "epoch": 29.50392464678179,
      "grad_norm": 4.618541240692139,
      "learning_rate": 3.156004709576138e-05,
      "loss": 0.6043,
      "step": 1879400
    },
    {
      "epoch": 29.505494505494504,
      "grad_norm": 4.460763931274414,
      "learning_rate": 3.155906593406593e-05,
      "loss": 0.5749,
      "step": 1879500
    },
    {
      "epoch": 29.50706436420722,
      "grad_norm": 3.2978382110595703,
      "learning_rate": 3.155808477237049e-05,
      "loss": 0.5762,
      "step": 1879600
    },
    {
      "epoch": 29.508634222919937,
      "grad_norm": 3.610278367996216,
      "learning_rate": 3.155710361067504e-05,
      "loss": 0.5518,
      "step": 1879700
    },
    {
      "epoch": 29.510204081632654,
      "grad_norm": 3.983429193496704,
      "learning_rate": 3.155612244897959e-05,
      "loss": 0.5641,
      "step": 1879800
    },
    {
      "epoch": 29.511773940345368,
      "grad_norm": 2.752645254135132,
      "learning_rate": 3.1555141287284144e-05,
      "loss": 0.6341,
      "step": 1879900
    },
    {
      "epoch": 29.513343799058084,
      "grad_norm": 4.837891578674316,
      "learning_rate": 3.15541601255887e-05,
      "loss": 0.6081,
      "step": 1880000
    },
    {
      "epoch": 29.5149136577708,
      "grad_norm": 3.153513193130493,
      "learning_rate": 3.1553178963893246e-05,
      "loss": 0.5885,
      "step": 1880100
    },
    {
      "epoch": 29.516483516483518,
      "grad_norm": 3.782942295074463,
      "learning_rate": 3.1552197802197804e-05,
      "loss": 0.587,
      "step": 1880200
    },
    {
      "epoch": 29.51805337519623,
      "grad_norm": 4.426451683044434,
      "learning_rate": 3.1551216640502355e-05,
      "loss": 0.5801,
      "step": 1880300
    },
    {
      "epoch": 29.51962323390895,
      "grad_norm": 4.066258907318115,
      "learning_rate": 3.155023547880691e-05,
      "loss": 0.5593,
      "step": 1880400
    },
    {
      "epoch": 29.521193092621665,
      "grad_norm": 3.4637293815612793,
      "learning_rate": 3.154925431711146e-05,
      "loss": 0.5768,
      "step": 1880500
    },
    {
      "epoch": 29.52276295133438,
      "grad_norm": 3.426003932952881,
      "learning_rate": 3.1548273155416014e-05,
      "loss": 0.5914,
      "step": 1880600
    },
    {
      "epoch": 29.524332810047095,
      "grad_norm": 3.2424168586730957,
      "learning_rate": 3.1547291993720565e-05,
      "loss": 0.5867,
      "step": 1880700
    },
    {
      "epoch": 29.525902668759812,
      "grad_norm": 2.850296974182129,
      "learning_rate": 3.1546310832025116e-05,
      "loss": 0.5683,
      "step": 1880800
    },
    {
      "epoch": 29.52747252747253,
      "grad_norm": 3.2756550312042236,
      "learning_rate": 3.1545329670329674e-05,
      "loss": 0.5612,
      "step": 1880900
    },
    {
      "epoch": 29.529042386185242,
      "grad_norm": 2.98995304107666,
      "learning_rate": 3.1544348508634225e-05,
      "loss": 0.545,
      "step": 1881000
    },
    {
      "epoch": 29.53061224489796,
      "grad_norm": 3.837371587753296,
      "learning_rate": 3.1543367346938776e-05,
      "loss": 0.5498,
      "step": 1881100
    },
    {
      "epoch": 29.532182103610676,
      "grad_norm": 2.8850159645080566,
      "learning_rate": 3.154238618524333e-05,
      "loss": 0.6212,
      "step": 1881200
    },
    {
      "epoch": 29.53375196232339,
      "grad_norm": 4.672418594360352,
      "learning_rate": 3.1541405023547885e-05,
      "loss": 0.6159,
      "step": 1881300
    },
    {
      "epoch": 29.535321821036106,
      "grad_norm": 2.884751796722412,
      "learning_rate": 3.1540423861852436e-05,
      "loss": 0.6069,
      "step": 1881400
    },
    {
      "epoch": 29.536891679748823,
      "grad_norm": 4.012197494506836,
      "learning_rate": 3.153944270015699e-05,
      "loss": 0.5937,
      "step": 1881500
    },
    {
      "epoch": 29.53846153846154,
      "grad_norm": 3.265068769454956,
      "learning_rate": 3.153846153846154e-05,
      "loss": 0.5555,
      "step": 1881600
    },
    {
      "epoch": 29.540031397174253,
      "grad_norm": 3.3822667598724365,
      "learning_rate": 3.1537480376766096e-05,
      "loss": 0.5715,
      "step": 1881700
    },
    {
      "epoch": 29.54160125588697,
      "grad_norm": 4.217189311981201,
      "learning_rate": 3.1536499215070647e-05,
      "loss": 0.6182,
      "step": 1881800
    },
    {
      "epoch": 29.543171114599687,
      "grad_norm": 2.7682015895843506,
      "learning_rate": 3.15355180533752e-05,
      "loss": 0.5813,
      "step": 1881900
    },
    {
      "epoch": 29.5447409733124,
      "grad_norm": 4.400865077972412,
      "learning_rate": 3.153453689167975e-05,
      "loss": 0.5899,
      "step": 1882000
    },
    {
      "epoch": 29.546310832025117,
      "grad_norm": 3.3435401916503906,
      "learning_rate": 3.1533555729984306e-05,
      "loss": 0.5956,
      "step": 1882100
    },
    {
      "epoch": 29.547880690737834,
      "grad_norm": 2.4432365894317627,
      "learning_rate": 3.153257456828885e-05,
      "loss": 0.6088,
      "step": 1882200
    },
    {
      "epoch": 29.54945054945055,
      "grad_norm": 2.8442142009735107,
      "learning_rate": 3.153159340659341e-05,
      "loss": 0.6174,
      "step": 1882300
    },
    {
      "epoch": 29.551020408163264,
      "grad_norm": 3.4562153816223145,
      "learning_rate": 3.153061224489796e-05,
      "loss": 0.6104,
      "step": 1882400
    },
    {
      "epoch": 29.55259026687598,
      "grad_norm": 3.8156721591949463,
      "learning_rate": 3.152963108320252e-05,
      "loss": 0.5701,
      "step": 1882500
    },
    {
      "epoch": 29.554160125588698,
      "grad_norm": 4.500407695770264,
      "learning_rate": 3.152864992150706e-05,
      "loss": 0.5938,
      "step": 1882600
    },
    {
      "epoch": 29.55572998430141,
      "grad_norm": 4.172529220581055,
      "learning_rate": 3.152766875981162e-05,
      "loss": 0.5868,
      "step": 1882700
    },
    {
      "epoch": 29.55729984301413,
      "grad_norm": 3.556654453277588,
      "learning_rate": 3.152668759811617e-05,
      "loss": 0.6024,
      "step": 1882800
    },
    {
      "epoch": 29.558869701726845,
      "grad_norm": 3.274991512298584,
      "learning_rate": 3.152570643642072e-05,
      "loss": 0.6156,
      "step": 1882900
    },
    {
      "epoch": 29.560439560439562,
      "grad_norm": 3.692075729370117,
      "learning_rate": 3.152472527472528e-05,
      "loss": 0.5665,
      "step": 1883000
    },
    {
      "epoch": 29.562009419152275,
      "grad_norm": 4.397940635681152,
      "learning_rate": 3.152374411302983e-05,
      "loss": 0.574,
      "step": 1883100
    },
    {
      "epoch": 29.563579277864992,
      "grad_norm": 4.5034403800964355,
      "learning_rate": 3.152276295133438e-05,
      "loss": 0.5884,
      "step": 1883200
    },
    {
      "epoch": 29.56514913657771,
      "grad_norm": 4.255587577819824,
      "learning_rate": 3.152178178963893e-05,
      "loss": 0.6,
      "step": 1883300
    },
    {
      "epoch": 29.566718995290422,
      "grad_norm": 1.9459558725357056,
      "learning_rate": 3.152080062794349e-05,
      "loss": 0.6086,
      "step": 1883400
    },
    {
      "epoch": 29.56828885400314,
      "grad_norm": 4.522871017456055,
      "learning_rate": 3.151981946624804e-05,
      "loss": 0.5949,
      "step": 1883500
    },
    {
      "epoch": 29.569858712715856,
      "grad_norm": 3.3032021522521973,
      "learning_rate": 3.151883830455259e-05,
      "loss": 0.5621,
      "step": 1883600
    },
    {
      "epoch": 29.571428571428573,
      "grad_norm": 2.214994430541992,
      "learning_rate": 3.151785714285714e-05,
      "loss": 0.6129,
      "step": 1883700
    },
    {
      "epoch": 29.572998430141286,
      "grad_norm": 3.928093433380127,
      "learning_rate": 3.15168759811617e-05,
      "loss": 0.5862,
      "step": 1883800
    },
    {
      "epoch": 29.574568288854003,
      "grad_norm": 3.6689727306365967,
      "learning_rate": 3.151589481946625e-05,
      "loss": 0.6125,
      "step": 1883900
    },
    {
      "epoch": 29.57613814756672,
      "grad_norm": 3.367917776107788,
      "learning_rate": 3.15149136577708e-05,
      "loss": 0.5879,
      "step": 1884000
    },
    {
      "epoch": 29.577708006279433,
      "grad_norm": 4.618058204650879,
      "learning_rate": 3.151393249607535e-05,
      "loss": 0.5855,
      "step": 1884100
    },
    {
      "epoch": 29.57927786499215,
      "grad_norm": 3.3106024265289307,
      "learning_rate": 3.151295133437991e-05,
      "loss": 0.6277,
      "step": 1884200
    },
    {
      "epoch": 29.580847723704867,
      "grad_norm": 3.1326873302459717,
      "learning_rate": 3.1511970172684455e-05,
      "loss": 0.5974,
      "step": 1884300
    },
    {
      "epoch": 29.582417582417584,
      "grad_norm": 3.8717474937438965,
      "learning_rate": 3.151098901098901e-05,
      "loss": 0.6056,
      "step": 1884400
    },
    {
      "epoch": 29.583987441130297,
      "grad_norm": 2.8833835124969482,
      "learning_rate": 3.1510007849293564e-05,
      "loss": 0.5562,
      "step": 1884500
    },
    {
      "epoch": 29.585557299843014,
      "grad_norm": 3.9870924949645996,
      "learning_rate": 3.150902668759812e-05,
      "loss": 0.6196,
      "step": 1884600
    },
    {
      "epoch": 29.58712715855573,
      "grad_norm": 3.731236457824707,
      "learning_rate": 3.1508045525902666e-05,
      "loss": 0.5384,
      "step": 1884700
    },
    {
      "epoch": 29.588697017268444,
      "grad_norm": 3.822084665298462,
      "learning_rate": 3.1507064364207223e-05,
      "loss": 0.578,
      "step": 1884800
    },
    {
      "epoch": 29.59026687598116,
      "grad_norm": 3.5137779712677,
      "learning_rate": 3.1506083202511774e-05,
      "loss": 0.5989,
      "step": 1884900
    },
    {
      "epoch": 29.591836734693878,
      "grad_norm": 4.464301586151123,
      "learning_rate": 3.1505102040816325e-05,
      "loss": 0.5622,
      "step": 1885000
    },
    {
      "epoch": 29.593406593406595,
      "grad_norm": 2.696387529373169,
      "learning_rate": 3.1504120879120876e-05,
      "loss": 0.6089,
      "step": 1885100
    },
    {
      "epoch": 29.594976452119308,
      "grad_norm": 4.0073161125183105,
      "learning_rate": 3.1503139717425434e-05,
      "loss": 0.5836,
      "step": 1885200
    },
    {
      "epoch": 29.596546310832025,
      "grad_norm": 3.3390443325042725,
      "learning_rate": 3.1502158555729985e-05,
      "loss": 0.567,
      "step": 1885300
    },
    {
      "epoch": 29.598116169544742,
      "grad_norm": 4.19586706161499,
      "learning_rate": 3.1501177394034536e-05,
      "loss": 0.6059,
      "step": 1885400
    },
    {
      "epoch": 29.599686028257455,
      "grad_norm": 5.687660217285156,
      "learning_rate": 3.1500196232339094e-05,
      "loss": 0.6075,
      "step": 1885500
    },
    {
      "epoch": 29.601255886970172,
      "grad_norm": 4.913822174072266,
      "learning_rate": 3.1499215070643645e-05,
      "loss": 0.5565,
      "step": 1885600
    },
    {
      "epoch": 29.60282574568289,
      "grad_norm": 4.047585487365723,
      "learning_rate": 3.1498233908948196e-05,
      "loss": 0.5828,
      "step": 1885700
    },
    {
      "epoch": 29.604395604395606,
      "grad_norm": 2.4116504192352295,
      "learning_rate": 3.149725274725275e-05,
      "loss": 0.5645,
      "step": 1885800
    },
    {
      "epoch": 29.60596546310832,
      "grad_norm": 3.6586899757385254,
      "learning_rate": 3.1496271585557305e-05,
      "loss": 0.5828,
      "step": 1885900
    },
    {
      "epoch": 29.607535321821036,
      "grad_norm": 2.556293487548828,
      "learning_rate": 3.1495290423861856e-05,
      "loss": 0.6091,
      "step": 1886000
    },
    {
      "epoch": 29.609105180533753,
      "grad_norm": 3.612156867980957,
      "learning_rate": 3.1494309262166406e-05,
      "loss": 0.5452,
      "step": 1886100
    },
    {
      "epoch": 29.610675039246466,
      "grad_norm": 4.303739070892334,
      "learning_rate": 3.149332810047096e-05,
      "loss": 0.5597,
      "step": 1886200
    },
    {
      "epoch": 29.612244897959183,
      "grad_norm": 3.221189260482788,
      "learning_rate": 3.1492346938775515e-05,
      "loss": 0.5915,
      "step": 1886300
    },
    {
      "epoch": 29.6138147566719,
      "grad_norm": 4.0899658203125,
      "learning_rate": 3.149136577708006e-05,
      "loss": 0.5568,
      "step": 1886400
    },
    {
      "epoch": 29.615384615384617,
      "grad_norm": 3.104783773422241,
      "learning_rate": 3.149038461538462e-05,
      "loss": 0.6192,
      "step": 1886500
    },
    {
      "epoch": 29.61695447409733,
      "grad_norm": 4.453060626983643,
      "learning_rate": 3.148940345368917e-05,
      "loss": 0.6079,
      "step": 1886600
    },
    {
      "epoch": 29.618524332810047,
      "grad_norm": 4.562200546264648,
      "learning_rate": 3.1488422291993726e-05,
      "loss": 0.5584,
      "step": 1886700
    },
    {
      "epoch": 29.620094191522764,
      "grad_norm": 3.7522926330566406,
      "learning_rate": 3.148744113029827e-05,
      "loss": 0.5892,
      "step": 1886800
    },
    {
      "epoch": 29.621664050235477,
      "grad_norm": 3.179232597351074,
      "learning_rate": 3.148645996860283e-05,
      "loss": 0.6116,
      "step": 1886900
    },
    {
      "epoch": 29.623233908948194,
      "grad_norm": 2.6708500385284424,
      "learning_rate": 3.148547880690738e-05,
      "loss": 0.6059,
      "step": 1887000
    },
    {
      "epoch": 29.62480376766091,
      "grad_norm": 4.667202949523926,
      "learning_rate": 3.148449764521193e-05,
      "loss": 0.5782,
      "step": 1887100
    },
    {
      "epoch": 29.626373626373628,
      "grad_norm": 2.3106088638305664,
      "learning_rate": 3.148351648351648e-05,
      "loss": 0.5574,
      "step": 1887200
    },
    {
      "epoch": 29.62794348508634,
      "grad_norm": 3.5163238048553467,
      "learning_rate": 3.148253532182104e-05,
      "loss": 0.5905,
      "step": 1887300
    },
    {
      "epoch": 29.629513343799058,
      "grad_norm": 3.001789093017578,
      "learning_rate": 3.148155416012559e-05,
      "loss": 0.5577,
      "step": 1887400
    },
    {
      "epoch": 29.631083202511775,
      "grad_norm": 3.4110076427459717,
      "learning_rate": 3.148057299843014e-05,
      "loss": 0.5971,
      "step": 1887500
    },
    {
      "epoch": 29.632653061224488,
      "grad_norm": 4.718012809753418,
      "learning_rate": 3.14795918367347e-05,
      "loss": 0.5614,
      "step": 1887600
    },
    {
      "epoch": 29.634222919937205,
      "grad_norm": 3.106271505355835,
      "learning_rate": 3.147861067503925e-05,
      "loss": 0.5977,
      "step": 1887700
    },
    {
      "epoch": 29.635792778649922,
      "grad_norm": 3.6608617305755615,
      "learning_rate": 3.14776295133438e-05,
      "loss": 0.6341,
      "step": 1887800
    },
    {
      "epoch": 29.63736263736264,
      "grad_norm": 4.680441379547119,
      "learning_rate": 3.147664835164835e-05,
      "loss": 0.5366,
      "step": 1887900
    },
    {
      "epoch": 29.638932496075352,
      "grad_norm": 4.175521373748779,
      "learning_rate": 3.147566718995291e-05,
      "loss": 0.6099,
      "step": 1888000
    },
    {
      "epoch": 29.64050235478807,
      "grad_norm": 3.443842649459839,
      "learning_rate": 3.147468602825746e-05,
      "loss": 0.5914,
      "step": 1888100
    },
    {
      "epoch": 29.642072213500786,
      "grad_norm": 3.3182871341705322,
      "learning_rate": 3.147370486656201e-05,
      "loss": 0.5862,
      "step": 1888200
    },
    {
      "epoch": 29.643642072213503,
      "grad_norm": 1.4222086668014526,
      "learning_rate": 3.147272370486656e-05,
      "loss": 0.5481,
      "step": 1888300
    },
    {
      "epoch": 29.645211930926216,
      "grad_norm": 3.2367281913757324,
      "learning_rate": 3.147174254317112e-05,
      "loss": 0.573,
      "step": 1888400
    },
    {
      "epoch": 29.646781789638933,
      "grad_norm": 3.8676788806915283,
      "learning_rate": 3.1470761381475664e-05,
      "loss": 0.5841,
      "step": 1888500
    },
    {
      "epoch": 29.64835164835165,
      "grad_norm": 3.842275619506836,
      "learning_rate": 3.146978021978022e-05,
      "loss": 0.5519,
      "step": 1888600
    },
    {
      "epoch": 29.649921507064363,
      "grad_norm": 3.655186653137207,
      "learning_rate": 3.146879905808477e-05,
      "loss": 0.5923,
      "step": 1888700
    },
    {
      "epoch": 29.65149136577708,
      "grad_norm": 2.731072187423706,
      "learning_rate": 3.146781789638933e-05,
      "loss": 0.5868,
      "step": 1888800
    },
    {
      "epoch": 29.653061224489797,
      "grad_norm": 3.550819158554077,
      "learning_rate": 3.1466836734693875e-05,
      "loss": 0.5831,
      "step": 1888900
    },
    {
      "epoch": 29.65463108320251,
      "grad_norm": 2.9283313751220703,
      "learning_rate": 3.146585557299843e-05,
      "loss": 0.6223,
      "step": 1889000
    },
    {
      "epoch": 29.656200941915227,
      "grad_norm": 3.9543728828430176,
      "learning_rate": 3.146487441130298e-05,
      "loss": 0.6089,
      "step": 1889100
    },
    {
      "epoch": 29.657770800627944,
      "grad_norm": 4.478196620941162,
      "learning_rate": 3.1463893249607534e-05,
      "loss": 0.6045,
      "step": 1889200
    },
    {
      "epoch": 29.65934065934066,
      "grad_norm": 3.966945171356201,
      "learning_rate": 3.1462912087912085e-05,
      "loss": 0.5976,
      "step": 1889300
    },
    {
      "epoch": 29.660910518053374,
      "grad_norm": 3.24753999710083,
      "learning_rate": 3.146193092621664e-05,
      "loss": 0.5956,
      "step": 1889400
    },
    {
      "epoch": 29.66248037676609,
      "grad_norm": 3.675520658493042,
      "learning_rate": 3.1460949764521194e-05,
      "loss": 0.6028,
      "step": 1889500
    },
    {
      "epoch": 29.664050235478808,
      "grad_norm": 2.9354214668273926,
      "learning_rate": 3.1459968602825745e-05,
      "loss": 0.57,
      "step": 1889600
    },
    {
      "epoch": 29.665620094191524,
      "grad_norm": 3.1024892330169678,
      "learning_rate": 3.14589874411303e-05,
      "loss": 0.6067,
      "step": 1889700
    },
    {
      "epoch": 29.667189952904238,
      "grad_norm": 4.242003917694092,
      "learning_rate": 3.1458006279434854e-05,
      "loss": 0.6094,
      "step": 1889800
    },
    {
      "epoch": 29.668759811616955,
      "grad_norm": 3.598428964614868,
      "learning_rate": 3.1457025117739405e-05,
      "loss": 0.5628,
      "step": 1889900
    },
    {
      "epoch": 29.67032967032967,
      "grad_norm": 3.4158434867858887,
      "learning_rate": 3.1456043956043956e-05,
      "loss": 0.5673,
      "step": 1890000
    },
    {
      "epoch": 29.671899529042385,
      "grad_norm": 3.9785993099212646,
      "learning_rate": 3.1455062794348514e-05,
      "loss": 0.6192,
      "step": 1890100
    },
    {
      "epoch": 29.6734693877551,
      "grad_norm": 4.230240821838379,
      "learning_rate": 3.1454081632653064e-05,
      "loss": 0.5645,
      "step": 1890200
    },
    {
      "epoch": 29.67503924646782,
      "grad_norm": 3.125352382659912,
      "learning_rate": 3.1453100470957615e-05,
      "loss": 0.6112,
      "step": 1890300
    },
    {
      "epoch": 29.676609105180535,
      "grad_norm": 3.2069406509399414,
      "learning_rate": 3.1452119309262166e-05,
      "loss": 0.6164,
      "step": 1890400
    },
    {
      "epoch": 29.67817896389325,
      "grad_norm": 3.875129461288452,
      "learning_rate": 3.1451138147566724e-05,
      "loss": 0.5923,
      "step": 1890500
    },
    {
      "epoch": 29.679748822605966,
      "grad_norm": 3.644289970397949,
      "learning_rate": 3.145015698587127e-05,
      "loss": 0.5735,
      "step": 1890600
    },
    {
      "epoch": 29.681318681318682,
      "grad_norm": 3.51182222366333,
      "learning_rate": 3.1449175824175826e-05,
      "loss": 0.6098,
      "step": 1890700
    },
    {
      "epoch": 29.682888540031396,
      "grad_norm": 3.7756941318511963,
      "learning_rate": 3.144819466248038e-05,
      "loss": 0.5941,
      "step": 1890800
    },
    {
      "epoch": 29.684458398744113,
      "grad_norm": 2.0610837936401367,
      "learning_rate": 3.1447213500784935e-05,
      "loss": 0.6043,
      "step": 1890900
    },
    {
      "epoch": 29.68602825745683,
      "grad_norm": 4.3729166984558105,
      "learning_rate": 3.144623233908948e-05,
      "loss": 0.5903,
      "step": 1891000
    },
    {
      "epoch": 29.687598116169546,
      "grad_norm": 4.977774143218994,
      "learning_rate": 3.144525117739404e-05,
      "loss": 0.5857,
      "step": 1891100
    },
    {
      "epoch": 29.68916797488226,
      "grad_norm": 3.3751935958862305,
      "learning_rate": 3.144427001569859e-05,
      "loss": 0.5821,
      "step": 1891200
    },
    {
      "epoch": 29.690737833594977,
      "grad_norm": 4.3850531578063965,
      "learning_rate": 3.144328885400314e-05,
      "loss": 0.566,
      "step": 1891300
    },
    {
      "epoch": 29.692307692307693,
      "grad_norm": 3.454930067062378,
      "learning_rate": 3.144230769230769e-05,
      "loss": 0.5848,
      "step": 1891400
    },
    {
      "epoch": 29.693877551020407,
      "grad_norm": 4.222689628601074,
      "learning_rate": 3.144132653061225e-05,
      "loss": 0.5804,
      "step": 1891500
    },
    {
      "epoch": 29.695447409733124,
      "grad_norm": 3.80475115776062,
      "learning_rate": 3.14403453689168e-05,
      "loss": 0.5955,
      "step": 1891600
    },
    {
      "epoch": 29.69701726844584,
      "grad_norm": 3.1879308223724365,
      "learning_rate": 3.143936420722135e-05,
      "loss": 0.5876,
      "step": 1891700
    },
    {
      "epoch": 29.698587127158557,
      "grad_norm": 3.7531213760375977,
      "learning_rate": 3.143838304552591e-05,
      "loss": 0.6055,
      "step": 1891800
    },
    {
      "epoch": 29.70015698587127,
      "grad_norm": 2.2958664894104004,
      "learning_rate": 3.143740188383046e-05,
      "loss": 0.6198,
      "step": 1891900
    },
    {
      "epoch": 29.701726844583987,
      "grad_norm": 3.8462090492248535,
      "learning_rate": 3.143642072213501e-05,
      "loss": 0.6078,
      "step": 1892000
    },
    {
      "epoch": 29.703296703296704,
      "grad_norm": 4.305612564086914,
      "learning_rate": 3.143543956043956e-05,
      "loss": 0.5469,
      "step": 1892100
    },
    {
      "epoch": 29.704866562009418,
      "grad_norm": 3.546531915664673,
      "learning_rate": 3.143445839874412e-05,
      "loss": 0.5938,
      "step": 1892200
    },
    {
      "epoch": 29.706436420722135,
      "grad_norm": 3.384779453277588,
      "learning_rate": 3.143347723704867e-05,
      "loss": 0.6292,
      "step": 1892300
    },
    {
      "epoch": 29.70800627943485,
      "grad_norm": 3.4633398056030273,
      "learning_rate": 3.143249607535322e-05,
      "loss": 0.6056,
      "step": 1892400
    },
    {
      "epoch": 29.70957613814757,
      "grad_norm": 4.44428825378418,
      "learning_rate": 3.143151491365777e-05,
      "loss": 0.618,
      "step": 1892500
    },
    {
      "epoch": 29.71114599686028,
      "grad_norm": 3.7843310832977295,
      "learning_rate": 3.143053375196233e-05,
      "loss": 0.5705,
      "step": 1892600
    },
    {
      "epoch": 29.712715855573,
      "grad_norm": 4.253385543823242,
      "learning_rate": 3.142955259026687e-05,
      "loss": 0.6229,
      "step": 1892700
    },
    {
      "epoch": 29.714285714285715,
      "grad_norm": 3.5232105255126953,
      "learning_rate": 3.142857142857143e-05,
      "loss": 0.576,
      "step": 1892800
    },
    {
      "epoch": 29.71585557299843,
      "grad_norm": 4.850215435028076,
      "learning_rate": 3.142759026687598e-05,
      "loss": 0.5816,
      "step": 1892900
    },
    {
      "epoch": 29.717425431711145,
      "grad_norm": 3.1644058227539062,
      "learning_rate": 3.142660910518054e-05,
      "loss": 0.5912,
      "step": 1893000
    },
    {
      "epoch": 29.718995290423862,
      "grad_norm": 4.063641548156738,
      "learning_rate": 3.1425627943485084e-05,
      "loss": 0.5881,
      "step": 1893100
    },
    {
      "epoch": 29.72056514913658,
      "grad_norm": 4.370164394378662,
      "learning_rate": 3.142464678178964e-05,
      "loss": 0.6098,
      "step": 1893200
    },
    {
      "epoch": 29.722135007849293,
      "grad_norm": 3.7504117488861084,
      "learning_rate": 3.142366562009419e-05,
      "loss": 0.6126,
      "step": 1893300
    },
    {
      "epoch": 29.72370486656201,
      "grad_norm": 3.9814836978912354,
      "learning_rate": 3.142268445839874e-05,
      "loss": 0.6102,
      "step": 1893400
    },
    {
      "epoch": 29.725274725274726,
      "grad_norm": 2.873486280441284,
      "learning_rate": 3.1421703296703294e-05,
      "loss": 0.567,
      "step": 1893500
    },
    {
      "epoch": 29.72684458398744,
      "grad_norm": 3.776653289794922,
      "learning_rate": 3.142072213500785e-05,
      "loss": 0.5572,
      "step": 1893600
    },
    {
      "epoch": 29.728414442700156,
      "grad_norm": 4.346223831176758,
      "learning_rate": 3.14197409733124e-05,
      "loss": 0.6177,
      "step": 1893700
    },
    {
      "epoch": 29.729984301412873,
      "grad_norm": 4.0598344802856445,
      "learning_rate": 3.1418759811616954e-05,
      "loss": 0.5742,
      "step": 1893800
    },
    {
      "epoch": 29.73155416012559,
      "grad_norm": 2.9089837074279785,
      "learning_rate": 3.141777864992151e-05,
      "loss": 0.5872,
      "step": 1893900
    },
    {
      "epoch": 29.733124018838303,
      "grad_norm": 3.9330503940582275,
      "learning_rate": 3.141679748822606e-05,
      "loss": 0.5866,
      "step": 1894000
    },
    {
      "epoch": 29.73469387755102,
      "grad_norm": 3.2596137523651123,
      "learning_rate": 3.1415816326530614e-05,
      "loss": 0.6013,
      "step": 1894100
    },
    {
      "epoch": 29.736263736263737,
      "grad_norm": 3.70525860786438,
      "learning_rate": 3.1414835164835165e-05,
      "loss": 0.6194,
      "step": 1894200
    },
    {
      "epoch": 29.73783359497645,
      "grad_norm": 3.828955888748169,
      "learning_rate": 3.141385400313972e-05,
      "loss": 0.5553,
      "step": 1894300
    },
    {
      "epoch": 29.739403453689167,
      "grad_norm": 3.65549373626709,
      "learning_rate": 3.1412872841444273e-05,
      "loss": 0.5882,
      "step": 1894400
    },
    {
      "epoch": 29.740973312401884,
      "grad_norm": 3.3924739360809326,
      "learning_rate": 3.1411891679748824e-05,
      "loss": 0.6008,
      "step": 1894500
    },
    {
      "epoch": 29.7425431711146,
      "grad_norm": 3.901181936264038,
      "learning_rate": 3.1410910518053375e-05,
      "loss": 0.5986,
      "step": 1894600
    },
    {
      "epoch": 29.744113029827314,
      "grad_norm": 3.0061914920806885,
      "learning_rate": 3.140992935635793e-05,
      "loss": 0.5802,
      "step": 1894700
    },
    {
      "epoch": 29.74568288854003,
      "grad_norm": 4.015455722808838,
      "learning_rate": 3.140894819466248e-05,
      "loss": 0.5913,
      "step": 1894800
    },
    {
      "epoch": 29.747252747252748,
      "grad_norm": 4.021846771240234,
      "learning_rate": 3.1407967032967035e-05,
      "loss": 0.5987,
      "step": 1894900
    },
    {
      "epoch": 29.74882260596546,
      "grad_norm": 4.278708457946777,
      "learning_rate": 3.1406985871271586e-05,
      "loss": 0.6026,
      "step": 1895000
    },
    {
      "epoch": 29.75039246467818,
      "grad_norm": 3.448613405227661,
      "learning_rate": 3.1406004709576144e-05,
      "loss": 0.589,
      "step": 1895100
    },
    {
      "epoch": 29.751962323390895,
      "grad_norm": 3.079289674758911,
      "learning_rate": 3.140502354788069e-05,
      "loss": 0.6143,
      "step": 1895200
    },
    {
      "epoch": 29.753532182103612,
      "grad_norm": 4.361108779907227,
      "learning_rate": 3.1404042386185246e-05,
      "loss": 0.6201,
      "step": 1895300
    },
    {
      "epoch": 29.755102040816325,
      "grad_norm": 4.217097759246826,
      "learning_rate": 3.14030612244898e-05,
      "loss": 0.5356,
      "step": 1895400
    },
    {
      "epoch": 29.756671899529042,
      "grad_norm": 4.171976566314697,
      "learning_rate": 3.140208006279435e-05,
      "loss": 0.5668,
      "step": 1895500
    },
    {
      "epoch": 29.75824175824176,
      "grad_norm": 4.139290809631348,
      "learning_rate": 3.14010989010989e-05,
      "loss": 0.6012,
      "step": 1895600
    },
    {
      "epoch": 29.759811616954472,
      "grad_norm": 3.167692184448242,
      "learning_rate": 3.1400117739403457e-05,
      "loss": 0.5948,
      "step": 1895700
    },
    {
      "epoch": 29.76138147566719,
      "grad_norm": 3.7881228923797607,
      "learning_rate": 3.139913657770801e-05,
      "loss": 0.5533,
      "step": 1895800
    },
    {
      "epoch": 29.762951334379906,
      "grad_norm": 3.597722053527832,
      "learning_rate": 3.139815541601256e-05,
      "loss": 0.5849,
      "step": 1895900
    },
    {
      "epoch": 29.764521193092623,
      "grad_norm": 4.1559672355651855,
      "learning_rate": 3.1397174254317116e-05,
      "loss": 0.5928,
      "step": 1896000
    },
    {
      "epoch": 29.766091051805336,
      "grad_norm": 3.003366231918335,
      "learning_rate": 3.139619309262167e-05,
      "loss": 0.6055,
      "step": 1896100
    },
    {
      "epoch": 29.767660910518053,
      "grad_norm": 4.294378280639648,
      "learning_rate": 3.139521193092622e-05,
      "loss": 0.5753,
      "step": 1896200
    },
    {
      "epoch": 29.76923076923077,
      "grad_norm": 3.2093608379364014,
      "learning_rate": 3.139423076923077e-05,
      "loss": 0.636,
      "step": 1896300
    },
    {
      "epoch": 29.770800627943487,
      "grad_norm": 3.67317795753479,
      "learning_rate": 3.139324960753533e-05,
      "loss": 0.5706,
      "step": 1896400
    },
    {
      "epoch": 29.7723704866562,
      "grad_norm": 4.1230854988098145,
      "learning_rate": 3.139226844583988e-05,
      "loss": 0.6265,
      "step": 1896500
    },
    {
      "epoch": 29.773940345368917,
      "grad_norm": 4.198601722717285,
      "learning_rate": 3.139128728414443e-05,
      "loss": 0.6156,
      "step": 1896600
    },
    {
      "epoch": 29.775510204081634,
      "grad_norm": 3.7571585178375244,
      "learning_rate": 3.139030612244898e-05,
      "loss": 0.5744,
      "step": 1896700
    },
    {
      "epoch": 29.777080062794347,
      "grad_norm": 3.689540147781372,
      "learning_rate": 3.138932496075354e-05,
      "loss": 0.6319,
      "step": 1896800
    },
    {
      "epoch": 29.778649921507064,
      "grad_norm": 4.53533411026001,
      "learning_rate": 3.138834379905808e-05,
      "loss": 0.5684,
      "step": 1896900
    },
    {
      "epoch": 29.78021978021978,
      "grad_norm": 3.035322666168213,
      "learning_rate": 3.138736263736264e-05,
      "loss": 0.569,
      "step": 1897000
    },
    {
      "epoch": 29.781789638932494,
      "grad_norm": 3.8452720642089844,
      "learning_rate": 3.138638147566719e-05,
      "loss": 0.6082,
      "step": 1897100
    },
    {
      "epoch": 29.78335949764521,
      "grad_norm": 3.8598077297210693,
      "learning_rate": 3.138540031397175e-05,
      "loss": 0.5664,
      "step": 1897200
    },
    {
      "epoch": 29.784929356357928,
      "grad_norm": 4.276504993438721,
      "learning_rate": 3.138441915227629e-05,
      "loss": 0.5934,
      "step": 1897300
    },
    {
      "epoch": 29.786499215070645,
      "grad_norm": 3.9706954956054688,
      "learning_rate": 3.138343799058085e-05,
      "loss": 0.6436,
      "step": 1897400
    },
    {
      "epoch": 29.788069073783358,
      "grad_norm": 3.485640048980713,
      "learning_rate": 3.13824568288854e-05,
      "loss": 0.5911,
      "step": 1897500
    },
    {
      "epoch": 29.789638932496075,
      "grad_norm": 3.1166160106658936,
      "learning_rate": 3.138147566718995e-05,
      "loss": 0.5958,
      "step": 1897600
    },
    {
      "epoch": 29.791208791208792,
      "grad_norm": 3.4521992206573486,
      "learning_rate": 3.13804945054945e-05,
      "loss": 0.5622,
      "step": 1897700
    },
    {
      "epoch": 29.79277864992151,
      "grad_norm": 2.941575288772583,
      "learning_rate": 3.137951334379906e-05,
      "loss": 0.5944,
      "step": 1897800
    },
    {
      "epoch": 29.794348508634222,
      "grad_norm": 3.4678733348846436,
      "learning_rate": 3.137853218210361e-05,
      "loss": 0.578,
      "step": 1897900
    },
    {
      "epoch": 29.79591836734694,
      "grad_norm": 3.843951940536499,
      "learning_rate": 3.137755102040816e-05,
      "loss": 0.5779,
      "step": 1898000
    },
    {
      "epoch": 29.797488226059656,
      "grad_norm": 2.306638717651367,
      "learning_rate": 3.137656985871272e-05,
      "loss": 0.5824,
      "step": 1898100
    },
    {
      "epoch": 29.79905808477237,
      "grad_norm": 4.045745372772217,
      "learning_rate": 3.137558869701727e-05,
      "loss": 0.5905,
      "step": 1898200
    },
    {
      "epoch": 29.800627943485086,
      "grad_norm": 2.936028003692627,
      "learning_rate": 3.137460753532182e-05,
      "loss": 0.5902,
      "step": 1898300
    },
    {
      "epoch": 29.802197802197803,
      "grad_norm": 5.321037292480469,
      "learning_rate": 3.1373626373626374e-05,
      "loss": 0.6167,
      "step": 1898400
    },
    {
      "epoch": 29.80376766091052,
      "grad_norm": 3.60056734085083,
      "learning_rate": 3.137264521193093e-05,
      "loss": 0.5642,
      "step": 1898500
    },
    {
      "epoch": 29.805337519623233,
      "grad_norm": 3.0651955604553223,
      "learning_rate": 3.137166405023548e-05,
      "loss": 0.5673,
      "step": 1898600
    },
    {
      "epoch": 29.80690737833595,
      "grad_norm": 4.048456192016602,
      "learning_rate": 3.1370682888540033e-05,
      "loss": 0.5898,
      "step": 1898700
    },
    {
      "epoch": 29.808477237048667,
      "grad_norm": 5.053376197814941,
      "learning_rate": 3.1369701726844584e-05,
      "loss": 0.5889,
      "step": 1898800
    },
    {
      "epoch": 29.81004709576138,
      "grad_norm": 3.2458302974700928,
      "learning_rate": 3.136872056514914e-05,
      "loss": 0.6495,
      "step": 1898900
    },
    {
      "epoch": 29.811616954474097,
      "grad_norm": 3.891983985900879,
      "learning_rate": 3.1367739403453686e-05,
      "loss": 0.5932,
      "step": 1899000
    },
    {
      "epoch": 29.813186813186814,
      "grad_norm": 3.088794231414795,
      "learning_rate": 3.1366758241758244e-05,
      "loss": 0.5672,
      "step": 1899100
    },
    {
      "epoch": 29.81475667189953,
      "grad_norm": 3.4111900329589844,
      "learning_rate": 3.1365777080062795e-05,
      "loss": 0.5688,
      "step": 1899200
    },
    {
      "epoch": 29.816326530612244,
      "grad_norm": 3.8677728176116943,
      "learning_rate": 3.136479591836735e-05,
      "loss": 0.5618,
      "step": 1899300
    },
    {
      "epoch": 29.81789638932496,
      "grad_norm": 3.7629640102386475,
      "learning_rate": 3.13638147566719e-05,
      "loss": 0.5634,
      "step": 1899400
    },
    {
      "epoch": 29.819466248037678,
      "grad_norm": 4.56727933883667,
      "learning_rate": 3.1362833594976455e-05,
      "loss": 0.6028,
      "step": 1899500
    },
    {
      "epoch": 29.82103610675039,
      "grad_norm": 4.404661178588867,
      "learning_rate": 3.1361852433281006e-05,
      "loss": 0.5652,
      "step": 1899600
    },
    {
      "epoch": 29.822605965463108,
      "grad_norm": 3.199733257293701,
      "learning_rate": 3.136087127158556e-05,
      "loss": 0.54,
      "step": 1899700
    },
    {
      "epoch": 29.824175824175825,
      "grad_norm": 3.945775032043457,
      "learning_rate": 3.135989010989011e-05,
      "loss": 0.6083,
      "step": 1899800
    },
    {
      "epoch": 29.82574568288854,
      "grad_norm": 4.9842939376831055,
      "learning_rate": 3.1358908948194666e-05,
      "loss": 0.5716,
      "step": 1899900
    },
    {
      "epoch": 29.827315541601255,
      "grad_norm": 4.991030216217041,
      "learning_rate": 3.1357927786499217e-05,
      "loss": 0.5952,
      "step": 1900000
    },
    {
      "epoch": 29.828885400313972,
      "grad_norm": 3.509849786758423,
      "learning_rate": 3.135694662480377e-05,
      "loss": 0.6646,
      "step": 1900100
    },
    {
      "epoch": 29.83045525902669,
      "grad_norm": 3.8774373531341553,
      "learning_rate": 3.1355965463108325e-05,
      "loss": 0.5778,
      "step": 1900200
    },
    {
      "epoch": 29.832025117739402,
      "grad_norm": 3.7117655277252197,
      "learning_rate": 3.1354984301412876e-05,
      "loss": 0.6172,
      "step": 1900300
    },
    {
      "epoch": 29.83359497645212,
      "grad_norm": 4.698375701904297,
      "learning_rate": 3.135400313971743e-05,
      "loss": 0.58,
      "step": 1900400
    },
    {
      "epoch": 29.835164835164836,
      "grad_norm": 3.943739652633667,
      "learning_rate": 3.135302197802198e-05,
      "loss": 0.5948,
      "step": 1900500
    },
    {
      "epoch": 29.836734693877553,
      "grad_norm": 3.942836046218872,
      "learning_rate": 3.1352040816326536e-05,
      "loss": 0.5978,
      "step": 1900600
    },
    {
      "epoch": 29.838304552590266,
      "grad_norm": 2.957850217819214,
      "learning_rate": 3.135105965463108e-05,
      "loss": 0.5542,
      "step": 1900700
    },
    {
      "epoch": 29.839874411302983,
      "grad_norm": 2.757150650024414,
      "learning_rate": 3.135007849293564e-05,
      "loss": 0.561,
      "step": 1900800
    },
    {
      "epoch": 29.8414442700157,
      "grad_norm": 3.75685453414917,
      "learning_rate": 3.134909733124019e-05,
      "loss": 0.5685,
      "step": 1900900
    },
    {
      "epoch": 29.843014128728413,
      "grad_norm": 3.40445876121521,
      "learning_rate": 3.134811616954475e-05,
      "loss": 0.6137,
      "step": 1901000
    },
    {
      "epoch": 29.84458398744113,
      "grad_norm": 4.07276725769043,
      "learning_rate": 3.134713500784929e-05,
      "loss": 0.6171,
      "step": 1901100
    },
    {
      "epoch": 29.846153846153847,
      "grad_norm": 2.097052812576294,
      "learning_rate": 3.134615384615385e-05,
      "loss": 0.5866,
      "step": 1901200
    },
    {
      "epoch": 29.847723704866564,
      "grad_norm": 4.617480278015137,
      "learning_rate": 3.13451726844584e-05,
      "loss": 0.5519,
      "step": 1901300
    },
    {
      "epoch": 29.849293563579277,
      "grad_norm": 4.122788906097412,
      "learning_rate": 3.134419152276295e-05,
      "loss": 0.5682,
      "step": 1901400
    },
    {
      "epoch": 29.850863422291994,
      "grad_norm": 3.8960540294647217,
      "learning_rate": 3.13432103610675e-05,
      "loss": 0.5993,
      "step": 1901500
    },
    {
      "epoch": 29.85243328100471,
      "grad_norm": 3.7238657474517822,
      "learning_rate": 3.134222919937206e-05,
      "loss": 0.6007,
      "step": 1901600
    },
    {
      "epoch": 29.854003139717424,
      "grad_norm": 5.606888771057129,
      "learning_rate": 3.134124803767661e-05,
      "loss": 0.5854,
      "step": 1901700
    },
    {
      "epoch": 29.85557299843014,
      "grad_norm": 3.31196665763855,
      "learning_rate": 3.134026687598116e-05,
      "loss": 0.6019,
      "step": 1901800
    },
    {
      "epoch": 29.857142857142858,
      "grad_norm": 4.004339694976807,
      "learning_rate": 3.133928571428571e-05,
      "loss": 0.6058,
      "step": 1901900
    },
    {
      "epoch": 29.858712715855575,
      "grad_norm": 3.673189163208008,
      "learning_rate": 3.133830455259027e-05,
      "loss": 0.5972,
      "step": 1902000
    },
    {
      "epoch": 29.860282574568288,
      "grad_norm": 4.211580276489258,
      "learning_rate": 3.133732339089482e-05,
      "loss": 0.6116,
      "step": 1902100
    },
    {
      "epoch": 29.861852433281005,
      "grad_norm": 3.6653361320495605,
      "learning_rate": 3.133634222919937e-05,
      "loss": 0.6156,
      "step": 1902200
    },
    {
      "epoch": 29.86342229199372,
      "grad_norm": 4.509166717529297,
      "learning_rate": 3.133536106750393e-05,
      "loss": 0.5919,
      "step": 1902300
    },
    {
      "epoch": 29.864992150706435,
      "grad_norm": 3.7600245475769043,
      "learning_rate": 3.133437990580848e-05,
      "loss": 0.6167,
      "step": 1902400
    },
    {
      "epoch": 29.86656200941915,
      "grad_norm": 3.155292510986328,
      "learning_rate": 3.133339874411303e-05,
      "loss": 0.6116,
      "step": 1902500
    },
    {
      "epoch": 29.86813186813187,
      "grad_norm": 3.8324520587921143,
      "learning_rate": 3.133241758241758e-05,
      "loss": 0.571,
      "step": 1902600
    },
    {
      "epoch": 29.869701726844585,
      "grad_norm": 3.672755241394043,
      "learning_rate": 3.133143642072214e-05,
      "loss": 0.5733,
      "step": 1902700
    },
    {
      "epoch": 29.8712715855573,
      "grad_norm": 3.5662617683410645,
      "learning_rate": 3.1330455259026685e-05,
      "loss": 0.609,
      "step": 1902800
    },
    {
      "epoch": 29.872841444270016,
      "grad_norm": 4.059433937072754,
      "learning_rate": 3.132947409733124e-05,
      "loss": 0.5782,
      "step": 1902900
    },
    {
      "epoch": 29.874411302982733,
      "grad_norm": 3.155303478240967,
      "learning_rate": 3.1328492935635793e-05,
      "loss": 0.6239,
      "step": 1903000
    },
    {
      "epoch": 29.875981161695446,
      "grad_norm": 4.120247840881348,
      "learning_rate": 3.132751177394035e-05,
      "loss": 0.6079,
      "step": 1903100
    },
    {
      "epoch": 29.877551020408163,
      "grad_norm": 4.569539546966553,
      "learning_rate": 3.1326530612244895e-05,
      "loss": 0.6255,
      "step": 1903200
    },
    {
      "epoch": 29.87912087912088,
      "grad_norm": 3.2323625087738037,
      "learning_rate": 3.132554945054945e-05,
      "loss": 0.5413,
      "step": 1903300
    },
    {
      "epoch": 29.880690737833596,
      "grad_norm": 4.266348361968994,
      "learning_rate": 3.1324568288854004e-05,
      "loss": 0.5859,
      "step": 1903400
    },
    {
      "epoch": 29.88226059654631,
      "grad_norm": 3.349665880203247,
      "learning_rate": 3.1323587127158555e-05,
      "loss": 0.5844,
      "step": 1903500
    },
    {
      "epoch": 29.883830455259027,
      "grad_norm": 3.595440149307251,
      "learning_rate": 3.1322605965463106e-05,
      "loss": 0.5738,
      "step": 1903600
    },
    {
      "epoch": 29.885400313971743,
      "grad_norm": 3.9782416820526123,
      "learning_rate": 3.1321624803767664e-05,
      "loss": 0.6005,
      "step": 1903700
    },
    {
      "epoch": 29.886970172684457,
      "grad_norm": 4.834331512451172,
      "learning_rate": 3.1320643642072215e-05,
      "loss": 0.6179,
      "step": 1903800
    },
    {
      "epoch": 29.888540031397174,
      "grad_norm": 3.2989156246185303,
      "learning_rate": 3.1319662480376766e-05,
      "loss": 0.5601,
      "step": 1903900
    },
    {
      "epoch": 29.89010989010989,
      "grad_norm": 3.4123313426971436,
      "learning_rate": 3.131868131868132e-05,
      "loss": 0.5757,
      "step": 1904000
    },
    {
      "epoch": 29.891679748822607,
      "grad_norm": 4.731511116027832,
      "learning_rate": 3.1317700156985875e-05,
      "loss": 0.5863,
      "step": 1904100
    },
    {
      "epoch": 29.89324960753532,
      "grad_norm": 4.0943169593811035,
      "learning_rate": 3.1316718995290425e-05,
      "loss": 0.548,
      "step": 1904200
    },
    {
      "epoch": 29.894819466248038,
      "grad_norm": 2.4195361137390137,
      "learning_rate": 3.1315737833594976e-05,
      "loss": 0.5948,
      "step": 1904300
    },
    {
      "epoch": 29.896389324960754,
      "grad_norm": 5.425191879272461,
      "learning_rate": 3.1314756671899534e-05,
      "loss": 0.5773,
      "step": 1904400
    },
    {
      "epoch": 29.897959183673468,
      "grad_norm": 2.9214982986450195,
      "learning_rate": 3.1313775510204085e-05,
      "loss": 0.5747,
      "step": 1904500
    },
    {
      "epoch": 29.899529042386185,
      "grad_norm": 3.7916910648345947,
      "learning_rate": 3.1312794348508636e-05,
      "loss": 0.6302,
      "step": 1904600
    },
    {
      "epoch": 29.9010989010989,
      "grad_norm": 2.990065336227417,
      "learning_rate": 3.131181318681319e-05,
      "loss": 0.6112,
      "step": 1904700
    },
    {
      "epoch": 29.90266875981162,
      "grad_norm": 3.194632053375244,
      "learning_rate": 3.1310832025117745e-05,
      "loss": 0.566,
      "step": 1904800
    },
    {
      "epoch": 29.90423861852433,
      "grad_norm": 3.9842123985290527,
      "learning_rate": 3.130985086342229e-05,
      "loss": 0.6438,
      "step": 1904900
    },
    {
      "epoch": 29.90580847723705,
      "grad_norm": 3.0248923301696777,
      "learning_rate": 3.130886970172685e-05,
      "loss": 0.5806,
      "step": 1905000
    },
    {
      "epoch": 29.907378335949765,
      "grad_norm": 3.6478497982025146,
      "learning_rate": 3.13078885400314e-05,
      "loss": 0.5467,
      "step": 1905100
    },
    {
      "epoch": 29.90894819466248,
      "grad_norm": 4.6151909828186035,
      "learning_rate": 3.1306907378335956e-05,
      "loss": 0.5988,
      "step": 1905200
    },
    {
      "epoch": 29.910518053375196,
      "grad_norm": 3.42179799079895,
      "learning_rate": 3.13059262166405e-05,
      "loss": 0.5826,
      "step": 1905300
    },
    {
      "epoch": 29.912087912087912,
      "grad_norm": 4.7384114265441895,
      "learning_rate": 3.130494505494506e-05,
      "loss": 0.5924,
      "step": 1905400
    },
    {
      "epoch": 29.91365777080063,
      "grad_norm": 3.4705841541290283,
      "learning_rate": 3.130396389324961e-05,
      "loss": 0.6027,
      "step": 1905500
    },
    {
      "epoch": 29.915227629513343,
      "grad_norm": 4.280548095703125,
      "learning_rate": 3.130298273155416e-05,
      "loss": 0.584,
      "step": 1905600
    },
    {
      "epoch": 29.91679748822606,
      "grad_norm": 4.335675239562988,
      "learning_rate": 3.130200156985871e-05,
      "loss": 0.5807,
      "step": 1905700
    },
    {
      "epoch": 29.918367346938776,
      "grad_norm": 3.622959613800049,
      "learning_rate": 3.130102040816327e-05,
      "loss": 0.5754,
      "step": 1905800
    },
    {
      "epoch": 29.919937205651493,
      "grad_norm": 3.9601035118103027,
      "learning_rate": 3.130003924646782e-05,
      "loss": 0.5952,
      "step": 1905900
    },
    {
      "epoch": 29.921507064364206,
      "grad_norm": 2.3533902168273926,
      "learning_rate": 3.129905808477237e-05,
      "loss": 0.604,
      "step": 1906000
    },
    {
      "epoch": 29.923076923076923,
      "grad_norm": 2.306739091873169,
      "learning_rate": 3.129807692307692e-05,
      "loss": 0.5738,
      "step": 1906100
    },
    {
      "epoch": 29.92464678178964,
      "grad_norm": 4.672026634216309,
      "learning_rate": 3.129709576138148e-05,
      "loss": 0.6218,
      "step": 1906200
    },
    {
      "epoch": 29.926216640502354,
      "grad_norm": 4.079353332519531,
      "learning_rate": 3.129611459968603e-05,
      "loss": 0.6185,
      "step": 1906300
    },
    {
      "epoch": 29.92778649921507,
      "grad_norm": 3.6919450759887695,
      "learning_rate": 3.129513343799058e-05,
      "loss": 0.5959,
      "step": 1906400
    },
    {
      "epoch": 29.929356357927787,
      "grad_norm": 3.5494537353515625,
      "learning_rate": 3.129415227629514e-05,
      "loss": 0.635,
      "step": 1906500
    },
    {
      "epoch": 29.9309262166405,
      "grad_norm": 3.303452253341675,
      "learning_rate": 3.129317111459969e-05,
      "loss": 0.5845,
      "step": 1906600
    },
    {
      "epoch": 29.932496075353217,
      "grad_norm": 2.716590404510498,
      "learning_rate": 3.129218995290424e-05,
      "loss": 0.6625,
      "step": 1906700
    },
    {
      "epoch": 29.934065934065934,
      "grad_norm": 5.266934394836426,
      "learning_rate": 3.129120879120879e-05,
      "loss": 0.5828,
      "step": 1906800
    },
    {
      "epoch": 29.93563579277865,
      "grad_norm": 3.0038607120513916,
      "learning_rate": 3.129022762951335e-05,
      "loss": 0.5664,
      "step": 1906900
    },
    {
      "epoch": 29.937205651491364,
      "grad_norm": 2.682101011276245,
      "learning_rate": 3.1289246467817894e-05,
      "loss": 0.6091,
      "step": 1907000
    },
    {
      "epoch": 29.93877551020408,
      "grad_norm": 4.056082248687744,
      "learning_rate": 3.128826530612245e-05,
      "loss": 0.5842,
      "step": 1907100
    },
    {
      "epoch": 29.940345368916798,
      "grad_norm": 5.07436466217041,
      "learning_rate": 3.1287284144427e-05,
      "loss": 0.5874,
      "step": 1907200
    },
    {
      "epoch": 29.941915227629515,
      "grad_norm": 3.2771852016448975,
      "learning_rate": 3.128630298273156e-05,
      "loss": 0.5894,
      "step": 1907300
    },
    {
      "epoch": 29.94348508634223,
      "grad_norm": 3.611999750137329,
      "learning_rate": 3.1285321821036104e-05,
      "loss": 0.6158,
      "step": 1907400
    },
    {
      "epoch": 29.945054945054945,
      "grad_norm": 4.4135541915893555,
      "learning_rate": 3.128434065934066e-05,
      "loss": 0.5333,
      "step": 1907500
    },
    {
      "epoch": 29.946624803767662,
      "grad_norm": 3.2161409854888916,
      "learning_rate": 3.128335949764521e-05,
      "loss": 0.5915,
      "step": 1907600
    },
    {
      "epoch": 29.948194662480375,
      "grad_norm": 3.0263233184814453,
      "learning_rate": 3.1282378335949764e-05,
      "loss": 0.6303,
      "step": 1907700
    },
    {
      "epoch": 29.949764521193092,
      "grad_norm": 3.1814475059509277,
      "learning_rate": 3.1281397174254315e-05,
      "loss": 0.6061,
      "step": 1907800
    },
    {
      "epoch": 29.95133437990581,
      "grad_norm": 4.003307819366455,
      "learning_rate": 3.128041601255887e-05,
      "loss": 0.6186,
      "step": 1907900
    },
    {
      "epoch": 29.952904238618526,
      "grad_norm": 3.9773569107055664,
      "learning_rate": 3.1279434850863424e-05,
      "loss": 0.6025,
      "step": 1908000
    },
    {
      "epoch": 29.95447409733124,
      "grad_norm": 3.8430521488189697,
      "learning_rate": 3.1278453689167975e-05,
      "loss": 0.5681,
      "step": 1908100
    },
    {
      "epoch": 29.956043956043956,
      "grad_norm": 4.304145336151123,
      "learning_rate": 3.1277472527472526e-05,
      "loss": 0.5805,
      "step": 1908200
    },
    {
      "epoch": 29.957613814756673,
      "grad_norm": 3.8677773475646973,
      "learning_rate": 3.1276491365777083e-05,
      "loss": 0.5888,
      "step": 1908300
    },
    {
      "epoch": 29.959183673469386,
      "grad_norm": 4.159830093383789,
      "learning_rate": 3.1275510204081634e-05,
      "loss": 0.5768,
      "step": 1908400
    },
    {
      "epoch": 29.960753532182103,
      "grad_norm": 3.8035130500793457,
      "learning_rate": 3.1274529042386185e-05,
      "loss": 0.5864,
      "step": 1908500
    },
    {
      "epoch": 29.96232339089482,
      "grad_norm": 4.480921745300293,
      "learning_rate": 3.127354788069074e-05,
      "loss": 0.5795,
      "step": 1908600
    },
    {
      "epoch": 29.963893249607537,
      "grad_norm": 3.92020583152771,
      "learning_rate": 3.1272566718995294e-05,
      "loss": 0.595,
      "step": 1908700
    },
    {
      "epoch": 29.96546310832025,
      "grad_norm": 3.807704448699951,
      "learning_rate": 3.1271585557299845e-05,
      "loss": 0.5668,
      "step": 1908800
    },
    {
      "epoch": 29.967032967032967,
      "grad_norm": 4.862771987915039,
      "learning_rate": 3.1270604395604396e-05,
      "loss": 0.6325,
      "step": 1908900
    },
    {
      "epoch": 29.968602825745684,
      "grad_norm": 2.9338815212249756,
      "learning_rate": 3.1269623233908954e-05,
      "loss": 0.5974,
      "step": 1909000
    },
    {
      "epoch": 29.970172684458397,
      "grad_norm": 4.003837585449219,
      "learning_rate": 3.12686420722135e-05,
      "loss": 0.5676,
      "step": 1909100
    },
    {
      "epoch": 29.971742543171114,
      "grad_norm": 5.336184024810791,
      "learning_rate": 3.1267660910518056e-05,
      "loss": 0.5798,
      "step": 1909200
    },
    {
      "epoch": 29.97331240188383,
      "grad_norm": 3.2366650104522705,
      "learning_rate": 3.126667974882261e-05,
      "loss": 0.5689,
      "step": 1909300
    },
    {
      "epoch": 29.974882260596548,
      "grad_norm": 2.8112361431121826,
      "learning_rate": 3.1265698587127165e-05,
      "loss": 0.6022,
      "step": 1909400
    },
    {
      "epoch": 29.97645211930926,
      "grad_norm": 2.7232887744903564,
      "learning_rate": 3.126471742543171e-05,
      "loss": 0.5729,
      "step": 1909500
    },
    {
      "epoch": 29.978021978021978,
      "grad_norm": 3.5378379821777344,
      "learning_rate": 3.1263736263736267e-05,
      "loss": 0.6007,
      "step": 1909600
    },
    {
      "epoch": 29.979591836734695,
      "grad_norm": 2.996530055999756,
      "learning_rate": 3.126275510204082e-05,
      "loss": 0.6027,
      "step": 1909700
    },
    {
      "epoch": 29.98116169544741,
      "grad_norm": 3.254542827606201,
      "learning_rate": 3.126177394034537e-05,
      "loss": 0.5664,
      "step": 1909800
    },
    {
      "epoch": 29.982731554160125,
      "grad_norm": 3.057971477508545,
      "learning_rate": 3.126079277864992e-05,
      "loss": 0.5519,
      "step": 1909900
    },
    {
      "epoch": 29.984301412872842,
      "grad_norm": 4.148421287536621,
      "learning_rate": 3.125981161695448e-05,
      "loss": 0.5841,
      "step": 1910000
    },
    {
      "epoch": 29.98587127158556,
      "grad_norm": 4.191372871398926,
      "learning_rate": 3.125883045525903e-05,
      "loss": 0.6025,
      "step": 1910100
    },
    {
      "epoch": 29.987441130298272,
      "grad_norm": 3.967714548110962,
      "learning_rate": 3.125784929356358e-05,
      "loss": 0.5776,
      "step": 1910200
    },
    {
      "epoch": 29.98901098901099,
      "grad_norm": 3.035384178161621,
      "learning_rate": 3.125686813186813e-05,
      "loss": 0.5818,
      "step": 1910300
    },
    {
      "epoch": 29.990580847723706,
      "grad_norm": 3.393050193786621,
      "learning_rate": 3.125588697017269e-05,
      "loss": 0.6299,
      "step": 1910400
    },
    {
      "epoch": 29.99215070643642,
      "grad_norm": 4.5452399253845215,
      "learning_rate": 3.125490580847724e-05,
      "loss": 0.5822,
      "step": 1910500
    },
    {
      "epoch": 29.993720565149136,
      "grad_norm": 2.652184247970581,
      "learning_rate": 3.125392464678179e-05,
      "loss": 0.574,
      "step": 1910600
    },
    {
      "epoch": 29.995290423861853,
      "grad_norm": 3.955326557159424,
      "learning_rate": 3.125294348508635e-05,
      "loss": 0.5935,
      "step": 1910700
    },
    {
      "epoch": 29.99686028257457,
      "grad_norm": 4.301224708557129,
      "learning_rate": 3.12519623233909e-05,
      "loss": 0.6143,
      "step": 1910800
    },
    {
      "epoch": 29.998430141287283,
      "grad_norm": 5.584810733795166,
      "learning_rate": 3.125098116169545e-05,
      "loss": 0.6092,
      "step": 1910900
    },
    {
      "epoch": 30.0,
      "grad_norm": 3.4400389194488525,
      "learning_rate": 3.125e-05,
      "loss": 0.6246,
      "step": 1911000
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.0235217809677124,
      "eval_runtime": 14.7473,
      "eval_samples_per_second": 227.363,
      "eval_steps_per_second": 227.363,
      "step": 1911000
    },
    {
      "epoch": 30.0,
      "eval_loss": 0.4534769356250763,
      "eval_runtime": 281.4784,
      "eval_samples_per_second": 226.305,
      "eval_steps_per_second": 226.305,
      "step": 1911000
    }
  ],
  "logging_steps": 100,
  "max_steps": 5096000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 80,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1182542699520000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
