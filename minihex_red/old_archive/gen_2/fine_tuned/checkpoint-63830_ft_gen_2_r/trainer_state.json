{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 63830,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007833307222309259,
      "grad_norm": 9.889351844787598,
      "learning_rate": 4.999347224398141e-05,
      "loss": 3.0309,
      "step": 100
    },
    {
      "epoch": 0.015666614444618518,
      "grad_norm": 7.452866554260254,
      "learning_rate": 4.998694448796282e-05,
      "loss": 2.6641,
      "step": 200
    },
    {
      "epoch": 0.023499921666927777,
      "grad_norm": 9.381114959716797,
      "learning_rate": 4.9980416731944226e-05,
      "loss": 2.4979,
      "step": 300
    },
    {
      "epoch": 0.031333228889237036,
      "grad_norm": 8.785229682922363,
      "learning_rate": 4.997388897592564e-05,
      "loss": 2.5016,
      "step": 400
    },
    {
      "epoch": 0.039166536111546295,
      "grad_norm": 7.8530378341674805,
      "learning_rate": 4.996736121990705e-05,
      "loss": 2.5435,
      "step": 500
    },
    {
      "epoch": 0.046999843333855554,
      "grad_norm": 7.387740612030029,
      "learning_rate": 4.996083346388846e-05,
      "loss": 2.4072,
      "step": 600
    },
    {
      "epoch": 0.05483315055616481,
      "grad_norm": 6.232944488525391,
      "learning_rate": 4.995430570786986e-05,
      "loss": 2.3947,
      "step": 700
    },
    {
      "epoch": 0.06266645777847407,
      "grad_norm": 7.020564079284668,
      "learning_rate": 4.9947777951851276e-05,
      "loss": 2.3762,
      "step": 800
    },
    {
      "epoch": 0.07049976500078334,
      "grad_norm": 6.896212577819824,
      "learning_rate": 4.994125019583268e-05,
      "loss": 2.2451,
      "step": 900
    },
    {
      "epoch": 0.07833307222309259,
      "grad_norm": 7.043675899505615,
      "learning_rate": 4.9934722439814094e-05,
      "loss": 2.3266,
      "step": 1000
    },
    {
      "epoch": 0.08616637944540186,
      "grad_norm": 5.520274639129639,
      "learning_rate": 4.99281946837955e-05,
      "loss": 2.3942,
      "step": 1100
    },
    {
      "epoch": 0.09399968666771111,
      "grad_norm": 7.796804428100586,
      "learning_rate": 4.992166692777691e-05,
      "loss": 2.2616,
      "step": 1200
    },
    {
      "epoch": 0.10183299389002037,
      "grad_norm": 7.348849296569824,
      "learning_rate": 4.991513917175832e-05,
      "loss": 2.3218,
      "step": 1300
    },
    {
      "epoch": 0.10966630111232963,
      "grad_norm": 7.272326946258545,
      "learning_rate": 4.9908611415739724e-05,
      "loss": 2.3044,
      "step": 1400
    },
    {
      "epoch": 0.11749960833463889,
      "grad_norm": 8.055939674377441,
      "learning_rate": 4.9902083659721136e-05,
      "loss": 2.3589,
      "step": 1500
    },
    {
      "epoch": 0.12533291555694814,
      "grad_norm": 5.982602596282959,
      "learning_rate": 4.989555590370255e-05,
      "loss": 2.2864,
      "step": 1600
    },
    {
      "epoch": 0.1331662227792574,
      "grad_norm": 7.760680198669434,
      "learning_rate": 4.9889028147683955e-05,
      "loss": 2.2013,
      "step": 1700
    },
    {
      "epoch": 0.14099953000156668,
      "grad_norm": 6.712719440460205,
      "learning_rate": 4.988250039166537e-05,
      "loss": 2.2972,
      "step": 1800
    },
    {
      "epoch": 0.1488328372238759,
      "grad_norm": 7.971512317657471,
      "learning_rate": 4.987597263564677e-05,
      "loss": 2.2711,
      "step": 1900
    },
    {
      "epoch": 0.15666614444618518,
      "grad_norm": 7.198053359985352,
      "learning_rate": 4.986944487962818e-05,
      "loss": 2.2437,
      "step": 2000
    },
    {
      "epoch": 0.16449945166849445,
      "grad_norm": 6.670984268188477,
      "learning_rate": 4.986291712360959e-05,
      "loss": 2.2549,
      "step": 2100
    },
    {
      "epoch": 0.1723327588908037,
      "grad_norm": 6.8612470626831055,
      "learning_rate": 4.9856389367591e-05,
      "loss": 2.2321,
      "step": 2200
    },
    {
      "epoch": 0.18016606611311295,
      "grad_norm": 6.981351375579834,
      "learning_rate": 4.984986161157241e-05,
      "loss": 2.3233,
      "step": 2300
    },
    {
      "epoch": 0.18799937333542222,
      "grad_norm": 5.5516815185546875,
      "learning_rate": 4.984333385555382e-05,
      "loss": 2.1512,
      "step": 2400
    },
    {
      "epoch": 0.19583268055773148,
      "grad_norm": 6.162785530090332,
      "learning_rate": 4.983680609953523e-05,
      "loss": 2.3066,
      "step": 2500
    },
    {
      "epoch": 0.20366598778004075,
      "grad_norm": 5.558867931365967,
      "learning_rate": 4.9830278343516634e-05,
      "loss": 2.3165,
      "step": 2600
    },
    {
      "epoch": 0.21149929500234999,
      "grad_norm": 6.5229411125183105,
      "learning_rate": 4.982375058749804e-05,
      "loss": 2.2614,
      "step": 2700
    },
    {
      "epoch": 0.21933260222465925,
      "grad_norm": 5.639630317687988,
      "learning_rate": 4.981722283147945e-05,
      "loss": 2.2394,
      "step": 2800
    },
    {
      "epoch": 0.22716590944696852,
      "grad_norm": 6.924002170562744,
      "learning_rate": 4.9810695075460865e-05,
      "loss": 2.2342,
      "step": 2900
    },
    {
      "epoch": 0.23499921666927778,
      "grad_norm": 6.589237213134766,
      "learning_rate": 4.980416731944227e-05,
      "loss": 2.2822,
      "step": 3000
    },
    {
      "epoch": 0.24283252389158702,
      "grad_norm": 8.434619903564453,
      "learning_rate": 4.979763956342368e-05,
      "loss": 2.2336,
      "step": 3100
    },
    {
      "epoch": 0.2506658311138963,
      "grad_norm": 7.7639875411987305,
      "learning_rate": 4.979111180740509e-05,
      "loss": 2.3198,
      "step": 3200
    },
    {
      "epoch": 0.2584991383362055,
      "grad_norm": 7.102499485015869,
      "learning_rate": 4.9784584051386494e-05,
      "loss": 2.1374,
      "step": 3300
    },
    {
      "epoch": 0.2663324455585148,
      "grad_norm": 6.8900837898254395,
      "learning_rate": 4.977805629536791e-05,
      "loss": 2.2547,
      "step": 3400
    },
    {
      "epoch": 0.27416575278082406,
      "grad_norm": 8.312372207641602,
      "learning_rate": 4.977152853934931e-05,
      "loss": 2.207,
      "step": 3500
    },
    {
      "epoch": 0.28199906000313335,
      "grad_norm": 5.141456604003906,
      "learning_rate": 4.9765000783330725e-05,
      "loss": 2.2249,
      "step": 3600
    },
    {
      "epoch": 0.2898323672254426,
      "grad_norm": 7.200782299041748,
      "learning_rate": 4.975847302731214e-05,
      "loss": 2.2484,
      "step": 3700
    },
    {
      "epoch": 0.2976656744477518,
      "grad_norm": 6.014896869659424,
      "learning_rate": 4.9751945271293544e-05,
      "loss": 2.136,
      "step": 3800
    },
    {
      "epoch": 0.3054989816700611,
      "grad_norm": 5.487762451171875,
      "learning_rate": 4.974541751527495e-05,
      "loss": 2.143,
      "step": 3900
    },
    {
      "epoch": 0.31333228889237036,
      "grad_norm": 6.315128326416016,
      "learning_rate": 4.973888975925636e-05,
      "loss": 2.2679,
      "step": 4000
    },
    {
      "epoch": 0.3211655961146796,
      "grad_norm": 7.192989349365234,
      "learning_rate": 4.973236200323777e-05,
      "loss": 2.2197,
      "step": 4100
    },
    {
      "epoch": 0.3289989033369889,
      "grad_norm": 5.973138332366943,
      "learning_rate": 4.972583424721918e-05,
      "loss": 2.1963,
      "step": 4200
    },
    {
      "epoch": 0.33683221055929813,
      "grad_norm": 6.011201858520508,
      "learning_rate": 4.9719306491200586e-05,
      "loss": 2.2118,
      "step": 4300
    },
    {
      "epoch": 0.3446655177816074,
      "grad_norm": 5.40524435043335,
      "learning_rate": 4.9712778735182e-05,
      "loss": 2.1853,
      "step": 4400
    },
    {
      "epoch": 0.35249882500391666,
      "grad_norm": 5.945638179779053,
      "learning_rate": 4.9706250979163404e-05,
      "loss": 2.2803,
      "step": 4500
    },
    {
      "epoch": 0.3603321322262259,
      "grad_norm": 5.7043938636779785,
      "learning_rate": 4.969972322314481e-05,
      "loss": 2.2027,
      "step": 4600
    },
    {
      "epoch": 0.3681654394485352,
      "grad_norm": 5.165133953094482,
      "learning_rate": 4.969319546712622e-05,
      "loss": 2.1998,
      "step": 4700
    },
    {
      "epoch": 0.37599874667084443,
      "grad_norm": 8.49649715423584,
      "learning_rate": 4.9686667711107635e-05,
      "loss": 2.1778,
      "step": 4800
    },
    {
      "epoch": 0.38383205389315367,
      "grad_norm": 6.4036760330200195,
      "learning_rate": 4.968013995508904e-05,
      "loss": 2.1969,
      "step": 4900
    },
    {
      "epoch": 0.39166536111546296,
      "grad_norm": 6.462640285491943,
      "learning_rate": 4.9673612199070453e-05,
      "loss": 2.1526,
      "step": 5000
    },
    {
      "epoch": 0.3994986683377722,
      "grad_norm": 7.625897407531738,
      "learning_rate": 4.966708444305186e-05,
      "loss": 2.2266,
      "step": 5100
    },
    {
      "epoch": 0.4073319755600815,
      "grad_norm": 6.159130573272705,
      "learning_rate": 4.9660556687033265e-05,
      "loss": 2.1995,
      "step": 5200
    },
    {
      "epoch": 0.41516528278239073,
      "grad_norm": 6.393376350402832,
      "learning_rate": 4.965402893101468e-05,
      "loss": 2.2454,
      "step": 5300
    },
    {
      "epoch": 0.42299859000469997,
      "grad_norm": 5.1790337562561035,
      "learning_rate": 4.964750117499608e-05,
      "loss": 2.1638,
      "step": 5400
    },
    {
      "epoch": 0.43083189722700926,
      "grad_norm": 6.204867362976074,
      "learning_rate": 4.9640973418977496e-05,
      "loss": 2.2506,
      "step": 5500
    },
    {
      "epoch": 0.4386652044493185,
      "grad_norm": 6.411115646362305,
      "learning_rate": 4.963444566295891e-05,
      "loss": 2.1712,
      "step": 5600
    },
    {
      "epoch": 0.44649851167162774,
      "grad_norm": 6.3819074630737305,
      "learning_rate": 4.9627917906940314e-05,
      "loss": 2.2672,
      "step": 5700
    },
    {
      "epoch": 0.45433181889393703,
      "grad_norm": 6.014433860778809,
      "learning_rate": 4.962139015092172e-05,
      "loss": 2.1711,
      "step": 5800
    },
    {
      "epoch": 0.46216512611624627,
      "grad_norm": 6.4459757804870605,
      "learning_rate": 4.9614862394903126e-05,
      "loss": 2.1662,
      "step": 5900
    },
    {
      "epoch": 0.46999843333855557,
      "grad_norm": 5.916441917419434,
      "learning_rate": 4.960833463888454e-05,
      "loss": 2.2356,
      "step": 6000
    },
    {
      "epoch": 0.4778317405608648,
      "grad_norm": 6.845850944519043,
      "learning_rate": 4.960180688286595e-05,
      "loss": 2.1019,
      "step": 6100
    },
    {
      "epoch": 0.48566504778317404,
      "grad_norm": 6.905365943908691,
      "learning_rate": 4.9595279126847357e-05,
      "loss": 2.2058,
      "step": 6200
    },
    {
      "epoch": 0.49349835500548334,
      "grad_norm": 5.011289596557617,
      "learning_rate": 4.958875137082877e-05,
      "loss": 2.157,
      "step": 6300
    },
    {
      "epoch": 0.5013316622277926,
      "grad_norm": 5.865724086761475,
      "learning_rate": 4.9582223614810175e-05,
      "loss": 2.1079,
      "step": 6400
    },
    {
      "epoch": 0.5091649694501018,
      "grad_norm": 6.625700950622559,
      "learning_rate": 4.957569585879158e-05,
      "loss": 2.1603,
      "step": 6500
    },
    {
      "epoch": 0.516998276672411,
      "grad_norm": 6.243629455566406,
      "learning_rate": 4.956916810277299e-05,
      "loss": 2.0854,
      "step": 6600
    },
    {
      "epoch": 0.5248315838947204,
      "grad_norm": 6.422800540924072,
      "learning_rate": 4.95626403467544e-05,
      "loss": 2.2297,
      "step": 6700
    },
    {
      "epoch": 0.5326648911170296,
      "grad_norm": 7.740489482879639,
      "learning_rate": 4.955611259073581e-05,
      "loss": 2.1884,
      "step": 6800
    },
    {
      "epoch": 0.5404981983393389,
      "grad_norm": 6.726064205169678,
      "learning_rate": 4.9549584834717224e-05,
      "loss": 2.1747,
      "step": 6900
    },
    {
      "epoch": 0.5483315055616481,
      "grad_norm": 6.533786296844482,
      "learning_rate": 4.954305707869863e-05,
      "loss": 2.1797,
      "step": 7000
    },
    {
      "epoch": 0.5561648127839574,
      "grad_norm": 6.425151824951172,
      "learning_rate": 4.9536529322680036e-05,
      "loss": 2.2604,
      "step": 7100
    },
    {
      "epoch": 0.5639981200062667,
      "grad_norm": 8.320115089416504,
      "learning_rate": 4.953000156666145e-05,
      "loss": 2.1385,
      "step": 7200
    },
    {
      "epoch": 0.5718314272285759,
      "grad_norm": 7.871435165405273,
      "learning_rate": 4.9523473810642854e-05,
      "loss": 2.1674,
      "step": 7300
    },
    {
      "epoch": 0.5796647344508852,
      "grad_norm": 4.779173374176025,
      "learning_rate": 4.9516946054624266e-05,
      "loss": 2.1226,
      "step": 7400
    },
    {
      "epoch": 0.5874980416731944,
      "grad_norm": 7.8425092697143555,
      "learning_rate": 4.951041829860568e-05,
      "loss": 2.1277,
      "step": 7500
    },
    {
      "epoch": 0.5953313488955037,
      "grad_norm": 7.394635200500488,
      "learning_rate": 4.9503890542587085e-05,
      "loss": 2.1736,
      "step": 7600
    },
    {
      "epoch": 0.6031646561178129,
      "grad_norm": 6.965882301330566,
      "learning_rate": 4.949736278656849e-05,
      "loss": 2.0679,
      "step": 7700
    },
    {
      "epoch": 0.6109979633401222,
      "grad_norm": 6.668418884277344,
      "learning_rate": 4.9490835030549896e-05,
      "loss": 2.1238,
      "step": 7800
    },
    {
      "epoch": 0.6188312705624315,
      "grad_norm": 7.177570343017578,
      "learning_rate": 4.948430727453131e-05,
      "loss": 2.1788,
      "step": 7900
    },
    {
      "epoch": 0.6266645777847407,
      "grad_norm": 5.8232622146606445,
      "learning_rate": 4.947777951851272e-05,
      "loss": 2.091,
      "step": 8000
    },
    {
      "epoch": 0.63449788500705,
      "grad_norm": 6.408426761627197,
      "learning_rate": 4.947125176249413e-05,
      "loss": 2.157,
      "step": 8100
    },
    {
      "epoch": 0.6423311922293592,
      "grad_norm": 5.913483142852783,
      "learning_rate": 4.946472400647554e-05,
      "loss": 2.092,
      "step": 8200
    },
    {
      "epoch": 0.6501644994516685,
      "grad_norm": 7.382790565490723,
      "learning_rate": 4.9458196250456945e-05,
      "loss": 2.0705,
      "step": 8300
    },
    {
      "epoch": 0.6579978066739778,
      "grad_norm": 7.897890567779541,
      "learning_rate": 4.945166849443835e-05,
      "loss": 2.1063,
      "step": 8400
    },
    {
      "epoch": 0.665831113896287,
      "grad_norm": 7.480864524841309,
      "learning_rate": 4.9445140738419764e-05,
      "loss": 2.2236,
      "step": 8500
    },
    {
      "epoch": 0.6736644211185963,
      "grad_norm": 6.077307224273682,
      "learning_rate": 4.943861298240117e-05,
      "loss": 2.2618,
      "step": 8600
    },
    {
      "epoch": 0.6814977283409055,
      "grad_norm": 6.6552886962890625,
      "learning_rate": 4.943208522638258e-05,
      "loss": 2.0947,
      "step": 8700
    },
    {
      "epoch": 0.6893310355632148,
      "grad_norm": 6.623555660247803,
      "learning_rate": 4.9425557470363995e-05,
      "loss": 2.1316,
      "step": 8800
    },
    {
      "epoch": 0.6971643427855241,
      "grad_norm": 6.651762008666992,
      "learning_rate": 4.94190297143454e-05,
      "loss": 2.1266,
      "step": 8900
    },
    {
      "epoch": 0.7049976500078333,
      "grad_norm": 5.725539207458496,
      "learning_rate": 4.9412501958326806e-05,
      "loss": 2.2154,
      "step": 9000
    },
    {
      "epoch": 0.7128309572301426,
      "grad_norm": 7.1718854904174805,
      "learning_rate": 4.940597420230822e-05,
      "loss": 2.1298,
      "step": 9100
    },
    {
      "epoch": 0.7206642644524518,
      "grad_norm": 8.624910354614258,
      "learning_rate": 4.9399446446289624e-05,
      "loss": 2.0753,
      "step": 9200
    },
    {
      "epoch": 0.728497571674761,
      "grad_norm": 6.356025218963623,
      "learning_rate": 4.939291869027104e-05,
      "loss": 2.1346,
      "step": 9300
    },
    {
      "epoch": 0.7363308788970704,
      "grad_norm": 6.011891841888428,
      "learning_rate": 4.938639093425244e-05,
      "loss": 2.1031,
      "step": 9400
    },
    {
      "epoch": 0.7441641861193796,
      "grad_norm": 6.387389659881592,
      "learning_rate": 4.9379863178233855e-05,
      "loss": 2.1353,
      "step": 9500
    },
    {
      "epoch": 0.7519974933416889,
      "grad_norm": 7.592201232910156,
      "learning_rate": 4.937333542221526e-05,
      "loss": 2.1348,
      "step": 9600
    },
    {
      "epoch": 0.7598308005639981,
      "grad_norm": 6.814454078674316,
      "learning_rate": 4.936680766619667e-05,
      "loss": 2.1739,
      "step": 9700
    },
    {
      "epoch": 0.7676641077863073,
      "grad_norm": 8.172882080078125,
      "learning_rate": 4.936027991017808e-05,
      "loss": 2.0806,
      "step": 9800
    },
    {
      "epoch": 0.7754974150086167,
      "grad_norm": 6.620243072509766,
      "learning_rate": 4.9353752154159485e-05,
      "loss": 2.1625,
      "step": 9900
    },
    {
      "epoch": 0.7833307222309259,
      "grad_norm": 6.795835971832275,
      "learning_rate": 4.93472243981409e-05,
      "loss": 2.2624,
      "step": 10000
    },
    {
      "epoch": 0.7911640294532352,
      "grad_norm": 6.765786647796631,
      "learning_rate": 4.934069664212231e-05,
      "loss": 2.2173,
      "step": 10100
    },
    {
      "epoch": 0.7989973366755444,
      "grad_norm": 7.465338706970215,
      "learning_rate": 4.9334168886103716e-05,
      "loss": 2.1376,
      "step": 10200
    },
    {
      "epoch": 0.8068306438978536,
      "grad_norm": 6.440117359161377,
      "learning_rate": 4.932764113008512e-05,
      "loss": 2.0497,
      "step": 10300
    },
    {
      "epoch": 0.814663951120163,
      "grad_norm": 6.074404239654541,
      "learning_rate": 4.9321113374066534e-05,
      "loss": 2.1178,
      "step": 10400
    },
    {
      "epoch": 0.8224972583424722,
      "grad_norm": 6.882252216339111,
      "learning_rate": 4.931458561804794e-05,
      "loss": 2.0777,
      "step": 10500
    },
    {
      "epoch": 0.8303305655647815,
      "grad_norm": 6.638382434844971,
      "learning_rate": 4.930805786202935e-05,
      "loss": 2.1628,
      "step": 10600
    },
    {
      "epoch": 0.8381638727870907,
      "grad_norm": 7.3805766105651855,
      "learning_rate": 4.9301530106010765e-05,
      "loss": 2.1373,
      "step": 10700
    },
    {
      "epoch": 0.8459971800093999,
      "grad_norm": 6.840793132781982,
      "learning_rate": 4.929500234999217e-05,
      "loss": 2.0826,
      "step": 10800
    },
    {
      "epoch": 0.8538304872317092,
      "grad_norm": 5.570561408996582,
      "learning_rate": 4.928847459397358e-05,
      "loss": 2.0634,
      "step": 10900
    },
    {
      "epoch": 0.8616637944540185,
      "grad_norm": 6.93506383895874,
      "learning_rate": 4.928194683795498e-05,
      "loss": 2.0885,
      "step": 11000
    },
    {
      "epoch": 0.8694971016763278,
      "grad_norm": 9.51860237121582,
      "learning_rate": 4.9275419081936395e-05,
      "loss": 2.2345,
      "step": 11100
    },
    {
      "epoch": 0.877330408898637,
      "grad_norm": 5.752577304840088,
      "learning_rate": 4.926889132591781e-05,
      "loss": 2.1826,
      "step": 11200
    },
    {
      "epoch": 0.8851637161209462,
      "grad_norm": 5.814443588256836,
      "learning_rate": 4.926236356989921e-05,
      "loss": 2.2044,
      "step": 11300
    },
    {
      "epoch": 0.8929970233432555,
      "grad_norm": 6.684855937957764,
      "learning_rate": 4.9255835813880626e-05,
      "loss": 2.0938,
      "step": 11400
    },
    {
      "epoch": 0.9008303305655648,
      "grad_norm": 8.385662078857422,
      "learning_rate": 4.924930805786203e-05,
      "loss": 2.2565,
      "step": 11500
    },
    {
      "epoch": 0.9086636377878741,
      "grad_norm": 5.691100597381592,
      "learning_rate": 4.924278030184344e-05,
      "loss": 2.0838,
      "step": 11600
    },
    {
      "epoch": 0.9164969450101833,
      "grad_norm": 7.553969860076904,
      "learning_rate": 4.923625254582485e-05,
      "loss": 2.0709,
      "step": 11700
    },
    {
      "epoch": 0.9243302522324925,
      "grad_norm": 6.7658467292785645,
      "learning_rate": 4.9229724789806256e-05,
      "loss": 2.0961,
      "step": 11800
    },
    {
      "epoch": 0.9321635594548018,
      "grad_norm": 8.197063446044922,
      "learning_rate": 4.922319703378767e-05,
      "loss": 2.0842,
      "step": 11900
    },
    {
      "epoch": 0.9399968666771111,
      "grad_norm": 7.251062393188477,
      "learning_rate": 4.921666927776908e-05,
      "loss": 2.1333,
      "step": 12000
    },
    {
      "epoch": 0.9478301738994204,
      "grad_norm": 5.568418025970459,
      "learning_rate": 4.921014152175049e-05,
      "loss": 2.2254,
      "step": 12100
    },
    {
      "epoch": 0.9556634811217296,
      "grad_norm": 5.666238784790039,
      "learning_rate": 4.920361376573189e-05,
      "loss": 2.1831,
      "step": 12200
    },
    {
      "epoch": 0.9634967883440388,
      "grad_norm": 8.039314270019531,
      "learning_rate": 4.9197086009713305e-05,
      "loss": 2.0638,
      "step": 12300
    },
    {
      "epoch": 0.9713300955663481,
      "grad_norm": 6.114645957946777,
      "learning_rate": 4.919055825369471e-05,
      "loss": 2.1161,
      "step": 12400
    },
    {
      "epoch": 0.9791634027886573,
      "grad_norm": 6.543319225311279,
      "learning_rate": 4.918403049767612e-05,
      "loss": 2.1643,
      "step": 12500
    },
    {
      "epoch": 0.9869967100109667,
      "grad_norm": 5.581549644470215,
      "learning_rate": 4.9177502741657536e-05,
      "loss": 2.1026,
      "step": 12600
    },
    {
      "epoch": 0.9948300172332759,
      "grad_norm": 7.20421838760376,
      "learning_rate": 4.917097498563894e-05,
      "loss": 2.1199,
      "step": 12700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.0184824466705322,
      "eval_runtime": 2.8834,
      "eval_samples_per_second": 233.058,
      "eval_steps_per_second": 233.058,
      "step": 12766
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9163345098495483,
      "eval_runtime": 35.35,
      "eval_samples_per_second": 361.132,
      "eval_steps_per_second": 361.132,
      "step": 12766
    },
    {
      "epoch": 1.0026633244555851,
      "grad_norm": 6.324655055999756,
      "learning_rate": 4.916444722962035e-05,
      "loss": 2.0696,
      "step": 12800
    },
    {
      "epoch": 1.0104966316778945,
      "grad_norm": 6.8038649559021,
      "learning_rate": 4.915791947360175e-05,
      "loss": 2.1152,
      "step": 12900
    },
    {
      "epoch": 1.0183299389002036,
      "grad_norm": 7.042905330657959,
      "learning_rate": 4.9151391717583166e-05,
      "loss": 2.0619,
      "step": 13000
    },
    {
      "epoch": 1.026163246122513,
      "grad_norm": 6.275902271270752,
      "learning_rate": 4.914486396156457e-05,
      "loss": 2.034,
      "step": 13100
    },
    {
      "epoch": 1.033996553344822,
      "grad_norm": 6.379561901092529,
      "learning_rate": 4.9138336205545984e-05,
      "loss": 2.0028,
      "step": 13200
    },
    {
      "epoch": 1.0418298605671314,
      "grad_norm": 5.969287872314453,
      "learning_rate": 4.9131808449527396e-05,
      "loss": 2.0751,
      "step": 13300
    },
    {
      "epoch": 1.0496631677894408,
      "grad_norm": 7.606876850128174,
      "learning_rate": 4.91252806935088e-05,
      "loss": 2.0818,
      "step": 13400
    },
    {
      "epoch": 1.05749647501175,
      "grad_norm": 7.6151909828186035,
      "learning_rate": 4.911875293749021e-05,
      "loss": 2.0422,
      "step": 13500
    },
    {
      "epoch": 1.0653297822340593,
      "grad_norm": 5.601588249206543,
      "learning_rate": 4.911222518147162e-05,
      "loss": 2.1089,
      "step": 13600
    },
    {
      "epoch": 1.0731630894563684,
      "grad_norm": 6.8966569900512695,
      "learning_rate": 4.9105697425453026e-05,
      "loss": 2.0776,
      "step": 13700
    },
    {
      "epoch": 1.0809963966786778,
      "grad_norm": 7.150012493133545,
      "learning_rate": 4.909916966943444e-05,
      "loss": 2.0679,
      "step": 13800
    },
    {
      "epoch": 1.088829703900987,
      "grad_norm": 7.827211380004883,
      "learning_rate": 4.909264191341585e-05,
      "loss": 2.0861,
      "step": 13900
    },
    {
      "epoch": 1.0966630111232962,
      "grad_norm": 4.198551177978516,
      "learning_rate": 4.908611415739726e-05,
      "loss": 2.0563,
      "step": 14000
    },
    {
      "epoch": 1.1044963183456056,
      "grad_norm": 7.628843307495117,
      "learning_rate": 4.907958640137866e-05,
      "loss": 2.0735,
      "step": 14100
    },
    {
      "epoch": 1.1123296255679147,
      "grad_norm": 7.192502498626709,
      "learning_rate": 4.9073058645360076e-05,
      "loss": 2.0454,
      "step": 14200
    },
    {
      "epoch": 1.120162932790224,
      "grad_norm": 5.716953277587891,
      "learning_rate": 4.906653088934148e-05,
      "loss": 2.0766,
      "step": 14300
    },
    {
      "epoch": 1.1279962400125334,
      "grad_norm": 6.243908882141113,
      "learning_rate": 4.9060003133322894e-05,
      "loss": 2.0622,
      "step": 14400
    },
    {
      "epoch": 1.1358295472348425,
      "grad_norm": 8.28734302520752,
      "learning_rate": 4.90534753773043e-05,
      "loss": 1.9754,
      "step": 14500
    },
    {
      "epoch": 1.1436628544571519,
      "grad_norm": 6.853830337524414,
      "learning_rate": 4.904694762128571e-05,
      "loss": 2.1,
      "step": 14600
    },
    {
      "epoch": 1.151496161679461,
      "grad_norm": 7.495238304138184,
      "learning_rate": 4.904041986526712e-05,
      "loss": 2.1058,
      "step": 14700
    },
    {
      "epoch": 1.1593294689017704,
      "grad_norm": 6.801480770111084,
      "learning_rate": 4.9033892109248524e-05,
      "loss": 2.141,
      "step": 14800
    },
    {
      "epoch": 1.1671627761240795,
      "grad_norm": 5.7169084548950195,
      "learning_rate": 4.9027364353229936e-05,
      "loss": 2.0617,
      "step": 14900
    },
    {
      "epoch": 1.1749960833463888,
      "grad_norm": 9.674238204956055,
      "learning_rate": 4.902083659721134e-05,
      "loss": 2.0174,
      "step": 15000
    },
    {
      "epoch": 1.1828293905686982,
      "grad_norm": 6.4778594970703125,
      "learning_rate": 4.9014308841192755e-05,
      "loss": 2.0422,
      "step": 15100
    },
    {
      "epoch": 1.1906626977910073,
      "grad_norm": 7.032618999481201,
      "learning_rate": 4.900778108517417e-05,
      "loss": 2.0813,
      "step": 15200
    },
    {
      "epoch": 1.1984960050133167,
      "grad_norm": 6.965595245361328,
      "learning_rate": 4.900125332915557e-05,
      "loss": 2.0628,
      "step": 15300
    },
    {
      "epoch": 1.206329312235626,
      "grad_norm": 7.475944519042969,
      "learning_rate": 4.899472557313698e-05,
      "loss": 2.1108,
      "step": 15400
    },
    {
      "epoch": 1.2141626194579351,
      "grad_norm": 5.627975940704346,
      "learning_rate": 4.898819781711839e-05,
      "loss": 2.0579,
      "step": 15500
    },
    {
      "epoch": 1.2219959266802445,
      "grad_norm": 7.763607025146484,
      "learning_rate": 4.89816700610998e-05,
      "loss": 2.1512,
      "step": 15600
    },
    {
      "epoch": 1.2298292339025536,
      "grad_norm": 5.936602592468262,
      "learning_rate": 4.897514230508121e-05,
      "loss": 2.0644,
      "step": 15700
    },
    {
      "epoch": 1.237662541124863,
      "grad_norm": 7.0839457511901855,
      "learning_rate": 4.896861454906262e-05,
      "loss": 2.1471,
      "step": 15800
    },
    {
      "epoch": 1.245495848347172,
      "grad_norm": 5.713009357452393,
      "learning_rate": 4.896208679304403e-05,
      "loss": 2.1342,
      "step": 15900
    },
    {
      "epoch": 1.2533291555694814,
      "grad_norm": 5.165163040161133,
      "learning_rate": 4.8955559037025434e-05,
      "loss": 2.1524,
      "step": 16000
    },
    {
      "epoch": 1.2611624627917908,
      "grad_norm": 6.778467178344727,
      "learning_rate": 4.894903128100684e-05,
      "loss": 2.1168,
      "step": 16100
    },
    {
      "epoch": 1.2689957700141,
      "grad_norm": 6.9240498542785645,
      "learning_rate": 4.894250352498825e-05,
      "loss": 2.1147,
      "step": 16200
    },
    {
      "epoch": 1.2768290772364093,
      "grad_norm": 5.597258567810059,
      "learning_rate": 4.893597576896966e-05,
      "loss": 2.1285,
      "step": 16300
    },
    {
      "epoch": 1.2846623844587186,
      "grad_norm": 5.457404136657715,
      "learning_rate": 4.892944801295107e-05,
      "loss": 2.1304,
      "step": 16400
    },
    {
      "epoch": 1.2924956916810277,
      "grad_norm": 7.125478744506836,
      "learning_rate": 4.892292025693248e-05,
      "loss": 2.098,
      "step": 16500
    },
    {
      "epoch": 1.3003289989033369,
      "grad_norm": 5.870748996734619,
      "learning_rate": 4.891639250091389e-05,
      "loss": 2.1433,
      "step": 16600
    },
    {
      "epoch": 1.3081623061256462,
      "grad_norm": 6.4367475509643555,
      "learning_rate": 4.8909864744895294e-05,
      "loss": 2.0515,
      "step": 16700
    },
    {
      "epoch": 1.3159956133479556,
      "grad_norm": 5.998950004577637,
      "learning_rate": 4.890333698887671e-05,
      "loss": 2.0343,
      "step": 16800
    },
    {
      "epoch": 1.3238289205702647,
      "grad_norm": 7.076148986816406,
      "learning_rate": 4.889680923285811e-05,
      "loss": 2.0749,
      "step": 16900
    },
    {
      "epoch": 1.331662227792574,
      "grad_norm": 6.800472259521484,
      "learning_rate": 4.8890281476839525e-05,
      "loss": 2.0458,
      "step": 17000
    },
    {
      "epoch": 1.3394955350148834,
      "grad_norm": 10.454846382141113,
      "learning_rate": 4.888375372082094e-05,
      "loss": 2.1273,
      "step": 17100
    },
    {
      "epoch": 1.3473288422371925,
      "grad_norm": 6.171381950378418,
      "learning_rate": 4.8877225964802343e-05,
      "loss": 2.0819,
      "step": 17200
    },
    {
      "epoch": 1.3551621494595019,
      "grad_norm": 6.913208961486816,
      "learning_rate": 4.887069820878375e-05,
      "loss": 1.9963,
      "step": 17300
    },
    {
      "epoch": 1.362995456681811,
      "grad_norm": 5.486148357391357,
      "learning_rate": 4.886417045276516e-05,
      "loss": 2.0244,
      "step": 17400
    },
    {
      "epoch": 1.3708287639041203,
      "grad_norm": 4.900354385375977,
      "learning_rate": 4.885764269674657e-05,
      "loss": 2.0371,
      "step": 17500
    },
    {
      "epoch": 1.3786620711264295,
      "grad_norm": 5.089940547943115,
      "learning_rate": 4.885111494072798e-05,
      "loss": 2.1584,
      "step": 17600
    },
    {
      "epoch": 1.3864953783487388,
      "grad_norm": 7.07856559753418,
      "learning_rate": 4.8844587184709386e-05,
      "loss": 2.038,
      "step": 17700
    },
    {
      "epoch": 1.3943286855710482,
      "grad_norm": 6.936547756195068,
      "learning_rate": 4.88380594286908e-05,
      "loss": 2.2036,
      "step": 17800
    },
    {
      "epoch": 1.4021619927933573,
      "grad_norm": 7.00590705871582,
      "learning_rate": 4.8831531672672204e-05,
      "loss": 1.9643,
      "step": 17900
    },
    {
      "epoch": 1.4099953000156666,
      "grad_norm": 6.371906757354736,
      "learning_rate": 4.882500391665361e-05,
      "loss": 1.9948,
      "step": 18000
    },
    {
      "epoch": 1.417828607237976,
      "grad_norm": 7.553290367126465,
      "learning_rate": 4.881847616063502e-05,
      "loss": 2.008,
      "step": 18100
    },
    {
      "epoch": 1.4256619144602851,
      "grad_norm": 6.225111484527588,
      "learning_rate": 4.881194840461643e-05,
      "loss": 2.0832,
      "step": 18200
    },
    {
      "epoch": 1.4334952216825945,
      "grad_norm": 8.36999797821045,
      "learning_rate": 4.880542064859784e-05,
      "loss": 2.0074,
      "step": 18300
    },
    {
      "epoch": 1.4413285289049036,
      "grad_norm": 5.475383758544922,
      "learning_rate": 4.879889289257925e-05,
      "loss": 2.1561,
      "step": 18400
    },
    {
      "epoch": 1.449161836127213,
      "grad_norm": 5.472922325134277,
      "learning_rate": 4.879236513656066e-05,
      "loss": 2.0716,
      "step": 18500
    },
    {
      "epoch": 1.456995143349522,
      "grad_norm": 6.5317816734313965,
      "learning_rate": 4.8785837380542065e-05,
      "loss": 2.09,
      "step": 18600
    },
    {
      "epoch": 1.4648284505718314,
      "grad_norm": 6.865780353546143,
      "learning_rate": 4.877930962452348e-05,
      "loss": 2.0386,
      "step": 18700
    },
    {
      "epoch": 1.4726617577941408,
      "grad_norm": 6.855094909667969,
      "learning_rate": 4.877278186850488e-05,
      "loss": 2.001,
      "step": 18800
    },
    {
      "epoch": 1.48049506501645,
      "grad_norm": 5.902965545654297,
      "learning_rate": 4.8766254112486296e-05,
      "loss": 2.1725,
      "step": 18900
    },
    {
      "epoch": 1.4883283722387592,
      "grad_norm": 6.692051887512207,
      "learning_rate": 4.875972635646771e-05,
      "loss": 1.9901,
      "step": 19000
    },
    {
      "epoch": 1.4961616794610686,
      "grad_norm": 8.52558708190918,
      "learning_rate": 4.8753198600449114e-05,
      "loss": 2.0394,
      "step": 19100
    },
    {
      "epoch": 1.5039949866833777,
      "grad_norm": 8.117634773254395,
      "learning_rate": 4.874667084443052e-05,
      "loss": 2.1303,
      "step": 19200
    },
    {
      "epoch": 1.5118282939056868,
      "grad_norm": 7.810457229614258,
      "learning_rate": 4.874014308841193e-05,
      "loss": 2.136,
      "step": 19300
    },
    {
      "epoch": 1.5196616011279962,
      "grad_norm": 5.593348503112793,
      "learning_rate": 4.873361533239334e-05,
      "loss": 2.055,
      "step": 19400
    },
    {
      "epoch": 1.5274949083503055,
      "grad_norm": 7.206170082092285,
      "learning_rate": 4.8727087576374744e-05,
      "loss": 2.057,
      "step": 19500
    },
    {
      "epoch": 1.5353282155726147,
      "grad_norm": 5.396327495574951,
      "learning_rate": 4.8720559820356156e-05,
      "loss": 2.0391,
      "step": 19600
    },
    {
      "epoch": 1.543161522794924,
      "grad_norm": 6.542226314544678,
      "learning_rate": 4.871403206433757e-05,
      "loss": 2.0148,
      "step": 19700
    },
    {
      "epoch": 1.5509948300172334,
      "grad_norm": 12.691354751586914,
      "learning_rate": 4.8707504308318975e-05,
      "loss": 2.0416,
      "step": 19800
    },
    {
      "epoch": 1.5588281372395425,
      "grad_norm": 8.272102355957031,
      "learning_rate": 4.870097655230038e-05,
      "loss": 2.0266,
      "step": 19900
    },
    {
      "epoch": 1.5666614444618518,
      "grad_norm": 5.485255718231201,
      "learning_rate": 4.869444879628179e-05,
      "loss": 2.0448,
      "step": 20000
    },
    {
      "epoch": 1.5744947516841612,
      "grad_norm": 5.383591175079346,
      "learning_rate": 4.86879210402632e-05,
      "loss": 2.1448,
      "step": 20100
    },
    {
      "epoch": 1.5823280589064703,
      "grad_norm": 6.651650905609131,
      "learning_rate": 4.868139328424461e-05,
      "loss": 1.9991,
      "step": 20200
    },
    {
      "epoch": 1.5901613661287795,
      "grad_norm": 9.438446998596191,
      "learning_rate": 4.8674865528226024e-05,
      "loss": 2.082,
      "step": 20300
    },
    {
      "epoch": 1.5979946733510888,
      "grad_norm": 3.82412052154541,
      "learning_rate": 4.866833777220743e-05,
      "loss": 1.969,
      "step": 20400
    },
    {
      "epoch": 1.6058279805733982,
      "grad_norm": 7.242066860198975,
      "learning_rate": 4.8661810016188835e-05,
      "loss": 2.0063,
      "step": 20500
    },
    {
      "epoch": 1.6136612877957073,
      "grad_norm": 8.690362930297852,
      "learning_rate": 4.865528226017025e-05,
      "loss": 2.0296,
      "step": 20600
    },
    {
      "epoch": 1.6214945950180166,
      "grad_norm": 6.331714153289795,
      "learning_rate": 4.8648754504151654e-05,
      "loss": 1.9605,
      "step": 20700
    },
    {
      "epoch": 1.629327902240326,
      "grad_norm": 6.972808361053467,
      "learning_rate": 4.8642226748133066e-05,
      "loss": 2.0348,
      "step": 20800
    },
    {
      "epoch": 1.637161209462635,
      "grad_norm": 7.670622825622559,
      "learning_rate": 4.863569899211448e-05,
      "loss": 2.0513,
      "step": 20900
    },
    {
      "epoch": 1.6449945166849442,
      "grad_norm": 9.073395729064941,
      "learning_rate": 4.8629171236095885e-05,
      "loss": 2.0736,
      "step": 21000
    },
    {
      "epoch": 1.6528278239072538,
      "grad_norm": 10.389143943786621,
      "learning_rate": 4.862264348007729e-05,
      "loss": 2.0262,
      "step": 21100
    },
    {
      "epoch": 1.660661131129563,
      "grad_norm": 6.092132568359375,
      "learning_rate": 4.8616115724058696e-05,
      "loss": 2.0716,
      "step": 21200
    },
    {
      "epoch": 1.668494438351872,
      "grad_norm": 5.893816947937012,
      "learning_rate": 4.860958796804011e-05,
      "loss": 2.0805,
      "step": 21300
    },
    {
      "epoch": 1.6763277455741814,
      "grad_norm": 6.096791744232178,
      "learning_rate": 4.8603060212021514e-05,
      "loss": 2.1431,
      "step": 21400
    },
    {
      "epoch": 1.6841610527964908,
      "grad_norm": 6.354844093322754,
      "learning_rate": 4.859653245600293e-05,
      "loss": 2.0048,
      "step": 21500
    },
    {
      "epoch": 1.6919943600187999,
      "grad_norm": 6.280371189117432,
      "learning_rate": 4.859000469998434e-05,
      "loss": 2.119,
      "step": 21600
    },
    {
      "epoch": 1.6998276672411092,
      "grad_norm": 5.3791985511779785,
      "learning_rate": 4.8583476943965745e-05,
      "loss": 2.0455,
      "step": 21700
    },
    {
      "epoch": 1.7076609744634186,
      "grad_norm": 7.255735874176025,
      "learning_rate": 4.857694918794715e-05,
      "loss": 2.0669,
      "step": 21800
    },
    {
      "epoch": 1.7154942816857277,
      "grad_norm": 6.711012363433838,
      "learning_rate": 4.8570421431928564e-05,
      "loss": 1.9765,
      "step": 21900
    },
    {
      "epoch": 1.7233275889080368,
      "grad_norm": 7.545569896697998,
      "learning_rate": 4.856389367590997e-05,
      "loss": 2.0742,
      "step": 22000
    },
    {
      "epoch": 1.7311608961303462,
      "grad_norm": 5.11443567276001,
      "learning_rate": 4.855736591989138e-05,
      "loss": 2.0759,
      "step": 22100
    },
    {
      "epoch": 1.7389942033526555,
      "grad_norm": 7.697194576263428,
      "learning_rate": 4.8550838163872794e-05,
      "loss": 2.0744,
      "step": 22200
    },
    {
      "epoch": 1.7468275105749647,
      "grad_norm": 9.578873634338379,
      "learning_rate": 4.85443104078542e-05,
      "loss": 2.1073,
      "step": 22300
    },
    {
      "epoch": 1.754660817797274,
      "grad_norm": 5.920336723327637,
      "learning_rate": 4.8537782651835606e-05,
      "loss": 1.9827,
      "step": 22400
    },
    {
      "epoch": 1.7624941250195834,
      "grad_norm": 8.039865493774414,
      "learning_rate": 4.853125489581702e-05,
      "loss": 2.0524,
      "step": 22500
    },
    {
      "epoch": 1.7703274322418925,
      "grad_norm": 6.023388862609863,
      "learning_rate": 4.8524727139798424e-05,
      "loss": 2.0798,
      "step": 22600
    },
    {
      "epoch": 1.7781607394642018,
      "grad_norm": 4.93147611618042,
      "learning_rate": 4.851819938377983e-05,
      "loss": 2.0476,
      "step": 22700
    },
    {
      "epoch": 1.7859940466865112,
      "grad_norm": 4.799201965332031,
      "learning_rate": 4.851167162776124e-05,
      "loss": 1.9534,
      "step": 22800
    },
    {
      "epoch": 1.7938273539088203,
      "grad_norm": 5.937392234802246,
      "learning_rate": 4.8505143871742655e-05,
      "loss": 2.1058,
      "step": 22900
    },
    {
      "epoch": 1.8016606611311294,
      "grad_norm": 6.01974630355835,
      "learning_rate": 4.849861611572406e-05,
      "loss": 2.1789,
      "step": 23000
    },
    {
      "epoch": 1.8094939683534388,
      "grad_norm": 7.989223957061768,
      "learning_rate": 4.849208835970547e-05,
      "loss": 1.9841,
      "step": 23100
    },
    {
      "epoch": 1.8173272755757481,
      "grad_norm": 7.655680179595947,
      "learning_rate": 4.848556060368688e-05,
      "loss": 1.9712,
      "step": 23200
    },
    {
      "epoch": 1.8251605827980573,
      "grad_norm": 5.782766342163086,
      "learning_rate": 4.8479032847668285e-05,
      "loss": 2.0702,
      "step": 23300
    },
    {
      "epoch": 1.8329938900203666,
      "grad_norm": 7.8149638175964355,
      "learning_rate": 4.84725050916497e-05,
      "loss": 1.9852,
      "step": 23400
    },
    {
      "epoch": 1.840827197242676,
      "grad_norm": 6.177658557891846,
      "learning_rate": 4.846597733563111e-05,
      "loss": 2.0936,
      "step": 23500
    },
    {
      "epoch": 1.848660504464985,
      "grad_norm": 5.421632289886475,
      "learning_rate": 4.8459449579612516e-05,
      "loss": 2.0713,
      "step": 23600
    },
    {
      "epoch": 1.8564938116872942,
      "grad_norm": 6.877612590789795,
      "learning_rate": 4.845292182359392e-05,
      "loss": 2.0327,
      "step": 23700
    },
    {
      "epoch": 1.8643271189096038,
      "grad_norm": 7.927372932434082,
      "learning_rate": 4.8446394067575334e-05,
      "loss": 2.0463,
      "step": 23800
    },
    {
      "epoch": 1.872160426131913,
      "grad_norm": 6.331596851348877,
      "learning_rate": 4.843986631155674e-05,
      "loss": 2.0571,
      "step": 23900
    },
    {
      "epoch": 1.879993733354222,
      "grad_norm": 9.822507858276367,
      "learning_rate": 4.843333855553815e-05,
      "loss": 2.053,
      "step": 24000
    },
    {
      "epoch": 1.8878270405765314,
      "grad_norm": 6.760190486907959,
      "learning_rate": 4.8426810799519565e-05,
      "loss": 1.9627,
      "step": 24100
    },
    {
      "epoch": 1.8956603477988407,
      "grad_norm": 6.146420955657959,
      "learning_rate": 4.842028304350097e-05,
      "loss": 2.02,
      "step": 24200
    },
    {
      "epoch": 1.9034936550211499,
      "grad_norm": 5.902420997619629,
      "learning_rate": 4.8413755287482377e-05,
      "loss": 2.0475,
      "step": 24300
    },
    {
      "epoch": 1.9113269622434592,
      "grad_norm": 6.456366062164307,
      "learning_rate": 4.840722753146379e-05,
      "loss": 1.9368,
      "step": 24400
    },
    {
      "epoch": 1.9191602694657686,
      "grad_norm": 5.714493751525879,
      "learning_rate": 4.8400699775445195e-05,
      "loss": 2.023,
      "step": 24500
    },
    {
      "epoch": 1.9269935766880777,
      "grad_norm": 5.285961627960205,
      "learning_rate": 4.83941720194266e-05,
      "loss": 2.0366,
      "step": 24600
    },
    {
      "epoch": 1.9348268839103868,
      "grad_norm": 6.2441792488098145,
      "learning_rate": 4.838764426340801e-05,
      "loss": 2.0242,
      "step": 24700
    },
    {
      "epoch": 1.9426601911326964,
      "grad_norm": 7.852536201477051,
      "learning_rate": 4.8381116507389426e-05,
      "loss": 2.1591,
      "step": 24800
    },
    {
      "epoch": 1.9504934983550055,
      "grad_norm": 7.45185661315918,
      "learning_rate": 4.837458875137083e-05,
      "loss": 1.9926,
      "step": 24900
    },
    {
      "epoch": 1.9583268055773146,
      "grad_norm": 6.4149489402771,
      "learning_rate": 4.836806099535224e-05,
      "loss": 2.0424,
      "step": 25000
    },
    {
      "epoch": 1.966160112799624,
      "grad_norm": 6.064977169036865,
      "learning_rate": 4.836153323933365e-05,
      "loss": 2.0747,
      "step": 25100
    },
    {
      "epoch": 1.9739934200219333,
      "grad_norm": 8.704774856567383,
      "learning_rate": 4.8355005483315056e-05,
      "loss": 1.9715,
      "step": 25200
    },
    {
      "epoch": 1.9818267272442425,
      "grad_norm": 7.326056003570557,
      "learning_rate": 4.834847772729647e-05,
      "loss": 2.0528,
      "step": 25300
    },
    {
      "epoch": 1.9896600344665518,
      "grad_norm": 5.429208278656006,
      "learning_rate": 4.834194997127788e-05,
      "loss": 1.9533,
      "step": 25400
    },
    {
      "epoch": 1.9974933416888612,
      "grad_norm": 7.603304862976074,
      "learning_rate": 4.8335422215259286e-05,
      "loss": 2.0409,
      "step": 25500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9622827768325806,
      "eval_runtime": 1.4403,
      "eval_samples_per_second": 466.554,
      "eval_steps_per_second": 466.554,
      "step": 25532
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.833534598350525,
      "eval_runtime": 27.7453,
      "eval_samples_per_second": 460.114,
      "eval_steps_per_second": 460.114,
      "step": 25532
    },
    {
      "epoch": 2.0053266489111703,
      "grad_norm": 5.651968955993652,
      "learning_rate": 4.832889445924069e-05,
      "loss": 1.9628,
      "step": 25600
    },
    {
      "epoch": 2.0131599561334794,
      "grad_norm": 7.000604629516602,
      "learning_rate": 4.8322366703222105e-05,
      "loss": 2.0299,
      "step": 25700
    },
    {
      "epoch": 2.020993263355789,
      "grad_norm": 5.893112659454346,
      "learning_rate": 4.831583894720351e-05,
      "loss": 2.0085,
      "step": 25800
    },
    {
      "epoch": 2.028826570578098,
      "grad_norm": 7.489804744720459,
      "learning_rate": 4.8309311191184916e-05,
      "loss": 1.9652,
      "step": 25900
    },
    {
      "epoch": 2.0366598778004072,
      "grad_norm": 5.5080461502075195,
      "learning_rate": 4.830278343516633e-05,
      "loss": 1.9561,
      "step": 26000
    },
    {
      "epoch": 2.0444931850227164,
      "grad_norm": 11.020781517028809,
      "learning_rate": 4.829625567914774e-05,
      "loss": 2.0742,
      "step": 26100
    },
    {
      "epoch": 2.052326492245026,
      "grad_norm": 6.711004734039307,
      "learning_rate": 4.828972792312915e-05,
      "loss": 1.9663,
      "step": 26200
    },
    {
      "epoch": 2.060159799467335,
      "grad_norm": 5.126227378845215,
      "learning_rate": 4.828320016711055e-05,
      "loss": 1.9259,
      "step": 26300
    },
    {
      "epoch": 2.067993106689644,
      "grad_norm": 8.00275993347168,
      "learning_rate": 4.8276672411091966e-05,
      "loss": 1.9359,
      "step": 26400
    },
    {
      "epoch": 2.0758264139119538,
      "grad_norm": 5.566828727722168,
      "learning_rate": 4.827014465507337e-05,
      "loss": 1.9683,
      "step": 26500
    },
    {
      "epoch": 2.083659721134263,
      "grad_norm": 7.269411563873291,
      "learning_rate": 4.8263616899054784e-05,
      "loss": 2.0481,
      "step": 26600
    },
    {
      "epoch": 2.091493028356572,
      "grad_norm": 5.840382099151611,
      "learning_rate": 4.8257089143036196e-05,
      "loss": 1.9776,
      "step": 26700
    },
    {
      "epoch": 2.0993263355788816,
      "grad_norm": 6.3608198165893555,
      "learning_rate": 4.82505613870176e-05,
      "loss": 2.0563,
      "step": 26800
    },
    {
      "epoch": 2.1071596428011907,
      "grad_norm": 7.352105140686035,
      "learning_rate": 4.824403363099901e-05,
      "loss": 1.9513,
      "step": 26900
    },
    {
      "epoch": 2.1149929500235,
      "grad_norm": 6.854551792144775,
      "learning_rate": 4.823750587498042e-05,
      "loss": 1.9445,
      "step": 27000
    },
    {
      "epoch": 2.122826257245809,
      "grad_norm": 6.10230016708374,
      "learning_rate": 4.8230978118961826e-05,
      "loss": 1.9541,
      "step": 27100
    },
    {
      "epoch": 2.1306595644681185,
      "grad_norm": 6.754837989807129,
      "learning_rate": 4.822445036294324e-05,
      "loss": 2.0336,
      "step": 27200
    },
    {
      "epoch": 2.1384928716904277,
      "grad_norm": 5.142934322357178,
      "learning_rate": 4.821792260692465e-05,
      "loss": 2.0107,
      "step": 27300
    },
    {
      "epoch": 2.146326178912737,
      "grad_norm": 6.229458332061768,
      "learning_rate": 4.821139485090606e-05,
      "loss": 2.0276,
      "step": 27400
    },
    {
      "epoch": 2.1541594861350464,
      "grad_norm": 5.782587051391602,
      "learning_rate": 4.820486709488746e-05,
      "loss": 1.9949,
      "step": 27500
    },
    {
      "epoch": 2.1619927933573555,
      "grad_norm": 8.36993408203125,
      "learning_rate": 4.8198339338868875e-05,
      "loss": 1.961,
      "step": 27600
    },
    {
      "epoch": 2.1698261005796646,
      "grad_norm": 5.966054439544678,
      "learning_rate": 4.819181158285028e-05,
      "loss": 1.9772,
      "step": 27700
    },
    {
      "epoch": 2.177659407801974,
      "grad_norm": 7.475893497467041,
      "learning_rate": 4.818528382683169e-05,
      "loss": 2.0356,
      "step": 27800
    },
    {
      "epoch": 2.1854927150242833,
      "grad_norm": 6.816789150238037,
      "learning_rate": 4.81787560708131e-05,
      "loss": 2.0139,
      "step": 27900
    },
    {
      "epoch": 2.1933260222465925,
      "grad_norm": 8.73304271697998,
      "learning_rate": 4.817222831479451e-05,
      "loss": 2.0152,
      "step": 28000
    },
    {
      "epoch": 2.2011593294689016,
      "grad_norm": 5.105018138885498,
      "learning_rate": 4.816570055877592e-05,
      "loss": 2.1875,
      "step": 28100
    },
    {
      "epoch": 2.208992636691211,
      "grad_norm": 6.885828971862793,
      "learning_rate": 4.8159172802757324e-05,
      "loss": 1.9807,
      "step": 28200
    },
    {
      "epoch": 2.2168259439135203,
      "grad_norm": 6.767153739929199,
      "learning_rate": 4.8152645046738736e-05,
      "loss": 2.0516,
      "step": 28300
    },
    {
      "epoch": 2.2246592511358294,
      "grad_norm": 7.693289279937744,
      "learning_rate": 4.814611729072014e-05,
      "loss": 1.9674,
      "step": 28400
    },
    {
      "epoch": 2.232492558358139,
      "grad_norm": 5.590367794036865,
      "learning_rate": 4.8139589534701554e-05,
      "loss": 1.977,
      "step": 28500
    },
    {
      "epoch": 2.240325865580448,
      "grad_norm": 7.478643894195557,
      "learning_rate": 4.813306177868297e-05,
      "loss": 2.0712,
      "step": 28600
    },
    {
      "epoch": 2.2481591728027572,
      "grad_norm": 6.526484489440918,
      "learning_rate": 4.812653402266437e-05,
      "loss": 1.9899,
      "step": 28700
    },
    {
      "epoch": 2.255992480025067,
      "grad_norm": 8.848775863647461,
      "learning_rate": 4.812000626664578e-05,
      "loss": 2.0335,
      "step": 28800
    },
    {
      "epoch": 2.263825787247376,
      "grad_norm": 8.005647659301758,
      "learning_rate": 4.811347851062719e-05,
      "loss": 2.013,
      "step": 28900
    },
    {
      "epoch": 2.271659094469685,
      "grad_norm": 5.988755702972412,
      "learning_rate": 4.81069507546086e-05,
      "loss": 1.9923,
      "step": 29000
    },
    {
      "epoch": 2.279492401691994,
      "grad_norm": 6.907281398773193,
      "learning_rate": 4.810042299859e-05,
      "loss": 2.0375,
      "step": 29100
    },
    {
      "epoch": 2.2873257089143038,
      "grad_norm": 5.7645134925842285,
      "learning_rate": 4.8093895242571415e-05,
      "loss": 2.0158,
      "step": 29200
    },
    {
      "epoch": 2.295159016136613,
      "grad_norm": 7.035396575927734,
      "learning_rate": 4.808736748655283e-05,
      "loss": 1.9597,
      "step": 29300
    },
    {
      "epoch": 2.302992323358922,
      "grad_norm": 4.8996992111206055,
      "learning_rate": 4.8080839730534233e-05,
      "loss": 1.9697,
      "step": 29400
    },
    {
      "epoch": 2.3108256305812316,
      "grad_norm": 7.381188869476318,
      "learning_rate": 4.807431197451564e-05,
      "loss": 1.893,
      "step": 29500
    },
    {
      "epoch": 2.3186589378035407,
      "grad_norm": 6.583010196685791,
      "learning_rate": 4.806778421849705e-05,
      "loss": 2.0189,
      "step": 29600
    },
    {
      "epoch": 2.32649224502585,
      "grad_norm": 7.9594316482543945,
      "learning_rate": 4.806125646247846e-05,
      "loss": 2.0149,
      "step": 29700
    },
    {
      "epoch": 2.334325552248159,
      "grad_norm": 6.908137798309326,
      "learning_rate": 4.805472870645987e-05,
      "loss": 2.0294,
      "step": 29800
    },
    {
      "epoch": 2.3421588594704685,
      "grad_norm": 9.12617301940918,
      "learning_rate": 4.804820095044128e-05,
      "loss": 1.9932,
      "step": 29900
    },
    {
      "epoch": 2.3499921666927777,
      "grad_norm": 7.674236297607422,
      "learning_rate": 4.804167319442269e-05,
      "loss": 1.9801,
      "step": 30000
    },
    {
      "epoch": 2.357825473915087,
      "grad_norm": 7.840075492858887,
      "learning_rate": 4.8035145438404094e-05,
      "loss": 2.0686,
      "step": 30100
    },
    {
      "epoch": 2.3656587811373964,
      "grad_norm": 7.329992294311523,
      "learning_rate": 4.802861768238551e-05,
      "loss": 2.041,
      "step": 30200
    },
    {
      "epoch": 2.3734920883597055,
      "grad_norm": 5.574245452880859,
      "learning_rate": 4.802208992636691e-05,
      "loss": 2.0675,
      "step": 30300
    },
    {
      "epoch": 2.3813253955820146,
      "grad_norm": 6.236850738525391,
      "learning_rate": 4.8015562170348325e-05,
      "loss": 1.9896,
      "step": 30400
    },
    {
      "epoch": 2.389158702804324,
      "grad_norm": 5.04591178894043,
      "learning_rate": 4.800903441432974e-05,
      "loss": 2.0794,
      "step": 30500
    },
    {
      "epoch": 2.3969920100266333,
      "grad_norm": 7.461435794830322,
      "learning_rate": 4.800250665831114e-05,
      "loss": 2.0933,
      "step": 30600
    },
    {
      "epoch": 2.4048253172489424,
      "grad_norm": 6.261764049530029,
      "learning_rate": 4.799597890229255e-05,
      "loss": 2.0454,
      "step": 30700
    },
    {
      "epoch": 2.412658624471252,
      "grad_norm": 7.641841411590576,
      "learning_rate": 4.798945114627396e-05,
      "loss": 1.9415,
      "step": 30800
    },
    {
      "epoch": 2.420491931693561,
      "grad_norm": 7.9167304039001465,
      "learning_rate": 4.798292339025537e-05,
      "loss": 2.0609,
      "step": 30900
    },
    {
      "epoch": 2.4283252389158703,
      "grad_norm": 7.760875225067139,
      "learning_rate": 4.797639563423677e-05,
      "loss": 2.0555,
      "step": 31000
    },
    {
      "epoch": 2.4361585461381794,
      "grad_norm": 7.2191057205200195,
      "learning_rate": 4.7969867878218186e-05,
      "loss": 2.0459,
      "step": 31100
    },
    {
      "epoch": 2.443991853360489,
      "grad_norm": 9.143528938293457,
      "learning_rate": 4.79633401221996e-05,
      "loss": 1.9445,
      "step": 31200
    },
    {
      "epoch": 2.451825160582798,
      "grad_norm": 8.025609016418457,
      "learning_rate": 4.7956812366181004e-05,
      "loss": 2.0032,
      "step": 31300
    },
    {
      "epoch": 2.459658467805107,
      "grad_norm": 7.129694938659668,
      "learning_rate": 4.795028461016241e-05,
      "loss": 1.9787,
      "step": 31400
    },
    {
      "epoch": 2.4674917750274163,
      "grad_norm": 8.501152992248535,
      "learning_rate": 4.794375685414382e-05,
      "loss": 1.9333,
      "step": 31500
    },
    {
      "epoch": 2.475325082249726,
      "grad_norm": 6.5194196701049805,
      "learning_rate": 4.793722909812523e-05,
      "loss": 1.9679,
      "step": 31600
    },
    {
      "epoch": 2.483158389472035,
      "grad_norm": 7.178741455078125,
      "learning_rate": 4.793070134210664e-05,
      "loss": 2.0048,
      "step": 31700
    },
    {
      "epoch": 2.490991696694344,
      "grad_norm": 6.698935031890869,
      "learning_rate": 4.792417358608805e-05,
      "loss": 1.9989,
      "step": 31800
    },
    {
      "epoch": 2.4988250039166537,
      "grad_norm": 7.764402389526367,
      "learning_rate": 4.791764583006946e-05,
      "loss": 2.0124,
      "step": 31900
    },
    {
      "epoch": 2.506658311138963,
      "grad_norm": 4.940192699432373,
      "learning_rate": 4.7911118074050865e-05,
      "loss": 2.0224,
      "step": 32000
    },
    {
      "epoch": 2.514491618361272,
      "grad_norm": 7.069710731506348,
      "learning_rate": 4.790459031803228e-05,
      "loss": 1.9472,
      "step": 32100
    },
    {
      "epoch": 2.5223249255835816,
      "grad_norm": 9.328753471374512,
      "learning_rate": 4.789806256201368e-05,
      "loss": 1.9486,
      "step": 32200
    },
    {
      "epoch": 2.5301582328058907,
      "grad_norm": 6.91868782043457,
      "learning_rate": 4.789153480599509e-05,
      "loss": 2.0969,
      "step": 32300
    },
    {
      "epoch": 2.5379915400282,
      "grad_norm": 6.536069869995117,
      "learning_rate": 4.78850070499765e-05,
      "loss": 2.0452,
      "step": 32400
    },
    {
      "epoch": 2.5458248472505094,
      "grad_norm": 6.681578636169434,
      "learning_rate": 4.7878479293957914e-05,
      "loss": 2.1065,
      "step": 32500
    },
    {
      "epoch": 2.5536581544728185,
      "grad_norm": 6.32171630859375,
      "learning_rate": 4.787195153793932e-05,
      "loss": 1.8711,
      "step": 32600
    },
    {
      "epoch": 2.5614914616951276,
      "grad_norm": 4.808777809143066,
      "learning_rate": 4.786542378192073e-05,
      "loss": 1.8798,
      "step": 32700
    },
    {
      "epoch": 2.569324768917437,
      "grad_norm": 6.581122398376465,
      "learning_rate": 4.785889602590214e-05,
      "loss": 1.9668,
      "step": 32800
    },
    {
      "epoch": 2.5771580761397463,
      "grad_norm": 5.756503105163574,
      "learning_rate": 4.7852368269883544e-05,
      "loss": 2.0763,
      "step": 32900
    },
    {
      "epoch": 2.5849913833620555,
      "grad_norm": 6.561789512634277,
      "learning_rate": 4.7845840513864956e-05,
      "loss": 2.0825,
      "step": 33000
    },
    {
      "epoch": 2.5928246905843646,
      "grad_norm": 7.240208625793457,
      "learning_rate": 4.783931275784637e-05,
      "loss": 2.0068,
      "step": 33100
    },
    {
      "epoch": 2.6006579978066737,
      "grad_norm": 5.845768928527832,
      "learning_rate": 4.7832785001827775e-05,
      "loss": 2.0035,
      "step": 33200
    },
    {
      "epoch": 2.6084913050289833,
      "grad_norm": 8.741083145141602,
      "learning_rate": 4.782625724580918e-05,
      "loss": 2.0757,
      "step": 33300
    },
    {
      "epoch": 2.6163246122512924,
      "grad_norm": 6.910199165344238,
      "learning_rate": 4.781972948979059e-05,
      "loss": 1.9694,
      "step": 33400
    },
    {
      "epoch": 2.6241579194736016,
      "grad_norm": 10.130996704101562,
      "learning_rate": 4.7813201733772e-05,
      "loss": 2.0929,
      "step": 33500
    },
    {
      "epoch": 2.631991226695911,
      "grad_norm": 6.184483051300049,
      "learning_rate": 4.780667397775341e-05,
      "loss": 2.048,
      "step": 33600
    },
    {
      "epoch": 2.6398245339182203,
      "grad_norm": 6.869164943695068,
      "learning_rate": 4.7800146221734824e-05,
      "loss": 1.9673,
      "step": 33700
    },
    {
      "epoch": 2.6476578411405294,
      "grad_norm": 6.081740856170654,
      "learning_rate": 4.779361846571623e-05,
      "loss": 2.0949,
      "step": 33800
    },
    {
      "epoch": 2.655491148362839,
      "grad_norm": 8.577507972717285,
      "learning_rate": 4.7787090709697635e-05,
      "loss": 2.0038,
      "step": 33900
    },
    {
      "epoch": 2.663324455585148,
      "grad_norm": 6.81169319152832,
      "learning_rate": 4.778056295367905e-05,
      "loss": 2.0489,
      "step": 34000
    },
    {
      "epoch": 2.671157762807457,
      "grad_norm": 5.974483489990234,
      "learning_rate": 4.7774035197660454e-05,
      "loss": 1.951,
      "step": 34100
    },
    {
      "epoch": 2.6789910700297668,
      "grad_norm": 7.492208003997803,
      "learning_rate": 4.776750744164186e-05,
      "loss": 1.9176,
      "step": 34200
    },
    {
      "epoch": 2.686824377252076,
      "grad_norm": 7.214206218719482,
      "learning_rate": 4.776097968562327e-05,
      "loss": 2.0237,
      "step": 34300
    },
    {
      "epoch": 2.694657684474385,
      "grad_norm": 6.067623615264893,
      "learning_rate": 4.7754451929604684e-05,
      "loss": 2.0244,
      "step": 34400
    },
    {
      "epoch": 2.7024909916966946,
      "grad_norm": 6.138227462768555,
      "learning_rate": 4.774792417358609e-05,
      "loss": 1.933,
      "step": 34500
    },
    {
      "epoch": 2.7103242989190037,
      "grad_norm": 7.566463470458984,
      "learning_rate": 4.7741396417567496e-05,
      "loss": 2.0768,
      "step": 34600
    },
    {
      "epoch": 2.718157606141313,
      "grad_norm": 5.618873596191406,
      "learning_rate": 4.773486866154891e-05,
      "loss": 2.089,
      "step": 34700
    },
    {
      "epoch": 2.725990913363622,
      "grad_norm": 5.048804759979248,
      "learning_rate": 4.7728340905530314e-05,
      "loss": 1.9924,
      "step": 34800
    },
    {
      "epoch": 2.733824220585931,
      "grad_norm": 5.958756923675537,
      "learning_rate": 4.772181314951173e-05,
      "loss": 1.9981,
      "step": 34900
    },
    {
      "epoch": 2.7416575278082407,
      "grad_norm": 7.7681565284729,
      "learning_rate": 4.771528539349314e-05,
      "loss": 2.0137,
      "step": 35000
    },
    {
      "epoch": 2.74949083503055,
      "grad_norm": 7.860630989074707,
      "learning_rate": 4.7708757637474545e-05,
      "loss": 1.9169,
      "step": 35100
    },
    {
      "epoch": 2.757324142252859,
      "grad_norm": 7.475915908813477,
      "learning_rate": 4.770222988145595e-05,
      "loss": 1.9427,
      "step": 35200
    },
    {
      "epoch": 2.7651574494751685,
      "grad_norm": 7.543491840362549,
      "learning_rate": 4.7695702125437363e-05,
      "loss": 1.9813,
      "step": 35300
    },
    {
      "epoch": 2.7729907566974776,
      "grad_norm": 5.083280563354492,
      "learning_rate": 4.768917436941877e-05,
      "loss": 2.0122,
      "step": 35400
    },
    {
      "epoch": 2.7808240639197868,
      "grad_norm": 5.401878356933594,
      "learning_rate": 4.7682646613400175e-05,
      "loss": 1.9517,
      "step": 35500
    },
    {
      "epoch": 2.7886573711420963,
      "grad_norm": 6.571840286254883,
      "learning_rate": 4.767611885738159e-05,
      "loss": 1.9503,
      "step": 35600
    },
    {
      "epoch": 2.7964906783644055,
      "grad_norm": 8.390883445739746,
      "learning_rate": 4.7669591101363e-05,
      "loss": 1.9434,
      "step": 35700
    },
    {
      "epoch": 2.8043239855867146,
      "grad_norm": 7.43930721282959,
      "learning_rate": 4.7663063345344406e-05,
      "loss": 1.9103,
      "step": 35800
    },
    {
      "epoch": 2.812157292809024,
      "grad_norm": 7.438504695892334,
      "learning_rate": 4.765653558932582e-05,
      "loss": 2.0386,
      "step": 35900
    },
    {
      "epoch": 2.8199906000313333,
      "grad_norm": 9.306150436401367,
      "learning_rate": 4.7650007833307224e-05,
      "loss": 2.0372,
      "step": 36000
    },
    {
      "epoch": 2.8278239072536424,
      "grad_norm": 6.17804479598999,
      "learning_rate": 4.764348007728863e-05,
      "loss": 1.9714,
      "step": 36100
    },
    {
      "epoch": 2.835657214475952,
      "grad_norm": 5.434910774230957,
      "learning_rate": 4.763695232127004e-05,
      "loss": 2.0134,
      "step": 36200
    },
    {
      "epoch": 2.843490521698261,
      "grad_norm": 6.529047966003418,
      "learning_rate": 4.7630424565251455e-05,
      "loss": 2.0244,
      "step": 36300
    },
    {
      "epoch": 2.8513238289205702,
      "grad_norm": 8.697051048278809,
      "learning_rate": 4.762389680923286e-05,
      "loss": 1.8789,
      "step": 36400
    },
    {
      "epoch": 2.8591571361428794,
      "grad_norm": 6.380539894104004,
      "learning_rate": 4.7617369053214267e-05,
      "loss": 2.075,
      "step": 36500
    },
    {
      "epoch": 2.866990443365189,
      "grad_norm": 7.532377243041992,
      "learning_rate": 4.761084129719568e-05,
      "loss": 1.9471,
      "step": 36600
    },
    {
      "epoch": 2.874823750587498,
      "grad_norm": 12.56596851348877,
      "learning_rate": 4.7604313541177085e-05,
      "loss": 2.0128,
      "step": 36700
    },
    {
      "epoch": 2.882657057809807,
      "grad_norm": 6.30033540725708,
      "learning_rate": 4.75977857851585e-05,
      "loss": 1.9687,
      "step": 36800
    },
    {
      "epoch": 2.8904903650321163,
      "grad_norm": 5.657102584838867,
      "learning_rate": 4.759125802913991e-05,
      "loss": 1.9082,
      "step": 36900
    },
    {
      "epoch": 2.898323672254426,
      "grad_norm": 5.026503562927246,
      "learning_rate": 4.7584730273121316e-05,
      "loss": 1.9369,
      "step": 37000
    },
    {
      "epoch": 2.906156979476735,
      "grad_norm": 5.286486625671387,
      "learning_rate": 4.757820251710272e-05,
      "loss": 2.0897,
      "step": 37100
    },
    {
      "epoch": 2.913990286699044,
      "grad_norm": 6.3227152824401855,
      "learning_rate": 4.7571674761084134e-05,
      "loss": 1.9976,
      "step": 37200
    },
    {
      "epoch": 2.9218235939213537,
      "grad_norm": 6.603045463562012,
      "learning_rate": 4.756514700506554e-05,
      "loss": 1.9976,
      "step": 37300
    },
    {
      "epoch": 2.929656901143663,
      "grad_norm": 7.872734546661377,
      "learning_rate": 4.7558619249046946e-05,
      "loss": 2.0103,
      "step": 37400
    },
    {
      "epoch": 2.937490208365972,
      "grad_norm": 5.218966960906982,
      "learning_rate": 4.755209149302836e-05,
      "loss": 1.9883,
      "step": 37500
    },
    {
      "epoch": 2.9453235155882815,
      "grad_norm": 7.7841033935546875,
      "learning_rate": 4.754556373700977e-05,
      "loss": 1.9935,
      "step": 37600
    },
    {
      "epoch": 2.9531568228105907,
      "grad_norm": 7.411628246307373,
      "learning_rate": 4.7539035980991176e-05,
      "loss": 2.0277,
      "step": 37700
    },
    {
      "epoch": 2.9609901300329,
      "grad_norm": 5.668209552764893,
      "learning_rate": 4.753250822497259e-05,
      "loss": 2.0588,
      "step": 37800
    },
    {
      "epoch": 2.9688234372552094,
      "grad_norm": 4.591775894165039,
      "learning_rate": 4.7525980468953995e-05,
      "loss": 1.9811,
      "step": 37900
    },
    {
      "epoch": 2.9766567444775185,
      "grad_norm": 8.340641021728516,
      "learning_rate": 4.75194527129354e-05,
      "loss": 1.9266,
      "step": 38000
    },
    {
      "epoch": 2.9844900516998276,
      "grad_norm": 8.138605117797852,
      "learning_rate": 4.751292495691681e-05,
      "loss": 1.993,
      "step": 38100
    },
    {
      "epoch": 2.992323358922137,
      "grad_norm": 8.295817375183105,
      "learning_rate": 4.7506397200898226e-05,
      "loss": 1.9956,
      "step": 38200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9333786964416504,
      "eval_runtime": 2.922,
      "eval_samples_per_second": 229.976,
      "eval_steps_per_second": 229.976,
      "step": 38298
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.7879115343093872,
      "eval_runtime": 56.4142,
      "eval_samples_per_second": 226.291,
      "eval_steps_per_second": 226.291,
      "step": 38298
    },
    {
      "epoch": 3.0001566661444463,
      "grad_norm": 6.0084075927734375,
      "learning_rate": 4.749986944487963e-05,
      "loss": 1.9484,
      "step": 38300
    },
    {
      "epoch": 3.0079899733667554,
      "grad_norm": 6.9230637550354,
      "learning_rate": 4.749334168886104e-05,
      "loss": 1.9591,
      "step": 38400
    },
    {
      "epoch": 3.0158232805890646,
      "grad_norm": 6.8120198249816895,
      "learning_rate": 4.748681393284245e-05,
      "loss": 1.9012,
      "step": 38500
    },
    {
      "epoch": 3.023656587811374,
      "grad_norm": 6.8733015060424805,
      "learning_rate": 4.7480286176823855e-05,
      "loss": 1.9397,
      "step": 38600
    },
    {
      "epoch": 3.0314898950336833,
      "grad_norm": 7.493002891540527,
      "learning_rate": 4.747375842080526e-05,
      "loss": 1.9522,
      "step": 38700
    },
    {
      "epoch": 3.0393232022559924,
      "grad_norm": 5.50239372253418,
      "learning_rate": 4.7467230664786674e-05,
      "loss": 2.0456,
      "step": 38800
    },
    {
      "epoch": 3.0471565094783015,
      "grad_norm": 7.5401082038879395,
      "learning_rate": 4.7460702908768086e-05,
      "loss": 1.9899,
      "step": 38900
    },
    {
      "epoch": 3.054989816700611,
      "grad_norm": 5.847183704376221,
      "learning_rate": 4.745417515274949e-05,
      "loss": 1.9797,
      "step": 39000
    },
    {
      "epoch": 3.06282312392292,
      "grad_norm": 6.767307758331299,
      "learning_rate": 4.7447647396730905e-05,
      "loss": 1.9279,
      "step": 39100
    },
    {
      "epoch": 3.0706564311452293,
      "grad_norm": 12.360281944274902,
      "learning_rate": 4.744111964071231e-05,
      "loss": 1.9589,
      "step": 39200
    },
    {
      "epoch": 3.078489738367539,
      "grad_norm": 6.8523945808410645,
      "learning_rate": 4.7434591884693716e-05,
      "loss": 1.9133,
      "step": 39300
    },
    {
      "epoch": 3.086323045589848,
      "grad_norm": 7.806612968444824,
      "learning_rate": 4.742806412867513e-05,
      "loss": 2.011,
      "step": 39400
    },
    {
      "epoch": 3.094156352812157,
      "grad_norm": 6.515204429626465,
      "learning_rate": 4.742153637265654e-05,
      "loss": 2.0485,
      "step": 39500
    },
    {
      "epoch": 3.1019896600344667,
      "grad_norm": 6.403046607971191,
      "learning_rate": 4.741500861663795e-05,
      "loss": 2.0898,
      "step": 39600
    },
    {
      "epoch": 3.109822967256776,
      "grad_norm": 7.421962261199951,
      "learning_rate": 4.740848086061935e-05,
      "loss": 1.9919,
      "step": 39700
    },
    {
      "epoch": 3.117656274479085,
      "grad_norm": 6.18856954574585,
      "learning_rate": 4.7401953104600765e-05,
      "loss": 1.9568,
      "step": 39800
    },
    {
      "epoch": 3.125489581701394,
      "grad_norm": 5.984738826751709,
      "learning_rate": 4.739542534858217e-05,
      "loss": 1.9607,
      "step": 39900
    },
    {
      "epoch": 3.1333228889237037,
      "grad_norm": 5.904287338256836,
      "learning_rate": 4.7388897592563584e-05,
      "loss": 1.9587,
      "step": 40000
    },
    {
      "epoch": 3.141156196146013,
      "grad_norm": 6.729703426361084,
      "learning_rate": 4.7382369836544996e-05,
      "loss": 1.9545,
      "step": 40100
    },
    {
      "epoch": 3.148989503368322,
      "grad_norm": 6.093860626220703,
      "learning_rate": 4.73758420805264e-05,
      "loss": 2.0302,
      "step": 40200
    },
    {
      "epoch": 3.1568228105906315,
      "grad_norm": 6.5391154289245605,
      "learning_rate": 4.736931432450781e-05,
      "loss": 1.8941,
      "step": 40300
    },
    {
      "epoch": 3.1646561178129406,
      "grad_norm": 4.602065086364746,
      "learning_rate": 4.736278656848922e-05,
      "loss": 1.9413,
      "step": 40400
    },
    {
      "epoch": 3.1724894250352498,
      "grad_norm": 5.070751190185547,
      "learning_rate": 4.7356258812470626e-05,
      "loss": 1.9033,
      "step": 40500
    },
    {
      "epoch": 3.1803227322575593,
      "grad_norm": 6.3531999588012695,
      "learning_rate": 4.734973105645203e-05,
      "loss": 1.9815,
      "step": 40600
    },
    {
      "epoch": 3.1881560394798685,
      "grad_norm": 7.206649303436279,
      "learning_rate": 4.7343203300433444e-05,
      "loss": 1.9344,
      "step": 40700
    },
    {
      "epoch": 3.1959893467021776,
      "grad_norm": 5.718532085418701,
      "learning_rate": 4.733667554441486e-05,
      "loss": 2.0089,
      "step": 40800
    },
    {
      "epoch": 3.2038226539244867,
      "grad_norm": 9.675889015197754,
      "learning_rate": 4.733014778839626e-05,
      "loss": 1.9708,
      "step": 40900
    },
    {
      "epoch": 3.2116559611467963,
      "grad_norm": 7.840482234954834,
      "learning_rate": 4.7323620032377675e-05,
      "loss": 1.9372,
      "step": 41000
    },
    {
      "epoch": 3.2194892683691054,
      "grad_norm": 7.633354663848877,
      "learning_rate": 4.731709227635908e-05,
      "loss": 2.0209,
      "step": 41100
    },
    {
      "epoch": 3.2273225755914146,
      "grad_norm": 6.767846584320068,
      "learning_rate": 4.731056452034049e-05,
      "loss": 2.0228,
      "step": 41200
    },
    {
      "epoch": 3.235155882813724,
      "grad_norm": 5.298370838165283,
      "learning_rate": 4.73040367643219e-05,
      "loss": 1.9645,
      "step": 41300
    },
    {
      "epoch": 3.2429891900360333,
      "grad_norm": 5.0669846534729,
      "learning_rate": 4.729750900830331e-05,
      "loss": 1.8769,
      "step": 41400
    },
    {
      "epoch": 3.2508224972583424,
      "grad_norm": 6.170541763305664,
      "learning_rate": 4.729098125228472e-05,
      "loss": 2.0527,
      "step": 41500
    },
    {
      "epoch": 3.258655804480652,
      "grad_norm": 5.805488109588623,
      "learning_rate": 4.7284453496266123e-05,
      "loss": 2.0021,
      "step": 41600
    },
    {
      "epoch": 3.266489111702961,
      "grad_norm": 6.396935939788818,
      "learning_rate": 4.7277925740247536e-05,
      "loss": 1.9032,
      "step": 41700
    },
    {
      "epoch": 3.27432241892527,
      "grad_norm": 7.436779499053955,
      "learning_rate": 4.727139798422894e-05,
      "loss": 1.9862,
      "step": 41800
    },
    {
      "epoch": 3.2821557261475793,
      "grad_norm": 7.575311183929443,
      "learning_rate": 4.726487022821035e-05,
      "loss": 1.8431,
      "step": 41900
    },
    {
      "epoch": 3.289989033369889,
      "grad_norm": 6.483281135559082,
      "learning_rate": 4.725834247219176e-05,
      "loss": 1.9654,
      "step": 42000
    },
    {
      "epoch": 3.297822340592198,
      "grad_norm": 8.276163101196289,
      "learning_rate": 4.725181471617317e-05,
      "loss": 1.9512,
      "step": 42100
    },
    {
      "epoch": 3.305655647814507,
      "grad_norm": 7.370298862457275,
      "learning_rate": 4.724528696015458e-05,
      "loss": 1.9623,
      "step": 42200
    },
    {
      "epoch": 3.3134889550368167,
      "grad_norm": 7.1265869140625,
      "learning_rate": 4.723875920413599e-05,
      "loss": 1.9517,
      "step": 42300
    },
    {
      "epoch": 3.321322262259126,
      "grad_norm": 6.453105449676514,
      "learning_rate": 4.72322314481174e-05,
      "loss": 1.971,
      "step": 42400
    },
    {
      "epoch": 3.329155569481435,
      "grad_norm": 8.160308837890625,
      "learning_rate": 4.72257036920988e-05,
      "loss": 2.0256,
      "step": 42500
    },
    {
      "epoch": 3.336988876703744,
      "grad_norm": 5.642753601074219,
      "learning_rate": 4.7219175936080215e-05,
      "loss": 1.9995,
      "step": 42600
    },
    {
      "epoch": 3.3448221839260537,
      "grad_norm": 6.046187877655029,
      "learning_rate": 4.721264818006163e-05,
      "loss": 2.1228,
      "step": 42700
    },
    {
      "epoch": 3.352655491148363,
      "grad_norm": 5.878297328948975,
      "learning_rate": 4.720612042404303e-05,
      "loss": 1.9519,
      "step": 42800
    },
    {
      "epoch": 3.360488798370672,
      "grad_norm": 5.272262096405029,
      "learning_rate": 4.7199592668024446e-05,
      "loss": 2.0221,
      "step": 42900
    },
    {
      "epoch": 3.3683221055929815,
      "grad_norm": 5.881475925445557,
      "learning_rate": 4.719306491200585e-05,
      "loss": 2.0284,
      "step": 43000
    },
    {
      "epoch": 3.3761554128152906,
      "grad_norm": 6.278016567230225,
      "learning_rate": 4.718653715598726e-05,
      "loss": 1.9064,
      "step": 43100
    },
    {
      "epoch": 3.3839887200375998,
      "grad_norm": 5.404463768005371,
      "learning_rate": 4.718000939996867e-05,
      "loss": 1.9511,
      "step": 43200
    },
    {
      "epoch": 3.3918220272599093,
      "grad_norm": 7.419687747955322,
      "learning_rate": 4.717348164395008e-05,
      "loss": 1.9801,
      "step": 43300
    },
    {
      "epoch": 3.3996553344822185,
      "grad_norm": 6.985759258270264,
      "learning_rate": 4.716695388793149e-05,
      "loss": 2.0016,
      "step": 43400
    },
    {
      "epoch": 3.4074886417045276,
      "grad_norm": 5.055195331573486,
      "learning_rate": 4.7160426131912894e-05,
      "loss": 1.9613,
      "step": 43500
    },
    {
      "epoch": 3.415321948926837,
      "grad_norm": 6.0386552810668945,
      "learning_rate": 4.7153898375894307e-05,
      "loss": 1.8867,
      "step": 43600
    },
    {
      "epoch": 3.4231552561491463,
      "grad_norm": 6.552380561828613,
      "learning_rate": 4.714737061987571e-05,
      "loss": 1.9696,
      "step": 43700
    },
    {
      "epoch": 3.4309885633714554,
      "grad_norm": 7.933419227600098,
      "learning_rate": 4.714084286385712e-05,
      "loss": 1.8999,
      "step": 43800
    },
    {
      "epoch": 3.4388218705937645,
      "grad_norm": 9.193648338317871,
      "learning_rate": 4.713431510783853e-05,
      "loss": 1.9689,
      "step": 43900
    },
    {
      "epoch": 3.446655177816074,
      "grad_norm": 6.4341349601745605,
      "learning_rate": 4.712778735181994e-05,
      "loss": 1.965,
      "step": 44000
    },
    {
      "epoch": 3.4544884850383832,
      "grad_norm": 7.222376346588135,
      "learning_rate": 4.712125959580135e-05,
      "loss": 1.9942,
      "step": 44100
    },
    {
      "epoch": 3.4623217922606924,
      "grad_norm": 7.36007833480835,
      "learning_rate": 4.711473183978276e-05,
      "loss": 1.9318,
      "step": 44200
    },
    {
      "epoch": 3.4701550994830015,
      "grad_norm": 8.744588851928711,
      "learning_rate": 4.710820408376417e-05,
      "loss": 1.8825,
      "step": 44300
    },
    {
      "epoch": 3.477988406705311,
      "grad_norm": 5.579815864562988,
      "learning_rate": 4.710167632774557e-05,
      "loss": 1.9491,
      "step": 44400
    },
    {
      "epoch": 3.48582171392762,
      "grad_norm": 11.853327751159668,
      "learning_rate": 4.7095148571726986e-05,
      "loss": 1.9623,
      "step": 44500
    },
    {
      "epoch": 3.4936550211499293,
      "grad_norm": 6.989988803863525,
      "learning_rate": 4.70886208157084e-05,
      "loss": 1.9923,
      "step": 44600
    },
    {
      "epoch": 3.501488328372239,
      "grad_norm": 6.299351215362549,
      "learning_rate": 4.7082093059689804e-05,
      "loss": 2.0313,
      "step": 44700
    },
    {
      "epoch": 3.509321635594548,
      "grad_norm": 7.413650035858154,
      "learning_rate": 4.707556530367121e-05,
      "loss": 1.8314,
      "step": 44800
    },
    {
      "epoch": 3.517154942816857,
      "grad_norm": 5.840681552886963,
      "learning_rate": 4.706903754765262e-05,
      "loss": 2.0161,
      "step": 44900
    },
    {
      "epoch": 3.5249882500391667,
      "grad_norm": 4.913244724273682,
      "learning_rate": 4.706250979163403e-05,
      "loss": 1.9481,
      "step": 45000
    },
    {
      "epoch": 3.532821557261476,
      "grad_norm": 5.6979660987854,
      "learning_rate": 4.7055982035615434e-05,
      "loss": 1.9933,
      "step": 45100
    },
    {
      "epoch": 3.540654864483785,
      "grad_norm": 5.3857293128967285,
      "learning_rate": 4.7049454279596846e-05,
      "loss": 2.008,
      "step": 45200
    },
    {
      "epoch": 3.5484881717060945,
      "grad_norm": 7.623622417449951,
      "learning_rate": 4.704292652357826e-05,
      "loss": 1.9907,
      "step": 45300
    },
    {
      "epoch": 3.5563214789284037,
      "grad_norm": 5.507002353668213,
      "learning_rate": 4.7036398767559665e-05,
      "loss": 1.9308,
      "step": 45400
    },
    {
      "epoch": 3.564154786150713,
      "grad_norm": 7.5735344886779785,
      "learning_rate": 4.702987101154108e-05,
      "loss": 1.9978,
      "step": 45500
    },
    {
      "epoch": 3.5719880933730224,
      "grad_norm": 8.016491889953613,
      "learning_rate": 4.702334325552248e-05,
      "loss": 1.9412,
      "step": 45600
    },
    {
      "epoch": 3.5798214005953315,
      "grad_norm": 5.586874008178711,
      "learning_rate": 4.701681549950389e-05,
      "loss": 1.9335,
      "step": 45700
    },
    {
      "epoch": 3.5876547078176406,
      "grad_norm": 8.109602928161621,
      "learning_rate": 4.70102877434853e-05,
      "loss": 1.9587,
      "step": 45800
    },
    {
      "epoch": 3.5954880150399497,
      "grad_norm": 7.263306140899658,
      "learning_rate": 4.7003759987466714e-05,
      "loss": 1.9271,
      "step": 45900
    },
    {
      "epoch": 3.603321322262259,
      "grad_norm": 5.436116695404053,
      "learning_rate": 4.699723223144812e-05,
      "loss": 2.0079,
      "step": 46000
    },
    {
      "epoch": 3.6111546294845684,
      "grad_norm": 9.446998596191406,
      "learning_rate": 4.699070447542953e-05,
      "loss": 1.992,
      "step": 46100
    },
    {
      "epoch": 3.6189879367068776,
      "grad_norm": 5.672733306884766,
      "learning_rate": 4.698417671941094e-05,
      "loss": 1.9076,
      "step": 46200
    },
    {
      "epoch": 3.6268212439291867,
      "grad_norm": 5.727181434631348,
      "learning_rate": 4.6977648963392344e-05,
      "loss": 1.8859,
      "step": 46300
    },
    {
      "epoch": 3.6346545511514963,
      "grad_norm": 5.328070640563965,
      "learning_rate": 4.6971121207373756e-05,
      "loss": 1.9066,
      "step": 46400
    },
    {
      "epoch": 3.6424878583738054,
      "grad_norm": 6.695876121520996,
      "learning_rate": 4.696459345135517e-05,
      "loss": 1.9383,
      "step": 46500
    },
    {
      "epoch": 3.6503211655961145,
      "grad_norm": 7.604464530944824,
      "learning_rate": 4.6958065695336574e-05,
      "loss": 1.9497,
      "step": 46600
    },
    {
      "epoch": 3.658154472818424,
      "grad_norm": 8.992817878723145,
      "learning_rate": 4.695153793931798e-05,
      "loss": 2.0736,
      "step": 46700
    },
    {
      "epoch": 3.6659877800407332,
      "grad_norm": 6.268909454345703,
      "learning_rate": 4.694501018329939e-05,
      "loss": 1.8786,
      "step": 46800
    },
    {
      "epoch": 3.6738210872630424,
      "grad_norm": 9.43589973449707,
      "learning_rate": 4.69384824272808e-05,
      "loss": 2.0092,
      "step": 46900
    },
    {
      "epoch": 3.681654394485352,
      "grad_norm": 6.975180625915527,
      "learning_rate": 4.6931954671262204e-05,
      "loss": 1.9681,
      "step": 47000
    },
    {
      "epoch": 3.689487701707661,
      "grad_norm": 6.947650909423828,
      "learning_rate": 4.692542691524362e-05,
      "loss": 1.9914,
      "step": 47100
    },
    {
      "epoch": 3.69732100892997,
      "grad_norm": 5.948134899139404,
      "learning_rate": 4.691889915922503e-05,
      "loss": 1.9817,
      "step": 47200
    },
    {
      "epoch": 3.7051543161522797,
      "grad_norm": 4.822868824005127,
      "learning_rate": 4.6912371403206435e-05,
      "loss": 1.9795,
      "step": 47300
    },
    {
      "epoch": 3.712987623374589,
      "grad_norm": 7.678108215332031,
      "learning_rate": 4.690584364718785e-05,
      "loss": 1.8883,
      "step": 47400
    },
    {
      "epoch": 3.720820930596898,
      "grad_norm": 6.783167362213135,
      "learning_rate": 4.6899315891169253e-05,
      "loss": 1.981,
      "step": 47500
    },
    {
      "epoch": 3.728654237819207,
      "grad_norm": 5.073479652404785,
      "learning_rate": 4.689278813515066e-05,
      "loss": 1.9127,
      "step": 47600
    },
    {
      "epoch": 3.7364875450415163,
      "grad_norm": 6.310993671417236,
      "learning_rate": 4.688626037913207e-05,
      "loss": 2.0025,
      "step": 47700
    },
    {
      "epoch": 3.744320852263826,
      "grad_norm": 5.451137542724609,
      "learning_rate": 4.6879732623113484e-05,
      "loss": 1.8618,
      "step": 47800
    },
    {
      "epoch": 3.752154159486135,
      "grad_norm": 6.234206199645996,
      "learning_rate": 4.687320486709489e-05,
      "loss": 1.9327,
      "step": 47900
    },
    {
      "epoch": 3.759987466708444,
      "grad_norm": 6.648550510406494,
      "learning_rate": 4.6866677111076296e-05,
      "loss": 1.9216,
      "step": 48000
    },
    {
      "epoch": 3.7678207739307537,
      "grad_norm": 7.092433452606201,
      "learning_rate": 4.686014935505771e-05,
      "loss": 2.0394,
      "step": 48100
    },
    {
      "epoch": 3.775654081153063,
      "grad_norm": 6.163690090179443,
      "learning_rate": 4.6853621599039114e-05,
      "loss": 1.9132,
      "step": 48200
    },
    {
      "epoch": 3.783487388375372,
      "grad_norm": 4.962888717651367,
      "learning_rate": 4.684709384302052e-05,
      "loss": 1.9453,
      "step": 48300
    },
    {
      "epoch": 3.7913206955976815,
      "grad_norm": 6.448905944824219,
      "learning_rate": 4.684056608700193e-05,
      "loss": 2.008,
      "step": 48400
    },
    {
      "epoch": 3.7991540028199906,
      "grad_norm": 13.00483512878418,
      "learning_rate": 4.6834038330983345e-05,
      "loss": 1.8897,
      "step": 48500
    },
    {
      "epoch": 3.8069873100422997,
      "grad_norm": 5.87309455871582,
      "learning_rate": 4.682751057496475e-05,
      "loss": 1.9396,
      "step": 48600
    },
    {
      "epoch": 3.8148206172646093,
      "grad_norm": 6.0919413566589355,
      "learning_rate": 4.682098281894616e-05,
      "loss": 1.9987,
      "step": 48700
    },
    {
      "epoch": 3.8226539244869184,
      "grad_norm": 8.616244316101074,
      "learning_rate": 4.681445506292757e-05,
      "loss": 1.9686,
      "step": 48800
    },
    {
      "epoch": 3.8304872317092276,
      "grad_norm": 5.314887523651123,
      "learning_rate": 4.6807927306908975e-05,
      "loss": 1.8945,
      "step": 48900
    },
    {
      "epoch": 3.838320538931537,
      "grad_norm": 6.809070587158203,
      "learning_rate": 4.680139955089039e-05,
      "loss": 1.9405,
      "step": 49000
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 5.219339847564697,
      "learning_rate": 4.67948717948718e-05,
      "loss": 2.022,
      "step": 49100
    },
    {
      "epoch": 3.8539871533761554,
      "grad_norm": 6.520021438598633,
      "learning_rate": 4.6788344038853206e-05,
      "loss": 1.9229,
      "step": 49200
    },
    {
      "epoch": 3.8618204605984645,
      "grad_norm": 7.044384002685547,
      "learning_rate": 4.678181628283462e-05,
      "loss": 1.8883,
      "step": 49300
    },
    {
      "epoch": 3.869653767820774,
      "grad_norm": 5.727638244628906,
      "learning_rate": 4.6775288526816024e-05,
      "loss": 1.9846,
      "step": 49400
    },
    {
      "epoch": 3.877487075043083,
      "grad_norm": 9.838738441467285,
      "learning_rate": 4.676876077079743e-05,
      "loss": 1.893,
      "step": 49500
    },
    {
      "epoch": 3.8853203822653923,
      "grad_norm": 7.429324150085449,
      "learning_rate": 4.676223301477884e-05,
      "loss": 1.964,
      "step": 49600
    },
    {
      "epoch": 3.8931536894877015,
      "grad_norm": 6.651976585388184,
      "learning_rate": 4.6755705258760255e-05,
      "loss": 2.0404,
      "step": 49700
    },
    {
      "epoch": 3.900986996710011,
      "grad_norm": 9.36438274383545,
      "learning_rate": 4.674917750274166e-05,
      "loss": 1.9202,
      "step": 49800
    },
    {
      "epoch": 3.90882030393232,
      "grad_norm": 6.849676609039307,
      "learning_rate": 4.6742649746723066e-05,
      "loss": 1.9185,
      "step": 49900
    },
    {
      "epoch": 3.9166536111546293,
      "grad_norm": 7.997569561004639,
      "learning_rate": 4.673612199070448e-05,
      "loss": 1.9909,
      "step": 50000
    },
    {
      "epoch": 3.924486918376939,
      "grad_norm": 7.7529497146606445,
      "learning_rate": 4.6729594234685885e-05,
      "loss": 1.9023,
      "step": 50100
    },
    {
      "epoch": 3.932320225599248,
      "grad_norm": 6.118818759918213,
      "learning_rate": 4.672306647866729e-05,
      "loss": 1.9555,
      "step": 50200
    },
    {
      "epoch": 3.940153532821557,
      "grad_norm": 6.849771022796631,
      "learning_rate": 4.67165387226487e-05,
      "loss": 1.9884,
      "step": 50300
    },
    {
      "epoch": 3.9479868400438667,
      "grad_norm": 5.609009742736816,
      "learning_rate": 4.6710010966630116e-05,
      "loss": 1.9018,
      "step": 50400
    },
    {
      "epoch": 3.955820147266176,
      "grad_norm": 7.3078532218933105,
      "learning_rate": 4.670348321061152e-05,
      "loss": 1.908,
      "step": 50500
    },
    {
      "epoch": 3.963653454488485,
      "grad_norm": 5.451830863952637,
      "learning_rate": 4.6696955454592934e-05,
      "loss": 1.984,
      "step": 50600
    },
    {
      "epoch": 3.9714867617107945,
      "grad_norm": 6.637599945068359,
      "learning_rate": 4.669042769857434e-05,
      "loss": 2.0233,
      "step": 50700
    },
    {
      "epoch": 3.9793200689331036,
      "grad_norm": 4.868288993835449,
      "learning_rate": 4.6683899942555745e-05,
      "loss": 2.0283,
      "step": 50800
    },
    {
      "epoch": 3.9871533761554128,
      "grad_norm": 6.839499473571777,
      "learning_rate": 4.667737218653716e-05,
      "loss": 1.9816,
      "step": 50900
    },
    {
      "epoch": 3.9949866833777223,
      "grad_norm": 6.936305522918701,
      "learning_rate": 4.667084443051857e-05,
      "loss": 1.9654,
      "step": 51000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9087677001953125,
      "eval_runtime": 2.9312,
      "eval_samples_per_second": 229.257,
      "eval_steps_per_second": 229.257,
      "step": 51064
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.7496825456619263,
      "eval_runtime": 57.1846,
      "eval_samples_per_second": 223.242,
      "eval_steps_per_second": 223.242,
      "step": 51064
    },
    {
      "epoch": 4.002819990600031,
      "grad_norm": 5.627793312072754,
      "learning_rate": 4.6664316674499976e-05,
      "loss": 1.9584,
      "step": 51100
    },
    {
      "epoch": 4.010653297822341,
      "grad_norm": 5.089822769165039,
      "learning_rate": 4.665778891848139e-05,
      "loss": 1.9766,
      "step": 51200
    },
    {
      "epoch": 4.01848660504465,
      "grad_norm": 5.8685784339904785,
      "learning_rate": 4.6651261162462795e-05,
      "loss": 1.9629,
      "step": 51300
    },
    {
      "epoch": 4.026319912266959,
      "grad_norm": 5.078973770141602,
      "learning_rate": 4.66447334064442e-05,
      "loss": 1.8524,
      "step": 51400
    },
    {
      "epoch": 4.034153219489268,
      "grad_norm": 6.914746284484863,
      "learning_rate": 4.6638205650425606e-05,
      "loss": 1.9897,
      "step": 51500
    },
    {
      "epoch": 4.041986526711578,
      "grad_norm": 6.606836795806885,
      "learning_rate": 4.663167789440702e-05,
      "loss": 1.9367,
      "step": 51600
    },
    {
      "epoch": 4.049819833933887,
      "grad_norm": 7.806486129760742,
      "learning_rate": 4.662515013838843e-05,
      "loss": 1.9188,
      "step": 51700
    },
    {
      "epoch": 4.057653141156196,
      "grad_norm": 5.385204315185547,
      "learning_rate": 4.661862238236984e-05,
      "loss": 1.8368,
      "step": 51800
    },
    {
      "epoch": 4.065486448378506,
      "grad_norm": 6.821121692657471,
      "learning_rate": 4.661209462635125e-05,
      "loss": 1.8125,
      "step": 51900
    },
    {
      "epoch": 4.0733197556008145,
      "grad_norm": 6.74898624420166,
      "learning_rate": 4.6605566870332655e-05,
      "loss": 1.9032,
      "step": 52000
    },
    {
      "epoch": 4.081153062823124,
      "grad_norm": 6.753880500793457,
      "learning_rate": 4.659903911431406e-05,
      "loss": 1.9667,
      "step": 52100
    },
    {
      "epoch": 4.088986370045433,
      "grad_norm": 6.266134738922119,
      "learning_rate": 4.6592511358295474e-05,
      "loss": 2.0287,
      "step": 52200
    },
    {
      "epoch": 4.096819677267742,
      "grad_norm": 4.842983245849609,
      "learning_rate": 4.6585983602276886e-05,
      "loss": 1.9454,
      "step": 52300
    },
    {
      "epoch": 4.104652984490052,
      "grad_norm": 6.542972087860107,
      "learning_rate": 4.657945584625829e-05,
      "loss": 1.9729,
      "step": 52400
    },
    {
      "epoch": 4.112486291712361,
      "grad_norm": 6.859457492828369,
      "learning_rate": 4.6572928090239705e-05,
      "loss": 1.9181,
      "step": 52500
    },
    {
      "epoch": 4.12031959893467,
      "grad_norm": 7.990116596221924,
      "learning_rate": 4.656640033422111e-05,
      "loss": 1.9018,
      "step": 52600
    },
    {
      "epoch": 4.12815290615698,
      "grad_norm": 7.381930351257324,
      "learning_rate": 4.6559872578202516e-05,
      "loss": 1.8653,
      "step": 52700
    },
    {
      "epoch": 4.135986213379288,
      "grad_norm": 10.237261772155762,
      "learning_rate": 4.655334482218393e-05,
      "loss": 1.8938,
      "step": 52800
    },
    {
      "epoch": 4.143819520601598,
      "grad_norm": 7.729321002960205,
      "learning_rate": 4.654681706616534e-05,
      "loss": 1.8802,
      "step": 52900
    },
    {
      "epoch": 4.1516528278239075,
      "grad_norm": 6.747607231140137,
      "learning_rate": 4.654028931014675e-05,
      "loss": 1.9326,
      "step": 53000
    },
    {
      "epoch": 4.159486135046216,
      "grad_norm": 5.506112098693848,
      "learning_rate": 4.653376155412815e-05,
      "loss": 1.8686,
      "step": 53100
    },
    {
      "epoch": 4.167319442268526,
      "grad_norm": 7.8844733238220215,
      "learning_rate": 4.6527233798109565e-05,
      "loss": 1.9275,
      "step": 53200
    },
    {
      "epoch": 4.175152749490835,
      "grad_norm": 7.0597076416015625,
      "learning_rate": 4.652070604209097e-05,
      "loss": 1.9279,
      "step": 53300
    },
    {
      "epoch": 4.182986056713144,
      "grad_norm": 10.282665252685547,
      "learning_rate": 4.651417828607238e-05,
      "loss": 1.9891,
      "step": 53400
    },
    {
      "epoch": 4.190819363935454,
      "grad_norm": 5.6113176345825195,
      "learning_rate": 4.650765053005379e-05,
      "loss": 1.9453,
      "step": 53500
    },
    {
      "epoch": 4.198652671157763,
      "grad_norm": 3.846458911895752,
      "learning_rate": 4.65011227740352e-05,
      "loss": 1.9352,
      "step": 53600
    },
    {
      "epoch": 4.206485978380072,
      "grad_norm": 4.8010406494140625,
      "learning_rate": 4.649459501801661e-05,
      "loss": 1.8195,
      "step": 53700
    },
    {
      "epoch": 4.2143192856023814,
      "grad_norm": 5.5081400871276855,
      "learning_rate": 4.648806726199802e-05,
      "loss": 1.8641,
      "step": 53800
    },
    {
      "epoch": 4.22215259282469,
      "grad_norm": 6.958820343017578,
      "learning_rate": 4.6481539505979426e-05,
      "loss": 1.9283,
      "step": 53900
    },
    {
      "epoch": 4.229985900047,
      "grad_norm": 5.431410789489746,
      "learning_rate": 4.647501174996083e-05,
      "loss": 2.0177,
      "step": 54000
    },
    {
      "epoch": 4.237819207269309,
      "grad_norm": 6.209184169769287,
      "learning_rate": 4.6468483993942244e-05,
      "loss": 1.8946,
      "step": 54100
    },
    {
      "epoch": 4.245652514491618,
      "grad_norm": 7.401605129241943,
      "learning_rate": 4.646195623792366e-05,
      "loss": 1.8594,
      "step": 54200
    },
    {
      "epoch": 4.2534858217139275,
      "grad_norm": 5.9179487228393555,
      "learning_rate": 4.645542848190506e-05,
      "loss": 1.9881,
      "step": 54300
    },
    {
      "epoch": 4.261319128936237,
      "grad_norm": 6.4492597579956055,
      "learning_rate": 4.6448900725886475e-05,
      "loss": 1.9415,
      "step": 54400
    },
    {
      "epoch": 4.269152436158546,
      "grad_norm": 6.132691383361816,
      "learning_rate": 4.644237296986788e-05,
      "loss": 1.9386,
      "step": 54500
    },
    {
      "epoch": 4.276985743380855,
      "grad_norm": 5.942525863647461,
      "learning_rate": 4.643584521384929e-05,
      "loss": 1.9008,
      "step": 54600
    },
    {
      "epoch": 4.284819050603165,
      "grad_norm": 8.221936225891113,
      "learning_rate": 4.64293174578307e-05,
      "loss": 1.9024,
      "step": 54700
    },
    {
      "epoch": 4.292652357825474,
      "grad_norm": 5.308866500854492,
      "learning_rate": 4.6422789701812105e-05,
      "loss": 2.0888,
      "step": 54800
    },
    {
      "epoch": 4.300485665047783,
      "grad_norm": 8.440834999084473,
      "learning_rate": 4.641626194579352e-05,
      "loss": 1.9405,
      "step": 54900
    },
    {
      "epoch": 4.308318972270093,
      "grad_norm": 7.891136646270752,
      "learning_rate": 4.640973418977492e-05,
      "loss": 1.9765,
      "step": 55000
    },
    {
      "epoch": 4.316152279492401,
      "grad_norm": 6.9117889404296875,
      "learning_rate": 4.6403206433756336e-05,
      "loss": 1.964,
      "step": 55100
    },
    {
      "epoch": 4.323985586714711,
      "grad_norm": 6.903436660766602,
      "learning_rate": 4.639667867773774e-05,
      "loss": 1.9788,
      "step": 55200
    },
    {
      "epoch": 4.331818893937021,
      "grad_norm": 6.580559253692627,
      "learning_rate": 4.639015092171915e-05,
      "loss": 1.9273,
      "step": 55300
    },
    {
      "epoch": 4.339652201159329,
      "grad_norm": 6.561908721923828,
      "learning_rate": 4.638362316570056e-05,
      "loss": 1.8826,
      "step": 55400
    },
    {
      "epoch": 4.347485508381639,
      "grad_norm": 7.178048610687256,
      "learning_rate": 4.637709540968197e-05,
      "loss": 1.8682,
      "step": 55500
    },
    {
      "epoch": 4.355318815603948,
      "grad_norm": 9.422646522521973,
      "learning_rate": 4.637056765366338e-05,
      "loss": 1.9669,
      "step": 55600
    },
    {
      "epoch": 4.363152122826257,
      "grad_norm": 6.354737758636475,
      "learning_rate": 4.636403989764479e-05,
      "loss": 1.902,
      "step": 55700
    },
    {
      "epoch": 4.370985430048567,
      "grad_norm": 6.140805244445801,
      "learning_rate": 4.6357512141626197e-05,
      "loss": 1.9545,
      "step": 55800
    },
    {
      "epoch": 4.378818737270876,
      "grad_norm": 7.026853084564209,
      "learning_rate": 4.63509843856076e-05,
      "loss": 1.8313,
      "step": 55900
    },
    {
      "epoch": 4.386652044493185,
      "grad_norm": 6.357251167297363,
      "learning_rate": 4.6344456629589015e-05,
      "loss": 2.0078,
      "step": 56000
    },
    {
      "epoch": 4.3944853517154945,
      "grad_norm": 6.684267997741699,
      "learning_rate": 4.633792887357043e-05,
      "loss": 1.9815,
      "step": 56100
    },
    {
      "epoch": 4.402318658937803,
      "grad_norm": 6.625532150268555,
      "learning_rate": 4.633140111755183e-05,
      "loss": 1.9906,
      "step": 56200
    },
    {
      "epoch": 4.410151966160113,
      "grad_norm": 7.196678638458252,
      "learning_rate": 4.6324873361533246e-05,
      "loss": 1.8841,
      "step": 56300
    },
    {
      "epoch": 4.417985273382422,
      "grad_norm": 11.299699783325195,
      "learning_rate": 4.631834560551465e-05,
      "loss": 1.852,
      "step": 56400
    },
    {
      "epoch": 4.425818580604731,
      "grad_norm": 7.713845252990723,
      "learning_rate": 4.631181784949606e-05,
      "loss": 1.9877,
      "step": 56500
    },
    {
      "epoch": 4.433651887827041,
      "grad_norm": 6.1951584815979,
      "learning_rate": 4.630529009347746e-05,
      "loss": 1.8902,
      "step": 56600
    },
    {
      "epoch": 4.44148519504935,
      "grad_norm": 9.220013618469238,
      "learning_rate": 4.6298762337458876e-05,
      "loss": 1.9665,
      "step": 56700
    },
    {
      "epoch": 4.449318502271659,
      "grad_norm": 7.338654041290283,
      "learning_rate": 4.629223458144029e-05,
      "loss": 1.9987,
      "step": 56800
    },
    {
      "epoch": 4.457151809493968,
      "grad_norm": 7.11026668548584,
      "learning_rate": 4.6285706825421694e-05,
      "loss": 1.8959,
      "step": 56900
    },
    {
      "epoch": 4.464985116716278,
      "grad_norm": 7.399552345275879,
      "learning_rate": 4.6279179069403106e-05,
      "loss": 1.8556,
      "step": 57000
    },
    {
      "epoch": 4.472818423938587,
      "grad_norm": 8.33662223815918,
      "learning_rate": 4.627265131338451e-05,
      "loss": 2.0218,
      "step": 57100
    },
    {
      "epoch": 4.480651731160896,
      "grad_norm": 6.2934441566467285,
      "learning_rate": 4.626612355736592e-05,
      "loss": 1.8611,
      "step": 57200
    },
    {
      "epoch": 4.488485038383206,
      "grad_norm": 8.759044647216797,
      "learning_rate": 4.625959580134733e-05,
      "loss": 1.9855,
      "step": 57300
    },
    {
      "epoch": 4.4963183456055145,
      "grad_norm": 6.525569438934326,
      "learning_rate": 4.625306804532874e-05,
      "loss": 2.0194,
      "step": 57400
    },
    {
      "epoch": 4.504151652827824,
      "grad_norm": 6.029094696044922,
      "learning_rate": 4.624654028931015e-05,
      "loss": 2.0035,
      "step": 57500
    },
    {
      "epoch": 4.511984960050134,
      "grad_norm": 6.5029988288879395,
      "learning_rate": 4.624001253329156e-05,
      "loss": 1.8619,
      "step": 57600
    },
    {
      "epoch": 4.519818267272442,
      "grad_norm": 4.666445255279541,
      "learning_rate": 4.623348477727297e-05,
      "loss": 1.9127,
      "step": 57700
    },
    {
      "epoch": 4.527651574494752,
      "grad_norm": 8.261005401611328,
      "learning_rate": 4.622695702125437e-05,
      "loss": 1.8173,
      "step": 57800
    },
    {
      "epoch": 4.5354848817170605,
      "grad_norm": 7.792538166046143,
      "learning_rate": 4.6220429265235785e-05,
      "loss": 2.0197,
      "step": 57900
    },
    {
      "epoch": 4.54331818893937,
      "grad_norm": 6.217899322509766,
      "learning_rate": 4.621390150921719e-05,
      "loss": 1.8723,
      "step": 58000
    },
    {
      "epoch": 4.55115149616168,
      "grad_norm": 6.783936977386475,
      "learning_rate": 4.6207373753198604e-05,
      "loss": 1.8819,
      "step": 58100
    },
    {
      "epoch": 4.558984803383988,
      "grad_norm": 10.116847038269043,
      "learning_rate": 4.620084599718001e-05,
      "loss": 1.889,
      "step": 58200
    },
    {
      "epoch": 4.566818110606298,
      "grad_norm": 7.487165927886963,
      "learning_rate": 4.619431824116142e-05,
      "loss": 1.9652,
      "step": 58300
    },
    {
      "epoch": 4.5746514178286075,
      "grad_norm": 6.539846420288086,
      "learning_rate": 4.618779048514283e-05,
      "loss": 1.9427,
      "step": 58400
    },
    {
      "epoch": 4.582484725050916,
      "grad_norm": 6.765420436859131,
      "learning_rate": 4.6181262729124234e-05,
      "loss": 1.915,
      "step": 58500
    },
    {
      "epoch": 4.590318032273226,
      "grad_norm": 6.260663032531738,
      "learning_rate": 4.6174734973105646e-05,
      "loss": 1.9235,
      "step": 58600
    },
    {
      "epoch": 4.598151339495535,
      "grad_norm": 7.285091400146484,
      "learning_rate": 4.616820721708706e-05,
      "loss": 1.9651,
      "step": 58700
    },
    {
      "epoch": 4.605984646717844,
      "grad_norm": 6.419942855834961,
      "learning_rate": 4.6161679461068464e-05,
      "loss": 1.9846,
      "step": 58800
    },
    {
      "epoch": 4.613817953940154,
      "grad_norm": 5.964732646942139,
      "learning_rate": 4.615515170504988e-05,
      "loss": 1.9748,
      "step": 58900
    },
    {
      "epoch": 4.621651261162463,
      "grad_norm": 6.303775787353516,
      "learning_rate": 4.614862394903128e-05,
      "loss": 1.9933,
      "step": 59000
    },
    {
      "epoch": 4.629484568384772,
      "grad_norm": 5.982430934906006,
      "learning_rate": 4.614209619301269e-05,
      "loss": 1.9052,
      "step": 59100
    },
    {
      "epoch": 4.637317875607081,
      "grad_norm": 7.142662048339844,
      "learning_rate": 4.61355684369941e-05,
      "loss": 1.9705,
      "step": 59200
    },
    {
      "epoch": 4.645151182829391,
      "grad_norm": 6.04142951965332,
      "learning_rate": 4.6129040680975514e-05,
      "loss": 1.915,
      "step": 59300
    },
    {
      "epoch": 4.6529844900517,
      "grad_norm": 7.0542311668396,
      "learning_rate": 4.612251292495692e-05,
      "loss": 1.9823,
      "step": 59400
    },
    {
      "epoch": 4.660817797274009,
      "grad_norm": 6.775838851928711,
      "learning_rate": 4.611598516893833e-05,
      "loss": 1.8852,
      "step": 59500
    },
    {
      "epoch": 4.668651104496318,
      "grad_norm": 7.165940284729004,
      "learning_rate": 4.610945741291974e-05,
      "loss": 2.0043,
      "step": 59600
    },
    {
      "epoch": 4.6764844117186275,
      "grad_norm": 4.976128578186035,
      "learning_rate": 4.6102929656901143e-05,
      "loss": 1.9055,
      "step": 59700
    },
    {
      "epoch": 4.684317718940937,
      "grad_norm": 6.062273025512695,
      "learning_rate": 4.609640190088255e-05,
      "loss": 1.9637,
      "step": 59800
    },
    {
      "epoch": 4.692151026163246,
      "grad_norm": 6.819700717926025,
      "learning_rate": 4.608987414486396e-05,
      "loss": 1.8625,
      "step": 59900
    },
    {
      "epoch": 4.699984333385555,
      "grad_norm": 7.241395950317383,
      "learning_rate": 4.6083346388845374e-05,
      "loss": 1.9326,
      "step": 60000
    },
    {
      "epoch": 4.707817640607865,
      "grad_norm": 7.522886753082275,
      "learning_rate": 4.607681863282678e-05,
      "loss": 1.9852,
      "step": 60100
    },
    {
      "epoch": 4.715650947830174,
      "grad_norm": 10.318901062011719,
      "learning_rate": 4.607029087680819e-05,
      "loss": 1.9838,
      "step": 60200
    },
    {
      "epoch": 4.723484255052483,
      "grad_norm": 5.200283527374268,
      "learning_rate": 4.60637631207896e-05,
      "loss": 1.893,
      "step": 60300
    },
    {
      "epoch": 4.731317562274793,
      "grad_norm": 6.97939920425415,
      "learning_rate": 4.6057235364771004e-05,
      "loss": 1.9133,
      "step": 60400
    },
    {
      "epoch": 4.739150869497101,
      "grad_norm": 8.326264381408691,
      "learning_rate": 4.605070760875242e-05,
      "loss": 1.944,
      "step": 60500
    },
    {
      "epoch": 4.746984176719411,
      "grad_norm": 15.997597694396973,
      "learning_rate": 4.604417985273383e-05,
      "loss": 1.9677,
      "step": 60600
    },
    {
      "epoch": 4.7548174839417205,
      "grad_norm": 7.873353004455566,
      "learning_rate": 4.6037652096715235e-05,
      "loss": 2.0208,
      "step": 60700
    },
    {
      "epoch": 4.762650791164029,
      "grad_norm": 6.327679634094238,
      "learning_rate": 4.603112434069665e-05,
      "loss": 1.8435,
      "step": 60800
    },
    {
      "epoch": 4.770484098386339,
      "grad_norm": 7.766221046447754,
      "learning_rate": 4.602459658467805e-05,
      "loss": 1.9333,
      "step": 60900
    },
    {
      "epoch": 4.778317405608648,
      "grad_norm": 8.137613296508789,
      "learning_rate": 4.601806882865946e-05,
      "loss": 1.8695,
      "step": 61000
    },
    {
      "epoch": 4.786150712830957,
      "grad_norm": 6.885122299194336,
      "learning_rate": 4.601154107264087e-05,
      "loss": 1.9169,
      "step": 61100
    },
    {
      "epoch": 4.793984020053267,
      "grad_norm": 6.307811260223389,
      "learning_rate": 4.600501331662228e-05,
      "loss": 1.9742,
      "step": 61200
    },
    {
      "epoch": 4.801817327275575,
      "grad_norm": 6.233839988708496,
      "learning_rate": 4.599848556060369e-05,
      "loss": 1.8653,
      "step": 61300
    },
    {
      "epoch": 4.809650634497885,
      "grad_norm": 8.768632888793945,
      "learning_rate": 4.59919578045851e-05,
      "loss": 1.8912,
      "step": 61400
    },
    {
      "epoch": 4.8174839417201945,
      "grad_norm": 7.21713924407959,
      "learning_rate": 4.598543004856651e-05,
      "loss": 1.8768,
      "step": 61500
    },
    {
      "epoch": 4.825317248942504,
      "grad_norm": 7.786301136016846,
      "learning_rate": 4.5978902292547914e-05,
      "loss": 1.9189,
      "step": 61600
    },
    {
      "epoch": 4.833150556164813,
      "grad_norm": 5.852537155151367,
      "learning_rate": 4.597237453652932e-05,
      "loss": 1.9858,
      "step": 61700
    },
    {
      "epoch": 4.840983863387122,
      "grad_norm": 7.281437873840332,
      "learning_rate": 4.596584678051073e-05,
      "loss": 1.8659,
      "step": 61800
    },
    {
      "epoch": 4.848817170609431,
      "grad_norm": 7.621211528778076,
      "learning_rate": 4.5959319024492145e-05,
      "loss": 1.9587,
      "step": 61900
    },
    {
      "epoch": 4.8566504778317405,
      "grad_norm": 6.548924922943115,
      "learning_rate": 4.595279126847355e-05,
      "loss": 1.9513,
      "step": 62000
    },
    {
      "epoch": 4.86448378505405,
      "grad_norm": 7.643063068389893,
      "learning_rate": 4.594626351245496e-05,
      "loss": 2.0351,
      "step": 62100
    },
    {
      "epoch": 4.872317092276359,
      "grad_norm": 6.988159656524658,
      "learning_rate": 4.593973575643637e-05,
      "loss": 1.9064,
      "step": 62200
    },
    {
      "epoch": 4.880150399498668,
      "grad_norm": 6.775688171386719,
      "learning_rate": 4.5933208000417775e-05,
      "loss": 1.9742,
      "step": 62300
    },
    {
      "epoch": 4.887983706720978,
      "grad_norm": 6.221900939941406,
      "learning_rate": 4.592668024439919e-05,
      "loss": 1.9897,
      "step": 62400
    },
    {
      "epoch": 4.895817013943287,
      "grad_norm": 7.185501575469971,
      "learning_rate": 4.59201524883806e-05,
      "loss": 1.9006,
      "step": 62500
    },
    {
      "epoch": 4.903650321165596,
      "grad_norm": 6.207765102386475,
      "learning_rate": 4.5913624732362006e-05,
      "loss": 1.887,
      "step": 62600
    },
    {
      "epoch": 4.911483628387906,
      "grad_norm": 6.583959579467773,
      "learning_rate": 4.590709697634342e-05,
      "loss": 1.874,
      "step": 62700
    },
    {
      "epoch": 4.919316935610214,
      "grad_norm": 11.468113899230957,
      "learning_rate": 4.5900569220324824e-05,
      "loss": 1.9299,
      "step": 62800
    },
    {
      "epoch": 4.927150242832524,
      "grad_norm": 7.5785393714904785,
      "learning_rate": 4.589404146430623e-05,
      "loss": 2.0365,
      "step": 62900
    },
    {
      "epoch": 4.934983550054833,
      "grad_norm": 7.486793041229248,
      "learning_rate": 4.588751370828764e-05,
      "loss": 1.9762,
      "step": 63000
    },
    {
      "epoch": 4.942816857277142,
      "grad_norm": 6.748518466949463,
      "learning_rate": 4.588098595226905e-05,
      "loss": 1.8902,
      "step": 63100
    },
    {
      "epoch": 4.950650164499452,
      "grad_norm": 6.482296943664551,
      "learning_rate": 4.587445819625046e-05,
      "loss": 1.9387,
      "step": 63200
    },
    {
      "epoch": 4.958483471721761,
      "grad_norm": 6.074260234832764,
      "learning_rate": 4.5867930440231866e-05,
      "loss": 1.9321,
      "step": 63300
    },
    {
      "epoch": 4.96631677894407,
      "grad_norm": 6.27500057220459,
      "learning_rate": 4.586140268421328e-05,
      "loss": 1.9173,
      "step": 63400
    },
    {
      "epoch": 4.97415008616638,
      "grad_norm": 8.875887870788574,
      "learning_rate": 4.5854874928194685e-05,
      "loss": 1.836,
      "step": 63500
    },
    {
      "epoch": 4.981983393388688,
      "grad_norm": 4.919683456420898,
      "learning_rate": 4.584834717217609e-05,
      "loss": 1.9036,
      "step": 63600
    },
    {
      "epoch": 4.989816700610998,
      "grad_norm": 6.337075710296631,
      "learning_rate": 4.58418194161575e-05,
      "loss": 1.9125,
      "step": 63700
    },
    {
      "epoch": 4.9976500078333075,
      "grad_norm": 7.7595295906066895,
      "learning_rate": 4.5835291660138915e-05,
      "loss": 2.0577,
      "step": 63800
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.8907723426818848,
      "eval_runtime": 2.9559,
      "eval_samples_per_second": 227.338,
      "eval_steps_per_second": 227.338,
      "step": 63830
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7178525924682617,
      "eval_runtime": 55.2308,
      "eval_samples_per_second": 231.139,
      "eval_steps_per_second": 231.139,
      "step": 63830
    }
  ],
  "logging_steps": 100,
  "max_steps": 765960,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 39498535065600.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
