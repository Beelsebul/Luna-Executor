{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.0,
  "eval_steps": 500,
  "global_step": 382980,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007833307222309259,
      "grad_norm": 11.540173530578613,
      "learning_rate": 4.999347224398141e-05,
      "loss": 4.6282,
      "step": 100
    },
    {
      "epoch": 0.015666614444618518,
      "grad_norm": 7.978172302246094,
      "learning_rate": 4.998694448796282e-05,
      "loss": 3.1429,
      "step": 200
    },
    {
      "epoch": 0.023499921666927777,
      "grad_norm": 7.5821146965026855,
      "learning_rate": 4.9980416731944226e-05,
      "loss": 2.8438,
      "step": 300
    },
    {
      "epoch": 0.031333228889237036,
      "grad_norm": 6.46599006652832,
      "learning_rate": 4.997388897592564e-05,
      "loss": 2.828,
      "step": 400
    },
    {
      "epoch": 0.039166536111546295,
      "grad_norm": 6.479909420013428,
      "learning_rate": 4.996736121990705e-05,
      "loss": 2.8127,
      "step": 500
    },
    {
      "epoch": 0.046999843333855554,
      "grad_norm": 5.755207061767578,
      "learning_rate": 4.996083346388846e-05,
      "loss": 2.6612,
      "step": 600
    },
    {
      "epoch": 0.05483315055616481,
      "grad_norm": 6.2831621170043945,
      "learning_rate": 4.995430570786986e-05,
      "loss": 2.6428,
      "step": 700
    },
    {
      "epoch": 0.06266645777847407,
      "grad_norm": 5.780747413635254,
      "learning_rate": 4.9947777951851276e-05,
      "loss": 2.6434,
      "step": 800
    },
    {
      "epoch": 0.07049976500078334,
      "grad_norm": 5.451594352722168,
      "learning_rate": 4.994125019583268e-05,
      "loss": 2.4887,
      "step": 900
    },
    {
      "epoch": 0.07833307222309259,
      "grad_norm": 5.487960338592529,
      "learning_rate": 4.9934722439814094e-05,
      "loss": 2.5645,
      "step": 1000
    },
    {
      "epoch": 0.08616637944540186,
      "grad_norm": 4.614451885223389,
      "learning_rate": 4.99281946837955e-05,
      "loss": 2.6156,
      "step": 1100
    },
    {
      "epoch": 0.09399968666771111,
      "grad_norm": 7.149703025817871,
      "learning_rate": 4.992166692777691e-05,
      "loss": 2.4604,
      "step": 1200
    },
    {
      "epoch": 0.10183299389002037,
      "grad_norm": 6.2258100509643555,
      "learning_rate": 4.991513917175832e-05,
      "loss": 2.5228,
      "step": 1300
    },
    {
      "epoch": 0.10966630111232963,
      "grad_norm": 5.4452009201049805,
      "learning_rate": 4.9908611415739724e-05,
      "loss": 2.5155,
      "step": 1400
    },
    {
      "epoch": 0.11749960833463889,
      "grad_norm": 6.061800479888916,
      "learning_rate": 4.9902083659721136e-05,
      "loss": 2.5487,
      "step": 1500
    },
    {
      "epoch": 0.12533291555694814,
      "grad_norm": 3.8109185695648193,
      "learning_rate": 4.989555590370255e-05,
      "loss": 2.4751,
      "step": 1600
    },
    {
      "epoch": 0.1331662227792574,
      "grad_norm": 6.115899562835693,
      "learning_rate": 4.9889028147683955e-05,
      "loss": 2.3672,
      "step": 1700
    },
    {
      "epoch": 0.14099953000156668,
      "grad_norm": 6.503889083862305,
      "learning_rate": 4.988250039166537e-05,
      "loss": 2.4864,
      "step": 1800
    },
    {
      "epoch": 0.1488328372238759,
      "grad_norm": 5.980323314666748,
      "learning_rate": 4.987597263564677e-05,
      "loss": 2.4674,
      "step": 1900
    },
    {
      "epoch": 0.15666614444618518,
      "grad_norm": 6.22133731842041,
      "learning_rate": 4.986944487962818e-05,
      "loss": 2.4429,
      "step": 2000
    },
    {
      "epoch": 0.16449945166849445,
      "grad_norm": 4.67347526550293,
      "learning_rate": 4.986291712360959e-05,
      "loss": 2.4223,
      "step": 2100
    },
    {
      "epoch": 0.1723327588908037,
      "grad_norm": 5.363992691040039,
      "learning_rate": 4.9856389367591e-05,
      "loss": 2.3662,
      "step": 2200
    },
    {
      "epoch": 0.18016606611311295,
      "grad_norm": 5.601268291473389,
      "learning_rate": 4.984986161157241e-05,
      "loss": 2.4572,
      "step": 2300
    },
    {
      "epoch": 0.18799937333542222,
      "grad_norm": 4.384665489196777,
      "learning_rate": 4.984333385555382e-05,
      "loss": 2.2841,
      "step": 2400
    },
    {
      "epoch": 0.19583268055773148,
      "grad_norm": 4.9136271476745605,
      "learning_rate": 4.983680609953523e-05,
      "loss": 2.4559,
      "step": 2500
    },
    {
      "epoch": 0.20366598778004075,
      "grad_norm": 4.50607442855835,
      "learning_rate": 4.9830278343516634e-05,
      "loss": 2.4366,
      "step": 2600
    },
    {
      "epoch": 0.21149929500234999,
      "grad_norm": 6.407818794250488,
      "learning_rate": 4.982375058749804e-05,
      "loss": 2.39,
      "step": 2700
    },
    {
      "epoch": 0.21933260222465925,
      "grad_norm": 4.789163112640381,
      "learning_rate": 4.981722283147945e-05,
      "loss": 2.3475,
      "step": 2800
    },
    {
      "epoch": 0.22716590944696852,
      "grad_norm": 6.096295356750488,
      "learning_rate": 4.9810695075460865e-05,
      "loss": 2.3772,
      "step": 2900
    },
    {
      "epoch": 0.23499921666927778,
      "grad_norm": 4.49558687210083,
      "learning_rate": 4.980416731944227e-05,
      "loss": 2.4344,
      "step": 3000
    },
    {
      "epoch": 0.24283252389158702,
      "grad_norm": 7.013221740722656,
      "learning_rate": 4.979763956342368e-05,
      "loss": 2.386,
      "step": 3100
    },
    {
      "epoch": 0.2506658311138963,
      "grad_norm": 5.5625200271606445,
      "learning_rate": 4.979111180740509e-05,
      "loss": 2.4325,
      "step": 3200
    },
    {
      "epoch": 0.2584991383362055,
      "grad_norm": 5.087408065795898,
      "learning_rate": 4.9784584051386494e-05,
      "loss": 2.2724,
      "step": 3300
    },
    {
      "epoch": 0.2663324455585148,
      "grad_norm": 5.005785942077637,
      "learning_rate": 4.977805629536791e-05,
      "loss": 2.3708,
      "step": 3400
    },
    {
      "epoch": 0.27416575278082406,
      "grad_norm": 5.801197528839111,
      "learning_rate": 4.977152853934931e-05,
      "loss": 2.345,
      "step": 3500
    },
    {
      "epoch": 0.28199906000313335,
      "grad_norm": 4.471706867218018,
      "learning_rate": 4.9765000783330725e-05,
      "loss": 2.3535,
      "step": 3600
    },
    {
      "epoch": 0.2898323672254426,
      "grad_norm": 5.845236778259277,
      "learning_rate": 4.975847302731214e-05,
      "loss": 2.3785,
      "step": 3700
    },
    {
      "epoch": 0.2976656744477518,
      "grad_norm": 5.005009174346924,
      "learning_rate": 4.9751945271293544e-05,
      "loss": 2.269,
      "step": 3800
    },
    {
      "epoch": 0.3054989816700611,
      "grad_norm": 4.719980716705322,
      "learning_rate": 4.974541751527495e-05,
      "loss": 2.2941,
      "step": 3900
    },
    {
      "epoch": 0.31333228889237036,
      "grad_norm": 5.734504222869873,
      "learning_rate": 4.973888975925636e-05,
      "loss": 2.3889,
      "step": 4000
    },
    {
      "epoch": 0.3211655961146796,
      "grad_norm": 5.682164669036865,
      "learning_rate": 4.973236200323777e-05,
      "loss": 2.3217,
      "step": 4100
    },
    {
      "epoch": 0.3289989033369889,
      "grad_norm": 4.343432426452637,
      "learning_rate": 4.972583424721918e-05,
      "loss": 2.3216,
      "step": 4200
    },
    {
      "epoch": 0.33683221055929813,
      "grad_norm": 4.926867485046387,
      "learning_rate": 4.9719306491200586e-05,
      "loss": 2.3243,
      "step": 4300
    },
    {
      "epoch": 0.3446655177816074,
      "grad_norm": 5.18257474899292,
      "learning_rate": 4.9712778735182e-05,
      "loss": 2.3182,
      "step": 4400
    },
    {
      "epoch": 0.35249882500391666,
      "grad_norm": 5.198777198791504,
      "learning_rate": 4.9706250979163404e-05,
      "loss": 2.3698,
      "step": 4500
    },
    {
      "epoch": 0.3603321322262259,
      "grad_norm": 4.421065330505371,
      "learning_rate": 4.969972322314481e-05,
      "loss": 2.3161,
      "step": 4600
    },
    {
      "epoch": 0.3681654394485352,
      "grad_norm": 4.067595481872559,
      "learning_rate": 4.969319546712622e-05,
      "loss": 2.3156,
      "step": 4700
    },
    {
      "epoch": 0.37599874667084443,
      "grad_norm": 6.632781028747559,
      "learning_rate": 4.9686667711107635e-05,
      "loss": 2.3182,
      "step": 4800
    },
    {
      "epoch": 0.38383205389315367,
      "grad_norm": 4.6471848487854,
      "learning_rate": 4.968013995508904e-05,
      "loss": 2.2918,
      "step": 4900
    },
    {
      "epoch": 0.39166536111546296,
      "grad_norm": 4.728133678436279,
      "learning_rate": 4.9673612199070453e-05,
      "loss": 2.2637,
      "step": 5000
    },
    {
      "epoch": 0.3994986683377722,
      "grad_norm": 5.720607280731201,
      "learning_rate": 4.966708444305186e-05,
      "loss": 2.3336,
      "step": 5100
    },
    {
      "epoch": 0.4073319755600815,
      "grad_norm": 4.715705394744873,
      "learning_rate": 4.9660556687033265e-05,
      "loss": 2.3017,
      "step": 5200
    },
    {
      "epoch": 0.41516528278239073,
      "grad_norm": 5.488768100738525,
      "learning_rate": 4.965402893101468e-05,
      "loss": 2.3278,
      "step": 5300
    },
    {
      "epoch": 0.42299859000469997,
      "grad_norm": 3.787397861480713,
      "learning_rate": 4.964750117499608e-05,
      "loss": 2.2712,
      "step": 5400
    },
    {
      "epoch": 0.43083189722700926,
      "grad_norm": 4.577098846435547,
      "learning_rate": 4.9640973418977496e-05,
      "loss": 2.3534,
      "step": 5500
    },
    {
      "epoch": 0.4386652044493185,
      "grad_norm": 5.038837909698486,
      "learning_rate": 4.963444566295891e-05,
      "loss": 2.269,
      "step": 5600
    },
    {
      "epoch": 0.44649851167162774,
      "grad_norm": 4.650664806365967,
      "learning_rate": 4.9627917906940314e-05,
      "loss": 2.3771,
      "step": 5700
    },
    {
      "epoch": 0.45433181889393703,
      "grad_norm": 4.2072248458862305,
      "learning_rate": 4.962139015092172e-05,
      "loss": 2.2643,
      "step": 5800
    },
    {
      "epoch": 0.46216512611624627,
      "grad_norm": 4.1768317222595215,
      "learning_rate": 4.9614862394903126e-05,
      "loss": 2.2683,
      "step": 5900
    },
    {
      "epoch": 0.46999843333855557,
      "grad_norm": 4.833404064178467,
      "learning_rate": 4.960833463888454e-05,
      "loss": 2.3385,
      "step": 6000
    },
    {
      "epoch": 0.4778317405608648,
      "grad_norm": 4.691647529602051,
      "learning_rate": 4.960180688286595e-05,
      "loss": 2.2282,
      "step": 6100
    },
    {
      "epoch": 0.48566504778317404,
      "grad_norm": 5.124332904815674,
      "learning_rate": 4.9595279126847357e-05,
      "loss": 2.2906,
      "step": 6200
    },
    {
      "epoch": 0.49349835500548334,
      "grad_norm": 4.049105644226074,
      "learning_rate": 4.958875137082877e-05,
      "loss": 2.2569,
      "step": 6300
    },
    {
      "epoch": 0.5013316622277926,
      "grad_norm": 4.43967342376709,
      "learning_rate": 4.9582223614810175e-05,
      "loss": 2.184,
      "step": 6400
    },
    {
      "epoch": 0.5091649694501018,
      "grad_norm": 6.591732978820801,
      "learning_rate": 4.957569585879158e-05,
      "loss": 2.2487,
      "step": 6500
    },
    {
      "epoch": 0.516998276672411,
      "grad_norm": 4.613829612731934,
      "learning_rate": 4.956916810277299e-05,
      "loss": 2.1903,
      "step": 6600
    },
    {
      "epoch": 0.5248315838947204,
      "grad_norm": 5.528923034667969,
      "learning_rate": 4.95626403467544e-05,
      "loss": 2.2963,
      "step": 6700
    },
    {
      "epoch": 0.5326648911170296,
      "grad_norm": 6.3916015625,
      "learning_rate": 4.955611259073581e-05,
      "loss": 2.2907,
      "step": 6800
    },
    {
      "epoch": 0.5404981983393389,
      "grad_norm": 6.273540496826172,
      "learning_rate": 4.9549584834717224e-05,
      "loss": 2.2568,
      "step": 6900
    },
    {
      "epoch": 0.5483315055616481,
      "grad_norm": 5.479650497436523,
      "learning_rate": 4.954305707869863e-05,
      "loss": 2.2672,
      "step": 7000
    },
    {
      "epoch": 0.5561648127839574,
      "grad_norm": 4.9421234130859375,
      "learning_rate": 4.9536529322680036e-05,
      "loss": 2.3497,
      "step": 7100
    },
    {
      "epoch": 0.5639981200062667,
      "grad_norm": 6.754128932952881,
      "learning_rate": 4.953000156666145e-05,
      "loss": 2.2255,
      "step": 7200
    },
    {
      "epoch": 0.5718314272285759,
      "grad_norm": 6.953390598297119,
      "learning_rate": 4.9523473810642854e-05,
      "loss": 2.2478,
      "step": 7300
    },
    {
      "epoch": 0.5796647344508852,
      "grad_norm": 4.988915920257568,
      "learning_rate": 4.9516946054624266e-05,
      "loss": 2.1897,
      "step": 7400
    },
    {
      "epoch": 0.5874980416731944,
      "grad_norm": 5.7152791023254395,
      "learning_rate": 4.951041829860568e-05,
      "loss": 2.2343,
      "step": 7500
    },
    {
      "epoch": 0.5953313488955037,
      "grad_norm": 5.915982246398926,
      "learning_rate": 4.9503890542587085e-05,
      "loss": 2.2887,
      "step": 7600
    },
    {
      "epoch": 0.6031646561178129,
      "grad_norm": 5.462746620178223,
      "learning_rate": 4.949736278656849e-05,
      "loss": 2.155,
      "step": 7700
    },
    {
      "epoch": 0.6109979633401222,
      "grad_norm": 5.5770487785339355,
      "learning_rate": 4.9490835030549896e-05,
      "loss": 2.2191,
      "step": 7800
    },
    {
      "epoch": 0.6188312705624315,
      "grad_norm": 5.446353912353516,
      "learning_rate": 4.948430727453131e-05,
      "loss": 2.2595,
      "step": 7900
    },
    {
      "epoch": 0.6266645777847407,
      "grad_norm": 4.3125081062316895,
      "learning_rate": 4.947777951851272e-05,
      "loss": 2.1652,
      "step": 8000
    },
    {
      "epoch": 0.63449788500705,
      "grad_norm": 4.842166423797607,
      "learning_rate": 4.947125176249413e-05,
      "loss": 2.2172,
      "step": 8100
    },
    {
      "epoch": 0.6423311922293592,
      "grad_norm": 4.451799392700195,
      "learning_rate": 4.946472400647554e-05,
      "loss": 2.1637,
      "step": 8200
    },
    {
      "epoch": 0.6501644994516685,
      "grad_norm": 5.307904243469238,
      "learning_rate": 4.9458196250456945e-05,
      "loss": 2.1575,
      "step": 8300
    },
    {
      "epoch": 0.6579978066739778,
      "grad_norm": 6.706522464752197,
      "learning_rate": 4.945166849443835e-05,
      "loss": 2.1968,
      "step": 8400
    },
    {
      "epoch": 0.665831113896287,
      "grad_norm": 5.0111823081970215,
      "learning_rate": 4.9445140738419764e-05,
      "loss": 2.331,
      "step": 8500
    },
    {
      "epoch": 0.6736644211185963,
      "grad_norm": 4.672807693481445,
      "learning_rate": 4.943861298240117e-05,
      "loss": 2.3216,
      "step": 8600
    },
    {
      "epoch": 0.6814977283409055,
      "grad_norm": 6.1778645515441895,
      "learning_rate": 4.943208522638258e-05,
      "loss": 2.1729,
      "step": 8700
    },
    {
      "epoch": 0.6893310355632148,
      "grad_norm": 5.289903163909912,
      "learning_rate": 4.9425557470363995e-05,
      "loss": 2.2065,
      "step": 8800
    },
    {
      "epoch": 0.6971643427855241,
      "grad_norm": 4.657099723815918,
      "learning_rate": 4.94190297143454e-05,
      "loss": 2.1855,
      "step": 8900
    },
    {
      "epoch": 0.7049976500078333,
      "grad_norm": 4.221887588500977,
      "learning_rate": 4.9412501958326806e-05,
      "loss": 2.3026,
      "step": 9000
    },
    {
      "epoch": 0.7128309572301426,
      "grad_norm": 5.26032829284668,
      "learning_rate": 4.940597420230822e-05,
      "loss": 2.203,
      "step": 9100
    },
    {
      "epoch": 0.7206642644524518,
      "grad_norm": 7.589452266693115,
      "learning_rate": 4.9399446446289624e-05,
      "loss": 2.1524,
      "step": 9200
    },
    {
      "epoch": 0.728497571674761,
      "grad_norm": 4.6626482009887695,
      "learning_rate": 4.939291869027104e-05,
      "loss": 2.1899,
      "step": 9300
    },
    {
      "epoch": 0.7363308788970704,
      "grad_norm": 4.3108625411987305,
      "learning_rate": 4.938639093425244e-05,
      "loss": 2.1656,
      "step": 9400
    },
    {
      "epoch": 0.7441641861193796,
      "grad_norm": 5.290939807891846,
      "learning_rate": 4.9379863178233855e-05,
      "loss": 2.2053,
      "step": 9500
    },
    {
      "epoch": 0.7519974933416889,
      "grad_norm": 6.35273551940918,
      "learning_rate": 4.937333542221526e-05,
      "loss": 2.2058,
      "step": 9600
    },
    {
      "epoch": 0.7598308005639981,
      "grad_norm": 5.47995662689209,
      "learning_rate": 4.936680766619667e-05,
      "loss": 2.2339,
      "step": 9700
    },
    {
      "epoch": 0.7676641077863073,
      "grad_norm": 5.519262313842773,
      "learning_rate": 4.936027991017808e-05,
      "loss": 2.1605,
      "step": 9800
    },
    {
      "epoch": 0.7754974150086167,
      "grad_norm": 4.763408184051514,
      "learning_rate": 4.9353752154159485e-05,
      "loss": 2.22,
      "step": 9900
    },
    {
      "epoch": 0.7833307222309259,
      "grad_norm": 5.280427932739258,
      "learning_rate": 4.93472243981409e-05,
      "loss": 2.3148,
      "step": 10000
    },
    {
      "epoch": 0.7911640294532352,
      "grad_norm": 4.728559970855713,
      "learning_rate": 4.934069664212231e-05,
      "loss": 2.2671,
      "step": 10100
    },
    {
      "epoch": 0.7989973366755444,
      "grad_norm": 6.843000411987305,
      "learning_rate": 4.9334168886103716e-05,
      "loss": 2.2031,
      "step": 10200
    },
    {
      "epoch": 0.8068306438978536,
      "grad_norm": 4.864438056945801,
      "learning_rate": 4.932764113008512e-05,
      "loss": 2.0999,
      "step": 10300
    },
    {
      "epoch": 0.814663951120163,
      "grad_norm": 4.980437755584717,
      "learning_rate": 4.9321113374066534e-05,
      "loss": 2.1602,
      "step": 10400
    },
    {
      "epoch": 0.8224972583424722,
      "grad_norm": 4.805253982543945,
      "learning_rate": 4.931458561804794e-05,
      "loss": 2.1446,
      "step": 10500
    },
    {
      "epoch": 0.8303305655647815,
      "grad_norm": 5.506919860839844,
      "learning_rate": 4.930805786202935e-05,
      "loss": 2.2293,
      "step": 10600
    },
    {
      "epoch": 0.8381638727870907,
      "grad_norm": 5.158807277679443,
      "learning_rate": 4.9301530106010765e-05,
      "loss": 2.188,
      "step": 10700
    },
    {
      "epoch": 0.8459971800093999,
      "grad_norm": 5.235378742218018,
      "learning_rate": 4.929500234999217e-05,
      "loss": 2.1438,
      "step": 10800
    },
    {
      "epoch": 0.8538304872317092,
      "grad_norm": 4.964086055755615,
      "learning_rate": 4.928847459397358e-05,
      "loss": 2.1181,
      "step": 10900
    },
    {
      "epoch": 0.8616637944540185,
      "grad_norm": 4.715092182159424,
      "learning_rate": 4.928194683795498e-05,
      "loss": 2.1524,
      "step": 11000
    },
    {
      "epoch": 0.8694971016763278,
      "grad_norm": 8.22596263885498,
      "learning_rate": 4.9275419081936395e-05,
      "loss": 2.2792,
      "step": 11100
    },
    {
      "epoch": 0.877330408898637,
      "grad_norm": 4.21669340133667,
      "learning_rate": 4.926889132591781e-05,
      "loss": 2.2717,
      "step": 11200
    },
    {
      "epoch": 0.8851637161209462,
      "grad_norm": 4.555757999420166,
      "learning_rate": 4.926236356989921e-05,
      "loss": 2.2667,
      "step": 11300
    },
    {
      "epoch": 0.8929970233432555,
      "grad_norm": 5.1033244132995605,
      "learning_rate": 4.9255835813880626e-05,
      "loss": 2.1403,
      "step": 11400
    },
    {
      "epoch": 0.9008303305655648,
      "grad_norm": 6.477269649505615,
      "learning_rate": 4.924930805786203e-05,
      "loss": 2.3175,
      "step": 11500
    },
    {
      "epoch": 0.9086636377878741,
      "grad_norm": 4.364744186401367,
      "learning_rate": 4.924278030184344e-05,
      "loss": 2.1666,
      "step": 11600
    },
    {
      "epoch": 0.9164969450101833,
      "grad_norm": 6.397687911987305,
      "learning_rate": 4.923625254582485e-05,
      "loss": 2.126,
      "step": 11700
    },
    {
      "epoch": 0.9243302522324925,
      "grad_norm": 5.804946422576904,
      "learning_rate": 4.9229724789806256e-05,
      "loss": 2.19,
      "step": 11800
    },
    {
      "epoch": 0.9321635594548018,
      "grad_norm": 5.370138645172119,
      "learning_rate": 4.922319703378767e-05,
      "loss": 2.1406,
      "step": 11900
    },
    {
      "epoch": 0.9399968666771111,
      "grad_norm": 5.041280269622803,
      "learning_rate": 4.921666927776908e-05,
      "loss": 2.2183,
      "step": 12000
    },
    {
      "epoch": 0.9478301738994204,
      "grad_norm": 3.7907209396362305,
      "learning_rate": 4.921014152175049e-05,
      "loss": 2.2769,
      "step": 12100
    },
    {
      "epoch": 0.9556634811217296,
      "grad_norm": 4.192178249359131,
      "learning_rate": 4.920361376573189e-05,
      "loss": 2.239,
      "step": 12200
    },
    {
      "epoch": 0.9634967883440388,
      "grad_norm": 6.872035503387451,
      "learning_rate": 4.9197086009713305e-05,
      "loss": 2.1131,
      "step": 12300
    },
    {
      "epoch": 0.9713300955663481,
      "grad_norm": 4.8373494148254395,
      "learning_rate": 4.919055825369471e-05,
      "loss": 2.1748,
      "step": 12400
    },
    {
      "epoch": 0.9791634027886573,
      "grad_norm": 4.575335502624512,
      "learning_rate": 4.918403049767612e-05,
      "loss": 2.221,
      "step": 12500
    },
    {
      "epoch": 0.9869967100109667,
      "grad_norm": 4.573701858520508,
      "learning_rate": 4.9177502741657536e-05,
      "loss": 2.1481,
      "step": 12600
    },
    {
      "epoch": 0.9948300172332759,
      "grad_norm": 5.283926963806152,
      "learning_rate": 4.917097498563894e-05,
      "loss": 2.1947,
      "step": 12700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.069763660430908,
      "eval_runtime": 2.9834,
      "eval_samples_per_second": 225.249,
      "eval_steps_per_second": 225.249,
      "step": 12766
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.958105444908142,
      "eval_runtime": 58.6426,
      "eval_samples_per_second": 217.692,
      "eval_steps_per_second": 217.692,
      "step": 12766
    },
    {
      "epoch": 1.0026633244555851,
      "grad_norm": 4.797819137573242,
      "learning_rate": 4.916444722962035e-05,
      "loss": 2.1368,
      "step": 12800
    },
    {
      "epoch": 1.0104966316778945,
      "grad_norm": 4.901352882385254,
      "learning_rate": 4.915791947360175e-05,
      "loss": 2.1786,
      "step": 12900
    },
    {
      "epoch": 1.0183299389002036,
      "grad_norm": 5.069640159606934,
      "learning_rate": 4.9151391717583166e-05,
      "loss": 2.1312,
      "step": 13000
    },
    {
      "epoch": 1.026163246122513,
      "grad_norm": 4.644036769866943,
      "learning_rate": 4.914486396156457e-05,
      "loss": 2.098,
      "step": 13100
    },
    {
      "epoch": 1.033996553344822,
      "grad_norm": 4.969990253448486,
      "learning_rate": 4.9138336205545984e-05,
      "loss": 2.0808,
      "step": 13200
    },
    {
      "epoch": 1.0418298605671314,
      "grad_norm": 5.422013759613037,
      "learning_rate": 4.9131808449527396e-05,
      "loss": 2.1426,
      "step": 13300
    },
    {
      "epoch": 1.0496631677894408,
      "grad_norm": 5.716988563537598,
      "learning_rate": 4.91252806935088e-05,
      "loss": 2.1562,
      "step": 13400
    },
    {
      "epoch": 1.05749647501175,
      "grad_norm": 4.65447473526001,
      "learning_rate": 4.911875293749021e-05,
      "loss": 2.1109,
      "step": 13500
    },
    {
      "epoch": 1.0653297822340593,
      "grad_norm": 4.518239974975586,
      "learning_rate": 4.911222518147162e-05,
      "loss": 2.1415,
      "step": 13600
    },
    {
      "epoch": 1.0731630894563684,
      "grad_norm": 5.108067989349365,
      "learning_rate": 4.9105697425453026e-05,
      "loss": 2.1495,
      "step": 13700
    },
    {
      "epoch": 1.0809963966786778,
      "grad_norm": 5.551692962646484,
      "learning_rate": 4.909916966943444e-05,
      "loss": 2.1284,
      "step": 13800
    },
    {
      "epoch": 1.088829703900987,
      "grad_norm": 6.631732940673828,
      "learning_rate": 4.909264191341585e-05,
      "loss": 2.1699,
      "step": 13900
    },
    {
      "epoch": 1.0966630111232962,
      "grad_norm": 3.5112507343292236,
      "learning_rate": 4.908611415739726e-05,
      "loss": 2.1287,
      "step": 14000
    },
    {
      "epoch": 1.1044963183456056,
      "grad_norm": 5.482921123504639,
      "learning_rate": 4.907958640137866e-05,
      "loss": 2.1101,
      "step": 14100
    },
    {
      "epoch": 1.1123296255679147,
      "grad_norm": 5.71177864074707,
      "learning_rate": 4.9073058645360076e-05,
      "loss": 2.0993,
      "step": 14200
    },
    {
      "epoch": 1.120162932790224,
      "grad_norm": 5.233309745788574,
      "learning_rate": 4.906653088934148e-05,
      "loss": 2.1248,
      "step": 14300
    },
    {
      "epoch": 1.1279962400125334,
      "grad_norm": 4.268956661224365,
      "learning_rate": 4.9060003133322894e-05,
      "loss": 2.1269,
      "step": 14400
    },
    {
      "epoch": 1.1358295472348425,
      "grad_norm": 4.8624491691589355,
      "learning_rate": 4.90534753773043e-05,
      "loss": 2.0442,
      "step": 14500
    },
    {
      "epoch": 1.1436628544571519,
      "grad_norm": 5.60540771484375,
      "learning_rate": 4.904694762128571e-05,
      "loss": 2.1624,
      "step": 14600
    },
    {
      "epoch": 1.151496161679461,
      "grad_norm": 7.3562397956848145,
      "learning_rate": 4.904041986526712e-05,
      "loss": 2.1696,
      "step": 14700
    },
    {
      "epoch": 1.1593294689017704,
      "grad_norm": 5.473916053771973,
      "learning_rate": 4.9033892109248524e-05,
      "loss": 2.1813,
      "step": 14800
    },
    {
      "epoch": 1.1671627761240795,
      "grad_norm": 4.109088897705078,
      "learning_rate": 4.9027364353229936e-05,
      "loss": 2.1025,
      "step": 14900
    },
    {
      "epoch": 1.1749960833463888,
      "grad_norm": 7.242049694061279,
      "learning_rate": 4.902083659721134e-05,
      "loss": 2.0953,
      "step": 15000
    },
    {
      "epoch": 1.1828293905686982,
      "grad_norm": 5.121238708496094,
      "learning_rate": 4.9014308841192755e-05,
      "loss": 2.0978,
      "step": 15100
    },
    {
      "epoch": 1.1906626977910073,
      "grad_norm": 5.301401615142822,
      "learning_rate": 4.900778108517417e-05,
      "loss": 2.1584,
      "step": 15200
    },
    {
      "epoch": 1.1984960050133167,
      "grad_norm": 5.459949970245361,
      "learning_rate": 4.900125332915557e-05,
      "loss": 2.1273,
      "step": 15300
    },
    {
      "epoch": 1.206329312235626,
      "grad_norm": 6.621047019958496,
      "learning_rate": 4.899472557313698e-05,
      "loss": 2.1576,
      "step": 15400
    },
    {
      "epoch": 1.2141626194579351,
      "grad_norm": 4.139832019805908,
      "learning_rate": 4.898819781711839e-05,
      "loss": 2.0992,
      "step": 15500
    },
    {
      "epoch": 1.2219959266802445,
      "grad_norm": 6.305038928985596,
      "learning_rate": 4.89816700610998e-05,
      "loss": 2.1827,
      "step": 15600
    },
    {
      "epoch": 1.2298292339025536,
      "grad_norm": 4.325533390045166,
      "learning_rate": 4.897514230508121e-05,
      "loss": 2.1032,
      "step": 15700
    },
    {
      "epoch": 1.237662541124863,
      "grad_norm": 6.65619421005249,
      "learning_rate": 4.896861454906262e-05,
      "loss": 2.2169,
      "step": 15800
    },
    {
      "epoch": 1.245495848347172,
      "grad_norm": 4.267451286315918,
      "learning_rate": 4.896208679304403e-05,
      "loss": 2.1631,
      "step": 15900
    },
    {
      "epoch": 1.2533291555694814,
      "grad_norm": 4.514301300048828,
      "learning_rate": 4.8955559037025434e-05,
      "loss": 2.1983,
      "step": 16000
    },
    {
      "epoch": 1.2611624627917908,
      "grad_norm": 4.758921146392822,
      "learning_rate": 4.894903128100684e-05,
      "loss": 2.172,
      "step": 16100
    },
    {
      "epoch": 1.2689957700141,
      "grad_norm": 5.194299697875977,
      "learning_rate": 4.894250352498825e-05,
      "loss": 2.1682,
      "step": 16200
    },
    {
      "epoch": 1.2768290772364093,
      "grad_norm": 4.1105804443359375,
      "learning_rate": 4.893597576896966e-05,
      "loss": 2.1983,
      "step": 16300
    },
    {
      "epoch": 1.2846623844587186,
      "grad_norm": 4.361325263977051,
      "learning_rate": 4.892944801295107e-05,
      "loss": 2.1616,
      "step": 16400
    },
    {
      "epoch": 1.2924956916810277,
      "grad_norm": 4.6535563468933105,
      "learning_rate": 4.892292025693248e-05,
      "loss": 2.1683,
      "step": 16500
    },
    {
      "epoch": 1.3003289989033369,
      "grad_norm": 4.2735490798950195,
      "learning_rate": 4.891639250091389e-05,
      "loss": 2.1878,
      "step": 16600
    },
    {
      "epoch": 1.3081623061256462,
      "grad_norm": 5.239178657531738,
      "learning_rate": 4.8909864744895294e-05,
      "loss": 2.0747,
      "step": 16700
    },
    {
      "epoch": 1.3159956133479556,
      "grad_norm": 4.508316993713379,
      "learning_rate": 4.890333698887671e-05,
      "loss": 2.0831,
      "step": 16800
    },
    {
      "epoch": 1.3238289205702647,
      "grad_norm": 5.145707130432129,
      "learning_rate": 4.889680923285811e-05,
      "loss": 2.1291,
      "step": 16900
    },
    {
      "epoch": 1.331662227792574,
      "grad_norm": 5.75711727142334,
      "learning_rate": 4.8890281476839525e-05,
      "loss": 2.0739,
      "step": 17000
    },
    {
      "epoch": 1.3394955350148834,
      "grad_norm": 8.339942932128906,
      "learning_rate": 4.888375372082094e-05,
      "loss": 2.1796,
      "step": 17100
    },
    {
      "epoch": 1.3473288422371925,
      "grad_norm": 5.613107681274414,
      "learning_rate": 4.8877225964802343e-05,
      "loss": 2.1361,
      "step": 17200
    },
    {
      "epoch": 1.3551621494595019,
      "grad_norm": 5.384558200836182,
      "learning_rate": 4.887069820878375e-05,
      "loss": 2.0309,
      "step": 17300
    },
    {
      "epoch": 1.362995456681811,
      "grad_norm": 4.268584728240967,
      "learning_rate": 4.886417045276516e-05,
      "loss": 2.071,
      "step": 17400
    },
    {
      "epoch": 1.3708287639041203,
      "grad_norm": 3.901287794113159,
      "learning_rate": 4.885764269674657e-05,
      "loss": 2.069,
      "step": 17500
    },
    {
      "epoch": 1.3786620711264295,
      "grad_norm": 4.0657958984375,
      "learning_rate": 4.885111494072798e-05,
      "loss": 2.1827,
      "step": 17600
    },
    {
      "epoch": 1.3864953783487388,
      "grad_norm": 5.088072299957275,
      "learning_rate": 4.8844587184709386e-05,
      "loss": 2.1095,
      "step": 17700
    },
    {
      "epoch": 1.3943286855710482,
      "grad_norm": 5.862856864929199,
      "learning_rate": 4.88380594286908e-05,
      "loss": 2.2273,
      "step": 17800
    },
    {
      "epoch": 1.4021619927933573,
      "grad_norm": 5.161714553833008,
      "learning_rate": 4.8831531672672204e-05,
      "loss": 1.9926,
      "step": 17900
    },
    {
      "epoch": 1.4099953000156666,
      "grad_norm": 4.77968692779541,
      "learning_rate": 4.882500391665361e-05,
      "loss": 2.0401,
      "step": 18000
    },
    {
      "epoch": 1.417828607237976,
      "grad_norm": 5.626577377319336,
      "learning_rate": 4.881847616063502e-05,
      "loss": 2.0558,
      "step": 18100
    },
    {
      "epoch": 1.4256619144602851,
      "grad_norm": 4.7220282554626465,
      "learning_rate": 4.881194840461643e-05,
      "loss": 2.1226,
      "step": 18200
    },
    {
      "epoch": 1.4334952216825945,
      "grad_norm": 7.1769866943359375,
      "learning_rate": 4.880542064859784e-05,
      "loss": 2.0468,
      "step": 18300
    },
    {
      "epoch": 1.4413285289049036,
      "grad_norm": 4.1683669090271,
      "learning_rate": 4.879889289257925e-05,
      "loss": 2.2046,
      "step": 18400
    },
    {
      "epoch": 1.449161836127213,
      "grad_norm": 4.47035551071167,
      "learning_rate": 4.879236513656066e-05,
      "loss": 2.092,
      "step": 18500
    },
    {
      "epoch": 1.456995143349522,
      "grad_norm": 5.748067855834961,
      "learning_rate": 4.8785837380542065e-05,
      "loss": 2.1413,
      "step": 18600
    },
    {
      "epoch": 1.4648284505718314,
      "grad_norm": 5.2364606857299805,
      "learning_rate": 4.877930962452348e-05,
      "loss": 2.0636,
      "step": 18700
    },
    {
      "epoch": 1.4726617577941408,
      "grad_norm": 5.188057899475098,
      "learning_rate": 4.877278186850488e-05,
      "loss": 2.0378,
      "step": 18800
    },
    {
      "epoch": 1.48049506501645,
      "grad_norm": 4.300225734710693,
      "learning_rate": 4.8766254112486296e-05,
      "loss": 2.2095,
      "step": 18900
    },
    {
      "epoch": 1.4883283722387592,
      "grad_norm": 4.768073558807373,
      "learning_rate": 4.875972635646771e-05,
      "loss": 2.0524,
      "step": 19000
    },
    {
      "epoch": 1.4961616794610686,
      "grad_norm": 5.824512958526611,
      "learning_rate": 4.8753198600449114e-05,
      "loss": 2.0873,
      "step": 19100
    },
    {
      "epoch": 1.5039949866833777,
      "grad_norm": 5.633890628814697,
      "learning_rate": 4.874667084443052e-05,
      "loss": 2.1749,
      "step": 19200
    },
    {
      "epoch": 1.5118282939056868,
      "grad_norm": 6.51552677154541,
      "learning_rate": 4.874014308841193e-05,
      "loss": 2.1776,
      "step": 19300
    },
    {
      "epoch": 1.5196616011279962,
      "grad_norm": 4.0044474601745605,
      "learning_rate": 4.873361533239334e-05,
      "loss": 2.069,
      "step": 19400
    },
    {
      "epoch": 1.5274949083503055,
      "grad_norm": 6.214383602142334,
      "learning_rate": 4.8727087576374744e-05,
      "loss": 2.1173,
      "step": 19500
    },
    {
      "epoch": 1.5353282155726147,
      "grad_norm": 3.920518159866333,
      "learning_rate": 4.8720559820356156e-05,
      "loss": 2.0903,
      "step": 19600
    },
    {
      "epoch": 1.543161522794924,
      "grad_norm": 5.436163425445557,
      "learning_rate": 4.871403206433757e-05,
      "loss": 2.0448,
      "step": 19700
    },
    {
      "epoch": 1.5509948300172334,
      "grad_norm": 10.286617279052734,
      "learning_rate": 4.8707504308318975e-05,
      "loss": 2.0818,
      "step": 19800
    },
    {
      "epoch": 1.5588281372395425,
      "grad_norm": 6.115528106689453,
      "learning_rate": 4.870097655230038e-05,
      "loss": 2.063,
      "step": 19900
    },
    {
      "epoch": 1.5666614444618518,
      "grad_norm": 4.746628761291504,
      "learning_rate": 4.869444879628179e-05,
      "loss": 2.0866,
      "step": 20000
    },
    {
      "epoch": 1.5744947516841612,
      "grad_norm": 3.9922258853912354,
      "learning_rate": 4.86879210402632e-05,
      "loss": 2.1833,
      "step": 20100
    },
    {
      "epoch": 1.5823280589064703,
      "grad_norm": 4.578382968902588,
      "learning_rate": 4.868139328424461e-05,
      "loss": 2.0414,
      "step": 20200
    },
    {
      "epoch": 1.5901613661287795,
      "grad_norm": 5.890382289886475,
      "learning_rate": 4.8674865528226024e-05,
      "loss": 2.1318,
      "step": 20300
    },
    {
      "epoch": 1.5979946733510888,
      "grad_norm": 3.634385108947754,
      "learning_rate": 4.866833777220743e-05,
      "loss": 2.006,
      "step": 20400
    },
    {
      "epoch": 1.6058279805733982,
      "grad_norm": 5.451206684112549,
      "learning_rate": 4.8661810016188835e-05,
      "loss": 2.0362,
      "step": 20500
    },
    {
      "epoch": 1.6136612877957073,
      "grad_norm": 5.999538421630859,
      "learning_rate": 4.865528226017025e-05,
      "loss": 2.0817,
      "step": 20600
    },
    {
      "epoch": 1.6214945950180166,
      "grad_norm": 5.559510231018066,
      "learning_rate": 4.8648754504151654e-05,
      "loss": 2.0125,
      "step": 20700
    },
    {
      "epoch": 1.629327902240326,
      "grad_norm": 4.709551811218262,
      "learning_rate": 4.8642226748133066e-05,
      "loss": 2.0719,
      "step": 20800
    },
    {
      "epoch": 1.637161209462635,
      "grad_norm": 6.041172504425049,
      "learning_rate": 4.863569899211448e-05,
      "loss": 2.0954,
      "step": 20900
    },
    {
      "epoch": 1.6449945166849442,
      "grad_norm": 7.327115535736084,
      "learning_rate": 4.8629171236095885e-05,
      "loss": 2.1016,
      "step": 21000
    },
    {
      "epoch": 1.6528278239072538,
      "grad_norm": 7.45778226852417,
      "learning_rate": 4.862264348007729e-05,
      "loss": 2.0561,
      "step": 21100
    },
    {
      "epoch": 1.660661131129563,
      "grad_norm": 4.678922176361084,
      "learning_rate": 4.8616115724058696e-05,
      "loss": 2.1001,
      "step": 21200
    },
    {
      "epoch": 1.668494438351872,
      "grad_norm": 4.452406406402588,
      "learning_rate": 4.860958796804011e-05,
      "loss": 2.1256,
      "step": 21300
    },
    {
      "epoch": 1.6763277455741814,
      "grad_norm": 5.279855728149414,
      "learning_rate": 4.8603060212021514e-05,
      "loss": 2.1818,
      "step": 21400
    },
    {
      "epoch": 1.6841610527964908,
      "grad_norm": 4.0176215171813965,
      "learning_rate": 4.859653245600293e-05,
      "loss": 2.0457,
      "step": 21500
    },
    {
      "epoch": 1.6919943600187999,
      "grad_norm": 4.816084861755371,
      "learning_rate": 4.859000469998434e-05,
      "loss": 2.1614,
      "step": 21600
    },
    {
      "epoch": 1.6998276672411092,
      "grad_norm": 3.8773934841156006,
      "learning_rate": 4.8583476943965745e-05,
      "loss": 2.0626,
      "step": 21700
    },
    {
      "epoch": 1.7076609744634186,
      "grad_norm": 5.974483489990234,
      "learning_rate": 4.857694918794715e-05,
      "loss": 2.0787,
      "step": 21800
    },
    {
      "epoch": 1.7154942816857277,
      "grad_norm": 5.126582145690918,
      "learning_rate": 4.8570421431928564e-05,
      "loss": 2.0319,
      "step": 21900
    },
    {
      "epoch": 1.7233275889080368,
      "grad_norm": 5.642197608947754,
      "learning_rate": 4.856389367590997e-05,
      "loss": 2.1135,
      "step": 22000
    },
    {
      "epoch": 1.7311608961303462,
      "grad_norm": 3.667330741882324,
      "learning_rate": 4.855736591989138e-05,
      "loss": 2.1022,
      "step": 22100
    },
    {
      "epoch": 1.7389942033526555,
      "grad_norm": 6.468472480773926,
      "learning_rate": 4.8550838163872794e-05,
      "loss": 2.1112,
      "step": 22200
    },
    {
      "epoch": 1.7468275105749647,
      "grad_norm": 7.7331037521362305,
      "learning_rate": 4.85443104078542e-05,
      "loss": 2.135,
      "step": 22300
    },
    {
      "epoch": 1.754660817797274,
      "grad_norm": 5.047895908355713,
      "learning_rate": 4.8537782651835606e-05,
      "loss": 2.0381,
      "step": 22400
    },
    {
      "epoch": 1.7624941250195834,
      "grad_norm": 6.224412441253662,
      "learning_rate": 4.853125489581702e-05,
      "loss": 2.1075,
      "step": 22500
    },
    {
      "epoch": 1.7703274322418925,
      "grad_norm": 5.196223258972168,
      "learning_rate": 4.8524727139798424e-05,
      "loss": 2.1162,
      "step": 22600
    },
    {
      "epoch": 1.7781607394642018,
      "grad_norm": 3.587786912918091,
      "learning_rate": 4.851819938377983e-05,
      "loss": 2.0819,
      "step": 22700
    },
    {
      "epoch": 1.7859940466865112,
      "grad_norm": 3.8359479904174805,
      "learning_rate": 4.851167162776124e-05,
      "loss": 1.9985,
      "step": 22800
    },
    {
      "epoch": 1.7938273539088203,
      "grad_norm": 4.5959601402282715,
      "learning_rate": 4.8505143871742655e-05,
      "loss": 2.113,
      "step": 22900
    },
    {
      "epoch": 1.8016606611311294,
      "grad_norm": 4.782068252563477,
      "learning_rate": 4.849861611572406e-05,
      "loss": 2.1971,
      "step": 23000
    },
    {
      "epoch": 1.8094939683534388,
      "grad_norm": 5.934356212615967,
      "learning_rate": 4.849208835970547e-05,
      "loss": 2.0287,
      "step": 23100
    },
    {
      "epoch": 1.8173272755757481,
      "grad_norm": 5.647505283355713,
      "learning_rate": 4.848556060368688e-05,
      "loss": 2.0189,
      "step": 23200
    },
    {
      "epoch": 1.8251605827980573,
      "grad_norm": 4.303274631500244,
      "learning_rate": 4.8479032847668285e-05,
      "loss": 2.103,
      "step": 23300
    },
    {
      "epoch": 1.8329938900203666,
      "grad_norm": 6.072854042053223,
      "learning_rate": 4.84725050916497e-05,
      "loss": 2.0455,
      "step": 23400
    },
    {
      "epoch": 1.840827197242676,
      "grad_norm": 5.149067401885986,
      "learning_rate": 4.846597733563111e-05,
      "loss": 2.117,
      "step": 23500
    },
    {
      "epoch": 1.848660504464985,
      "grad_norm": 4.510815143585205,
      "learning_rate": 4.8459449579612516e-05,
      "loss": 2.1137,
      "step": 23600
    },
    {
      "epoch": 1.8564938116872942,
      "grad_norm": 5.205973148345947,
      "learning_rate": 4.845292182359392e-05,
      "loss": 2.0724,
      "step": 23700
    },
    {
      "epoch": 1.8643271189096038,
      "grad_norm": 5.6367363929748535,
      "learning_rate": 4.8446394067575334e-05,
      "loss": 2.0644,
      "step": 23800
    },
    {
      "epoch": 1.872160426131913,
      "grad_norm": 5.174858093261719,
      "learning_rate": 4.843986631155674e-05,
      "loss": 2.0895,
      "step": 23900
    },
    {
      "epoch": 1.879993733354222,
      "grad_norm": 6.238369464874268,
      "learning_rate": 4.843333855553815e-05,
      "loss": 2.0931,
      "step": 24000
    },
    {
      "epoch": 1.8878270405765314,
      "grad_norm": 5.053220748901367,
      "learning_rate": 4.8426810799519565e-05,
      "loss": 1.9801,
      "step": 24100
    },
    {
      "epoch": 1.8956603477988407,
      "grad_norm": 4.525503635406494,
      "learning_rate": 4.842028304350097e-05,
      "loss": 2.0371,
      "step": 24200
    },
    {
      "epoch": 1.9034936550211499,
      "grad_norm": 4.4244866371154785,
      "learning_rate": 4.8413755287482377e-05,
      "loss": 2.0858,
      "step": 24300
    },
    {
      "epoch": 1.9113269622434592,
      "grad_norm": 4.525151252746582,
      "learning_rate": 4.840722753146379e-05,
      "loss": 1.9738,
      "step": 24400
    },
    {
      "epoch": 1.9191602694657686,
      "grad_norm": 4.490096569061279,
      "learning_rate": 4.8400699775445195e-05,
      "loss": 2.0558,
      "step": 24500
    },
    {
      "epoch": 1.9269935766880777,
      "grad_norm": 4.1084747314453125,
      "learning_rate": 4.83941720194266e-05,
      "loss": 2.0753,
      "step": 24600
    },
    {
      "epoch": 1.9348268839103868,
      "grad_norm": 5.050004482269287,
      "learning_rate": 4.838764426340801e-05,
      "loss": 2.0647,
      "step": 24700
    },
    {
      "epoch": 1.9426601911326964,
      "grad_norm": 5.98903226852417,
      "learning_rate": 4.8381116507389426e-05,
      "loss": 2.2006,
      "step": 24800
    },
    {
      "epoch": 1.9504934983550055,
      "grad_norm": 5.696963787078857,
      "learning_rate": 4.837458875137083e-05,
      "loss": 2.0155,
      "step": 24900
    },
    {
      "epoch": 1.9583268055773146,
      "grad_norm": 4.867893218994141,
      "learning_rate": 4.836806099535224e-05,
      "loss": 2.0841,
      "step": 25000
    },
    {
      "epoch": 1.966160112799624,
      "grad_norm": 4.5824713706970215,
      "learning_rate": 4.836153323933365e-05,
      "loss": 2.0984,
      "step": 25100
    },
    {
      "epoch": 1.9739934200219333,
      "grad_norm": 7.60143518447876,
      "learning_rate": 4.8355005483315056e-05,
      "loss": 1.9979,
      "step": 25200
    },
    {
      "epoch": 1.9818267272442425,
      "grad_norm": 5.19178581237793,
      "learning_rate": 4.834847772729647e-05,
      "loss": 2.086,
      "step": 25300
    },
    {
      "epoch": 1.9896600344665518,
      "grad_norm": 4.1170172691345215,
      "learning_rate": 4.834194997127788e-05,
      "loss": 1.9972,
      "step": 25400
    },
    {
      "epoch": 1.9974933416888612,
      "grad_norm": 5.304464340209961,
      "learning_rate": 4.8335422215259286e-05,
      "loss": 2.0785,
      "step": 25500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9926196336746216,
      "eval_runtime": 3.0986,
      "eval_samples_per_second": 216.871,
      "eval_steps_per_second": 216.871,
      "step": 25532
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8570411205291748,
      "eval_runtime": 56.9504,
      "eval_samples_per_second": 224.16,
      "eval_steps_per_second": 224.16,
      "step": 25532
    },
    {
      "epoch": 2.0053266489111703,
      "grad_norm": 4.647586345672607,
      "learning_rate": 4.832889445924069e-05,
      "loss": 1.9907,
      "step": 25600
    },
    {
      "epoch": 2.0131599561334794,
      "grad_norm": 4.918145656585693,
      "learning_rate": 4.8322366703222105e-05,
      "loss": 2.0435,
      "step": 25700
    },
    {
      "epoch": 2.020993263355789,
      "grad_norm": 5.131568431854248,
      "learning_rate": 4.831583894720351e-05,
      "loss": 2.0264,
      "step": 25800
    },
    {
      "epoch": 2.028826570578098,
      "grad_norm": 6.219016075134277,
      "learning_rate": 4.8309311191184916e-05,
      "loss": 2.011,
      "step": 25900
    },
    {
      "epoch": 2.0366598778004072,
      "grad_norm": 4.211695671081543,
      "learning_rate": 4.830278343516633e-05,
      "loss": 1.9835,
      "step": 26000
    },
    {
      "epoch": 2.0444931850227164,
      "grad_norm": 8.442184448242188,
      "learning_rate": 4.829625567914774e-05,
      "loss": 2.1073,
      "step": 26100
    },
    {
      "epoch": 2.052326492245026,
      "grad_norm": 5.090169429779053,
      "learning_rate": 4.828972792312915e-05,
      "loss": 1.9926,
      "step": 26200
    },
    {
      "epoch": 2.060159799467335,
      "grad_norm": 4.051022529602051,
      "learning_rate": 4.828320016711055e-05,
      "loss": 1.9418,
      "step": 26300
    },
    {
      "epoch": 2.067993106689644,
      "grad_norm": 5.895142555236816,
      "learning_rate": 4.8276672411091966e-05,
      "loss": 1.9812,
      "step": 26400
    },
    {
      "epoch": 2.0758264139119538,
      "grad_norm": 4.491461753845215,
      "learning_rate": 4.827014465507337e-05,
      "loss": 2.0044,
      "step": 26500
    },
    {
      "epoch": 2.083659721134263,
      "grad_norm": 5.628273010253906,
      "learning_rate": 4.8263616899054784e-05,
      "loss": 2.0867,
      "step": 26600
    },
    {
      "epoch": 2.091493028356572,
      "grad_norm": 4.808006286621094,
      "learning_rate": 4.8257089143036196e-05,
      "loss": 2.0164,
      "step": 26700
    },
    {
      "epoch": 2.0993263355788816,
      "grad_norm": 4.4526286125183105,
      "learning_rate": 4.82505613870176e-05,
      "loss": 2.1019,
      "step": 26800
    },
    {
      "epoch": 2.1071596428011907,
      "grad_norm": 5.413354396820068,
      "learning_rate": 4.824403363099901e-05,
      "loss": 1.9749,
      "step": 26900
    },
    {
      "epoch": 2.1149929500235,
      "grad_norm": 5.673831462860107,
      "learning_rate": 4.823750587498042e-05,
      "loss": 1.9838,
      "step": 27000
    },
    {
      "epoch": 2.122826257245809,
      "grad_norm": 4.428493022918701,
      "learning_rate": 4.8230978118961826e-05,
      "loss": 2.0,
      "step": 27100
    },
    {
      "epoch": 2.1306595644681185,
      "grad_norm": 4.539832592010498,
      "learning_rate": 4.822445036294324e-05,
      "loss": 2.0592,
      "step": 27200
    },
    {
      "epoch": 2.1384928716904277,
      "grad_norm": 4.693939208984375,
      "learning_rate": 4.821792260692465e-05,
      "loss": 2.0501,
      "step": 27300
    },
    {
      "epoch": 2.146326178912737,
      "grad_norm": 4.423044204711914,
      "learning_rate": 4.821139485090606e-05,
      "loss": 2.0658,
      "step": 27400
    },
    {
      "epoch": 2.1541594861350464,
      "grad_norm": 4.509613037109375,
      "learning_rate": 4.820486709488746e-05,
      "loss": 2.0188,
      "step": 27500
    },
    {
      "epoch": 2.1619927933573555,
      "grad_norm": 5.899482727050781,
      "learning_rate": 4.8198339338868875e-05,
      "loss": 1.9908,
      "step": 27600
    },
    {
      "epoch": 2.1698261005796646,
      "grad_norm": 4.978906631469727,
      "learning_rate": 4.819181158285028e-05,
      "loss": 2.0053,
      "step": 27700
    },
    {
      "epoch": 2.177659407801974,
      "grad_norm": 5.497633457183838,
      "learning_rate": 4.818528382683169e-05,
      "loss": 2.0575,
      "step": 27800
    },
    {
      "epoch": 2.1854927150242833,
      "grad_norm": 5.5597710609436035,
      "learning_rate": 4.81787560708131e-05,
      "loss": 2.0517,
      "step": 27900
    },
    {
      "epoch": 2.1933260222465925,
      "grad_norm": 7.378326416015625,
      "learning_rate": 4.817222831479451e-05,
      "loss": 2.0427,
      "step": 28000
    },
    {
      "epoch": 2.2011593294689016,
      "grad_norm": 4.1333723068237305,
      "learning_rate": 4.816570055877592e-05,
      "loss": 2.2187,
      "step": 28100
    },
    {
      "epoch": 2.208992636691211,
      "grad_norm": 4.790182113647461,
      "learning_rate": 4.8159172802757324e-05,
      "loss": 1.9944,
      "step": 28200
    },
    {
      "epoch": 2.2168259439135203,
      "grad_norm": 4.856724262237549,
      "learning_rate": 4.8152645046738736e-05,
      "loss": 2.0849,
      "step": 28300
    },
    {
      "epoch": 2.2246592511358294,
      "grad_norm": 5.2033538818359375,
      "learning_rate": 4.814611729072014e-05,
      "loss": 1.9844,
      "step": 28400
    },
    {
      "epoch": 2.232492558358139,
      "grad_norm": 4.204851150512695,
      "learning_rate": 4.8139589534701554e-05,
      "loss": 1.9913,
      "step": 28500
    },
    {
      "epoch": 2.240325865580448,
      "grad_norm": 5.331219673156738,
      "learning_rate": 4.813306177868297e-05,
      "loss": 2.1017,
      "step": 28600
    },
    {
      "epoch": 2.2481591728027572,
      "grad_norm": 5.712644577026367,
      "learning_rate": 4.812653402266437e-05,
      "loss": 2.0101,
      "step": 28700
    },
    {
      "epoch": 2.255992480025067,
      "grad_norm": 6.39498233795166,
      "learning_rate": 4.812000626664578e-05,
      "loss": 2.0544,
      "step": 28800
    },
    {
      "epoch": 2.263825787247376,
      "grad_norm": 5.979741096496582,
      "learning_rate": 4.811347851062719e-05,
      "loss": 2.0442,
      "step": 28900
    },
    {
      "epoch": 2.271659094469685,
      "grad_norm": 4.897599697113037,
      "learning_rate": 4.81069507546086e-05,
      "loss": 2.0231,
      "step": 29000
    },
    {
      "epoch": 2.279492401691994,
      "grad_norm": 5.766157627105713,
      "learning_rate": 4.810042299859e-05,
      "loss": 2.0718,
      "step": 29100
    },
    {
      "epoch": 2.2873257089143038,
      "grad_norm": 4.085831642150879,
      "learning_rate": 4.8093895242571415e-05,
      "loss": 2.051,
      "step": 29200
    },
    {
      "epoch": 2.295159016136613,
      "grad_norm": 5.157279968261719,
      "learning_rate": 4.808736748655283e-05,
      "loss": 1.9919,
      "step": 29300
    },
    {
      "epoch": 2.302992323358922,
      "grad_norm": 3.5968217849731445,
      "learning_rate": 4.8080839730534233e-05,
      "loss": 2.002,
      "step": 29400
    },
    {
      "epoch": 2.3108256305812316,
      "grad_norm": 6.232082366943359,
      "learning_rate": 4.807431197451564e-05,
      "loss": 1.9398,
      "step": 29500
    },
    {
      "epoch": 2.3186589378035407,
      "grad_norm": 4.789041042327881,
      "learning_rate": 4.806778421849705e-05,
      "loss": 2.0289,
      "step": 29600
    },
    {
      "epoch": 2.32649224502585,
      "grad_norm": 5.949288845062256,
      "learning_rate": 4.806125646247846e-05,
      "loss": 2.0367,
      "step": 29700
    },
    {
      "epoch": 2.334325552248159,
      "grad_norm": 5.601947784423828,
      "learning_rate": 4.805472870645987e-05,
      "loss": 2.0666,
      "step": 29800
    },
    {
      "epoch": 2.3421588594704685,
      "grad_norm": 5.9471330642700195,
      "learning_rate": 4.804820095044128e-05,
      "loss": 2.0434,
      "step": 29900
    },
    {
      "epoch": 2.3499921666927777,
      "grad_norm": 6.798578262329102,
      "learning_rate": 4.804167319442269e-05,
      "loss": 2.0212,
      "step": 30000
    },
    {
      "epoch": 2.357825473915087,
      "grad_norm": 6.152658462524414,
      "learning_rate": 4.8035145438404094e-05,
      "loss": 2.1209,
      "step": 30100
    },
    {
      "epoch": 2.3656587811373964,
      "grad_norm": 5.509180068969727,
      "learning_rate": 4.802861768238551e-05,
      "loss": 2.0901,
      "step": 30200
    },
    {
      "epoch": 2.3734920883597055,
      "grad_norm": 4.957502365112305,
      "learning_rate": 4.802208992636691e-05,
      "loss": 2.0739,
      "step": 30300
    },
    {
      "epoch": 2.3813253955820146,
      "grad_norm": 4.160114765167236,
      "learning_rate": 4.8015562170348325e-05,
      "loss": 2.0093,
      "step": 30400
    },
    {
      "epoch": 2.389158702804324,
      "grad_norm": 3.3765249252319336,
      "learning_rate": 4.800903441432974e-05,
      "loss": 2.0956,
      "step": 30500
    },
    {
      "epoch": 2.3969920100266333,
      "grad_norm": 5.6804327964782715,
      "learning_rate": 4.800250665831114e-05,
      "loss": 2.1074,
      "step": 30600
    },
    {
      "epoch": 2.4048253172489424,
      "grad_norm": 4.528505802154541,
      "learning_rate": 4.799597890229255e-05,
      "loss": 2.087,
      "step": 30700
    },
    {
      "epoch": 2.412658624471252,
      "grad_norm": 6.331862449645996,
      "learning_rate": 4.798945114627396e-05,
      "loss": 1.9673,
      "step": 30800
    },
    {
      "epoch": 2.420491931693561,
      "grad_norm": 6.034541606903076,
      "learning_rate": 4.798292339025537e-05,
      "loss": 2.0849,
      "step": 30900
    },
    {
      "epoch": 2.4283252389158703,
      "grad_norm": 5.536960601806641,
      "learning_rate": 4.797639563423677e-05,
      "loss": 2.075,
      "step": 31000
    },
    {
      "epoch": 2.4361585461381794,
      "grad_norm": 6.1350555419921875,
      "learning_rate": 4.7969867878218186e-05,
      "loss": 2.0537,
      "step": 31100
    },
    {
      "epoch": 2.443991853360489,
      "grad_norm": 7.098172664642334,
      "learning_rate": 4.79633401221996e-05,
      "loss": 1.9613,
      "step": 31200
    },
    {
      "epoch": 2.451825160582798,
      "grad_norm": 5.754804611206055,
      "learning_rate": 4.7956812366181004e-05,
      "loss": 2.0291,
      "step": 31300
    },
    {
      "epoch": 2.459658467805107,
      "grad_norm": 4.887819766998291,
      "learning_rate": 4.795028461016241e-05,
      "loss": 1.9919,
      "step": 31400
    },
    {
      "epoch": 2.4674917750274163,
      "grad_norm": 5.117165565490723,
      "learning_rate": 4.794375685414382e-05,
      "loss": 1.9546,
      "step": 31500
    },
    {
      "epoch": 2.475325082249726,
      "grad_norm": 5.01502799987793,
      "learning_rate": 4.793722909812523e-05,
      "loss": 1.975,
      "step": 31600
    },
    {
      "epoch": 2.483158389472035,
      "grad_norm": 4.936805725097656,
      "learning_rate": 4.793070134210664e-05,
      "loss": 2.0295,
      "step": 31700
    },
    {
      "epoch": 2.490991696694344,
      "grad_norm": 5.335522174835205,
      "learning_rate": 4.792417358608805e-05,
      "loss": 2.0158,
      "step": 31800
    },
    {
      "epoch": 2.4988250039166537,
      "grad_norm": 5.413890361785889,
      "learning_rate": 4.791764583006946e-05,
      "loss": 2.0313,
      "step": 31900
    },
    {
      "epoch": 2.506658311138963,
      "grad_norm": 3.931697368621826,
      "learning_rate": 4.7911118074050865e-05,
      "loss": 2.0274,
      "step": 32000
    },
    {
      "epoch": 2.514491618361272,
      "grad_norm": 4.442944526672363,
      "learning_rate": 4.790459031803228e-05,
      "loss": 1.9546,
      "step": 32100
    },
    {
      "epoch": 2.5223249255835816,
      "grad_norm": 6.700174331665039,
      "learning_rate": 4.789806256201368e-05,
      "loss": 1.9775,
      "step": 32200
    },
    {
      "epoch": 2.5301582328058907,
      "grad_norm": 5.069040298461914,
      "learning_rate": 4.789153480599509e-05,
      "loss": 2.0949,
      "step": 32300
    },
    {
      "epoch": 2.5379915400282,
      "grad_norm": 5.111856460571289,
      "learning_rate": 4.78850070499765e-05,
      "loss": 2.0763,
      "step": 32400
    },
    {
      "epoch": 2.5458248472505094,
      "grad_norm": 5.5258469581604,
      "learning_rate": 4.7878479293957914e-05,
      "loss": 2.1291,
      "step": 32500
    },
    {
      "epoch": 2.5536581544728185,
      "grad_norm": 4.8423686027526855,
      "learning_rate": 4.787195153793932e-05,
      "loss": 1.8896,
      "step": 32600
    },
    {
      "epoch": 2.5614914616951276,
      "grad_norm": 3.407818555831909,
      "learning_rate": 4.786542378192073e-05,
      "loss": 1.8983,
      "step": 32700
    },
    {
      "epoch": 2.569324768917437,
      "grad_norm": 4.490142822265625,
      "learning_rate": 4.785889602590214e-05,
      "loss": 2.0167,
      "step": 32800
    },
    {
      "epoch": 2.5771580761397463,
      "grad_norm": 4.310635566711426,
      "learning_rate": 4.7852368269883544e-05,
      "loss": 2.1107,
      "step": 32900
    },
    {
      "epoch": 2.5849913833620555,
      "grad_norm": 4.995942115783691,
      "learning_rate": 4.7845840513864956e-05,
      "loss": 2.0942,
      "step": 33000
    },
    {
      "epoch": 2.5928246905843646,
      "grad_norm": 6.022937774658203,
      "learning_rate": 4.783931275784637e-05,
      "loss": 2.0169,
      "step": 33100
    },
    {
      "epoch": 2.6006579978066737,
      "grad_norm": 5.1805949211120605,
      "learning_rate": 4.7832785001827775e-05,
      "loss": 2.029,
      "step": 33200
    },
    {
      "epoch": 2.6084913050289833,
      "grad_norm": 6.150389194488525,
      "learning_rate": 4.782625724580918e-05,
      "loss": 2.1012,
      "step": 33300
    },
    {
      "epoch": 2.6163246122512924,
      "grad_norm": 4.912558078765869,
      "learning_rate": 4.781972948979059e-05,
      "loss": 1.98,
      "step": 33400
    },
    {
      "epoch": 2.6241579194736016,
      "grad_norm": 8.236820220947266,
      "learning_rate": 4.7813201733772e-05,
      "loss": 2.1108,
      "step": 33500
    },
    {
      "epoch": 2.631991226695911,
      "grad_norm": 4.543659210205078,
      "learning_rate": 4.780667397775341e-05,
      "loss": 2.0407,
      "step": 33600
    },
    {
      "epoch": 2.6398245339182203,
      "grad_norm": 5.348236083984375,
      "learning_rate": 4.7800146221734824e-05,
      "loss": 1.9951,
      "step": 33700
    },
    {
      "epoch": 2.6476578411405294,
      "grad_norm": 4.854011535644531,
      "learning_rate": 4.779361846571623e-05,
      "loss": 2.1174,
      "step": 33800
    },
    {
      "epoch": 2.655491148362839,
      "grad_norm": 6.149148941040039,
      "learning_rate": 4.7787090709697635e-05,
      "loss": 2.0303,
      "step": 33900
    },
    {
      "epoch": 2.663324455585148,
      "grad_norm": 5.227893352508545,
      "learning_rate": 4.778056295367905e-05,
      "loss": 2.0686,
      "step": 34000
    },
    {
      "epoch": 2.671157762807457,
      "grad_norm": 4.293994903564453,
      "learning_rate": 4.7774035197660454e-05,
      "loss": 1.9837,
      "step": 34100
    },
    {
      "epoch": 2.6789910700297668,
      "grad_norm": 5.301433086395264,
      "learning_rate": 4.776750744164186e-05,
      "loss": 1.944,
      "step": 34200
    },
    {
      "epoch": 2.686824377252076,
      "grad_norm": 5.991118431091309,
      "learning_rate": 4.776097968562327e-05,
      "loss": 2.0219,
      "step": 34300
    },
    {
      "epoch": 2.694657684474385,
      "grad_norm": 4.294913291931152,
      "learning_rate": 4.7754451929604684e-05,
      "loss": 2.023,
      "step": 34400
    },
    {
      "epoch": 2.7024909916966946,
      "grad_norm": 4.132788181304932,
      "learning_rate": 4.774792417358609e-05,
      "loss": 1.9586,
      "step": 34500
    },
    {
      "epoch": 2.7103242989190037,
      "grad_norm": 5.738503456115723,
      "learning_rate": 4.7741396417567496e-05,
      "loss": 2.0867,
      "step": 34600
    },
    {
      "epoch": 2.718157606141313,
      "grad_norm": 4.3815531730651855,
      "learning_rate": 4.773486866154891e-05,
      "loss": 2.1201,
      "step": 34700
    },
    {
      "epoch": 2.725990913363622,
      "grad_norm": 3.993252754211426,
      "learning_rate": 4.7728340905530314e-05,
      "loss": 2.0165,
      "step": 34800
    },
    {
      "epoch": 2.733824220585931,
      "grad_norm": 4.293881893157959,
      "learning_rate": 4.772181314951173e-05,
      "loss": 2.0249,
      "step": 34900
    },
    {
      "epoch": 2.7416575278082407,
      "grad_norm": 6.0127458572387695,
      "learning_rate": 4.771528539349314e-05,
      "loss": 2.0362,
      "step": 35000
    },
    {
      "epoch": 2.74949083503055,
      "grad_norm": 5.952290058135986,
      "learning_rate": 4.7708757637474545e-05,
      "loss": 1.9284,
      "step": 35100
    },
    {
      "epoch": 2.757324142252859,
      "grad_norm": 5.145452499389648,
      "learning_rate": 4.770222988145595e-05,
      "loss": 1.9584,
      "step": 35200
    },
    {
      "epoch": 2.7651574494751685,
      "grad_norm": 6.231517791748047,
      "learning_rate": 4.7695702125437363e-05,
      "loss": 2.007,
      "step": 35300
    },
    {
      "epoch": 2.7729907566974776,
      "grad_norm": 3.9751076698303223,
      "learning_rate": 4.768917436941877e-05,
      "loss": 2.0255,
      "step": 35400
    },
    {
      "epoch": 2.7808240639197868,
      "grad_norm": 4.392862319946289,
      "learning_rate": 4.7682646613400175e-05,
      "loss": 1.9803,
      "step": 35500
    },
    {
      "epoch": 2.7886573711420963,
      "grad_norm": 4.6267313957214355,
      "learning_rate": 4.767611885738159e-05,
      "loss": 1.9617,
      "step": 35600
    },
    {
      "epoch": 2.7964906783644055,
      "grad_norm": 5.865799427032471,
      "learning_rate": 4.7669591101363e-05,
      "loss": 1.9689,
      "step": 35700
    },
    {
      "epoch": 2.8043239855867146,
      "grad_norm": 5.371832847595215,
      "learning_rate": 4.7663063345344406e-05,
      "loss": 1.927,
      "step": 35800
    },
    {
      "epoch": 2.812157292809024,
      "grad_norm": 5.959019660949707,
      "learning_rate": 4.765653558932582e-05,
      "loss": 2.0654,
      "step": 35900
    },
    {
      "epoch": 2.8199906000313333,
      "grad_norm": 7.48266077041626,
      "learning_rate": 4.7650007833307224e-05,
      "loss": 2.0634,
      "step": 36000
    },
    {
      "epoch": 2.8278239072536424,
      "grad_norm": 4.054315567016602,
      "learning_rate": 4.764348007728863e-05,
      "loss": 1.9793,
      "step": 36100
    },
    {
      "epoch": 2.835657214475952,
      "grad_norm": 4.394195079803467,
      "learning_rate": 4.763695232127004e-05,
      "loss": 2.0373,
      "step": 36200
    },
    {
      "epoch": 2.843490521698261,
      "grad_norm": 5.215391635894775,
      "learning_rate": 4.7630424565251455e-05,
      "loss": 2.039,
      "step": 36300
    },
    {
      "epoch": 2.8513238289205702,
      "grad_norm": 7.325978755950928,
      "learning_rate": 4.762389680923286e-05,
      "loss": 1.9066,
      "step": 36400
    },
    {
      "epoch": 2.8591571361428794,
      "grad_norm": 4.393141269683838,
      "learning_rate": 4.7617369053214267e-05,
      "loss": 2.0871,
      "step": 36500
    },
    {
      "epoch": 2.866990443365189,
      "grad_norm": 5.580824851989746,
      "learning_rate": 4.761084129719568e-05,
      "loss": 1.9558,
      "step": 36600
    },
    {
      "epoch": 2.874823750587498,
      "grad_norm": 9.187762260437012,
      "learning_rate": 4.7604313541177085e-05,
      "loss": 2.0286,
      "step": 36700
    },
    {
      "epoch": 2.882657057809807,
      "grad_norm": 4.805226802825928,
      "learning_rate": 4.75977857851585e-05,
      "loss": 1.986,
      "step": 36800
    },
    {
      "epoch": 2.8904903650321163,
      "grad_norm": 5.553100109100342,
      "learning_rate": 4.759125802913991e-05,
      "loss": 1.9454,
      "step": 36900
    },
    {
      "epoch": 2.898323672254426,
      "grad_norm": 4.190942287445068,
      "learning_rate": 4.7584730273121316e-05,
      "loss": 1.9665,
      "step": 37000
    },
    {
      "epoch": 2.906156979476735,
      "grad_norm": 4.751959323883057,
      "learning_rate": 4.757820251710272e-05,
      "loss": 2.1286,
      "step": 37100
    },
    {
      "epoch": 2.913990286699044,
      "grad_norm": 4.421791076660156,
      "learning_rate": 4.7571674761084134e-05,
      "loss": 2.0396,
      "step": 37200
    },
    {
      "epoch": 2.9218235939213537,
      "grad_norm": 4.424722671508789,
      "learning_rate": 4.756514700506554e-05,
      "loss": 2.0157,
      "step": 37300
    },
    {
      "epoch": 2.929656901143663,
      "grad_norm": 6.847751140594482,
      "learning_rate": 4.7558619249046946e-05,
      "loss": 2.0194,
      "step": 37400
    },
    {
      "epoch": 2.937490208365972,
      "grad_norm": 4.041317462921143,
      "learning_rate": 4.755209149302836e-05,
      "loss": 2.0036,
      "step": 37500
    },
    {
      "epoch": 2.9453235155882815,
      "grad_norm": 6.053011417388916,
      "learning_rate": 4.754556373700977e-05,
      "loss": 2.0117,
      "step": 37600
    },
    {
      "epoch": 2.9531568228105907,
      "grad_norm": 5.634842395782471,
      "learning_rate": 4.7539035980991176e-05,
      "loss": 2.0174,
      "step": 37700
    },
    {
      "epoch": 2.9609901300329,
      "grad_norm": 4.756145477294922,
      "learning_rate": 4.753250822497259e-05,
      "loss": 2.0362,
      "step": 37800
    },
    {
      "epoch": 2.9688234372552094,
      "grad_norm": 3.5491766929626465,
      "learning_rate": 4.7525980468953995e-05,
      "loss": 1.9977,
      "step": 37900
    },
    {
      "epoch": 2.9766567444775185,
      "grad_norm": 6.277848243713379,
      "learning_rate": 4.75194527129354e-05,
      "loss": 1.96,
      "step": 38000
    },
    {
      "epoch": 2.9844900516998276,
      "grad_norm": 6.121026992797852,
      "learning_rate": 4.751292495691681e-05,
      "loss": 2.0172,
      "step": 38100
    },
    {
      "epoch": 2.992323358922137,
      "grad_norm": 6.077446937561035,
      "learning_rate": 4.7506397200898226e-05,
      "loss": 2.0143,
      "step": 38200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9475297927856445,
      "eval_runtime": 2.9215,
      "eval_samples_per_second": 230.017,
      "eval_steps_per_second": 230.017,
      "step": 38298
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.7918555736541748,
      "eval_runtime": 56.1154,
      "eval_samples_per_second": 227.496,
      "eval_steps_per_second": 227.496,
      "step": 38298
    },
    {
      "epoch": 3.0001566661444463,
      "grad_norm": 4.636927604675293,
      "learning_rate": 4.749986944487963e-05,
      "loss": 1.977,
      "step": 38300
    },
    {
      "epoch": 3.0079899733667554,
      "grad_norm": 5.9925737380981445,
      "learning_rate": 4.749334168886104e-05,
      "loss": 1.962,
      "step": 38400
    },
    {
      "epoch": 3.0158232805890646,
      "grad_norm": 5.154314041137695,
      "learning_rate": 4.748681393284245e-05,
      "loss": 1.9268,
      "step": 38500
    },
    {
      "epoch": 3.023656587811374,
      "grad_norm": 6.141176223754883,
      "learning_rate": 4.7480286176823855e-05,
      "loss": 1.973,
      "step": 38600
    },
    {
      "epoch": 3.0314898950336833,
      "grad_norm": 6.1372456550598145,
      "learning_rate": 4.747375842080526e-05,
      "loss": 2.0035,
      "step": 38700
    },
    {
      "epoch": 3.0393232022559924,
      "grad_norm": 4.307765007019043,
      "learning_rate": 4.7467230664786674e-05,
      "loss": 2.05,
      "step": 38800
    },
    {
      "epoch": 3.0471565094783015,
      "grad_norm": 6.1045002937316895,
      "learning_rate": 4.7460702908768086e-05,
      "loss": 2.0058,
      "step": 38900
    },
    {
      "epoch": 3.054989816700611,
      "grad_norm": 4.864141464233398,
      "learning_rate": 4.745417515274949e-05,
      "loss": 1.9875,
      "step": 39000
    },
    {
      "epoch": 3.06282312392292,
      "grad_norm": 5.343522548675537,
      "learning_rate": 4.7447647396730905e-05,
      "loss": 1.9602,
      "step": 39100
    },
    {
      "epoch": 3.0706564311452293,
      "grad_norm": 7.915684700012207,
      "learning_rate": 4.744111964071231e-05,
      "loss": 1.9677,
      "step": 39200
    },
    {
      "epoch": 3.078489738367539,
      "grad_norm": 6.613976955413818,
      "learning_rate": 4.7434591884693716e-05,
      "loss": 1.9345,
      "step": 39300
    },
    {
      "epoch": 3.086323045589848,
      "grad_norm": 5.661594867706299,
      "learning_rate": 4.742806412867513e-05,
      "loss": 2.0448,
      "step": 39400
    },
    {
      "epoch": 3.094156352812157,
      "grad_norm": 4.928496360778809,
      "learning_rate": 4.742153637265654e-05,
      "loss": 2.082,
      "step": 39500
    },
    {
      "epoch": 3.1019896600344667,
      "grad_norm": 4.719184398651123,
      "learning_rate": 4.741500861663795e-05,
      "loss": 2.0936,
      "step": 39600
    },
    {
      "epoch": 3.109822967256776,
      "grad_norm": 6.52072286605835,
      "learning_rate": 4.740848086061935e-05,
      "loss": 2.018,
      "step": 39700
    },
    {
      "epoch": 3.117656274479085,
      "grad_norm": 4.251194477081299,
      "learning_rate": 4.7401953104600765e-05,
      "loss": 1.9784,
      "step": 39800
    },
    {
      "epoch": 3.125489581701394,
      "grad_norm": 4.509903430938721,
      "learning_rate": 4.739542534858217e-05,
      "loss": 1.9693,
      "step": 39900
    },
    {
      "epoch": 3.1333228889237037,
      "grad_norm": 4.902053356170654,
      "learning_rate": 4.7388897592563584e-05,
      "loss": 1.963,
      "step": 40000
    },
    {
      "epoch": 3.141156196146013,
      "grad_norm": 5.82466459274292,
      "learning_rate": 4.7382369836544996e-05,
      "loss": 1.9775,
      "step": 40100
    },
    {
      "epoch": 3.148989503368322,
      "grad_norm": 4.110559463500977,
      "learning_rate": 4.73758420805264e-05,
      "loss": 2.037,
      "step": 40200
    },
    {
      "epoch": 3.1568228105906315,
      "grad_norm": 5.265739440917969,
      "learning_rate": 4.736931432450781e-05,
      "loss": 1.9122,
      "step": 40300
    },
    {
      "epoch": 3.1646561178129406,
      "grad_norm": 3.624466896057129,
      "learning_rate": 4.736278656848922e-05,
      "loss": 1.9449,
      "step": 40400
    },
    {
      "epoch": 3.1724894250352498,
      "grad_norm": 4.001105785369873,
      "learning_rate": 4.7356258812470626e-05,
      "loss": 1.911,
      "step": 40500
    },
    {
      "epoch": 3.1803227322575593,
      "grad_norm": 4.9102783203125,
      "learning_rate": 4.734973105645203e-05,
      "loss": 2.0037,
      "step": 40600
    },
    {
      "epoch": 3.1881560394798685,
      "grad_norm": 5.845912456512451,
      "learning_rate": 4.7343203300433444e-05,
      "loss": 1.9677,
      "step": 40700
    },
    {
      "epoch": 3.1959893467021776,
      "grad_norm": 4.6486496925354,
      "learning_rate": 4.733667554441486e-05,
      "loss": 2.0247,
      "step": 40800
    },
    {
      "epoch": 3.2038226539244867,
      "grad_norm": 7.366896629333496,
      "learning_rate": 4.733014778839626e-05,
      "loss": 1.9894,
      "step": 40900
    },
    {
      "epoch": 3.2116559611467963,
      "grad_norm": 7.3248162269592285,
      "learning_rate": 4.7323620032377675e-05,
      "loss": 1.9522,
      "step": 41000
    },
    {
      "epoch": 3.2194892683691054,
      "grad_norm": 6.005098342895508,
      "learning_rate": 4.731709227635908e-05,
      "loss": 2.0402,
      "step": 41100
    },
    {
      "epoch": 3.2273225755914146,
      "grad_norm": 4.742285251617432,
      "learning_rate": 4.731056452034049e-05,
      "loss": 2.0371,
      "step": 41200
    },
    {
      "epoch": 3.235155882813724,
      "grad_norm": 4.019186496734619,
      "learning_rate": 4.73040367643219e-05,
      "loss": 1.9666,
      "step": 41300
    },
    {
      "epoch": 3.2429891900360333,
      "grad_norm": 3.997689962387085,
      "learning_rate": 4.729750900830331e-05,
      "loss": 1.8907,
      "step": 41400
    },
    {
      "epoch": 3.2508224972583424,
      "grad_norm": 4.601377487182617,
      "learning_rate": 4.729098125228472e-05,
      "loss": 2.0752,
      "step": 41500
    },
    {
      "epoch": 3.258655804480652,
      "grad_norm": 4.831997871398926,
      "learning_rate": 4.7284453496266123e-05,
      "loss": 2.012,
      "step": 41600
    },
    {
      "epoch": 3.266489111702961,
      "grad_norm": 5.057933807373047,
      "learning_rate": 4.7277925740247536e-05,
      "loss": 1.9192,
      "step": 41700
    },
    {
      "epoch": 3.27432241892527,
      "grad_norm": 5.203285217285156,
      "learning_rate": 4.727139798422894e-05,
      "loss": 2.0032,
      "step": 41800
    },
    {
      "epoch": 3.2821557261475793,
      "grad_norm": 6.520449161529541,
      "learning_rate": 4.726487022821035e-05,
      "loss": 1.8405,
      "step": 41900
    },
    {
      "epoch": 3.289989033369889,
      "grad_norm": 5.729855060577393,
      "learning_rate": 4.725834247219176e-05,
      "loss": 1.9935,
      "step": 42000
    },
    {
      "epoch": 3.297822340592198,
      "grad_norm": 7.090335369110107,
      "learning_rate": 4.725181471617317e-05,
      "loss": 1.9801,
      "step": 42100
    },
    {
      "epoch": 3.305655647814507,
      "grad_norm": 5.082674503326416,
      "learning_rate": 4.724528696015458e-05,
      "loss": 1.9828,
      "step": 42200
    },
    {
      "epoch": 3.3134889550368167,
      "grad_norm": 5.225092887878418,
      "learning_rate": 4.723875920413599e-05,
      "loss": 1.9576,
      "step": 42300
    },
    {
      "epoch": 3.321322262259126,
      "grad_norm": 5.09206485748291,
      "learning_rate": 4.72322314481174e-05,
      "loss": 1.9702,
      "step": 42400
    },
    {
      "epoch": 3.329155569481435,
      "grad_norm": 6.480985641479492,
      "learning_rate": 4.72257036920988e-05,
      "loss": 2.0823,
      "step": 42500
    },
    {
      "epoch": 3.336988876703744,
      "grad_norm": 4.236398696899414,
      "learning_rate": 4.7219175936080215e-05,
      "loss": 2.0021,
      "step": 42600
    },
    {
      "epoch": 3.3448221839260537,
      "grad_norm": 5.4189887046813965,
      "learning_rate": 4.721264818006163e-05,
      "loss": 2.1497,
      "step": 42700
    },
    {
      "epoch": 3.352655491148363,
      "grad_norm": 4.3686347007751465,
      "learning_rate": 4.720612042404303e-05,
      "loss": 1.9452,
      "step": 42800
    },
    {
      "epoch": 3.360488798370672,
      "grad_norm": 4.215681076049805,
      "learning_rate": 4.7199592668024446e-05,
      "loss": 2.0485,
      "step": 42900
    },
    {
      "epoch": 3.3683221055929815,
      "grad_norm": 4.722554683685303,
      "learning_rate": 4.719306491200585e-05,
      "loss": 2.0408,
      "step": 43000
    },
    {
      "epoch": 3.3761554128152906,
      "grad_norm": 4.073475360870361,
      "learning_rate": 4.718653715598726e-05,
      "loss": 1.9482,
      "step": 43100
    },
    {
      "epoch": 3.3839887200375998,
      "grad_norm": 4.20788049697876,
      "learning_rate": 4.718000939996867e-05,
      "loss": 1.9533,
      "step": 43200
    },
    {
      "epoch": 3.3918220272599093,
      "grad_norm": 6.734785556793213,
      "learning_rate": 4.717348164395008e-05,
      "loss": 1.9929,
      "step": 43300
    },
    {
      "epoch": 3.3996553344822185,
      "grad_norm": 5.801131248474121,
      "learning_rate": 4.716695388793149e-05,
      "loss": 2.02,
      "step": 43400
    },
    {
      "epoch": 3.4074886417045276,
      "grad_norm": 4.165582180023193,
      "learning_rate": 4.7160426131912894e-05,
      "loss": 1.963,
      "step": 43500
    },
    {
      "epoch": 3.415321948926837,
      "grad_norm": 4.016812801361084,
      "learning_rate": 4.7153898375894307e-05,
      "loss": 1.8999,
      "step": 43600
    },
    {
      "epoch": 3.4231552561491463,
      "grad_norm": 4.89268159866333,
      "learning_rate": 4.714737061987571e-05,
      "loss": 2.0004,
      "step": 43700
    },
    {
      "epoch": 3.4309885633714554,
      "grad_norm": 5.934228420257568,
      "learning_rate": 4.714084286385712e-05,
      "loss": 1.9043,
      "step": 43800
    },
    {
      "epoch": 3.4388218705937645,
      "grad_norm": 6.935124397277832,
      "learning_rate": 4.713431510783853e-05,
      "loss": 1.9628,
      "step": 43900
    },
    {
      "epoch": 3.446655177816074,
      "grad_norm": 4.984536647796631,
      "learning_rate": 4.712778735181994e-05,
      "loss": 1.9879,
      "step": 44000
    },
    {
      "epoch": 3.4544884850383832,
      "grad_norm": 5.5293660163879395,
      "learning_rate": 4.712125959580135e-05,
      "loss": 2.0099,
      "step": 44100
    },
    {
      "epoch": 3.4623217922606924,
      "grad_norm": 5.373193264007568,
      "learning_rate": 4.711473183978276e-05,
      "loss": 1.9452,
      "step": 44200
    },
    {
      "epoch": 3.4701550994830015,
      "grad_norm": 6.674759864807129,
      "learning_rate": 4.710820408376417e-05,
      "loss": 1.9106,
      "step": 44300
    },
    {
      "epoch": 3.477988406705311,
      "grad_norm": 4.554877758026123,
      "learning_rate": 4.710167632774557e-05,
      "loss": 1.9475,
      "step": 44400
    },
    {
      "epoch": 3.48582171392762,
      "grad_norm": 7.310024738311768,
      "learning_rate": 4.7095148571726986e-05,
      "loss": 1.9478,
      "step": 44500
    },
    {
      "epoch": 3.4936550211499293,
      "grad_norm": 5.205700874328613,
      "learning_rate": 4.70886208157084e-05,
      "loss": 2.0159,
      "step": 44600
    },
    {
      "epoch": 3.501488328372239,
      "grad_norm": 5.314542293548584,
      "learning_rate": 4.7082093059689804e-05,
      "loss": 2.0313,
      "step": 44700
    },
    {
      "epoch": 3.509321635594548,
      "grad_norm": 5.6562347412109375,
      "learning_rate": 4.707556530367121e-05,
      "loss": 1.8731,
      "step": 44800
    },
    {
      "epoch": 3.517154942816857,
      "grad_norm": 4.740940093994141,
      "learning_rate": 4.706903754765262e-05,
      "loss": 2.0149,
      "step": 44900
    },
    {
      "epoch": 3.5249882500391667,
      "grad_norm": 4.116093158721924,
      "learning_rate": 4.706250979163403e-05,
      "loss": 1.9663,
      "step": 45000
    },
    {
      "epoch": 3.532821557261476,
      "grad_norm": 4.1569647789001465,
      "learning_rate": 4.7055982035615434e-05,
      "loss": 1.9989,
      "step": 45100
    },
    {
      "epoch": 3.540654864483785,
      "grad_norm": 3.822322130203247,
      "learning_rate": 4.7049454279596846e-05,
      "loss": 2.0168,
      "step": 45200
    },
    {
      "epoch": 3.5484881717060945,
      "grad_norm": 5.49286413192749,
      "learning_rate": 4.704292652357826e-05,
      "loss": 2.0146,
      "step": 45300
    },
    {
      "epoch": 3.5563214789284037,
      "grad_norm": 4.291873455047607,
      "learning_rate": 4.7036398767559665e-05,
      "loss": 1.9422,
      "step": 45400
    },
    {
      "epoch": 3.564154786150713,
      "grad_norm": 4.723999977111816,
      "learning_rate": 4.702987101154108e-05,
      "loss": 2.0069,
      "step": 45500
    },
    {
      "epoch": 3.5719880933730224,
      "grad_norm": 5.211782455444336,
      "learning_rate": 4.702334325552248e-05,
      "loss": 1.9439,
      "step": 45600
    },
    {
      "epoch": 3.5798214005953315,
      "grad_norm": 4.3615193367004395,
      "learning_rate": 4.701681549950389e-05,
      "loss": 1.9543,
      "step": 45700
    },
    {
      "epoch": 3.5876547078176406,
      "grad_norm": 6.0828351974487305,
      "learning_rate": 4.70102877434853e-05,
      "loss": 1.9728,
      "step": 45800
    },
    {
      "epoch": 3.5954880150399497,
      "grad_norm": 5.256207466125488,
      "learning_rate": 4.7003759987466714e-05,
      "loss": 1.9522,
      "step": 45900
    },
    {
      "epoch": 3.603321322262259,
      "grad_norm": 4.051787853240967,
      "learning_rate": 4.699723223144812e-05,
      "loss": 2.0374,
      "step": 46000
    },
    {
      "epoch": 3.6111546294845684,
      "grad_norm": 8.156350135803223,
      "learning_rate": 4.699070447542953e-05,
      "loss": 2.0142,
      "step": 46100
    },
    {
      "epoch": 3.6189879367068776,
      "grad_norm": 4.171003818511963,
      "learning_rate": 4.698417671941094e-05,
      "loss": 1.9106,
      "step": 46200
    },
    {
      "epoch": 3.6268212439291867,
      "grad_norm": 4.047712326049805,
      "learning_rate": 4.6977648963392344e-05,
      "loss": 1.8943,
      "step": 46300
    },
    {
      "epoch": 3.6346545511514963,
      "grad_norm": 4.081398010253906,
      "learning_rate": 4.6971121207373756e-05,
      "loss": 1.9172,
      "step": 46400
    },
    {
      "epoch": 3.6424878583738054,
      "grad_norm": 5.919658660888672,
      "learning_rate": 4.696459345135517e-05,
      "loss": 1.9506,
      "step": 46500
    },
    {
      "epoch": 3.6503211655961145,
      "grad_norm": 5.920458793640137,
      "learning_rate": 4.6958065695336574e-05,
      "loss": 1.9588,
      "step": 46600
    },
    {
      "epoch": 3.658154472818424,
      "grad_norm": 7.047016620635986,
      "learning_rate": 4.695153793931798e-05,
      "loss": 2.0866,
      "step": 46700
    },
    {
      "epoch": 3.6659877800407332,
      "grad_norm": 4.861125469207764,
      "learning_rate": 4.694501018329939e-05,
      "loss": 1.8911,
      "step": 46800
    },
    {
      "epoch": 3.6738210872630424,
      "grad_norm": 7.108783721923828,
      "learning_rate": 4.69384824272808e-05,
      "loss": 2.0136,
      "step": 46900
    },
    {
      "epoch": 3.681654394485352,
      "grad_norm": 6.572436332702637,
      "learning_rate": 4.6931954671262204e-05,
      "loss": 1.9627,
      "step": 47000
    },
    {
      "epoch": 3.689487701707661,
      "grad_norm": 5.4652628898620605,
      "learning_rate": 4.692542691524362e-05,
      "loss": 1.9804,
      "step": 47100
    },
    {
      "epoch": 3.69732100892997,
      "grad_norm": 4.791603088378906,
      "learning_rate": 4.691889915922503e-05,
      "loss": 1.9659,
      "step": 47200
    },
    {
      "epoch": 3.7051543161522797,
      "grad_norm": 3.7998054027557373,
      "learning_rate": 4.6912371403206435e-05,
      "loss": 1.9728,
      "step": 47300
    },
    {
      "epoch": 3.712987623374589,
      "grad_norm": 5.755447864532471,
      "learning_rate": 4.690584364718785e-05,
      "loss": 1.9111,
      "step": 47400
    },
    {
      "epoch": 3.720820930596898,
      "grad_norm": 5.247085094451904,
      "learning_rate": 4.6899315891169253e-05,
      "loss": 1.9948,
      "step": 47500
    },
    {
      "epoch": 3.728654237819207,
      "grad_norm": 3.909461498260498,
      "learning_rate": 4.689278813515066e-05,
      "loss": 1.9086,
      "step": 47600
    },
    {
      "epoch": 3.7364875450415163,
      "grad_norm": 4.993703365325928,
      "learning_rate": 4.688626037913207e-05,
      "loss": 1.9971,
      "step": 47700
    },
    {
      "epoch": 3.744320852263826,
      "grad_norm": 4.290987491607666,
      "learning_rate": 4.6879732623113484e-05,
      "loss": 1.8683,
      "step": 47800
    },
    {
      "epoch": 3.752154159486135,
      "grad_norm": 4.814509868621826,
      "learning_rate": 4.687320486709489e-05,
      "loss": 1.9713,
      "step": 47900
    },
    {
      "epoch": 3.759987466708444,
      "grad_norm": 4.895517349243164,
      "learning_rate": 4.6866677111076296e-05,
      "loss": 1.9361,
      "step": 48000
    },
    {
      "epoch": 3.7678207739307537,
      "grad_norm": 4.671459674835205,
      "learning_rate": 4.686014935505771e-05,
      "loss": 2.0604,
      "step": 48100
    },
    {
      "epoch": 3.775654081153063,
      "grad_norm": 4.8160624504089355,
      "learning_rate": 4.6853621599039114e-05,
      "loss": 1.9223,
      "step": 48200
    },
    {
      "epoch": 3.783487388375372,
      "grad_norm": 3.6596035957336426,
      "learning_rate": 4.684709384302052e-05,
      "loss": 1.9517,
      "step": 48300
    },
    {
      "epoch": 3.7913206955976815,
      "grad_norm": 4.883203029632568,
      "learning_rate": 4.684056608700193e-05,
      "loss": 1.9956,
      "step": 48400
    },
    {
      "epoch": 3.7991540028199906,
      "grad_norm": 10.296404838562012,
      "learning_rate": 4.6834038330983345e-05,
      "loss": 1.9206,
      "step": 48500
    },
    {
      "epoch": 3.8069873100422997,
      "grad_norm": 4.398230075836182,
      "learning_rate": 4.682751057496475e-05,
      "loss": 1.9431,
      "step": 48600
    },
    {
      "epoch": 3.8148206172646093,
      "grad_norm": 4.663690567016602,
      "learning_rate": 4.682098281894616e-05,
      "loss": 2.005,
      "step": 48700
    },
    {
      "epoch": 3.8226539244869184,
      "grad_norm": 7.266565322875977,
      "learning_rate": 4.681445506292757e-05,
      "loss": 1.9591,
      "step": 48800
    },
    {
      "epoch": 3.8304872317092276,
      "grad_norm": 4.052662372589111,
      "learning_rate": 4.6807927306908975e-05,
      "loss": 1.9075,
      "step": 48900
    },
    {
      "epoch": 3.838320538931537,
      "grad_norm": 4.892978668212891,
      "learning_rate": 4.680139955089039e-05,
      "loss": 1.9469,
      "step": 49000
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 4.058023929595947,
      "learning_rate": 4.67948717948718e-05,
      "loss": 2.0338,
      "step": 49100
    },
    {
      "epoch": 3.8539871533761554,
      "grad_norm": 5.16387939453125,
      "learning_rate": 4.6788344038853206e-05,
      "loss": 1.9466,
      "step": 49200
    },
    {
      "epoch": 3.8618204605984645,
      "grad_norm": 5.260441303253174,
      "learning_rate": 4.678181628283462e-05,
      "loss": 1.9135,
      "step": 49300
    },
    {
      "epoch": 3.869653767820774,
      "grad_norm": 5.09376859664917,
      "learning_rate": 4.6775288526816024e-05,
      "loss": 1.991,
      "step": 49400
    },
    {
      "epoch": 3.877487075043083,
      "grad_norm": 7.395714282989502,
      "learning_rate": 4.676876077079743e-05,
      "loss": 1.909,
      "step": 49500
    },
    {
      "epoch": 3.8853203822653923,
      "grad_norm": 5.353390216827393,
      "learning_rate": 4.676223301477884e-05,
      "loss": 1.9812,
      "step": 49600
    },
    {
      "epoch": 3.8931536894877015,
      "grad_norm": 4.7648725509643555,
      "learning_rate": 4.6755705258760255e-05,
      "loss": 2.0299,
      "step": 49700
    },
    {
      "epoch": 3.900986996710011,
      "grad_norm": 7.057986259460449,
      "learning_rate": 4.674917750274166e-05,
      "loss": 1.9521,
      "step": 49800
    },
    {
      "epoch": 3.90882030393232,
      "grad_norm": 5.142226219177246,
      "learning_rate": 4.6742649746723066e-05,
      "loss": 1.9243,
      "step": 49900
    },
    {
      "epoch": 3.9166536111546293,
      "grad_norm": 5.858100414276123,
      "learning_rate": 4.673612199070448e-05,
      "loss": 2.0,
      "step": 50000
    },
    {
      "epoch": 3.924486918376939,
      "grad_norm": 4.686342239379883,
      "learning_rate": 4.6729594234685885e-05,
      "loss": 1.9045,
      "step": 50100
    },
    {
      "epoch": 3.932320225599248,
      "grad_norm": 4.831070899963379,
      "learning_rate": 4.672306647866729e-05,
      "loss": 1.9593,
      "step": 50200
    },
    {
      "epoch": 3.940153532821557,
      "grad_norm": 4.784497261047363,
      "learning_rate": 4.67165387226487e-05,
      "loss": 1.9987,
      "step": 50300
    },
    {
      "epoch": 3.9479868400438667,
      "grad_norm": 4.021237373352051,
      "learning_rate": 4.6710010966630116e-05,
      "loss": 1.9143,
      "step": 50400
    },
    {
      "epoch": 3.955820147266176,
      "grad_norm": 5.357560634613037,
      "learning_rate": 4.670348321061152e-05,
      "loss": 1.9378,
      "step": 50500
    },
    {
      "epoch": 3.963653454488485,
      "grad_norm": 4.596705913543701,
      "learning_rate": 4.6696955454592934e-05,
      "loss": 1.9803,
      "step": 50600
    },
    {
      "epoch": 3.9714867617107945,
      "grad_norm": 5.299375057220459,
      "learning_rate": 4.669042769857434e-05,
      "loss": 2.0289,
      "step": 50700
    },
    {
      "epoch": 3.9793200689331036,
      "grad_norm": 3.8512747287750244,
      "learning_rate": 4.6683899942555745e-05,
      "loss": 2.0438,
      "step": 50800
    },
    {
      "epoch": 3.9871533761554128,
      "grad_norm": 5.033165454864502,
      "learning_rate": 4.667737218653716e-05,
      "loss": 1.9972,
      "step": 50900
    },
    {
      "epoch": 3.9949866833777223,
      "grad_norm": 6.007688999176025,
      "learning_rate": 4.667084443051857e-05,
      "loss": 1.9653,
      "step": 51000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9149173498153687,
      "eval_runtime": 2.9686,
      "eval_samples_per_second": 226.37,
      "eval_steps_per_second": 226.37,
      "step": 51064
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.747023344039917,
      "eval_runtime": 56.7346,
      "eval_samples_per_second": 225.013,
      "eval_steps_per_second": 225.013,
      "step": 51064
    },
    {
      "epoch": 4.002819990600031,
      "grad_norm": 4.73599100112915,
      "learning_rate": 4.6664316674499976e-05,
      "loss": 1.9587,
      "step": 51100
    },
    {
      "epoch": 4.010653297822341,
      "grad_norm": 4.116372585296631,
      "learning_rate": 4.665778891848139e-05,
      "loss": 1.9759,
      "step": 51200
    },
    {
      "epoch": 4.01848660504465,
      "grad_norm": 5.862637519836426,
      "learning_rate": 4.6651261162462795e-05,
      "loss": 1.9789,
      "step": 51300
    },
    {
      "epoch": 4.026319912266959,
      "grad_norm": 3.6694886684417725,
      "learning_rate": 4.66447334064442e-05,
      "loss": 1.8675,
      "step": 51400
    },
    {
      "epoch": 4.034153219489268,
      "grad_norm": 5.357460021972656,
      "learning_rate": 4.6638205650425606e-05,
      "loss": 1.9992,
      "step": 51500
    },
    {
      "epoch": 4.041986526711578,
      "grad_norm": 5.275086402893066,
      "learning_rate": 4.663167789440702e-05,
      "loss": 1.954,
      "step": 51600
    },
    {
      "epoch": 4.049819833933887,
      "grad_norm": 5.671922206878662,
      "learning_rate": 4.662515013838843e-05,
      "loss": 1.9461,
      "step": 51700
    },
    {
      "epoch": 4.057653141156196,
      "grad_norm": 4.344459533691406,
      "learning_rate": 4.661862238236984e-05,
      "loss": 1.8785,
      "step": 51800
    },
    {
      "epoch": 4.065486448378506,
      "grad_norm": 5.000631332397461,
      "learning_rate": 4.661209462635125e-05,
      "loss": 1.8334,
      "step": 51900
    },
    {
      "epoch": 4.0733197556008145,
      "grad_norm": 4.398412227630615,
      "learning_rate": 4.6605566870332655e-05,
      "loss": 1.909,
      "step": 52000
    },
    {
      "epoch": 4.081153062823124,
      "grad_norm": 5.303780555725098,
      "learning_rate": 4.659903911431406e-05,
      "loss": 1.971,
      "step": 52100
    },
    {
      "epoch": 4.088986370045433,
      "grad_norm": 5.425512313842773,
      "learning_rate": 4.6592511358295474e-05,
      "loss": 2.0456,
      "step": 52200
    },
    {
      "epoch": 4.096819677267742,
      "grad_norm": 3.5480504035949707,
      "learning_rate": 4.6585983602276886e-05,
      "loss": 1.9665,
      "step": 52300
    },
    {
      "epoch": 4.104652984490052,
      "grad_norm": 4.742230415344238,
      "learning_rate": 4.657945584625829e-05,
      "loss": 1.9756,
      "step": 52400
    },
    {
      "epoch": 4.112486291712361,
      "grad_norm": 4.7486724853515625,
      "learning_rate": 4.6572928090239705e-05,
      "loss": 1.92,
      "step": 52500
    },
    {
      "epoch": 4.12031959893467,
      "grad_norm": 5.577468395233154,
      "learning_rate": 4.656640033422111e-05,
      "loss": 1.9132,
      "step": 52600
    },
    {
      "epoch": 4.12815290615698,
      "grad_norm": 5.340426921844482,
      "learning_rate": 4.6559872578202516e-05,
      "loss": 1.8737,
      "step": 52700
    },
    {
      "epoch": 4.135986213379288,
      "grad_norm": 7.77296781539917,
      "learning_rate": 4.655334482218393e-05,
      "loss": 1.8886,
      "step": 52800
    },
    {
      "epoch": 4.143819520601598,
      "grad_norm": 6.200047016143799,
      "learning_rate": 4.654681706616534e-05,
      "loss": 1.8976,
      "step": 52900
    },
    {
      "epoch": 4.1516528278239075,
      "grad_norm": 5.212062358856201,
      "learning_rate": 4.654028931014675e-05,
      "loss": 1.956,
      "step": 53000
    },
    {
      "epoch": 4.159486135046216,
      "grad_norm": 5.024997234344482,
      "learning_rate": 4.653376155412815e-05,
      "loss": 1.8966,
      "step": 53100
    },
    {
      "epoch": 4.167319442268526,
      "grad_norm": 6.029552936553955,
      "learning_rate": 4.6527233798109565e-05,
      "loss": 1.9397,
      "step": 53200
    },
    {
      "epoch": 4.175152749490835,
      "grad_norm": 5.129266738891602,
      "learning_rate": 4.652070604209097e-05,
      "loss": 1.9084,
      "step": 53300
    },
    {
      "epoch": 4.182986056713144,
      "grad_norm": 7.777626037597656,
      "learning_rate": 4.651417828607238e-05,
      "loss": 1.9968,
      "step": 53400
    },
    {
      "epoch": 4.190819363935454,
      "grad_norm": 4.421821594238281,
      "learning_rate": 4.650765053005379e-05,
      "loss": 1.9779,
      "step": 53500
    },
    {
      "epoch": 4.198652671157763,
      "grad_norm": 3.0350399017333984,
      "learning_rate": 4.65011227740352e-05,
      "loss": 1.962,
      "step": 53600
    },
    {
      "epoch": 4.206485978380072,
      "grad_norm": 3.7959320545196533,
      "learning_rate": 4.649459501801661e-05,
      "loss": 1.8387,
      "step": 53700
    },
    {
      "epoch": 4.2143192856023814,
      "grad_norm": 4.628771781921387,
      "learning_rate": 4.648806726199802e-05,
      "loss": 1.8503,
      "step": 53800
    },
    {
      "epoch": 4.22215259282469,
      "grad_norm": 4.4930219650268555,
      "learning_rate": 4.6481539505979426e-05,
      "loss": 1.9192,
      "step": 53900
    },
    {
      "epoch": 4.229985900047,
      "grad_norm": 4.151788234710693,
      "learning_rate": 4.647501174996083e-05,
      "loss": 2.0152,
      "step": 54000
    },
    {
      "epoch": 4.237819207269309,
      "grad_norm": 4.571745872497559,
      "learning_rate": 4.6468483993942244e-05,
      "loss": 1.9153,
      "step": 54100
    },
    {
      "epoch": 4.245652514491618,
      "grad_norm": 5.496715545654297,
      "learning_rate": 4.646195623792366e-05,
      "loss": 1.8661,
      "step": 54200
    },
    {
      "epoch": 4.2534858217139275,
      "grad_norm": 4.2988691329956055,
      "learning_rate": 4.645542848190506e-05,
      "loss": 1.9865,
      "step": 54300
    },
    {
      "epoch": 4.261319128936237,
      "grad_norm": 4.263460636138916,
      "learning_rate": 4.6448900725886475e-05,
      "loss": 1.9643,
      "step": 54400
    },
    {
      "epoch": 4.269152436158546,
      "grad_norm": 4.280045986175537,
      "learning_rate": 4.644237296986788e-05,
      "loss": 1.9562,
      "step": 54500
    },
    {
      "epoch": 4.276985743380855,
      "grad_norm": 4.559769153594971,
      "learning_rate": 4.643584521384929e-05,
      "loss": 1.8967,
      "step": 54600
    },
    {
      "epoch": 4.284819050603165,
      "grad_norm": 5.882992744445801,
      "learning_rate": 4.64293174578307e-05,
      "loss": 1.8974,
      "step": 54700
    },
    {
      "epoch": 4.292652357825474,
      "grad_norm": 3.8449904918670654,
      "learning_rate": 4.6422789701812105e-05,
      "loss": 2.0923,
      "step": 54800
    },
    {
      "epoch": 4.300485665047783,
      "grad_norm": 5.681944370269775,
      "learning_rate": 4.641626194579352e-05,
      "loss": 1.9697,
      "step": 54900
    },
    {
      "epoch": 4.308318972270093,
      "grad_norm": 4.876407623291016,
      "learning_rate": 4.640973418977492e-05,
      "loss": 1.9933,
      "step": 55000
    },
    {
      "epoch": 4.316152279492401,
      "grad_norm": 5.0336503982543945,
      "learning_rate": 4.6403206433756336e-05,
      "loss": 1.9605,
      "step": 55100
    },
    {
      "epoch": 4.323985586714711,
      "grad_norm": 5.312171459197998,
      "learning_rate": 4.639667867773774e-05,
      "loss": 1.9589,
      "step": 55200
    },
    {
      "epoch": 4.331818893937021,
      "grad_norm": 4.728010177612305,
      "learning_rate": 4.639015092171915e-05,
      "loss": 1.9358,
      "step": 55300
    },
    {
      "epoch": 4.339652201159329,
      "grad_norm": 5.129451751708984,
      "learning_rate": 4.638362316570056e-05,
      "loss": 1.8881,
      "step": 55400
    },
    {
      "epoch": 4.347485508381639,
      "grad_norm": 6.500182628631592,
      "learning_rate": 4.637709540968197e-05,
      "loss": 1.8771,
      "step": 55500
    },
    {
      "epoch": 4.355318815603948,
      "grad_norm": 7.308565139770508,
      "learning_rate": 4.637056765366338e-05,
      "loss": 1.9841,
      "step": 55600
    },
    {
      "epoch": 4.363152122826257,
      "grad_norm": 5.475771903991699,
      "learning_rate": 4.636403989764479e-05,
      "loss": 1.8928,
      "step": 55700
    },
    {
      "epoch": 4.370985430048567,
      "grad_norm": 5.126159191131592,
      "learning_rate": 4.6357512141626197e-05,
      "loss": 1.9653,
      "step": 55800
    },
    {
      "epoch": 4.378818737270876,
      "grad_norm": 5.04464054107666,
      "learning_rate": 4.63509843856076e-05,
      "loss": 1.8468,
      "step": 55900
    },
    {
      "epoch": 4.386652044493185,
      "grad_norm": 5.185324668884277,
      "learning_rate": 4.6344456629589015e-05,
      "loss": 2.0299,
      "step": 56000
    },
    {
      "epoch": 4.3944853517154945,
      "grad_norm": 5.36722469329834,
      "learning_rate": 4.633792887357043e-05,
      "loss": 1.98,
      "step": 56100
    },
    {
      "epoch": 4.402318658937803,
      "grad_norm": 5.149343013763428,
      "learning_rate": 4.633140111755183e-05,
      "loss": 1.9782,
      "step": 56200
    },
    {
      "epoch": 4.410151966160113,
      "grad_norm": 4.906771183013916,
      "learning_rate": 4.6324873361533246e-05,
      "loss": 1.9005,
      "step": 56300
    },
    {
      "epoch": 4.417985273382422,
      "grad_norm": 7.8331379890441895,
      "learning_rate": 4.631834560551465e-05,
      "loss": 1.862,
      "step": 56400
    },
    {
      "epoch": 4.425818580604731,
      "grad_norm": 7.276994228363037,
      "learning_rate": 4.631181784949606e-05,
      "loss": 1.985,
      "step": 56500
    },
    {
      "epoch": 4.433651887827041,
      "grad_norm": 5.15367317199707,
      "learning_rate": 4.630529009347746e-05,
      "loss": 1.8899,
      "step": 56600
    },
    {
      "epoch": 4.44148519504935,
      "grad_norm": 6.927482604980469,
      "learning_rate": 4.6298762337458876e-05,
      "loss": 1.9639,
      "step": 56700
    },
    {
      "epoch": 4.449318502271659,
      "grad_norm": 5.940095901489258,
      "learning_rate": 4.629223458144029e-05,
      "loss": 1.9936,
      "step": 56800
    },
    {
      "epoch": 4.457151809493968,
      "grad_norm": 4.7363667488098145,
      "learning_rate": 4.6285706825421694e-05,
      "loss": 1.8935,
      "step": 56900
    },
    {
      "epoch": 4.464985116716278,
      "grad_norm": 5.728607654571533,
      "learning_rate": 4.6279179069403106e-05,
      "loss": 1.8644,
      "step": 57000
    },
    {
      "epoch": 4.472818423938587,
      "grad_norm": 6.137546539306641,
      "learning_rate": 4.627265131338451e-05,
      "loss": 2.0373,
      "step": 57100
    },
    {
      "epoch": 4.480651731160896,
      "grad_norm": 4.456728935241699,
      "learning_rate": 4.626612355736592e-05,
      "loss": 1.8568,
      "step": 57200
    },
    {
      "epoch": 4.488485038383206,
      "grad_norm": 6.564637184143066,
      "learning_rate": 4.625959580134733e-05,
      "loss": 1.9966,
      "step": 57300
    },
    {
      "epoch": 4.4963183456055145,
      "grad_norm": 4.697601318359375,
      "learning_rate": 4.625306804532874e-05,
      "loss": 2.0226,
      "step": 57400
    },
    {
      "epoch": 4.504151652827824,
      "grad_norm": 5.423552513122559,
      "learning_rate": 4.624654028931015e-05,
      "loss": 2.0282,
      "step": 57500
    },
    {
      "epoch": 4.511984960050134,
      "grad_norm": 4.056656360626221,
      "learning_rate": 4.624001253329156e-05,
      "loss": 1.8759,
      "step": 57600
    },
    {
      "epoch": 4.519818267272442,
      "grad_norm": 3.4788527488708496,
      "learning_rate": 4.623348477727297e-05,
      "loss": 1.9291,
      "step": 57700
    },
    {
      "epoch": 4.527651574494752,
      "grad_norm": 5.896433353424072,
      "learning_rate": 4.622695702125437e-05,
      "loss": 1.8309,
      "step": 57800
    },
    {
      "epoch": 4.5354848817170605,
      "grad_norm": 5.686951637268066,
      "learning_rate": 4.6220429265235785e-05,
      "loss": 2.0294,
      "step": 57900
    },
    {
      "epoch": 4.54331818893937,
      "grad_norm": 4.626039028167725,
      "learning_rate": 4.621390150921719e-05,
      "loss": 1.8874,
      "step": 58000
    },
    {
      "epoch": 4.55115149616168,
      "grad_norm": 5.361112594604492,
      "learning_rate": 4.6207373753198604e-05,
      "loss": 1.8962,
      "step": 58100
    },
    {
      "epoch": 4.558984803383988,
      "grad_norm": 7.768917560577393,
      "learning_rate": 4.620084599718001e-05,
      "loss": 1.8753,
      "step": 58200
    },
    {
      "epoch": 4.566818110606298,
      "grad_norm": 5.067788600921631,
      "learning_rate": 4.619431824116142e-05,
      "loss": 1.972,
      "step": 58300
    },
    {
      "epoch": 4.5746514178286075,
      "grad_norm": 4.722743034362793,
      "learning_rate": 4.618779048514283e-05,
      "loss": 1.9123,
      "step": 58400
    },
    {
      "epoch": 4.582484725050916,
      "grad_norm": 5.933180332183838,
      "learning_rate": 4.6181262729124234e-05,
      "loss": 1.9346,
      "step": 58500
    },
    {
      "epoch": 4.590318032273226,
      "grad_norm": 4.490505695343018,
      "learning_rate": 4.6174734973105646e-05,
      "loss": 1.9154,
      "step": 58600
    },
    {
      "epoch": 4.598151339495535,
      "grad_norm": 5.39658260345459,
      "learning_rate": 4.616820721708706e-05,
      "loss": 1.9577,
      "step": 58700
    },
    {
      "epoch": 4.605984646717844,
      "grad_norm": 5.341395854949951,
      "learning_rate": 4.6161679461068464e-05,
      "loss": 1.9963,
      "step": 58800
    },
    {
      "epoch": 4.613817953940154,
      "grad_norm": 4.147541522979736,
      "learning_rate": 4.615515170504988e-05,
      "loss": 1.9628,
      "step": 58900
    },
    {
      "epoch": 4.621651261162463,
      "grad_norm": 4.215973377227783,
      "learning_rate": 4.614862394903128e-05,
      "loss": 2.0125,
      "step": 59000
    },
    {
      "epoch": 4.629484568384772,
      "grad_norm": 4.402493000030518,
      "learning_rate": 4.614209619301269e-05,
      "loss": 1.9291,
      "step": 59100
    },
    {
      "epoch": 4.637317875607081,
      "grad_norm": 5.1038432121276855,
      "learning_rate": 4.61355684369941e-05,
      "loss": 1.9699,
      "step": 59200
    },
    {
      "epoch": 4.645151182829391,
      "grad_norm": 4.561689376831055,
      "learning_rate": 4.6129040680975514e-05,
      "loss": 1.9325,
      "step": 59300
    },
    {
      "epoch": 4.6529844900517,
      "grad_norm": 7.053696155548096,
      "learning_rate": 4.612251292495692e-05,
      "loss": 1.9705,
      "step": 59400
    },
    {
      "epoch": 4.660817797274009,
      "grad_norm": 4.661675930023193,
      "learning_rate": 4.611598516893833e-05,
      "loss": 1.9006,
      "step": 59500
    },
    {
      "epoch": 4.668651104496318,
      "grad_norm": 7.402167320251465,
      "learning_rate": 4.610945741291974e-05,
      "loss": 2.0031,
      "step": 59600
    },
    {
      "epoch": 4.6764844117186275,
      "grad_norm": 3.9730021953582764,
      "learning_rate": 4.6102929656901143e-05,
      "loss": 1.9046,
      "step": 59700
    },
    {
      "epoch": 4.684317718940937,
      "grad_norm": 4.787923336029053,
      "learning_rate": 4.609640190088255e-05,
      "loss": 1.9754,
      "step": 59800
    },
    {
      "epoch": 4.692151026163246,
      "grad_norm": 6.121640205383301,
      "learning_rate": 4.608987414486396e-05,
      "loss": 1.8781,
      "step": 59900
    },
    {
      "epoch": 4.699984333385555,
      "grad_norm": 5.577556610107422,
      "learning_rate": 4.6083346388845374e-05,
      "loss": 1.956,
      "step": 60000
    },
    {
      "epoch": 4.707817640607865,
      "grad_norm": 6.200906276702881,
      "learning_rate": 4.607681863282678e-05,
      "loss": 1.9942,
      "step": 60100
    },
    {
      "epoch": 4.715650947830174,
      "grad_norm": 6.5151472091674805,
      "learning_rate": 4.607029087680819e-05,
      "loss": 1.9838,
      "step": 60200
    },
    {
      "epoch": 4.723484255052483,
      "grad_norm": 3.950657844543457,
      "learning_rate": 4.60637631207896e-05,
      "loss": 1.8884,
      "step": 60300
    },
    {
      "epoch": 4.731317562274793,
      "grad_norm": 4.95066499710083,
      "learning_rate": 4.6057235364771004e-05,
      "loss": 1.9226,
      "step": 60400
    },
    {
      "epoch": 4.739150869497101,
      "grad_norm": 5.976319789886475,
      "learning_rate": 4.605070760875242e-05,
      "loss": 1.9579,
      "step": 60500
    },
    {
      "epoch": 4.746984176719411,
      "grad_norm": 14.596490859985352,
      "learning_rate": 4.604417985273383e-05,
      "loss": 1.9585,
      "step": 60600
    },
    {
      "epoch": 4.7548174839417205,
      "grad_norm": 4.651458263397217,
      "learning_rate": 4.6037652096715235e-05,
      "loss": 2.0064,
      "step": 60700
    },
    {
      "epoch": 4.762650791164029,
      "grad_norm": 4.446235179901123,
      "learning_rate": 4.603112434069665e-05,
      "loss": 1.8444,
      "step": 60800
    },
    {
      "epoch": 4.770484098386339,
      "grad_norm": 6.283600330352783,
      "learning_rate": 4.602459658467805e-05,
      "loss": 1.9519,
      "step": 60900
    },
    {
      "epoch": 4.778317405608648,
      "grad_norm": 6.495820045471191,
      "learning_rate": 4.601806882865946e-05,
      "loss": 1.9018,
      "step": 61000
    },
    {
      "epoch": 4.786150712830957,
      "grad_norm": 5.713624000549316,
      "learning_rate": 4.601154107264087e-05,
      "loss": 1.9228,
      "step": 61100
    },
    {
      "epoch": 4.793984020053267,
      "grad_norm": 4.563314914703369,
      "learning_rate": 4.600501331662228e-05,
      "loss": 1.9659,
      "step": 61200
    },
    {
      "epoch": 4.801817327275575,
      "grad_norm": 4.644680500030518,
      "learning_rate": 4.599848556060369e-05,
      "loss": 1.8518,
      "step": 61300
    },
    {
      "epoch": 4.809650634497885,
      "grad_norm": 6.122389793395996,
      "learning_rate": 4.59919578045851e-05,
      "loss": 1.8759,
      "step": 61400
    },
    {
      "epoch": 4.8174839417201945,
      "grad_norm": 4.81769323348999,
      "learning_rate": 4.598543004856651e-05,
      "loss": 1.8749,
      "step": 61500
    },
    {
      "epoch": 4.825317248942504,
      "grad_norm": 6.551992893218994,
      "learning_rate": 4.5978902292547914e-05,
      "loss": 1.9222,
      "step": 61600
    },
    {
      "epoch": 4.833150556164813,
      "grad_norm": 4.691587448120117,
      "learning_rate": 4.597237453652932e-05,
      "loss": 1.9928,
      "step": 61700
    },
    {
      "epoch": 4.840983863387122,
      "grad_norm": 5.501951217651367,
      "learning_rate": 4.596584678051073e-05,
      "loss": 1.863,
      "step": 61800
    },
    {
      "epoch": 4.848817170609431,
      "grad_norm": 5.22545862197876,
      "learning_rate": 4.5959319024492145e-05,
      "loss": 1.9689,
      "step": 61900
    },
    {
      "epoch": 4.8566504778317405,
      "grad_norm": 4.596985816955566,
      "learning_rate": 4.595279126847355e-05,
      "loss": 1.9537,
      "step": 62000
    },
    {
      "epoch": 4.86448378505405,
      "grad_norm": 5.174563407897949,
      "learning_rate": 4.594626351245496e-05,
      "loss": 2.0244,
      "step": 62100
    },
    {
      "epoch": 4.872317092276359,
      "grad_norm": 5.378050327301025,
      "learning_rate": 4.593973575643637e-05,
      "loss": 1.9074,
      "step": 62200
    },
    {
      "epoch": 4.880150399498668,
      "grad_norm": 4.832735061645508,
      "learning_rate": 4.5933208000417775e-05,
      "loss": 1.9424,
      "step": 62300
    },
    {
      "epoch": 4.887983706720978,
      "grad_norm": 4.615085601806641,
      "learning_rate": 4.592668024439919e-05,
      "loss": 1.9805,
      "step": 62400
    },
    {
      "epoch": 4.895817013943287,
      "grad_norm": 5.572385311126709,
      "learning_rate": 4.59201524883806e-05,
      "loss": 1.9134,
      "step": 62500
    },
    {
      "epoch": 4.903650321165596,
      "grad_norm": 4.020567417144775,
      "learning_rate": 4.5913624732362006e-05,
      "loss": 1.8987,
      "step": 62600
    },
    {
      "epoch": 4.911483628387906,
      "grad_norm": 5.641813278198242,
      "learning_rate": 4.590709697634342e-05,
      "loss": 1.8875,
      "step": 62700
    },
    {
      "epoch": 4.919316935610214,
      "grad_norm": 8.227210998535156,
      "learning_rate": 4.5900569220324824e-05,
      "loss": 1.9322,
      "step": 62800
    },
    {
      "epoch": 4.927150242832524,
      "grad_norm": 5.049123764038086,
      "learning_rate": 4.589404146430623e-05,
      "loss": 2.0361,
      "step": 62900
    },
    {
      "epoch": 4.934983550054833,
      "grad_norm": 5.518784046173096,
      "learning_rate": 4.588751370828764e-05,
      "loss": 1.9773,
      "step": 63000
    },
    {
      "epoch": 4.942816857277142,
      "grad_norm": 5.028581619262695,
      "learning_rate": 4.588098595226905e-05,
      "loss": 1.8717,
      "step": 63100
    },
    {
      "epoch": 4.950650164499452,
      "grad_norm": 4.9241437911987305,
      "learning_rate": 4.587445819625046e-05,
      "loss": 1.9541,
      "step": 63200
    },
    {
      "epoch": 4.958483471721761,
      "grad_norm": 4.767052173614502,
      "learning_rate": 4.5867930440231866e-05,
      "loss": 1.933,
      "step": 63300
    },
    {
      "epoch": 4.96631677894407,
      "grad_norm": 4.600002288818359,
      "learning_rate": 4.586140268421328e-05,
      "loss": 1.9381,
      "step": 63400
    },
    {
      "epoch": 4.97415008616638,
      "grad_norm": 5.7410173416137695,
      "learning_rate": 4.5854874928194685e-05,
      "loss": 1.8486,
      "step": 63500
    },
    {
      "epoch": 4.981983393388688,
      "grad_norm": 4.296876430511475,
      "learning_rate": 4.584834717217609e-05,
      "loss": 1.89,
      "step": 63600
    },
    {
      "epoch": 4.989816700610998,
      "grad_norm": 5.8164873123168945,
      "learning_rate": 4.58418194161575e-05,
      "loss": 1.9139,
      "step": 63700
    },
    {
      "epoch": 4.9976500078333075,
      "grad_norm": 5.5823845863342285,
      "learning_rate": 4.5835291660138915e-05,
      "loss": 2.0642,
      "step": 63800
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.8963106870651245,
      "eval_runtime": 3.0228,
      "eval_samples_per_second": 222.311,
      "eval_steps_per_second": 222.311,
      "step": 63830
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7168855667114258,
      "eval_runtime": 55.6317,
      "eval_samples_per_second": 229.473,
      "eval_steps_per_second": 229.473,
      "step": 63830
    },
    {
      "epoch": 5.005483315055616,
      "grad_norm": 4.524844646453857,
      "learning_rate": 4.582876390412032e-05,
      "loss": 1.93,
      "step": 63900
    },
    {
      "epoch": 5.013316622277926,
      "grad_norm": 4.7701616287231445,
      "learning_rate": 4.5822236148101734e-05,
      "loss": 1.9348,
      "step": 64000
    },
    {
      "epoch": 5.021149929500235,
      "grad_norm": 4.567166328430176,
      "learning_rate": 4.581570839208314e-05,
      "loss": 1.9252,
      "step": 64100
    },
    {
      "epoch": 5.028983236722544,
      "grad_norm": 5.095538139343262,
      "learning_rate": 4.5809180636064545e-05,
      "loss": 1.8133,
      "step": 64200
    },
    {
      "epoch": 5.036816543944854,
      "grad_norm": 5.090572834014893,
      "learning_rate": 4.580265288004596e-05,
      "loss": 1.9125,
      "step": 64300
    },
    {
      "epoch": 5.044649851167163,
      "grad_norm": 5.7410888671875,
      "learning_rate": 4.5796125124027364e-05,
      "loss": 1.918,
      "step": 64400
    },
    {
      "epoch": 5.052483158389472,
      "grad_norm": 4.718165397644043,
      "learning_rate": 4.5789597368008776e-05,
      "loss": 1.9036,
      "step": 64500
    },
    {
      "epoch": 5.060316465611781,
      "grad_norm": 4.802165985107422,
      "learning_rate": 4.578306961199019e-05,
      "loss": 1.9074,
      "step": 64600
    },
    {
      "epoch": 5.068149772834091,
      "grad_norm": 4.768837928771973,
      "learning_rate": 4.5776541855971595e-05,
      "loss": 1.8961,
      "step": 64700
    },
    {
      "epoch": 5.0759830800564,
      "grad_norm": 4.549527645111084,
      "learning_rate": 4.5770014099953e-05,
      "loss": 1.9221,
      "step": 64800
    },
    {
      "epoch": 5.083816387278709,
      "grad_norm": 5.975907802581787,
      "learning_rate": 4.5763486343934406e-05,
      "loss": 1.9385,
      "step": 64900
    },
    {
      "epoch": 5.091649694501018,
      "grad_norm": 3.9046790599823,
      "learning_rate": 4.575695858791582e-05,
      "loss": 1.8884,
      "step": 65000
    },
    {
      "epoch": 5.0994830017233275,
      "grad_norm": 7.664243221282959,
      "learning_rate": 4.575043083189723e-05,
      "loss": 1.9632,
      "step": 65100
    },
    {
      "epoch": 5.107316308945637,
      "grad_norm": 5.912238597869873,
      "learning_rate": 4.574390307587864e-05,
      "loss": 1.8859,
      "step": 65200
    },
    {
      "epoch": 5.115149616167946,
      "grad_norm": 4.023863792419434,
      "learning_rate": 4.573737531986005e-05,
      "loss": 1.9588,
      "step": 65300
    },
    {
      "epoch": 5.122982923390255,
      "grad_norm": 5.728212356567383,
      "learning_rate": 4.5730847563841455e-05,
      "loss": 1.8706,
      "step": 65400
    },
    {
      "epoch": 5.130816230612565,
      "grad_norm": 4.790277481079102,
      "learning_rate": 4.572431980782286e-05,
      "loss": 1.8832,
      "step": 65500
    },
    {
      "epoch": 5.1386495378348735,
      "grad_norm": 5.846380233764648,
      "learning_rate": 4.5717792051804274e-05,
      "loss": 1.8468,
      "step": 65600
    },
    {
      "epoch": 5.146482845057183,
      "grad_norm": 4.2373127937316895,
      "learning_rate": 4.5711264295785686e-05,
      "loss": 1.9042,
      "step": 65700
    },
    {
      "epoch": 5.154316152279493,
      "grad_norm": 4.443782329559326,
      "learning_rate": 4.570473653976709e-05,
      "loss": 1.9083,
      "step": 65800
    },
    {
      "epoch": 5.162149459501801,
      "grad_norm": 5.306912422180176,
      "learning_rate": 4.5698208783748504e-05,
      "loss": 1.85,
      "step": 65900
    },
    {
      "epoch": 5.169982766724111,
      "grad_norm": 6.483170509338379,
      "learning_rate": 4.569168102772991e-05,
      "loss": 1.91,
      "step": 66000
    },
    {
      "epoch": 5.1778160739464205,
      "grad_norm": 4.826560974121094,
      "learning_rate": 4.5685153271711316e-05,
      "loss": 1.9908,
      "step": 66100
    },
    {
      "epoch": 5.185649381168729,
      "grad_norm": 4.96148157119751,
      "learning_rate": 4.567862551569273e-05,
      "loss": 1.8726,
      "step": 66200
    },
    {
      "epoch": 5.193482688391039,
      "grad_norm": 4.591657638549805,
      "learning_rate": 4.5672097759674134e-05,
      "loss": 1.9229,
      "step": 66300
    },
    {
      "epoch": 5.201315995613348,
      "grad_norm": 4.9867658615112305,
      "learning_rate": 4.566557000365555e-05,
      "loss": 2.0193,
      "step": 66400
    },
    {
      "epoch": 5.209149302835657,
      "grad_norm": 5.780063152313232,
      "learning_rate": 4.565904224763696e-05,
      "loss": 1.8817,
      "step": 66500
    },
    {
      "epoch": 5.216982610057967,
      "grad_norm": 5.35037088394165,
      "learning_rate": 4.5652514491618365e-05,
      "loss": 1.9492,
      "step": 66600
    },
    {
      "epoch": 5.224815917280276,
      "grad_norm": 5.177664756774902,
      "learning_rate": 4.564598673559977e-05,
      "loss": 2.0198,
      "step": 66700
    },
    {
      "epoch": 5.232649224502585,
      "grad_norm": 4.439562797546387,
      "learning_rate": 4.563945897958118e-05,
      "loss": 1.823,
      "step": 66800
    },
    {
      "epoch": 5.240482531724894,
      "grad_norm": 3.5388548374176025,
      "learning_rate": 4.563293122356259e-05,
      "loss": 1.8981,
      "step": 66900
    },
    {
      "epoch": 5.248315838947203,
      "grad_norm": 6.232329845428467,
      "learning_rate": 4.5626403467544e-05,
      "loss": 1.8627,
      "step": 67000
    },
    {
      "epoch": 5.256149146169513,
      "grad_norm": 5.903597354888916,
      "learning_rate": 4.561987571152541e-05,
      "loss": 1.8831,
      "step": 67100
    },
    {
      "epoch": 5.263982453391822,
      "grad_norm": 6.215305805206299,
      "learning_rate": 4.561334795550682e-05,
      "loss": 1.9012,
      "step": 67200
    },
    {
      "epoch": 5.271815760614131,
      "grad_norm": 5.023486614227295,
      "learning_rate": 4.5606820199488226e-05,
      "loss": 1.944,
      "step": 67300
    },
    {
      "epoch": 5.2796490678364405,
      "grad_norm": 5.846163272857666,
      "learning_rate": 4.560029244346963e-05,
      "loss": 1.9234,
      "step": 67400
    },
    {
      "epoch": 5.28748237505875,
      "grad_norm": 5.388189792633057,
      "learning_rate": 4.5593764687451044e-05,
      "loss": 1.8185,
      "step": 67500
    },
    {
      "epoch": 5.295315682281059,
      "grad_norm": 4.440972328186035,
      "learning_rate": 4.558723693143245e-05,
      "loss": 1.9169,
      "step": 67600
    },
    {
      "epoch": 5.303148989503368,
      "grad_norm": 3.4728081226348877,
      "learning_rate": 4.558070917541386e-05,
      "loss": 1.8984,
      "step": 67700
    },
    {
      "epoch": 5.310982296725678,
      "grad_norm": 6.486695289611816,
      "learning_rate": 4.5574181419395275e-05,
      "loss": 1.9497,
      "step": 67800
    },
    {
      "epoch": 5.318815603947987,
      "grad_norm": 7.733552932739258,
      "learning_rate": 4.556765366337668e-05,
      "loss": 1.893,
      "step": 67900
    },
    {
      "epoch": 5.326648911170296,
      "grad_norm": 6.2664618492126465,
      "learning_rate": 4.5561125907358087e-05,
      "loss": 1.9613,
      "step": 68000
    },
    {
      "epoch": 5.334482218392606,
      "grad_norm": 4.2856268882751465,
      "learning_rate": 4.55545981513395e-05,
      "loss": 1.8269,
      "step": 68100
    },
    {
      "epoch": 5.342315525614914,
      "grad_norm": 5.957225322723389,
      "learning_rate": 4.5548070395320905e-05,
      "loss": 1.9835,
      "step": 68200
    },
    {
      "epoch": 5.350148832837224,
      "grad_norm": 5.12491512298584,
      "learning_rate": 4.554154263930232e-05,
      "loss": 1.9766,
      "step": 68300
    },
    {
      "epoch": 5.3579821400595335,
      "grad_norm": 4.999460697174072,
      "learning_rate": 4.553501488328372e-05,
      "loss": 1.8364,
      "step": 68400
    },
    {
      "epoch": 5.365815447281842,
      "grad_norm": 7.33103084564209,
      "learning_rate": 4.5528487127265136e-05,
      "loss": 1.9515,
      "step": 68500
    },
    {
      "epoch": 5.373648754504152,
      "grad_norm": 4.939382076263428,
      "learning_rate": 4.552195937124654e-05,
      "loss": 1.9806,
      "step": 68600
    },
    {
      "epoch": 5.3814820617264605,
      "grad_norm": 6.108592987060547,
      "learning_rate": 4.551543161522795e-05,
      "loss": 1.8779,
      "step": 68700
    },
    {
      "epoch": 5.38931536894877,
      "grad_norm": 7.209583759307861,
      "learning_rate": 4.550890385920936e-05,
      "loss": 1.8086,
      "step": 68800
    },
    {
      "epoch": 5.39714867617108,
      "grad_norm": 7.0337700843811035,
      "learning_rate": 4.550237610319077e-05,
      "loss": 1.9895,
      "step": 68900
    },
    {
      "epoch": 5.404981983393388,
      "grad_norm": 4.7371439933776855,
      "learning_rate": 4.549584834717218e-05,
      "loss": 1.8817,
      "step": 69000
    },
    {
      "epoch": 5.412815290615698,
      "grad_norm": 5.732283115386963,
      "learning_rate": 4.548932059115359e-05,
      "loss": 1.9446,
      "step": 69100
    },
    {
      "epoch": 5.4206485978380075,
      "grad_norm": 3.8652586936950684,
      "learning_rate": 4.5482792835134996e-05,
      "loss": 1.8372,
      "step": 69200
    },
    {
      "epoch": 5.428481905060316,
      "grad_norm": 5.721510887145996,
      "learning_rate": 4.54762650791164e-05,
      "loss": 1.9636,
      "step": 69300
    },
    {
      "epoch": 5.436315212282626,
      "grad_norm": 6.198986053466797,
      "learning_rate": 4.5469737323097815e-05,
      "loss": 1.8813,
      "step": 69400
    },
    {
      "epoch": 5.444148519504935,
      "grad_norm": 4.929405212402344,
      "learning_rate": 4.546320956707922e-05,
      "loss": 1.9118,
      "step": 69500
    },
    {
      "epoch": 5.451981826727244,
      "grad_norm": 3.911088228225708,
      "learning_rate": 4.545668181106063e-05,
      "loss": 1.8976,
      "step": 69600
    },
    {
      "epoch": 5.4598151339495535,
      "grad_norm": 5.583773136138916,
      "learning_rate": 4.5450154055042046e-05,
      "loss": 1.8614,
      "step": 69700
    },
    {
      "epoch": 5.467648441171863,
      "grad_norm": 4.837012767791748,
      "learning_rate": 4.544362629902345e-05,
      "loss": 1.942,
      "step": 69800
    },
    {
      "epoch": 5.475481748394172,
      "grad_norm": 4.893595218658447,
      "learning_rate": 4.543709854300486e-05,
      "loss": 1.8767,
      "step": 69900
    },
    {
      "epoch": 5.483315055616481,
      "grad_norm": 5.253508567810059,
      "learning_rate": 4.543057078698626e-05,
      "loss": 1.7956,
      "step": 70000
    },
    {
      "epoch": 5.491148362838791,
      "grad_norm": 5.720953464508057,
      "learning_rate": 4.5424043030967675e-05,
      "loss": 1.874,
      "step": 70100
    },
    {
      "epoch": 5.4989816700611,
      "grad_norm": 5.39008092880249,
      "learning_rate": 4.541751527494909e-05,
      "loss": 1.8989,
      "step": 70200
    },
    {
      "epoch": 5.506814977283409,
      "grad_norm": 5.830794334411621,
      "learning_rate": 4.5410987518930494e-05,
      "loss": 1.83,
      "step": 70300
    },
    {
      "epoch": 5.514648284505718,
      "grad_norm": 5.797290325164795,
      "learning_rate": 4.5404459762911906e-05,
      "loss": 1.8817,
      "step": 70400
    },
    {
      "epoch": 5.522481591728027,
      "grad_norm": 4.614675521850586,
      "learning_rate": 4.539793200689331e-05,
      "loss": 1.9107,
      "step": 70500
    },
    {
      "epoch": 5.530314898950337,
      "grad_norm": 5.746186256408691,
      "learning_rate": 4.539140425087472e-05,
      "loss": 1.8792,
      "step": 70600
    },
    {
      "epoch": 5.538148206172647,
      "grad_norm": 7.284931659698486,
      "learning_rate": 4.538487649485613e-05,
      "loss": 1.8269,
      "step": 70700
    },
    {
      "epoch": 5.545981513394955,
      "grad_norm": 4.044444561004639,
      "learning_rate": 4.5378348738837536e-05,
      "loss": 1.8943,
      "step": 70800
    },
    {
      "epoch": 5.553814820617265,
      "grad_norm": 5.108213901519775,
      "learning_rate": 4.537182098281895e-05,
      "loss": 2.0363,
      "step": 70900
    },
    {
      "epoch": 5.5616481278395735,
      "grad_norm": 5.276669979095459,
      "learning_rate": 4.536529322680036e-05,
      "loss": 1.9786,
      "step": 71000
    },
    {
      "epoch": 5.569481435061883,
      "grad_norm": 4.162188529968262,
      "learning_rate": 4.535876547078177e-05,
      "loss": 1.8735,
      "step": 71100
    },
    {
      "epoch": 5.577314742284193,
      "grad_norm": 5.970276832580566,
      "learning_rate": 4.535223771476317e-05,
      "loss": 1.8649,
      "step": 71200
    },
    {
      "epoch": 5.585148049506501,
      "grad_norm": 5.1196208000183105,
      "learning_rate": 4.5345709958744585e-05,
      "loss": 1.8859,
      "step": 71300
    },
    {
      "epoch": 5.592981356728811,
      "grad_norm": 3.9120137691497803,
      "learning_rate": 4.533918220272599e-05,
      "loss": 1.8428,
      "step": 71400
    },
    {
      "epoch": 5.6008146639511205,
      "grad_norm": 4.690179824829102,
      "learning_rate": 4.5332654446707404e-05,
      "loss": 1.9664,
      "step": 71500
    },
    {
      "epoch": 5.608647971173429,
      "grad_norm": 4.872270107269287,
      "learning_rate": 4.532612669068881e-05,
      "loss": 1.9775,
      "step": 71600
    },
    {
      "epoch": 5.616481278395739,
      "grad_norm": 6.489995956420898,
      "learning_rate": 4.531959893467022e-05,
      "loss": 1.898,
      "step": 71700
    },
    {
      "epoch": 5.624314585618048,
      "grad_norm": 5.011394500732422,
      "learning_rate": 4.531307117865163e-05,
      "loss": 1.9603,
      "step": 71800
    },
    {
      "epoch": 5.632147892840357,
      "grad_norm": 4.909083843231201,
      "learning_rate": 4.5306543422633033e-05,
      "loss": 1.8668,
      "step": 71900
    },
    {
      "epoch": 5.639981200062667,
      "grad_norm": 6.325141429901123,
      "learning_rate": 4.5300015666614446e-05,
      "loss": 1.8966,
      "step": 72000
    },
    {
      "epoch": 5.647814507284975,
      "grad_norm": 6.732221603393555,
      "learning_rate": 4.529348791059586e-05,
      "loss": 1.9336,
      "step": 72100
    },
    {
      "epoch": 5.655647814507285,
      "grad_norm": 7.962328910827637,
      "learning_rate": 4.5286960154577264e-05,
      "loss": 1.939,
      "step": 72200
    },
    {
      "epoch": 5.663481121729594,
      "grad_norm": 5.140292644500732,
      "learning_rate": 4.528043239855868e-05,
      "loss": 1.8907,
      "step": 72300
    },
    {
      "epoch": 5.671314428951904,
      "grad_norm": 6.490638256072998,
      "learning_rate": 4.527390464254008e-05,
      "loss": 1.9043,
      "step": 72400
    },
    {
      "epoch": 5.679147736174213,
      "grad_norm": 4.540446758270264,
      "learning_rate": 4.526737688652149e-05,
      "loss": 1.853,
      "step": 72500
    },
    {
      "epoch": 5.686981043396522,
      "grad_norm": 4.5813212394714355,
      "learning_rate": 4.52608491305029e-05,
      "loss": 1.8741,
      "step": 72600
    },
    {
      "epoch": 5.694814350618831,
      "grad_norm": 6.098787307739258,
      "learning_rate": 4.525432137448431e-05,
      "loss": 1.8846,
      "step": 72700
    },
    {
      "epoch": 5.7026476578411405,
      "grad_norm": 6.793752670288086,
      "learning_rate": 4.524779361846572e-05,
      "loss": 1.9236,
      "step": 72800
    },
    {
      "epoch": 5.71048096506345,
      "grad_norm": 4.868001937866211,
      "learning_rate": 4.524126586244713e-05,
      "loss": 1.9791,
      "step": 72900
    },
    {
      "epoch": 5.718314272285759,
      "grad_norm": 5.120480060577393,
      "learning_rate": 4.523473810642854e-05,
      "loss": 1.889,
      "step": 73000
    },
    {
      "epoch": 5.726147579508068,
      "grad_norm": 4.559901237487793,
      "learning_rate": 4.522821035040994e-05,
      "loss": 1.9489,
      "step": 73100
    },
    {
      "epoch": 5.733980886730378,
      "grad_norm": 6.060333728790283,
      "learning_rate": 4.5221682594391356e-05,
      "loss": 1.8987,
      "step": 73200
    },
    {
      "epoch": 5.7418141939526866,
      "grad_norm": 4.902194976806641,
      "learning_rate": 4.521515483837276e-05,
      "loss": 1.9854,
      "step": 73300
    },
    {
      "epoch": 5.749647501174996,
      "grad_norm": 5.530200958251953,
      "learning_rate": 4.5208627082354174e-05,
      "loss": 1.9158,
      "step": 73400
    },
    {
      "epoch": 5.757480808397306,
      "grad_norm": 5.82634973526001,
      "learning_rate": 4.520209932633558e-05,
      "loss": 1.8622,
      "step": 73500
    },
    {
      "epoch": 5.765314115619614,
      "grad_norm": 6.046175956726074,
      "learning_rate": 4.519557157031699e-05,
      "loss": 1.8556,
      "step": 73600
    },
    {
      "epoch": 5.773147422841924,
      "grad_norm": 3.4712960720062256,
      "learning_rate": 4.51890438142984e-05,
      "loss": 1.8976,
      "step": 73700
    },
    {
      "epoch": 5.780980730064233,
      "grad_norm": 4.903661251068115,
      "learning_rate": 4.5182516058279804e-05,
      "loss": 1.9179,
      "step": 73800
    },
    {
      "epoch": 5.788814037286542,
      "grad_norm": 4.745216369628906,
      "learning_rate": 4.5175988302261217e-05,
      "loss": 1.9179,
      "step": 73900
    },
    {
      "epoch": 5.796647344508852,
      "grad_norm": 7.0701189041137695,
      "learning_rate": 4.516946054624262e-05,
      "loss": 1.863,
      "step": 74000
    },
    {
      "epoch": 5.804480651731161,
      "grad_norm": 5.199631214141846,
      "learning_rate": 4.5162932790224035e-05,
      "loss": 1.9731,
      "step": 74100
    },
    {
      "epoch": 5.81231395895347,
      "grad_norm": 4.9573493003845215,
      "learning_rate": 4.515640503420545e-05,
      "loss": 1.7923,
      "step": 74200
    },
    {
      "epoch": 5.82014726617578,
      "grad_norm": 7.317769527435303,
      "learning_rate": 4.514987727818685e-05,
      "loss": 1.8806,
      "step": 74300
    },
    {
      "epoch": 5.827980573398088,
      "grad_norm": 3.8146755695343018,
      "learning_rate": 4.514334952216826e-05,
      "loss": 1.8647,
      "step": 74400
    },
    {
      "epoch": 5.835813880620398,
      "grad_norm": 3.4732656478881836,
      "learning_rate": 4.513682176614967e-05,
      "loss": 1.972,
      "step": 74500
    },
    {
      "epoch": 5.843647187842707,
      "grad_norm": 4.6508097648620605,
      "learning_rate": 4.513029401013108e-05,
      "loss": 1.8851,
      "step": 74600
    },
    {
      "epoch": 5.851480495065016,
      "grad_norm": 6.346247673034668,
      "learning_rate": 4.512376625411249e-05,
      "loss": 1.9584,
      "step": 74700
    },
    {
      "epoch": 5.859313802287326,
      "grad_norm": 5.19428825378418,
      "learning_rate": 4.51172384980939e-05,
      "loss": 1.9264,
      "step": 74800
    },
    {
      "epoch": 5.867147109509635,
      "grad_norm": 6.994325637817383,
      "learning_rate": 4.511071074207531e-05,
      "loss": 2.0086,
      "step": 74900
    },
    {
      "epoch": 5.874980416731944,
      "grad_norm": 6.30063533782959,
      "learning_rate": 4.5104182986056714e-05,
      "loss": 1.9781,
      "step": 75000
    },
    {
      "epoch": 5.8828137239542535,
      "grad_norm": 4.586233139038086,
      "learning_rate": 4.509765523003812e-05,
      "loss": 1.9655,
      "step": 75100
    },
    {
      "epoch": 5.890647031176563,
      "grad_norm": 4.3467583656311035,
      "learning_rate": 4.509112747401953e-05,
      "loss": 1.9113,
      "step": 75200
    },
    {
      "epoch": 5.898480338398872,
      "grad_norm": 5.452412128448486,
      "learning_rate": 4.5084599718000945e-05,
      "loss": 1.9389,
      "step": 75300
    },
    {
      "epoch": 5.906313645621181,
      "grad_norm": 5.093293190002441,
      "learning_rate": 4.507807196198235e-05,
      "loss": 1.8534,
      "step": 75400
    },
    {
      "epoch": 5.914146952843491,
      "grad_norm": 4.573155879974365,
      "learning_rate": 4.507154420596376e-05,
      "loss": 2.0305,
      "step": 75500
    },
    {
      "epoch": 5.9219802600658,
      "grad_norm": 5.018923759460449,
      "learning_rate": 4.506501644994517e-05,
      "loss": 1.925,
      "step": 75600
    },
    {
      "epoch": 5.929813567288109,
      "grad_norm": 4.109551429748535,
      "learning_rate": 4.5058488693926575e-05,
      "loss": 1.8924,
      "step": 75700
    },
    {
      "epoch": 5.937646874510419,
      "grad_norm": 5.007028579711914,
      "learning_rate": 4.505196093790799e-05,
      "loss": 1.9493,
      "step": 75800
    },
    {
      "epoch": 5.945480181732727,
      "grad_norm": 4.269765377044678,
      "learning_rate": 4.504543318188939e-05,
      "loss": 1.9817,
      "step": 75900
    },
    {
      "epoch": 5.953313488955037,
      "grad_norm": 4.576515197753906,
      "learning_rate": 4.5038905425870805e-05,
      "loss": 1.9661,
      "step": 76000
    },
    {
      "epoch": 5.961146796177346,
      "grad_norm": 6.2868146896362305,
      "learning_rate": 4.503237766985222e-05,
      "loss": 2.0103,
      "step": 76100
    },
    {
      "epoch": 5.968980103399655,
      "grad_norm": 4.502749443054199,
      "learning_rate": 4.5025849913833624e-05,
      "loss": 1.8947,
      "step": 76200
    },
    {
      "epoch": 5.976813410621965,
      "grad_norm": 5.862651348114014,
      "learning_rate": 4.501932215781503e-05,
      "loss": 1.8195,
      "step": 76300
    },
    {
      "epoch": 5.9846467178442735,
      "grad_norm": 4.982394695281982,
      "learning_rate": 4.501279440179644e-05,
      "loss": 1.9056,
      "step": 76400
    },
    {
      "epoch": 5.992480025066583,
      "grad_norm": 4.759921073913574,
      "learning_rate": 4.500626664577785e-05,
      "loss": 1.8364,
      "step": 76500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.867772102355957,
      "eval_runtime": 2.939,
      "eval_samples_per_second": 228.652,
      "eval_steps_per_second": 228.652,
      "step": 76596
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.6794581413269043,
      "eval_runtime": 56.5177,
      "eval_samples_per_second": 225.876,
      "eval_steps_per_second": 225.876,
      "step": 76596
    },
    {
      "epoch": 6.000313332288893,
      "grad_norm": 4.1915106773376465,
      "learning_rate": 4.499973888975926e-05,
      "loss": 2.0548,
      "step": 76600
    },
    {
      "epoch": 6.008146639511201,
      "grad_norm": 4.101052284240723,
      "learning_rate": 4.4993211133740666e-05,
      "loss": 1.9032,
      "step": 76700
    },
    {
      "epoch": 6.015979946733511,
      "grad_norm": 6.037019729614258,
      "learning_rate": 4.498668337772208e-05,
      "loss": 1.873,
      "step": 76800
    },
    {
      "epoch": 6.0238132539558205,
      "grad_norm": 5.5709638595581055,
      "learning_rate": 4.4980155621703485e-05,
      "loss": 2.0023,
      "step": 76900
    },
    {
      "epoch": 6.031646561178129,
      "grad_norm": 5.493380069732666,
      "learning_rate": 4.497362786568489e-05,
      "loss": 1.8703,
      "step": 77000
    },
    {
      "epoch": 6.039479868400439,
      "grad_norm": 5.706836223602295,
      "learning_rate": 4.49671001096663e-05,
      "loss": 1.9066,
      "step": 77100
    },
    {
      "epoch": 6.047313175622748,
      "grad_norm": 4.750121116638184,
      "learning_rate": 4.496057235364771e-05,
      "loss": 1.9404,
      "step": 77200
    },
    {
      "epoch": 6.055146482845057,
      "grad_norm": 4.485889434814453,
      "learning_rate": 4.495404459762912e-05,
      "loss": 1.8529,
      "step": 77300
    },
    {
      "epoch": 6.0629797900673665,
      "grad_norm": 4.57599401473999,
      "learning_rate": 4.4947516841610534e-05,
      "loss": 1.8285,
      "step": 77400
    },
    {
      "epoch": 6.070813097289676,
      "grad_norm": 5.019805431365967,
      "learning_rate": 4.494098908559194e-05,
      "loss": 1.884,
      "step": 77500
    },
    {
      "epoch": 6.078646404511985,
      "grad_norm": 6.550820827484131,
      "learning_rate": 4.4934461329573345e-05,
      "loss": 1.8916,
      "step": 77600
    },
    {
      "epoch": 6.086479711734294,
      "grad_norm": 4.7651896476745605,
      "learning_rate": 4.492793357355476e-05,
      "loss": 1.8269,
      "step": 77700
    },
    {
      "epoch": 6.094313018956603,
      "grad_norm": 6.260495662689209,
      "learning_rate": 4.4921405817536164e-05,
      "loss": 1.9092,
      "step": 77800
    },
    {
      "epoch": 6.102146326178913,
      "grad_norm": 4.538776874542236,
      "learning_rate": 4.4914878061517576e-05,
      "loss": 1.8872,
      "step": 77900
    },
    {
      "epoch": 6.109979633401222,
      "grad_norm": 5.7466325759887695,
      "learning_rate": 4.490835030549899e-05,
      "loss": 1.9071,
      "step": 78000
    },
    {
      "epoch": 6.117812940623531,
      "grad_norm": 5.322168350219727,
      "learning_rate": 4.4901822549480394e-05,
      "loss": 1.8691,
      "step": 78100
    },
    {
      "epoch": 6.12564624784584,
      "grad_norm": 5.718892574310303,
      "learning_rate": 4.48952947934618e-05,
      "loss": 1.9074,
      "step": 78200
    },
    {
      "epoch": 6.13347955506815,
      "grad_norm": 4.058574199676514,
      "learning_rate": 4.488876703744321e-05,
      "loss": 1.8653,
      "step": 78300
    },
    {
      "epoch": 6.141312862290459,
      "grad_norm": 5.2053399085998535,
      "learning_rate": 4.488223928142462e-05,
      "loss": 1.8814,
      "step": 78400
    },
    {
      "epoch": 6.149146169512768,
      "grad_norm": 5.648462772369385,
      "learning_rate": 4.487571152540603e-05,
      "loss": 1.8682,
      "step": 78500
    },
    {
      "epoch": 6.156979476735078,
      "grad_norm": 6.171565532684326,
      "learning_rate": 4.486918376938744e-05,
      "loss": 1.9984,
      "step": 78600
    },
    {
      "epoch": 6.1648127839573865,
      "grad_norm": 4.536208629608154,
      "learning_rate": 4.486265601336885e-05,
      "loss": 1.9019,
      "step": 78700
    },
    {
      "epoch": 6.172646091179696,
      "grad_norm": 4.748797416687012,
      "learning_rate": 4.4856128257350255e-05,
      "loss": 1.8452,
      "step": 78800
    },
    {
      "epoch": 6.180479398402006,
      "grad_norm": 6.341729164123535,
      "learning_rate": 4.484960050133166e-05,
      "loss": 1.8898,
      "step": 78900
    },
    {
      "epoch": 6.188312705624314,
      "grad_norm": 5.07527494430542,
      "learning_rate": 4.4843072745313073e-05,
      "loss": 1.9099,
      "step": 79000
    },
    {
      "epoch": 6.196146012846624,
      "grad_norm": 5.9496331214904785,
      "learning_rate": 4.483654498929448e-05,
      "loss": 1.9545,
      "step": 79100
    },
    {
      "epoch": 6.2039793200689335,
      "grad_norm": 5.686361789703369,
      "learning_rate": 4.483001723327589e-05,
      "loss": 1.8975,
      "step": 79200
    },
    {
      "epoch": 6.211812627291242,
      "grad_norm": 5.151727199554443,
      "learning_rate": 4.4823489477257304e-05,
      "loss": 1.9581,
      "step": 79300
    },
    {
      "epoch": 6.219645934513552,
      "grad_norm": 4.589910507202148,
      "learning_rate": 4.481696172123871e-05,
      "loss": 1.8731,
      "step": 79400
    },
    {
      "epoch": 6.22747924173586,
      "grad_norm": 5.16981840133667,
      "learning_rate": 4.4810433965220116e-05,
      "loss": 1.9005,
      "step": 79500
    },
    {
      "epoch": 6.23531254895817,
      "grad_norm": 5.0053300857543945,
      "learning_rate": 4.480390620920153e-05,
      "loss": 1.81,
      "step": 79600
    },
    {
      "epoch": 6.24314585618048,
      "grad_norm": 5.2259111404418945,
      "learning_rate": 4.4797378453182934e-05,
      "loss": 1.8709,
      "step": 79700
    },
    {
      "epoch": 6.250979163402788,
      "grad_norm": 5.287365913391113,
      "learning_rate": 4.479085069716435e-05,
      "loss": 1.7591,
      "step": 79800
    },
    {
      "epoch": 6.258812470625098,
      "grad_norm": 3.4519097805023193,
      "learning_rate": 4.478432294114576e-05,
      "loss": 1.8251,
      "step": 79900
    },
    {
      "epoch": 6.266645777847407,
      "grad_norm": 5.147543430328369,
      "learning_rate": 4.4777795185127165e-05,
      "loss": 1.9337,
      "step": 80000
    },
    {
      "epoch": 6.274479085069716,
      "grad_norm": 4.172781467437744,
      "learning_rate": 4.477126742910857e-05,
      "loss": 1.8103,
      "step": 80100
    },
    {
      "epoch": 6.282312392292026,
      "grad_norm": 5.355510711669922,
      "learning_rate": 4.4764739673089977e-05,
      "loss": 1.9107,
      "step": 80200
    },
    {
      "epoch": 6.290145699514335,
      "grad_norm": 4.905365943908691,
      "learning_rate": 4.475821191707139e-05,
      "loss": 1.903,
      "step": 80300
    },
    {
      "epoch": 6.297979006736644,
      "grad_norm": 4.739535331726074,
      "learning_rate": 4.4751684161052795e-05,
      "loss": 1.9432,
      "step": 80400
    },
    {
      "epoch": 6.3058123139589535,
      "grad_norm": 6.388297080993652,
      "learning_rate": 4.474515640503421e-05,
      "loss": 1.8176,
      "step": 80500
    },
    {
      "epoch": 6.313645621181263,
      "grad_norm": 4.920557022094727,
      "learning_rate": 4.473862864901562e-05,
      "loss": 1.8841,
      "step": 80600
    },
    {
      "epoch": 6.321478928403572,
      "grad_norm": 4.263242244720459,
      "learning_rate": 4.4732100892997026e-05,
      "loss": 1.9782,
      "step": 80700
    },
    {
      "epoch": 6.329312235625881,
      "grad_norm": 5.4849371910095215,
      "learning_rate": 4.472557313697843e-05,
      "loss": 1.9086,
      "step": 80800
    },
    {
      "epoch": 6.337145542848191,
      "grad_norm": 5.249056339263916,
      "learning_rate": 4.4719045380959844e-05,
      "loss": 1.7977,
      "step": 80900
    },
    {
      "epoch": 6.3449788500704996,
      "grad_norm": 6.1368088722229,
      "learning_rate": 4.471251762494125e-05,
      "loss": 1.9331,
      "step": 81000
    },
    {
      "epoch": 6.352812157292809,
      "grad_norm": 5.180891513824463,
      "learning_rate": 4.470598986892266e-05,
      "loss": 1.8518,
      "step": 81100
    },
    {
      "epoch": 6.360645464515119,
      "grad_norm": 4.820588111877441,
      "learning_rate": 4.4699462112904075e-05,
      "loss": 1.934,
      "step": 81200
    },
    {
      "epoch": 6.368478771737427,
      "grad_norm": 4.86692476272583,
      "learning_rate": 4.469293435688548e-05,
      "loss": 1.9202,
      "step": 81300
    },
    {
      "epoch": 6.376312078959737,
      "grad_norm": 6.052197456359863,
      "learning_rate": 4.4686406600866886e-05,
      "loss": 1.828,
      "step": 81400
    },
    {
      "epoch": 6.3841453861820465,
      "grad_norm": 4.64056921005249,
      "learning_rate": 4.46798788448483e-05,
      "loss": 1.8142,
      "step": 81500
    },
    {
      "epoch": 6.391978693404355,
      "grad_norm": 5.738348960876465,
      "learning_rate": 4.4673351088829705e-05,
      "loss": 1.8869,
      "step": 81600
    },
    {
      "epoch": 6.399812000626665,
      "grad_norm": 5.119624614715576,
      "learning_rate": 4.466682333281112e-05,
      "loss": 1.8516,
      "step": 81700
    },
    {
      "epoch": 6.4076453078489735,
      "grad_norm": 4.975688457489014,
      "learning_rate": 4.466029557679252e-05,
      "loss": 1.884,
      "step": 81800
    },
    {
      "epoch": 6.415478615071283,
      "grad_norm": 4.834557056427002,
      "learning_rate": 4.4653767820773936e-05,
      "loss": 1.9063,
      "step": 81900
    },
    {
      "epoch": 6.423311922293593,
      "grad_norm": 5.076468467712402,
      "learning_rate": 4.464724006475534e-05,
      "loss": 1.9076,
      "step": 82000
    },
    {
      "epoch": 6.431145229515901,
      "grad_norm": 4.847596645355225,
      "learning_rate": 4.464071230873675e-05,
      "loss": 1.8414,
      "step": 82100
    },
    {
      "epoch": 6.438978536738211,
      "grad_norm": 5.397582054138184,
      "learning_rate": 4.463418455271816e-05,
      "loss": 1.9037,
      "step": 82200
    },
    {
      "epoch": 6.44681184396052,
      "grad_norm": 4.873594760894775,
      "learning_rate": 4.4627656796699565e-05,
      "loss": 1.8368,
      "step": 82300
    },
    {
      "epoch": 6.454645151182829,
      "grad_norm": 6.103970527648926,
      "learning_rate": 4.462112904068098e-05,
      "loss": 1.865,
      "step": 82400
    },
    {
      "epoch": 6.462478458405139,
      "grad_norm": 5.092548847198486,
      "learning_rate": 4.461460128466239e-05,
      "loss": 1.8345,
      "step": 82500
    },
    {
      "epoch": 6.470311765627448,
      "grad_norm": 3.809267282485962,
      "learning_rate": 4.4608073528643796e-05,
      "loss": 1.8359,
      "step": 82600
    },
    {
      "epoch": 6.478145072849757,
      "grad_norm": 4.547642707824707,
      "learning_rate": 4.46015457726252e-05,
      "loss": 1.9997,
      "step": 82700
    },
    {
      "epoch": 6.4859783800720665,
      "grad_norm": 5.427545547485352,
      "learning_rate": 4.4595018016606615e-05,
      "loss": 1.9408,
      "step": 82800
    },
    {
      "epoch": 6.493811687294376,
      "grad_norm": 3.9555578231811523,
      "learning_rate": 4.458849026058802e-05,
      "loss": 1.9742,
      "step": 82900
    },
    {
      "epoch": 6.501644994516685,
      "grad_norm": 4.010596752166748,
      "learning_rate": 4.458196250456943e-05,
      "loss": 1.901,
      "step": 83000
    },
    {
      "epoch": 6.509478301738994,
      "grad_norm": 6.208824157714844,
      "learning_rate": 4.4575434748550845e-05,
      "loss": 1.9276,
      "step": 83100
    },
    {
      "epoch": 6.517311608961304,
      "grad_norm": 4.94537353515625,
      "learning_rate": 4.456890699253225e-05,
      "loss": 1.8828,
      "step": 83200
    },
    {
      "epoch": 6.525144916183613,
      "grad_norm": 6.604650497436523,
      "learning_rate": 4.456237923651366e-05,
      "loss": 1.8067,
      "step": 83300
    },
    {
      "epoch": 6.532978223405922,
      "grad_norm": 4.236048221588135,
      "learning_rate": 4.455585148049506e-05,
      "loss": 1.8933,
      "step": 83400
    },
    {
      "epoch": 6.540811530628231,
      "grad_norm": 5.718343257904053,
      "learning_rate": 4.4549323724476475e-05,
      "loss": 1.7853,
      "step": 83500
    },
    {
      "epoch": 6.54864483785054,
      "grad_norm": 4.885287761688232,
      "learning_rate": 4.454279596845788e-05,
      "loss": 1.8796,
      "step": 83600
    },
    {
      "epoch": 6.55647814507285,
      "grad_norm": 4.822196960449219,
      "learning_rate": 4.4536268212439294e-05,
      "loss": 1.8161,
      "step": 83700
    },
    {
      "epoch": 6.564311452295159,
      "grad_norm": 5.285719394683838,
      "learning_rate": 4.4529740456420706e-05,
      "loss": 1.8457,
      "step": 83800
    },
    {
      "epoch": 6.572144759517468,
      "grad_norm": 4.4988112449646,
      "learning_rate": 4.452321270040211e-05,
      "loss": 1.9092,
      "step": 83900
    },
    {
      "epoch": 6.579978066739778,
      "grad_norm": 5.438190937042236,
      "learning_rate": 4.451668494438352e-05,
      "loss": 1.847,
      "step": 84000
    },
    {
      "epoch": 6.5878113739620865,
      "grad_norm": 5.098347187042236,
      "learning_rate": 4.451015718836493e-05,
      "loss": 1.9015,
      "step": 84100
    },
    {
      "epoch": 6.595644681184396,
      "grad_norm": 6.450432777404785,
      "learning_rate": 4.4503629432346336e-05,
      "loss": 1.8271,
      "step": 84200
    },
    {
      "epoch": 6.603477988406706,
      "grad_norm": 4.119333267211914,
      "learning_rate": 4.449710167632775e-05,
      "loss": 1.8054,
      "step": 84300
    },
    {
      "epoch": 6.611311295629014,
      "grad_norm": 5.421143054962158,
      "learning_rate": 4.449057392030916e-05,
      "loss": 1.9047,
      "step": 84400
    },
    {
      "epoch": 6.619144602851324,
      "grad_norm": 5.675657749176025,
      "learning_rate": 4.448404616429057e-05,
      "loss": 1.7989,
      "step": 84500
    },
    {
      "epoch": 6.6269779100736335,
      "grad_norm": 4.604155540466309,
      "learning_rate": 4.447751840827197e-05,
      "loss": 1.8678,
      "step": 84600
    },
    {
      "epoch": 6.634811217295942,
      "grad_norm": 6.629822731018066,
      "learning_rate": 4.4470990652253385e-05,
      "loss": 1.8826,
      "step": 84700
    },
    {
      "epoch": 6.642644524518252,
      "grad_norm": 5.524709701538086,
      "learning_rate": 4.446446289623479e-05,
      "loss": 1.8644,
      "step": 84800
    },
    {
      "epoch": 6.650477831740561,
      "grad_norm": 7.591411590576172,
      "learning_rate": 4.4457935140216203e-05,
      "loss": 1.8286,
      "step": 84900
    },
    {
      "epoch": 6.65831113896287,
      "grad_norm": 4.686996936798096,
      "learning_rate": 4.4451407384197616e-05,
      "loss": 1.9372,
      "step": 85000
    },
    {
      "epoch": 6.6661444461851795,
      "grad_norm": 5.74160623550415,
      "learning_rate": 4.444487962817902e-05,
      "loss": 1.9674,
      "step": 85100
    },
    {
      "epoch": 6.673977753407488,
      "grad_norm": 4.9561309814453125,
      "learning_rate": 4.443835187216043e-05,
      "loss": 1.8918,
      "step": 85200
    },
    {
      "epoch": 6.681811060629798,
      "grad_norm": 5.591190338134766,
      "learning_rate": 4.443182411614183e-05,
      "loss": 1.8786,
      "step": 85300
    },
    {
      "epoch": 6.689644367852107,
      "grad_norm": 6.107593059539795,
      "learning_rate": 4.4425296360123246e-05,
      "loss": 1.9364,
      "step": 85400
    },
    {
      "epoch": 6.697477675074416,
      "grad_norm": 4.7317795753479,
      "learning_rate": 4.441876860410465e-05,
      "loss": 1.8516,
      "step": 85500
    },
    {
      "epoch": 6.705310982296726,
      "grad_norm": 7.954570293426514,
      "learning_rate": 4.4412240848086064e-05,
      "loss": 1.8526,
      "step": 85600
    },
    {
      "epoch": 6.713144289519035,
      "grad_norm": 4.504105091094971,
      "learning_rate": 4.440571309206748e-05,
      "loss": 1.9056,
      "step": 85700
    },
    {
      "epoch": 6.720977596741344,
      "grad_norm": 4.935783863067627,
      "learning_rate": 4.439918533604888e-05,
      "loss": 1.9122,
      "step": 85800
    },
    {
      "epoch": 6.7288109039636534,
      "grad_norm": 4.788303375244141,
      "learning_rate": 4.439265758003029e-05,
      "loss": 1.8789,
      "step": 85900
    },
    {
      "epoch": 6.736644211185963,
      "grad_norm": 6.581550121307373,
      "learning_rate": 4.43861298240117e-05,
      "loss": 1.968,
      "step": 86000
    },
    {
      "epoch": 6.744477518408272,
      "grad_norm": 3.987877607345581,
      "learning_rate": 4.4379602067993107e-05,
      "loss": 1.8874,
      "step": 86100
    },
    {
      "epoch": 6.752310825630581,
      "grad_norm": 6.595106601715088,
      "learning_rate": 4.437307431197452e-05,
      "loss": 1.8411,
      "step": 86200
    },
    {
      "epoch": 6.760144132852891,
      "grad_norm": 4.554531574249268,
      "learning_rate": 4.436654655595593e-05,
      "loss": 1.9071,
      "step": 86300
    },
    {
      "epoch": 6.7679774400751995,
      "grad_norm": 5.295754432678223,
      "learning_rate": 4.436001879993734e-05,
      "loss": 1.8783,
      "step": 86400
    },
    {
      "epoch": 6.775810747297509,
      "grad_norm": 5.124614238739014,
      "learning_rate": 4.435349104391874e-05,
      "loss": 1.883,
      "step": 86500
    },
    {
      "epoch": 6.783644054519819,
      "grad_norm": 4.368738174438477,
      "learning_rate": 4.4346963287900156e-05,
      "loss": 1.9424,
      "step": 86600
    },
    {
      "epoch": 6.791477361742127,
      "grad_norm": 5.263012409210205,
      "learning_rate": 4.434043553188156e-05,
      "loss": 1.9454,
      "step": 86700
    },
    {
      "epoch": 6.799310668964437,
      "grad_norm": 5.89384126663208,
      "learning_rate": 4.433390777586297e-05,
      "loss": 1.8522,
      "step": 86800
    },
    {
      "epoch": 6.807143976186746,
      "grad_norm": 5.274849891662598,
      "learning_rate": 4.432738001984438e-05,
      "loss": 1.9779,
      "step": 86900
    },
    {
      "epoch": 6.814977283409055,
      "grad_norm": 4.632771968841553,
      "learning_rate": 4.432085226382579e-05,
      "loss": 1.8908,
      "step": 87000
    },
    {
      "epoch": 6.822810590631365,
      "grad_norm": 4.20460844039917,
      "learning_rate": 4.43143245078072e-05,
      "loss": 1.8684,
      "step": 87100
    },
    {
      "epoch": 6.830643897853674,
      "grad_norm": 4.891379356384277,
      "learning_rate": 4.4307796751788604e-05,
      "loss": 1.8708,
      "step": 87200
    },
    {
      "epoch": 6.838477205075983,
      "grad_norm": 5.774539947509766,
      "learning_rate": 4.4301268995770016e-05,
      "loss": 1.8515,
      "step": 87300
    },
    {
      "epoch": 6.846310512298293,
      "grad_norm": 5.492575168609619,
      "learning_rate": 4.429474123975142e-05,
      "loss": 1.8912,
      "step": 87400
    },
    {
      "epoch": 6.854143819520601,
      "grad_norm": 4.398056507110596,
      "learning_rate": 4.4288213483732835e-05,
      "loss": 1.8965,
      "step": 87500
    },
    {
      "epoch": 6.861977126742911,
      "grad_norm": 6.250814914703369,
      "learning_rate": 4.428168572771425e-05,
      "loss": 1.9023,
      "step": 87600
    },
    {
      "epoch": 6.86981043396522,
      "grad_norm": 4.352684020996094,
      "learning_rate": 4.427515797169565e-05,
      "loss": 1.9492,
      "step": 87700
    },
    {
      "epoch": 6.877643741187529,
      "grad_norm": 5.8058762550354,
      "learning_rate": 4.426863021567706e-05,
      "loss": 1.8708,
      "step": 87800
    },
    {
      "epoch": 6.885477048409839,
      "grad_norm": 4.107537269592285,
      "learning_rate": 4.426210245965847e-05,
      "loss": 1.9121,
      "step": 87900
    },
    {
      "epoch": 6.893310355632148,
      "grad_norm": 4.467634201049805,
      "learning_rate": 4.425557470363988e-05,
      "loss": 1.9767,
      "step": 88000
    },
    {
      "epoch": 6.901143662854457,
      "grad_norm": 5.262410640716553,
      "learning_rate": 4.424904694762129e-05,
      "loss": 1.8488,
      "step": 88100
    },
    {
      "epoch": 6.9089769700767665,
      "grad_norm": 5.637834548950195,
      "learning_rate": 4.42425191916027e-05,
      "loss": 1.8325,
      "step": 88200
    },
    {
      "epoch": 6.916810277299076,
      "grad_norm": 7.634215354919434,
      "learning_rate": 4.423599143558411e-05,
      "loss": 1.8562,
      "step": 88300
    },
    {
      "epoch": 6.924643584521385,
      "grad_norm": 6.064032554626465,
      "learning_rate": 4.4229463679565514e-05,
      "loss": 1.9156,
      "step": 88400
    },
    {
      "epoch": 6.932476891743694,
      "grad_norm": 5.025382995605469,
      "learning_rate": 4.422293592354692e-05,
      "loss": 1.9571,
      "step": 88500
    },
    {
      "epoch": 6.940310198966003,
      "grad_norm": 4.433874607086182,
      "learning_rate": 4.421640816752833e-05,
      "loss": 1.9237,
      "step": 88600
    },
    {
      "epoch": 6.948143506188313,
      "grad_norm": 5.8554368019104,
      "learning_rate": 4.420988041150974e-05,
      "loss": 1.949,
      "step": 88700
    },
    {
      "epoch": 6.955976813410622,
      "grad_norm": 5.478399276733398,
      "learning_rate": 4.420335265549115e-05,
      "loss": 1.9383,
      "step": 88800
    },
    {
      "epoch": 6.963810120632932,
      "grad_norm": 4.845142364501953,
      "learning_rate": 4.419682489947256e-05,
      "loss": 1.8645,
      "step": 88900
    },
    {
      "epoch": 6.97164342785524,
      "grad_norm": 4.084550380706787,
      "learning_rate": 4.419029714345397e-05,
      "loss": 1.8595,
      "step": 89000
    },
    {
      "epoch": 6.97947673507755,
      "grad_norm": 6.044218063354492,
      "learning_rate": 4.4183769387435374e-05,
      "loss": 1.857,
      "step": 89100
    },
    {
      "epoch": 6.987310042299859,
      "grad_norm": 5.231873989105225,
      "learning_rate": 4.417724163141679e-05,
      "loss": 1.9044,
      "step": 89200
    },
    {
      "epoch": 6.995143349522168,
      "grad_norm": 5.063162326812744,
      "learning_rate": 4.417071387539819e-05,
      "loss": 1.8559,
      "step": 89300
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8605657815933228,
      "eval_runtime": 1.4457,
      "eval_samples_per_second": 464.821,
      "eval_steps_per_second": 464.821,
      "step": 89362
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.6574029922485352,
      "eval_runtime": 43.0164,
      "eval_samples_per_second": 296.77,
      "eval_steps_per_second": 296.77,
      "step": 89362
    },
    {
      "epoch": 7.002976656744478,
      "grad_norm": 6.499480724334717,
      "learning_rate": 4.4164186119379605e-05,
      "loss": 1.9201,
      "step": 89400
    },
    {
      "epoch": 7.0108099639667865,
      "grad_norm": 6.688878059387207,
      "learning_rate": 4.415765836336102e-05,
      "loss": 1.8702,
      "step": 89500
    },
    {
      "epoch": 7.018643271189096,
      "grad_norm": 5.679152965545654,
      "learning_rate": 4.4151130607342424e-05,
      "loss": 1.8964,
      "step": 89600
    },
    {
      "epoch": 7.026476578411406,
      "grad_norm": 5.525883197784424,
      "learning_rate": 4.414460285132383e-05,
      "loss": 1.8369,
      "step": 89700
    },
    {
      "epoch": 7.034309885633714,
      "grad_norm": 4.546505451202393,
      "learning_rate": 4.413807509530524e-05,
      "loss": 1.923,
      "step": 89800
    },
    {
      "epoch": 7.042143192856024,
      "grad_norm": 4.976408958435059,
      "learning_rate": 4.413154733928665e-05,
      "loss": 1.8453,
      "step": 89900
    },
    {
      "epoch": 7.049976500078333,
      "grad_norm": 5.76019811630249,
      "learning_rate": 4.4125019583268054e-05,
      "loss": 1.7996,
      "step": 90000
    },
    {
      "epoch": 7.057809807300642,
      "grad_norm": 4.413512229919434,
      "learning_rate": 4.411849182724947e-05,
      "loss": 1.9298,
      "step": 90100
    },
    {
      "epoch": 7.065643114522952,
      "grad_norm": 4.031225681304932,
      "learning_rate": 4.411196407123088e-05,
      "loss": 1.8699,
      "step": 90200
    },
    {
      "epoch": 7.073476421745261,
      "grad_norm": 4.239649295806885,
      "learning_rate": 4.4105436315212284e-05,
      "loss": 1.9258,
      "step": 90300
    },
    {
      "epoch": 7.08130972896757,
      "grad_norm": 4.375614166259766,
      "learning_rate": 4.409890855919369e-05,
      "loss": 1.8715,
      "step": 90400
    },
    {
      "epoch": 7.0891430361898795,
      "grad_norm": 5.6234450340271,
      "learning_rate": 4.40923808031751e-05,
      "loss": 1.8549,
      "step": 90500
    },
    {
      "epoch": 7.096976343412188,
      "grad_norm": 5.410249710083008,
      "learning_rate": 4.408585304715651e-05,
      "loss": 1.7921,
      "step": 90600
    },
    {
      "epoch": 7.104809650634498,
      "grad_norm": 4.5057549476623535,
      "learning_rate": 4.407932529113792e-05,
      "loss": 1.8443,
      "step": 90700
    },
    {
      "epoch": 7.112642957856807,
      "grad_norm": 7.101842403411865,
      "learning_rate": 4.4072797535119334e-05,
      "loss": 1.8474,
      "step": 90800
    },
    {
      "epoch": 7.120476265079116,
      "grad_norm": 4.568471908569336,
      "learning_rate": 4.406626977910074e-05,
      "loss": 1.8962,
      "step": 90900
    },
    {
      "epoch": 7.128309572301426,
      "grad_norm": 3.6762197017669678,
      "learning_rate": 4.4059742023082145e-05,
      "loss": 1.8391,
      "step": 91000
    },
    {
      "epoch": 7.136142879523735,
      "grad_norm": 5.571392059326172,
      "learning_rate": 4.405321426706356e-05,
      "loss": 1.7481,
      "step": 91100
    },
    {
      "epoch": 7.143976186746044,
      "grad_norm": 4.548376560211182,
      "learning_rate": 4.404668651104496e-05,
      "loss": 1.8306,
      "step": 91200
    },
    {
      "epoch": 7.151809493968353,
      "grad_norm": 4.851761817932129,
      "learning_rate": 4.4040158755026376e-05,
      "loss": 1.8056,
      "step": 91300
    },
    {
      "epoch": 7.159642801190663,
      "grad_norm": 5.339156150817871,
      "learning_rate": 4.403363099900779e-05,
      "loss": 1.8455,
      "step": 91400
    },
    {
      "epoch": 7.167476108412972,
      "grad_norm": 5.268421649932861,
      "learning_rate": 4.4027103242989194e-05,
      "loss": 1.8811,
      "step": 91500
    },
    {
      "epoch": 7.175309415635281,
      "grad_norm": 6.410618782043457,
      "learning_rate": 4.40205754869706e-05,
      "loss": 1.893,
      "step": 91600
    },
    {
      "epoch": 7.183142722857591,
      "grad_norm": 5.148377895355225,
      "learning_rate": 4.401404773095201e-05,
      "loss": 1.8104,
      "step": 91700
    },
    {
      "epoch": 7.1909760300798995,
      "grad_norm": 6.272737979888916,
      "learning_rate": 4.400751997493342e-05,
      "loss": 1.8325,
      "step": 91800
    },
    {
      "epoch": 7.198809337302209,
      "grad_norm": 3.938178777694702,
      "learning_rate": 4.4000992218914824e-05,
      "loss": 1.8802,
      "step": 91900
    },
    {
      "epoch": 7.206642644524519,
      "grad_norm": 3.7361228466033936,
      "learning_rate": 4.399446446289624e-05,
      "loss": 1.8714,
      "step": 92000
    },
    {
      "epoch": 7.214475951746827,
      "grad_norm": 4.937767028808594,
      "learning_rate": 4.398793670687765e-05,
      "loss": 1.8992,
      "step": 92100
    },
    {
      "epoch": 7.222309258969137,
      "grad_norm": 4.7627458572387695,
      "learning_rate": 4.3981408950859055e-05,
      "loss": 1.9238,
      "step": 92200
    },
    {
      "epoch": 7.2301425661914465,
      "grad_norm": 6.302828311920166,
      "learning_rate": 4.397488119484046e-05,
      "loss": 1.8532,
      "step": 92300
    },
    {
      "epoch": 7.237975873413755,
      "grad_norm": 3.928489923477173,
      "learning_rate": 4.396835343882187e-05,
      "loss": 1.9165,
      "step": 92400
    },
    {
      "epoch": 7.245809180636065,
      "grad_norm": 3.9136111736297607,
      "learning_rate": 4.396182568280328e-05,
      "loss": 1.978,
      "step": 92500
    },
    {
      "epoch": 7.253642487858373,
      "grad_norm": 3.695496082305908,
      "learning_rate": 4.395529792678469e-05,
      "loss": 1.8179,
      "step": 92600
    },
    {
      "epoch": 7.261475795080683,
      "grad_norm": 4.839295864105225,
      "learning_rate": 4.3948770170766104e-05,
      "loss": 1.9167,
      "step": 92700
    },
    {
      "epoch": 7.2693091023029925,
      "grad_norm": 6.126452445983887,
      "learning_rate": 4.394224241474751e-05,
      "loss": 1.8908,
      "step": 92800
    },
    {
      "epoch": 7.277142409525301,
      "grad_norm": 6.412555694580078,
      "learning_rate": 4.3935714658728916e-05,
      "loss": 1.9305,
      "step": 92900
    },
    {
      "epoch": 7.284975716747611,
      "grad_norm": 3.926610231399536,
      "learning_rate": 4.392918690271033e-05,
      "loss": 1.8667,
      "step": 93000
    },
    {
      "epoch": 7.29280902396992,
      "grad_norm": 5.136283874511719,
      "learning_rate": 4.3922659146691734e-05,
      "loss": 1.8486,
      "step": 93100
    },
    {
      "epoch": 7.300642331192229,
      "grad_norm": 5.384939193725586,
      "learning_rate": 4.391613139067314e-05,
      "loss": 1.8792,
      "step": 93200
    },
    {
      "epoch": 7.308475638414539,
      "grad_norm": 6.444245338439941,
      "learning_rate": 4.390960363465456e-05,
      "loss": 1.8024,
      "step": 93300
    },
    {
      "epoch": 7.316308945636848,
      "grad_norm": 7.313949108123779,
      "learning_rate": 4.3903075878635965e-05,
      "loss": 1.8406,
      "step": 93400
    },
    {
      "epoch": 7.324142252859157,
      "grad_norm": 7.9215192794799805,
      "learning_rate": 4.389654812261737e-05,
      "loss": 1.9644,
      "step": 93500
    },
    {
      "epoch": 7.3319755600814664,
      "grad_norm": 5.883944034576416,
      "learning_rate": 4.3890020366598776e-05,
      "loss": 1.9222,
      "step": 93600
    },
    {
      "epoch": 7.339808867303776,
      "grad_norm": 5.657112121582031,
      "learning_rate": 4.388349261058019e-05,
      "loss": 1.9182,
      "step": 93700
    },
    {
      "epoch": 7.347642174526085,
      "grad_norm": 5.3730950355529785,
      "learning_rate": 4.3876964854561595e-05,
      "loss": 1.7703,
      "step": 93800
    },
    {
      "epoch": 7.355475481748394,
      "grad_norm": 5.514874458312988,
      "learning_rate": 4.387043709854301e-05,
      "loss": 1.8814,
      "step": 93900
    },
    {
      "epoch": 7.363308788970704,
      "grad_norm": 6.421929836273193,
      "learning_rate": 4.386390934252442e-05,
      "loss": 1.852,
      "step": 94000
    },
    {
      "epoch": 7.3711420961930125,
      "grad_norm": 4.404657363891602,
      "learning_rate": 4.3857381586505826e-05,
      "loss": 1.8085,
      "step": 94100
    },
    {
      "epoch": 7.378975403415322,
      "grad_norm": 4.86623477935791,
      "learning_rate": 4.385085383048723e-05,
      "loss": 1.8456,
      "step": 94200
    },
    {
      "epoch": 7.386808710637631,
      "grad_norm": 5.411877632141113,
      "learning_rate": 4.3844326074468644e-05,
      "loss": 1.8136,
      "step": 94300
    },
    {
      "epoch": 7.39464201785994,
      "grad_norm": 5.870655059814453,
      "learning_rate": 4.383779831845005e-05,
      "loss": 1.8956,
      "step": 94400
    },
    {
      "epoch": 7.40247532508225,
      "grad_norm": 4.382625102996826,
      "learning_rate": 4.383127056243146e-05,
      "loss": 1.819,
      "step": 94500
    },
    {
      "epoch": 7.410308632304559,
      "grad_norm": 5.534652233123779,
      "learning_rate": 4.3824742806412875e-05,
      "loss": 1.8186,
      "step": 94600
    },
    {
      "epoch": 7.418141939526868,
      "grad_norm": 5.082809925079346,
      "learning_rate": 4.381821505039428e-05,
      "loss": 1.8814,
      "step": 94700
    },
    {
      "epoch": 7.425975246749178,
      "grad_norm": 5.485293865203857,
      "learning_rate": 4.3811687294375686e-05,
      "loss": 1.8572,
      "step": 94800
    },
    {
      "epoch": 7.433808553971486,
      "grad_norm": 8.611104965209961,
      "learning_rate": 4.38051595383571e-05,
      "loss": 1.8818,
      "step": 94900
    },
    {
      "epoch": 7.441641861193796,
      "grad_norm": 4.357416152954102,
      "learning_rate": 4.3798631782338505e-05,
      "loss": 1.7661,
      "step": 95000
    },
    {
      "epoch": 7.449475168416106,
      "grad_norm": 5.575433254241943,
      "learning_rate": 4.379210402631991e-05,
      "loss": 1.8564,
      "step": 95100
    },
    {
      "epoch": 7.457308475638414,
      "grad_norm": 5.230414867401123,
      "learning_rate": 4.378557627030132e-05,
      "loss": 1.9107,
      "step": 95200
    },
    {
      "epoch": 7.465141782860724,
      "grad_norm": 4.930791854858398,
      "learning_rate": 4.3779048514282735e-05,
      "loss": 1.8338,
      "step": 95300
    },
    {
      "epoch": 7.472975090083033,
      "grad_norm": 6.035702705383301,
      "learning_rate": 4.377252075826414e-05,
      "loss": 1.895,
      "step": 95400
    },
    {
      "epoch": 7.480808397305342,
      "grad_norm": 4.162937164306641,
      "learning_rate": 4.376599300224555e-05,
      "loss": 1.8117,
      "step": 95500
    },
    {
      "epoch": 7.488641704527652,
      "grad_norm": 4.528042793273926,
      "learning_rate": 4.375946524622696e-05,
      "loss": 1.9223,
      "step": 95600
    },
    {
      "epoch": 7.496475011749961,
      "grad_norm": 5.927042007446289,
      "learning_rate": 4.3752937490208365e-05,
      "loss": 1.9046,
      "step": 95700
    },
    {
      "epoch": 7.50430831897227,
      "grad_norm": 6.103978157043457,
      "learning_rate": 4.374640973418978e-05,
      "loss": 1.8714,
      "step": 95800
    },
    {
      "epoch": 7.5121416261945795,
      "grad_norm": 5.04842472076416,
      "learning_rate": 4.373988197817119e-05,
      "loss": 1.8666,
      "step": 95900
    },
    {
      "epoch": 7.519974933416888,
      "grad_norm": 4.13715934753418,
      "learning_rate": 4.3733354222152596e-05,
      "loss": 1.8438,
      "step": 96000
    },
    {
      "epoch": 7.527808240639198,
      "grad_norm": 5.689407825469971,
      "learning_rate": 4.3726826466134e-05,
      "loss": 1.8659,
      "step": 96100
    },
    {
      "epoch": 7.535641547861507,
      "grad_norm": 5.168278217315674,
      "learning_rate": 4.3720298710115414e-05,
      "loss": 1.8434,
      "step": 96200
    },
    {
      "epoch": 7.543474855083817,
      "grad_norm": 4.838292121887207,
      "learning_rate": 4.371377095409682e-05,
      "loss": 1.8349,
      "step": 96300
    },
    {
      "epoch": 7.551308162306126,
      "grad_norm": 3.344209671020508,
      "learning_rate": 4.3707243198078226e-05,
      "loss": 1.9918,
      "step": 96400
    },
    {
      "epoch": 7.559141469528435,
      "grad_norm": 5.948869228363037,
      "learning_rate": 4.3700715442059645e-05,
      "loss": 1.878,
      "step": 96500
    },
    {
      "epoch": 7.566974776750744,
      "grad_norm": 5.565150737762451,
      "learning_rate": 4.369418768604105e-05,
      "loss": 1.8788,
      "step": 96600
    },
    {
      "epoch": 7.574808083973053,
      "grad_norm": 4.887584686279297,
      "learning_rate": 4.368765993002246e-05,
      "loss": 1.853,
      "step": 96700
    },
    {
      "epoch": 7.582641391195363,
      "grad_norm": 5.971219539642334,
      "learning_rate": 4.368113217400387e-05,
      "loss": 1.865,
      "step": 96800
    },
    {
      "epoch": 7.590474698417672,
      "grad_norm": 5.72511100769043,
      "learning_rate": 4.3674604417985275e-05,
      "loss": 1.811,
      "step": 96900
    },
    {
      "epoch": 7.598308005639981,
      "grad_norm": 7.589619159698486,
      "learning_rate": 4.366807666196668e-05,
      "loss": 1.8823,
      "step": 97000
    },
    {
      "epoch": 7.606141312862291,
      "grad_norm": 7.245207786560059,
      "learning_rate": 4.3661548905948093e-05,
      "loss": 1.8321,
      "step": 97100
    },
    {
      "epoch": 7.6139746200845995,
      "grad_norm": 3.9786863327026367,
      "learning_rate": 4.3655021149929506e-05,
      "loss": 1.8142,
      "step": 97200
    },
    {
      "epoch": 7.621807927306909,
      "grad_norm": 5.338998317718506,
      "learning_rate": 4.364849339391091e-05,
      "loss": 1.9095,
      "step": 97300
    },
    {
      "epoch": 7.629641234529219,
      "grad_norm": 5.823686122894287,
      "learning_rate": 4.364196563789232e-05,
      "loss": 1.8579,
      "step": 97400
    },
    {
      "epoch": 7.637474541751527,
      "grad_norm": 5.580321788787842,
      "learning_rate": 4.363543788187373e-05,
      "loss": 1.8161,
      "step": 97500
    },
    {
      "epoch": 7.645307848973837,
      "grad_norm": 5.439297199249268,
      "learning_rate": 4.3628910125855136e-05,
      "loss": 1.9051,
      "step": 97600
    },
    {
      "epoch": 7.6531411561961455,
      "grad_norm": 4.845741271972656,
      "learning_rate": 4.362238236983655e-05,
      "loss": 1.9804,
      "step": 97700
    },
    {
      "epoch": 7.660974463418455,
      "grad_norm": 5.6002631187438965,
      "learning_rate": 4.361585461381796e-05,
      "loss": 1.8032,
      "step": 97800
    },
    {
      "epoch": 7.668807770640765,
      "grad_norm": 5.441030025482178,
      "learning_rate": 4.360932685779937e-05,
      "loss": 1.8654,
      "step": 97900
    },
    {
      "epoch": 7.676641077863074,
      "grad_norm": 4.140481472015381,
      "learning_rate": 4.360279910178077e-05,
      "loss": 1.8183,
      "step": 98000
    },
    {
      "epoch": 7.684474385085383,
      "grad_norm": 4.7139434814453125,
      "learning_rate": 4.3596271345762185e-05,
      "loss": 1.8488,
      "step": 98100
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 4.999976634979248,
      "learning_rate": 4.358974358974359e-05,
      "loss": 1.8829,
      "step": 98200
    },
    {
      "epoch": 7.700140999530001,
      "grad_norm": 4.594523906707764,
      "learning_rate": 4.3583215833724997e-05,
      "loss": 1.8311,
      "step": 98300
    },
    {
      "epoch": 7.707974306752311,
      "grad_norm": 5.269774913787842,
      "learning_rate": 4.357668807770641e-05,
      "loss": 1.7747,
      "step": 98400
    },
    {
      "epoch": 7.71580761397462,
      "grad_norm": 4.423227310180664,
      "learning_rate": 4.357016032168782e-05,
      "loss": 1.9283,
      "step": 98500
    },
    {
      "epoch": 7.723640921196929,
      "grad_norm": 3.9966726303100586,
      "learning_rate": 4.356363256566923e-05,
      "loss": 1.8254,
      "step": 98600
    },
    {
      "epoch": 7.731474228419239,
      "grad_norm": 6.267873764038086,
      "learning_rate": 4.355710480965063e-05,
      "loss": 1.8333,
      "step": 98700
    },
    {
      "epoch": 7.739307535641548,
      "grad_norm": 5.418694496154785,
      "learning_rate": 4.3550577053632046e-05,
      "loss": 1.8821,
      "step": 98800
    },
    {
      "epoch": 7.747140842863857,
      "grad_norm": 4.874550819396973,
      "learning_rate": 4.354404929761345e-05,
      "loss": 1.8335,
      "step": 98900
    },
    {
      "epoch": 7.754974150086166,
      "grad_norm": 4.624449253082275,
      "learning_rate": 4.3537521541594864e-05,
      "loss": 1.8599,
      "step": 99000
    },
    {
      "epoch": 7.762807457308476,
      "grad_norm": 6.050155162811279,
      "learning_rate": 4.3530993785576277e-05,
      "loss": 1.905,
      "step": 99100
    },
    {
      "epoch": 7.770640764530785,
      "grad_norm": 4.847387790679932,
      "learning_rate": 4.352446602955768e-05,
      "loss": 1.9604,
      "step": 99200
    },
    {
      "epoch": 7.778474071753094,
      "grad_norm": 7.587813377380371,
      "learning_rate": 4.351793827353909e-05,
      "loss": 1.9191,
      "step": 99300
    },
    {
      "epoch": 7.786307378975403,
      "grad_norm": 5.18441915512085,
      "learning_rate": 4.35114105175205e-05,
      "loss": 1.8135,
      "step": 99400
    },
    {
      "epoch": 7.7941406861977125,
      "grad_norm": 3.5489132404327393,
      "learning_rate": 4.3504882761501906e-05,
      "loss": 1.8632,
      "step": 99500
    },
    {
      "epoch": 7.801973993420022,
      "grad_norm": 4.949942111968994,
      "learning_rate": 4.349835500548331e-05,
      "loss": 1.8876,
      "step": 99600
    },
    {
      "epoch": 7.809807300642332,
      "grad_norm": 4.723047256469727,
      "learning_rate": 4.349182724946473e-05,
      "loss": 1.8791,
      "step": 99700
    },
    {
      "epoch": 7.81764060786464,
      "grad_norm": 6.019229412078857,
      "learning_rate": 4.348529949344614e-05,
      "loss": 1.8996,
      "step": 99800
    },
    {
      "epoch": 7.82547391508695,
      "grad_norm": 6.156996726989746,
      "learning_rate": 4.347877173742754e-05,
      "loss": 1.7937,
      "step": 99900
    },
    {
      "epoch": 7.833307222309259,
      "grad_norm": 4.7639479637146,
      "learning_rate": 4.3472243981408956e-05,
      "loss": 1.9373,
      "step": 100000
    },
    {
      "epoch": 7.841140529531568,
      "grad_norm": 4.921067714691162,
      "learning_rate": 4.346571622539036e-05,
      "loss": 1.8837,
      "step": 100100
    },
    {
      "epoch": 7.848973836753878,
      "grad_norm": 3.8996567726135254,
      "learning_rate": 4.345918846937177e-05,
      "loss": 1.9278,
      "step": 100200
    },
    {
      "epoch": 7.856807143976186,
      "grad_norm": 6.562733173370361,
      "learning_rate": 4.345266071335318e-05,
      "loss": 1.8012,
      "step": 100300
    },
    {
      "epoch": 7.864640451198496,
      "grad_norm": 4.8470778465271,
      "learning_rate": 4.344613295733459e-05,
      "loss": 1.9085,
      "step": 100400
    },
    {
      "epoch": 7.8724737584208055,
      "grad_norm": 5.996266841888428,
      "learning_rate": 4.3439605201316e-05,
      "loss": 1.8707,
      "step": 100500
    },
    {
      "epoch": 7.880307065643114,
      "grad_norm": 4.8102641105651855,
      "learning_rate": 4.3433077445297404e-05,
      "loss": 1.9247,
      "step": 100600
    },
    {
      "epoch": 7.888140372865424,
      "grad_norm": 4.944815158843994,
      "learning_rate": 4.3426549689278816e-05,
      "loss": 1.7684,
      "step": 100700
    },
    {
      "epoch": 7.895973680087733,
      "grad_norm": 5.340024948120117,
      "learning_rate": 4.342002193326022e-05,
      "loss": 1.9078,
      "step": 100800
    },
    {
      "epoch": 7.903806987310042,
      "grad_norm": 7.194905757904053,
      "learning_rate": 4.3413494177241635e-05,
      "loss": 1.9172,
      "step": 100900
    },
    {
      "epoch": 7.911640294532352,
      "grad_norm": 3.897456645965576,
      "learning_rate": 4.340696642122305e-05,
      "loss": 1.9765,
      "step": 101000
    },
    {
      "epoch": 7.919473601754661,
      "grad_norm": 4.6745147705078125,
      "learning_rate": 4.340043866520445e-05,
      "loss": 1.782,
      "step": 101100
    },
    {
      "epoch": 7.92730690897697,
      "grad_norm": 6.08458137512207,
      "learning_rate": 4.339391090918586e-05,
      "loss": 1.7858,
      "step": 101200
    },
    {
      "epoch": 7.9351402161992795,
      "grad_norm": 5.9355292320251465,
      "learning_rate": 4.338738315316727e-05,
      "loss": 1.8111,
      "step": 101300
    },
    {
      "epoch": 7.942973523421589,
      "grad_norm": 3.794377326965332,
      "learning_rate": 4.338085539714868e-05,
      "loss": 1.852,
      "step": 101400
    },
    {
      "epoch": 7.950806830643898,
      "grad_norm": 4.842844009399414,
      "learning_rate": 4.337432764113008e-05,
      "loss": 1.9099,
      "step": 101500
    },
    {
      "epoch": 7.958640137866207,
      "grad_norm": 4.5869951248168945,
      "learning_rate": 4.3367799885111495e-05,
      "loss": 1.9057,
      "step": 101600
    },
    {
      "epoch": 7.966473445088516,
      "grad_norm": 6.069438934326172,
      "learning_rate": 4.336127212909291e-05,
      "loss": 1.8576,
      "step": 101700
    },
    {
      "epoch": 7.9743067523108255,
      "grad_norm": 5.4702348709106445,
      "learning_rate": 4.3354744373074314e-05,
      "loss": 1.9353,
      "step": 101800
    },
    {
      "epoch": 7.982140059533135,
      "grad_norm": 4.635383605957031,
      "learning_rate": 4.334821661705572e-05,
      "loss": 1.9123,
      "step": 101900
    },
    {
      "epoch": 7.989973366755444,
      "grad_norm": 4.750859260559082,
      "learning_rate": 4.334168886103713e-05,
      "loss": 1.9203,
      "step": 102000
    },
    {
      "epoch": 7.997806673977753,
      "grad_norm": 5.247661113739014,
      "learning_rate": 4.333516110501854e-05,
      "loss": 1.7964,
      "step": 102100
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8454644680023193,
      "eval_runtime": 1.4231,
      "eval_samples_per_second": 472.218,
      "eval_steps_per_second": 472.218,
      "step": 102128
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6358996629714966,
      "eval_runtime": 27.1756,
      "eval_samples_per_second": 469.759,
      "eval_steps_per_second": 469.759,
      "step": 102128
    },
    {
      "epoch": 8.005639981200062,
      "grad_norm": 4.77195930480957,
      "learning_rate": 4.332863334899995e-05,
      "loss": 1.8489,
      "step": 102200
    },
    {
      "epoch": 8.013473288422372,
      "grad_norm": 13.42249870300293,
      "learning_rate": 4.332210559298136e-05,
      "loss": 1.8254,
      "step": 102300
    },
    {
      "epoch": 8.021306595644681,
      "grad_norm": 6.091625213623047,
      "learning_rate": 4.331557783696277e-05,
      "loss": 1.8643,
      "step": 102400
    },
    {
      "epoch": 8.02913990286699,
      "grad_norm": 6.181840419769287,
      "learning_rate": 4.3309050080944174e-05,
      "loss": 1.8544,
      "step": 102500
    },
    {
      "epoch": 8.0369732100893,
      "grad_norm": 4.126466274261475,
      "learning_rate": 4.330252232492559e-05,
      "loss": 1.7649,
      "step": 102600
    },
    {
      "epoch": 8.044806517311608,
      "grad_norm": 7.92780876159668,
      "learning_rate": 4.329599456890699e-05,
      "loss": 1.921,
      "step": 102700
    },
    {
      "epoch": 8.052639824533918,
      "grad_norm": 5.012399196624756,
      "learning_rate": 4.32894668128884e-05,
      "loss": 1.8707,
      "step": 102800
    },
    {
      "epoch": 8.060473131756227,
      "grad_norm": 6.020811080932617,
      "learning_rate": 4.328293905686982e-05,
      "loss": 1.8582,
      "step": 102900
    },
    {
      "epoch": 8.068306438978537,
      "grad_norm": 5.009047508239746,
      "learning_rate": 4.3276411300851224e-05,
      "loss": 1.8798,
      "step": 103000
    },
    {
      "epoch": 8.076139746200846,
      "grad_norm": 4.938540935516357,
      "learning_rate": 4.326988354483263e-05,
      "loss": 1.874,
      "step": 103100
    },
    {
      "epoch": 8.083973053423156,
      "grad_norm": 6.089754104614258,
      "learning_rate": 4.326335578881404e-05,
      "loss": 1.7207,
      "step": 103200
    },
    {
      "epoch": 8.091806360645464,
      "grad_norm": 4.8066487312316895,
      "learning_rate": 4.325682803279545e-05,
      "loss": 1.838,
      "step": 103300
    },
    {
      "epoch": 8.099639667867773,
      "grad_norm": 5.29758358001709,
      "learning_rate": 4.325030027677685e-05,
      "loss": 1.7786,
      "step": 103400
    },
    {
      "epoch": 8.107472975090083,
      "grad_norm": 4.431085586547852,
      "learning_rate": 4.3243772520758266e-05,
      "loss": 1.9229,
      "step": 103500
    },
    {
      "epoch": 8.115306282312392,
      "grad_norm": 4.960023403167725,
      "learning_rate": 4.323724476473968e-05,
      "loss": 1.7855,
      "step": 103600
    },
    {
      "epoch": 8.123139589534702,
      "grad_norm": 6.230507850646973,
      "learning_rate": 4.3230717008721084e-05,
      "loss": 1.8893,
      "step": 103700
    },
    {
      "epoch": 8.130972896757012,
      "grad_norm": 4.578603267669678,
      "learning_rate": 4.322418925270249e-05,
      "loss": 1.8866,
      "step": 103800
    },
    {
      "epoch": 8.13880620397932,
      "grad_norm": 4.6527099609375,
      "learning_rate": 4.32176614966839e-05,
      "loss": 1.8201,
      "step": 103900
    },
    {
      "epoch": 8.146639511201629,
      "grad_norm": 5.434842586517334,
      "learning_rate": 4.321113374066531e-05,
      "loss": 1.8673,
      "step": 104000
    },
    {
      "epoch": 8.154472818423939,
      "grad_norm": 6.400641918182373,
      "learning_rate": 4.320460598464672e-05,
      "loss": 1.8085,
      "step": 104100
    },
    {
      "epoch": 8.162306125646248,
      "grad_norm": 6.200074195861816,
      "learning_rate": 4.3198078228628133e-05,
      "loss": 1.8509,
      "step": 104200
    },
    {
      "epoch": 8.170139432868558,
      "grad_norm": 4.566040992736816,
      "learning_rate": 4.319155047260954e-05,
      "loss": 1.8824,
      "step": 104300
    },
    {
      "epoch": 8.177972740090865,
      "grad_norm": 5.145117282867432,
      "learning_rate": 4.3185022716590945e-05,
      "loss": 1.8234,
      "step": 104400
    },
    {
      "epoch": 8.185806047313175,
      "grad_norm": 5.089197158813477,
      "learning_rate": 4.317849496057236e-05,
      "loss": 1.7349,
      "step": 104500
    },
    {
      "epoch": 8.193639354535485,
      "grad_norm": 4.0673017501831055,
      "learning_rate": 4.317196720455376e-05,
      "loss": 1.7716,
      "step": 104600
    },
    {
      "epoch": 8.201472661757794,
      "grad_norm": 4.057898998260498,
      "learning_rate": 4.316543944853517e-05,
      "loss": 1.813,
      "step": 104700
    },
    {
      "epoch": 8.209305968980104,
      "grad_norm": 6.435952663421631,
      "learning_rate": 4.315891169251658e-05,
      "loss": 1.8639,
      "step": 104800
    },
    {
      "epoch": 8.217139276202413,
      "grad_norm": 4.72334623336792,
      "learning_rate": 4.3152383936497994e-05,
      "loss": 1.7741,
      "step": 104900
    },
    {
      "epoch": 8.224972583424721,
      "grad_norm": 6.119784355163574,
      "learning_rate": 4.31458561804794e-05,
      "loss": 1.7683,
      "step": 105000
    },
    {
      "epoch": 8.23280589064703,
      "grad_norm": 5.071747779846191,
      "learning_rate": 4.313932842446081e-05,
      "loss": 1.8996,
      "step": 105100
    },
    {
      "epoch": 8.24063919786934,
      "grad_norm": 4.020365238189697,
      "learning_rate": 4.313280066844222e-05,
      "loss": 1.8644,
      "step": 105200
    },
    {
      "epoch": 8.24847250509165,
      "grad_norm": 4.614313125610352,
      "learning_rate": 4.3126272912423624e-05,
      "loss": 1.84,
      "step": 105300
    },
    {
      "epoch": 8.25630581231396,
      "grad_norm": 5.145082473754883,
      "learning_rate": 4.3119745156405037e-05,
      "loss": 1.8692,
      "step": 105400
    },
    {
      "epoch": 8.264139119536269,
      "grad_norm": 4.421053409576416,
      "learning_rate": 4.311321740038645e-05,
      "loss": 1.8098,
      "step": 105500
    },
    {
      "epoch": 8.271972426758577,
      "grad_norm": 5.564326763153076,
      "learning_rate": 4.3106689644367855e-05,
      "loss": 1.8036,
      "step": 105600
    },
    {
      "epoch": 8.279805733980886,
      "grad_norm": 5.269363880157471,
      "learning_rate": 4.310016188834926e-05,
      "loss": 1.7949,
      "step": 105700
    },
    {
      "epoch": 8.287639041203196,
      "grad_norm": 4.419637203216553,
      "learning_rate": 4.309363413233067e-05,
      "loss": 1.7465,
      "step": 105800
    },
    {
      "epoch": 8.295472348425506,
      "grad_norm": 3.7312533855438232,
      "learning_rate": 4.308710637631208e-05,
      "loss": 1.7848,
      "step": 105900
    },
    {
      "epoch": 8.303305655647815,
      "grad_norm": 4.010397911071777,
      "learning_rate": 4.3080578620293485e-05,
      "loss": 1.9027,
      "step": 106000
    },
    {
      "epoch": 8.311138962870125,
      "grad_norm": 7.3138346672058105,
      "learning_rate": 4.3074050864274904e-05,
      "loss": 1.8864,
      "step": 106100
    },
    {
      "epoch": 8.318972270092432,
      "grad_norm": 6.5535430908203125,
      "learning_rate": 4.306752310825631e-05,
      "loss": 1.8165,
      "step": 106200
    },
    {
      "epoch": 8.326805577314742,
      "grad_norm": 5.436423301696777,
      "learning_rate": 4.3060995352237716e-05,
      "loss": 1.8347,
      "step": 106300
    },
    {
      "epoch": 8.334638884537052,
      "grad_norm": 5.369569778442383,
      "learning_rate": 4.305446759621913e-05,
      "loss": 1.7649,
      "step": 106400
    },
    {
      "epoch": 8.342472191759361,
      "grad_norm": 5.784290313720703,
      "learning_rate": 4.3047939840200534e-05,
      "loss": 1.8443,
      "step": 106500
    },
    {
      "epoch": 8.35030549898167,
      "grad_norm": 3.7602951526641846,
      "learning_rate": 4.304141208418194e-05,
      "loss": 1.8111,
      "step": 106600
    },
    {
      "epoch": 8.358138806203979,
      "grad_norm": 4.990518569946289,
      "learning_rate": 4.303488432816335e-05,
      "loss": 1.7736,
      "step": 106700
    },
    {
      "epoch": 8.365972113426288,
      "grad_norm": 5.936410903930664,
      "learning_rate": 4.3028356572144765e-05,
      "loss": 1.8614,
      "step": 106800
    },
    {
      "epoch": 8.373805420648598,
      "grad_norm": 6.040550708770752,
      "learning_rate": 4.302182881612617e-05,
      "loss": 1.916,
      "step": 106900
    },
    {
      "epoch": 8.381638727870907,
      "grad_norm": 4.74567985534668,
      "learning_rate": 4.3015301060107576e-05,
      "loss": 1.8365,
      "step": 107000
    },
    {
      "epoch": 8.389472035093217,
      "grad_norm": 4.630902290344238,
      "learning_rate": 4.300877330408899e-05,
      "loss": 1.8125,
      "step": 107100
    },
    {
      "epoch": 8.397305342315526,
      "grad_norm": 5.046821117401123,
      "learning_rate": 4.3002245548070395e-05,
      "loss": 1.8464,
      "step": 107200
    },
    {
      "epoch": 8.405138649537834,
      "grad_norm": 4.085670471191406,
      "learning_rate": 4.299571779205181e-05,
      "loss": 1.9021,
      "step": 107300
    },
    {
      "epoch": 8.412971956760144,
      "grad_norm": 5.515462875366211,
      "learning_rate": 4.298919003603322e-05,
      "loss": 1.8142,
      "step": 107400
    },
    {
      "epoch": 8.420805263982453,
      "grad_norm": 5.229372501373291,
      "learning_rate": 4.2982662280014625e-05,
      "loss": 1.7691,
      "step": 107500
    },
    {
      "epoch": 8.428638571204763,
      "grad_norm": 4.134882926940918,
      "learning_rate": 4.297613452399603e-05,
      "loss": 1.8104,
      "step": 107600
    },
    {
      "epoch": 8.436471878427072,
      "grad_norm": 5.227293968200684,
      "learning_rate": 4.2969606767977444e-05,
      "loss": 1.9712,
      "step": 107700
    },
    {
      "epoch": 8.44430518564938,
      "grad_norm": 4.260936260223389,
      "learning_rate": 4.296307901195885e-05,
      "loss": 1.9172,
      "step": 107800
    },
    {
      "epoch": 8.45213849287169,
      "grad_norm": 4.839412212371826,
      "learning_rate": 4.2956551255940255e-05,
      "loss": 1.8882,
      "step": 107900
    },
    {
      "epoch": 8.459971800094,
      "grad_norm": 6.925136566162109,
      "learning_rate": 4.295002349992167e-05,
      "loss": 1.7543,
      "step": 108000
    },
    {
      "epoch": 8.467805107316309,
      "grad_norm": 5.305649757385254,
      "learning_rate": 4.294349574390308e-05,
      "loss": 1.9497,
      "step": 108100
    },
    {
      "epoch": 8.475638414538619,
      "grad_norm": 4.109306335449219,
      "learning_rate": 4.2936967987884486e-05,
      "loss": 1.9273,
      "step": 108200
    },
    {
      "epoch": 8.483471721760928,
      "grad_norm": 7.058071613311768,
      "learning_rate": 4.29304402318659e-05,
      "loss": 1.749,
      "step": 108300
    },
    {
      "epoch": 8.491305028983236,
      "grad_norm": 6.961357593536377,
      "learning_rate": 4.2923912475847304e-05,
      "loss": 1.9168,
      "step": 108400
    },
    {
      "epoch": 8.499138336205545,
      "grad_norm": 4.373779773712158,
      "learning_rate": 4.291738471982871e-05,
      "loss": 1.801,
      "step": 108500
    },
    {
      "epoch": 8.506971643427855,
      "grad_norm": 5.244163513183594,
      "learning_rate": 4.291085696381012e-05,
      "loss": 1.8905,
      "step": 108600
    },
    {
      "epoch": 8.514804950650165,
      "grad_norm": 4.127632141113281,
      "learning_rate": 4.2904329207791535e-05,
      "loss": 1.8294,
      "step": 108700
    },
    {
      "epoch": 8.522638257872474,
      "grad_norm": 5.909824371337891,
      "learning_rate": 4.289780145177294e-05,
      "loss": 1.7002,
      "step": 108800
    },
    {
      "epoch": 8.530471565094784,
      "grad_norm": 4.983743190765381,
      "learning_rate": 4.289127369575435e-05,
      "loss": 1.8291,
      "step": 108900
    },
    {
      "epoch": 8.538304872317092,
      "grad_norm": 6.7876105308532715,
      "learning_rate": 4.288474593973576e-05,
      "loss": 1.8117,
      "step": 109000
    },
    {
      "epoch": 8.546138179539401,
      "grad_norm": 4.662162780761719,
      "learning_rate": 4.2878218183717165e-05,
      "loss": 1.8657,
      "step": 109100
    },
    {
      "epoch": 8.55397148676171,
      "grad_norm": 4.938812255859375,
      "learning_rate": 4.287169042769857e-05,
      "loss": 1.8643,
      "step": 109200
    },
    {
      "epoch": 8.56180479398402,
      "grad_norm": 6.24184513092041,
      "learning_rate": 4.286516267167999e-05,
      "loss": 1.8739,
      "step": 109300
    },
    {
      "epoch": 8.56963810120633,
      "grad_norm": 5.543754577636719,
      "learning_rate": 4.2858634915661396e-05,
      "loss": 1.8303,
      "step": 109400
    },
    {
      "epoch": 8.57747140842864,
      "grad_norm": 6.559927463531494,
      "learning_rate": 4.28521071596428e-05,
      "loss": 1.8174,
      "step": 109500
    },
    {
      "epoch": 8.585304715650947,
      "grad_norm": 4.350579261779785,
      "learning_rate": 4.2845579403624214e-05,
      "loss": 1.9471,
      "step": 109600
    },
    {
      "epoch": 8.593138022873257,
      "grad_norm": 4.175530433654785,
      "learning_rate": 4.283905164760562e-05,
      "loss": 1.8749,
      "step": 109700
    },
    {
      "epoch": 8.600971330095566,
      "grad_norm": 5.62775993347168,
      "learning_rate": 4.2832523891587026e-05,
      "loss": 1.8037,
      "step": 109800
    },
    {
      "epoch": 8.608804637317876,
      "grad_norm": 5.300795555114746,
      "learning_rate": 4.282599613556844e-05,
      "loss": 1.815,
      "step": 109900
    },
    {
      "epoch": 8.616637944540186,
      "grad_norm": 4.02878999710083,
      "learning_rate": 4.281946837954985e-05,
      "loss": 1.9248,
      "step": 110000
    },
    {
      "epoch": 8.624471251762493,
      "grad_norm": 4.290795803070068,
      "learning_rate": 4.281294062353126e-05,
      "loss": 1.8536,
      "step": 110100
    },
    {
      "epoch": 8.632304558984803,
      "grad_norm": 4.287947654724121,
      "learning_rate": 4.280641286751267e-05,
      "loss": 1.8487,
      "step": 110200
    },
    {
      "epoch": 8.640137866207112,
      "grad_norm": 7.200594902038574,
      "learning_rate": 4.2799885111494075e-05,
      "loss": 1.8341,
      "step": 110300
    },
    {
      "epoch": 8.647971173429422,
      "grad_norm": 4.652311325073242,
      "learning_rate": 4.279335735547548e-05,
      "loss": 1.8701,
      "step": 110400
    },
    {
      "epoch": 8.655804480651732,
      "grad_norm": 6.051876068115234,
      "learning_rate": 4.278682959945689e-05,
      "loss": 1.8843,
      "step": 110500
    },
    {
      "epoch": 8.663637787874041,
      "grad_norm": 4.989205360412598,
      "learning_rate": 4.2780301843438306e-05,
      "loss": 1.8816,
      "step": 110600
    },
    {
      "epoch": 8.671471095096349,
      "grad_norm": 5.321943283081055,
      "learning_rate": 4.277377408741971e-05,
      "loss": 1.8279,
      "step": 110700
    },
    {
      "epoch": 8.679304402318659,
      "grad_norm": 3.482396364212036,
      "learning_rate": 4.276724633140112e-05,
      "loss": 1.8349,
      "step": 110800
    },
    {
      "epoch": 8.687137709540968,
      "grad_norm": 5.373305797576904,
      "learning_rate": 4.276071857538253e-05,
      "loss": 1.8916,
      "step": 110900
    },
    {
      "epoch": 8.694971016763278,
      "grad_norm": 7.245319843292236,
      "learning_rate": 4.2754190819363936e-05,
      "loss": 1.828,
      "step": 111000
    },
    {
      "epoch": 8.702804323985587,
      "grad_norm": 4.183032035827637,
      "learning_rate": 4.274766306334534e-05,
      "loss": 1.8904,
      "step": 111100
    },
    {
      "epoch": 8.710637631207897,
      "grad_norm": 5.132030963897705,
      "learning_rate": 4.2741135307326754e-05,
      "loss": 1.8679,
      "step": 111200
    },
    {
      "epoch": 8.718470938430205,
      "grad_norm": 4.956594944000244,
      "learning_rate": 4.2734607551308167e-05,
      "loss": 1.8412,
      "step": 111300
    },
    {
      "epoch": 8.726304245652514,
      "grad_norm": 6.643080234527588,
      "learning_rate": 4.272807979528957e-05,
      "loss": 1.8922,
      "step": 111400
    },
    {
      "epoch": 8.734137552874824,
      "grad_norm": 5.400099754333496,
      "learning_rate": 4.2721552039270985e-05,
      "loss": 1.8671,
      "step": 111500
    },
    {
      "epoch": 8.741970860097133,
      "grad_norm": 6.244022846221924,
      "learning_rate": 4.271502428325239e-05,
      "loss": 1.8031,
      "step": 111600
    },
    {
      "epoch": 8.749804167319443,
      "grad_norm": 5.36941385269165,
      "learning_rate": 4.2708496527233796e-05,
      "loss": 1.7755,
      "step": 111700
    },
    {
      "epoch": 8.757637474541752,
      "grad_norm": 5.435319900512695,
      "learning_rate": 4.270196877121521e-05,
      "loss": 1.8524,
      "step": 111800
    },
    {
      "epoch": 8.76547078176406,
      "grad_norm": 4.77193546295166,
      "learning_rate": 4.269544101519662e-05,
      "loss": 1.9681,
      "step": 111900
    },
    {
      "epoch": 8.77330408898637,
      "grad_norm": 5.129307746887207,
      "learning_rate": 4.268891325917803e-05,
      "loss": 1.8778,
      "step": 112000
    },
    {
      "epoch": 8.78113739620868,
      "grad_norm": 2.4777417182922363,
      "learning_rate": 4.268238550315943e-05,
      "loss": 1.8505,
      "step": 112100
    },
    {
      "epoch": 8.788970703430989,
      "grad_norm": 4.083472728729248,
      "learning_rate": 4.2675857747140846e-05,
      "loss": 1.8671,
      "step": 112200
    },
    {
      "epoch": 8.796804010653299,
      "grad_norm": 8.298789978027344,
      "learning_rate": 4.266932999112225e-05,
      "loss": 1.9206,
      "step": 112300
    },
    {
      "epoch": 8.804637317875606,
      "grad_norm": 4.7663421630859375,
      "learning_rate": 4.266280223510366e-05,
      "loss": 1.8926,
      "step": 112400
    },
    {
      "epoch": 8.812470625097916,
      "grad_norm": 3.937570333480835,
      "learning_rate": 4.2656274479085076e-05,
      "loss": 1.8654,
      "step": 112500
    },
    {
      "epoch": 8.820303932320225,
      "grad_norm": 6.055078983306885,
      "learning_rate": 4.264974672306648e-05,
      "loss": 1.9034,
      "step": 112600
    },
    {
      "epoch": 8.828137239542535,
      "grad_norm": 4.219819068908691,
      "learning_rate": 4.264321896704789e-05,
      "loss": 1.8265,
      "step": 112700
    },
    {
      "epoch": 8.835970546764845,
      "grad_norm": 6.997762203216553,
      "learning_rate": 4.26366912110293e-05,
      "loss": 1.9207,
      "step": 112800
    },
    {
      "epoch": 8.843803853987154,
      "grad_norm": 4.579605579376221,
      "learning_rate": 4.2630163455010706e-05,
      "loss": 1.8679,
      "step": 112900
    },
    {
      "epoch": 8.851637161209462,
      "grad_norm": 5.055825233459473,
      "learning_rate": 4.262363569899211e-05,
      "loss": 1.8432,
      "step": 113000
    },
    {
      "epoch": 8.859470468431772,
      "grad_norm": 5.2589921951293945,
      "learning_rate": 4.2617107942973525e-05,
      "loss": 1.8419,
      "step": 113100
    },
    {
      "epoch": 8.867303775654081,
      "grad_norm": 4.466538906097412,
      "learning_rate": 4.261058018695494e-05,
      "loss": 1.906,
      "step": 113200
    },
    {
      "epoch": 8.87513708287639,
      "grad_norm": 3.9973413944244385,
      "learning_rate": 4.260405243093634e-05,
      "loss": 1.7985,
      "step": 113300
    },
    {
      "epoch": 8.8829703900987,
      "grad_norm": 5.870123386383057,
      "learning_rate": 4.2597524674917755e-05,
      "loss": 1.7875,
      "step": 113400
    },
    {
      "epoch": 8.890803697321008,
      "grad_norm": 4.7968058586120605,
      "learning_rate": 4.259099691889916e-05,
      "loss": 1.8268,
      "step": 113500
    },
    {
      "epoch": 8.898637004543318,
      "grad_norm": 4.917143821716309,
      "learning_rate": 4.258446916288057e-05,
      "loss": 1.8804,
      "step": 113600
    },
    {
      "epoch": 8.906470311765627,
      "grad_norm": 4.923303127288818,
      "learning_rate": 4.257794140686198e-05,
      "loss": 1.9548,
      "step": 113700
    },
    {
      "epoch": 8.914303618987937,
      "grad_norm": 5.200554847717285,
      "learning_rate": 4.257141365084339e-05,
      "loss": 1.7484,
      "step": 113800
    },
    {
      "epoch": 8.922136926210246,
      "grad_norm": 5.805744171142578,
      "learning_rate": 4.25648858948248e-05,
      "loss": 1.8545,
      "step": 113900
    },
    {
      "epoch": 8.929970233432556,
      "grad_norm": 5.804266929626465,
      "learning_rate": 4.2558358138806204e-05,
      "loss": 1.9131,
      "step": 114000
    },
    {
      "epoch": 8.937803540654864,
      "grad_norm": 6.640910625457764,
      "learning_rate": 4.2551830382787616e-05,
      "loss": 1.8189,
      "step": 114100
    },
    {
      "epoch": 8.945636847877173,
      "grad_norm": 4.846467971801758,
      "learning_rate": 4.254530262676902e-05,
      "loss": 1.8715,
      "step": 114200
    },
    {
      "epoch": 8.953470155099483,
      "grad_norm": 3.7274386882781982,
      "learning_rate": 4.253877487075043e-05,
      "loss": 1.7727,
      "step": 114300
    },
    {
      "epoch": 8.961303462321792,
      "grad_norm": 6.105944633483887,
      "learning_rate": 4.253224711473184e-05,
      "loss": 1.8677,
      "step": 114400
    },
    {
      "epoch": 8.969136769544102,
      "grad_norm": 5.0612664222717285,
      "learning_rate": 4.252571935871325e-05,
      "loss": 1.988,
      "step": 114500
    },
    {
      "epoch": 8.976970076766412,
      "grad_norm": 3.727548837661743,
      "learning_rate": 4.251919160269466e-05,
      "loss": 1.8204,
      "step": 114600
    },
    {
      "epoch": 8.98480338398872,
      "grad_norm": 5.175848484039307,
      "learning_rate": 4.251266384667607e-05,
      "loss": 1.8656,
      "step": 114700
    },
    {
      "epoch": 8.992636691211029,
      "grad_norm": 3.7586917877197266,
      "learning_rate": 4.250613609065748e-05,
      "loss": 1.8817,
      "step": 114800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8444123268127441,
      "eval_runtime": 1.4184,
      "eval_samples_per_second": 473.781,
      "eval_steps_per_second": 473.781,
      "step": 114894
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.624441385269165,
      "eval_runtime": 27.6889,
      "eval_samples_per_second": 461.051,
      "eval_steps_per_second": 461.051,
      "step": 114894
    },
    {
      "epoch": 9.000469998433339,
      "grad_norm": 5.2214531898498535,
      "learning_rate": 4.249960833463888e-05,
      "loss": 1.9007,
      "step": 114900
    },
    {
      "epoch": 9.008303305655648,
      "grad_norm": 4.398146629333496,
      "learning_rate": 4.2493080578620295e-05,
      "loss": 1.8164,
      "step": 115000
    },
    {
      "epoch": 9.016136612877958,
      "grad_norm": 4.934291362762451,
      "learning_rate": 4.248655282260171e-05,
      "loss": 1.882,
      "step": 115100
    },
    {
      "epoch": 9.023969920100267,
      "grad_norm": 4.567049980163574,
      "learning_rate": 4.2480025066583114e-05,
      "loss": 1.7643,
      "step": 115200
    },
    {
      "epoch": 9.031803227322575,
      "grad_norm": 3.8193273544311523,
      "learning_rate": 4.2473497310564526e-05,
      "loss": 1.7777,
      "step": 115300
    },
    {
      "epoch": 9.039636534544885,
      "grad_norm": 5.091353893280029,
      "learning_rate": 4.246696955454593e-05,
      "loss": 1.8385,
      "step": 115400
    },
    {
      "epoch": 9.047469841767194,
      "grad_norm": 4.652123928070068,
      "learning_rate": 4.246044179852734e-05,
      "loss": 1.8849,
      "step": 115500
    },
    {
      "epoch": 9.055303148989504,
      "grad_norm": 5.575620174407959,
      "learning_rate": 4.245391404250874e-05,
      "loss": 1.7658,
      "step": 115600
    },
    {
      "epoch": 9.063136456211813,
      "grad_norm": 5.094728946685791,
      "learning_rate": 4.244738628649016e-05,
      "loss": 1.8184,
      "step": 115700
    },
    {
      "epoch": 9.070969763434121,
      "grad_norm": 5.398164749145508,
      "learning_rate": 4.244085853047157e-05,
      "loss": 1.8092,
      "step": 115800
    },
    {
      "epoch": 9.07880307065643,
      "grad_norm": 3.721482038497925,
      "learning_rate": 4.2434330774452974e-05,
      "loss": 1.7339,
      "step": 115900
    },
    {
      "epoch": 9.08663637787874,
      "grad_norm": 5.366099834442139,
      "learning_rate": 4.242780301843439e-05,
      "loss": 1.8055,
      "step": 116000
    },
    {
      "epoch": 9.09446968510105,
      "grad_norm": 6.641279697418213,
      "learning_rate": 4.242127526241579e-05,
      "loss": 1.8362,
      "step": 116100
    },
    {
      "epoch": 9.10230299232336,
      "grad_norm": 4.8624114990234375,
      "learning_rate": 4.24147475063972e-05,
      "loss": 1.8044,
      "step": 116200
    },
    {
      "epoch": 9.110136299545669,
      "grad_norm": 4.420355796813965,
      "learning_rate": 4.240821975037861e-05,
      "loss": 1.7355,
      "step": 116300
    },
    {
      "epoch": 9.117969606767977,
      "grad_norm": 3.8168303966522217,
      "learning_rate": 4.2401691994360023e-05,
      "loss": 1.8376,
      "step": 116400
    },
    {
      "epoch": 9.125802913990286,
      "grad_norm": 4.497209072113037,
      "learning_rate": 4.239516423834143e-05,
      "loss": 1.7395,
      "step": 116500
    },
    {
      "epoch": 9.133636221212596,
      "grad_norm": 5.052112579345703,
      "learning_rate": 4.238863648232284e-05,
      "loss": 1.7909,
      "step": 116600
    },
    {
      "epoch": 9.141469528434905,
      "grad_norm": 4.384313106536865,
      "learning_rate": 4.238210872630425e-05,
      "loss": 1.8306,
      "step": 116700
    },
    {
      "epoch": 9.149302835657215,
      "grad_norm": 5.787904739379883,
      "learning_rate": 4.237558097028565e-05,
      "loss": 1.7756,
      "step": 116800
    },
    {
      "epoch": 9.157136142879525,
      "grad_norm": 4.794641494750977,
      "learning_rate": 4.2369053214267066e-05,
      "loss": 1.7552,
      "step": 116900
    },
    {
      "epoch": 9.164969450101832,
      "grad_norm": 4.420494556427002,
      "learning_rate": 4.236252545824848e-05,
      "loss": 1.8496,
      "step": 117000
    },
    {
      "epoch": 9.172802757324142,
      "grad_norm": 4.016876697540283,
      "learning_rate": 4.2355997702229884e-05,
      "loss": 1.8497,
      "step": 117100
    },
    {
      "epoch": 9.180636064546452,
      "grad_norm": 5.543155670166016,
      "learning_rate": 4.234946994621129e-05,
      "loss": 1.8397,
      "step": 117200
    },
    {
      "epoch": 9.188469371768761,
      "grad_norm": 4.6086015701293945,
      "learning_rate": 4.23429421901927e-05,
      "loss": 1.8198,
      "step": 117300
    },
    {
      "epoch": 9.19630267899107,
      "grad_norm": 5.800994396209717,
      "learning_rate": 4.233641443417411e-05,
      "loss": 1.8735,
      "step": 117400
    },
    {
      "epoch": 9.204135986213378,
      "grad_norm": 5.780801773071289,
      "learning_rate": 4.2329886678155514e-05,
      "loss": 1.8468,
      "step": 117500
    },
    {
      "epoch": 9.211969293435688,
      "grad_norm": 4.961777687072754,
      "learning_rate": 4.2323358922136926e-05,
      "loss": 1.8403,
      "step": 117600
    },
    {
      "epoch": 9.219802600657998,
      "grad_norm": 4.718387126922607,
      "learning_rate": 4.231683116611834e-05,
      "loss": 1.8734,
      "step": 117700
    },
    {
      "epoch": 9.227635907880307,
      "grad_norm": 8.9718656539917,
      "learning_rate": 4.2310303410099745e-05,
      "loss": 1.8698,
      "step": 117800
    },
    {
      "epoch": 9.235469215102617,
      "grad_norm": 5.127786159515381,
      "learning_rate": 4.230377565408116e-05,
      "loss": 1.9011,
      "step": 117900
    },
    {
      "epoch": 9.243302522324926,
      "grad_norm": 3.869992971420288,
      "learning_rate": 4.229724789806256e-05,
      "loss": 1.8159,
      "step": 118000
    },
    {
      "epoch": 9.251135829547234,
      "grad_norm": 6.6330366134643555,
      "learning_rate": 4.229072014204397e-05,
      "loss": 1.7892,
      "step": 118100
    },
    {
      "epoch": 9.258969136769544,
      "grad_norm": 8.731983184814453,
      "learning_rate": 4.228419238602538e-05,
      "loss": 1.7474,
      "step": 118200
    },
    {
      "epoch": 9.266802443991853,
      "grad_norm": 4.04683780670166,
      "learning_rate": 4.2277664630006794e-05,
      "loss": 1.768,
      "step": 118300
    },
    {
      "epoch": 9.274635751214163,
      "grad_norm": 7.077352523803711,
      "learning_rate": 4.22711368739882e-05,
      "loss": 1.8428,
      "step": 118400
    },
    {
      "epoch": 9.282469058436472,
      "grad_norm": 4.793759346008301,
      "learning_rate": 4.226460911796961e-05,
      "loss": 1.845,
      "step": 118500
    },
    {
      "epoch": 9.290302365658782,
      "grad_norm": 4.951678276062012,
      "learning_rate": 4.225808136195102e-05,
      "loss": 1.8753,
      "step": 118600
    },
    {
      "epoch": 9.29813567288109,
      "grad_norm": 6.0950164794921875,
      "learning_rate": 4.2251553605932424e-05,
      "loss": 1.8059,
      "step": 118700
    },
    {
      "epoch": 9.3059689801034,
      "grad_norm": 5.920011043548584,
      "learning_rate": 4.224502584991383e-05,
      "loss": 1.9386,
      "step": 118800
    },
    {
      "epoch": 9.313802287325709,
      "grad_norm": 6.461268901824951,
      "learning_rate": 4.223849809389525e-05,
      "loss": 1.7389,
      "step": 118900
    },
    {
      "epoch": 9.321635594548018,
      "grad_norm": 6.266295433044434,
      "learning_rate": 4.2231970337876655e-05,
      "loss": 1.7544,
      "step": 119000
    },
    {
      "epoch": 9.329468901770328,
      "grad_norm": 5.948571681976318,
      "learning_rate": 4.222544258185806e-05,
      "loss": 1.862,
      "step": 119100
    },
    {
      "epoch": 9.337302208992636,
      "grad_norm": 4.5383172035217285,
      "learning_rate": 4.221891482583947e-05,
      "loss": 1.8516,
      "step": 119200
    },
    {
      "epoch": 9.345135516214945,
      "grad_norm": 5.92747163772583,
      "learning_rate": 4.221238706982088e-05,
      "loss": 1.8685,
      "step": 119300
    },
    {
      "epoch": 9.352968823437255,
      "grad_norm": 5.127749919891357,
      "learning_rate": 4.2205859313802285e-05,
      "loss": 1.764,
      "step": 119400
    },
    {
      "epoch": 9.360802130659565,
      "grad_norm": 4.420724868774414,
      "learning_rate": 4.21993315577837e-05,
      "loss": 1.8792,
      "step": 119500
    },
    {
      "epoch": 9.368635437881874,
      "grad_norm": 4.457455635070801,
      "learning_rate": 4.219280380176511e-05,
      "loss": 1.9708,
      "step": 119600
    },
    {
      "epoch": 9.376468745104184,
      "grad_norm": 6.21583890914917,
      "learning_rate": 4.2186276045746515e-05,
      "loss": 1.8174,
      "step": 119700
    },
    {
      "epoch": 9.384302052326492,
      "grad_norm": 4.394556999206543,
      "learning_rate": 4.217974828972793e-05,
      "loss": 1.7092,
      "step": 119800
    },
    {
      "epoch": 9.392135359548801,
      "grad_norm": 5.562828063964844,
      "learning_rate": 4.2173220533709334e-05,
      "loss": 1.801,
      "step": 119900
    },
    {
      "epoch": 9.39996866677111,
      "grad_norm": 5.573365688323975,
      "learning_rate": 4.216669277769074e-05,
      "loss": 1.7692,
      "step": 120000
    },
    {
      "epoch": 9.40780197399342,
      "grad_norm": 4.58308219909668,
      "learning_rate": 4.216016502167215e-05,
      "loss": 1.8912,
      "step": 120100
    },
    {
      "epoch": 9.41563528121573,
      "grad_norm": 5.246338844299316,
      "learning_rate": 4.2153637265653565e-05,
      "loss": 1.8913,
      "step": 120200
    },
    {
      "epoch": 9.42346858843804,
      "grad_norm": 7.556779384613037,
      "learning_rate": 4.214710950963497e-05,
      "loss": 1.7605,
      "step": 120300
    },
    {
      "epoch": 9.431301895660347,
      "grad_norm": 4.603209972381592,
      "learning_rate": 4.214058175361638e-05,
      "loss": 1.886,
      "step": 120400
    },
    {
      "epoch": 9.439135202882657,
      "grad_norm": 5.857675552368164,
      "learning_rate": 4.213405399759779e-05,
      "loss": 1.8079,
      "step": 120500
    },
    {
      "epoch": 9.446968510104966,
      "grad_norm": 5.350635528564453,
      "learning_rate": 4.2127526241579194e-05,
      "loss": 1.8146,
      "step": 120600
    },
    {
      "epoch": 9.454801817327276,
      "grad_norm": 5.048267364501953,
      "learning_rate": 4.21209984855606e-05,
      "loss": 1.9508,
      "step": 120700
    },
    {
      "epoch": 9.462635124549585,
      "grad_norm": 3.629134178161621,
      "learning_rate": 4.211447072954201e-05,
      "loss": 1.8196,
      "step": 120800
    },
    {
      "epoch": 9.470468431771893,
      "grad_norm": 4.243254661560059,
      "learning_rate": 4.2107942973523425e-05,
      "loss": 1.8717,
      "step": 120900
    },
    {
      "epoch": 9.478301738994203,
      "grad_norm": 4.468811511993408,
      "learning_rate": 4.210141521750483e-05,
      "loss": 1.8758,
      "step": 121000
    },
    {
      "epoch": 9.486135046216512,
      "grad_norm": 4.758539199829102,
      "learning_rate": 4.2094887461486244e-05,
      "loss": 1.9,
      "step": 121100
    },
    {
      "epoch": 9.493968353438822,
      "grad_norm": 4.1851115226745605,
      "learning_rate": 4.208835970546765e-05,
      "loss": 1.9063,
      "step": 121200
    },
    {
      "epoch": 9.501801660661132,
      "grad_norm": 5.849780559539795,
      "learning_rate": 4.2081831949449055e-05,
      "loss": 1.8112,
      "step": 121300
    },
    {
      "epoch": 9.509634967883441,
      "grad_norm": 4.7482380867004395,
      "learning_rate": 4.207530419343047e-05,
      "loss": 1.9289,
      "step": 121400
    },
    {
      "epoch": 9.517468275105749,
      "grad_norm": 5.662220001220703,
      "learning_rate": 4.206877643741188e-05,
      "loss": 1.8237,
      "step": 121500
    },
    {
      "epoch": 9.525301582328058,
      "grad_norm": 5.508352756500244,
      "learning_rate": 4.2062248681393286e-05,
      "loss": 1.7841,
      "step": 121600
    },
    {
      "epoch": 9.533134889550368,
      "grad_norm": 4.872033596038818,
      "learning_rate": 4.20557209253747e-05,
      "loss": 1.861,
      "step": 121700
    },
    {
      "epoch": 9.540968196772678,
      "grad_norm": 5.268369674682617,
      "learning_rate": 4.2049193169356104e-05,
      "loss": 1.8657,
      "step": 121800
    },
    {
      "epoch": 9.548801503994987,
      "grad_norm": 6.42230224609375,
      "learning_rate": 4.204266541333751e-05,
      "loss": 1.8683,
      "step": 121900
    },
    {
      "epoch": 9.556634811217297,
      "grad_norm": 5.267703533172607,
      "learning_rate": 4.203613765731892e-05,
      "loss": 1.8884,
      "step": 122000
    },
    {
      "epoch": 9.564468118439605,
      "grad_norm": 6.76984977722168,
      "learning_rate": 4.2029609901300335e-05,
      "loss": 1.9702,
      "step": 122100
    },
    {
      "epoch": 9.572301425661914,
      "grad_norm": 7.5139851570129395,
      "learning_rate": 4.202308214528174e-05,
      "loss": 1.8572,
      "step": 122200
    },
    {
      "epoch": 9.580134732884224,
      "grad_norm": 4.415567874908447,
      "learning_rate": 4.201655438926315e-05,
      "loss": 1.8164,
      "step": 122300
    },
    {
      "epoch": 9.587968040106533,
      "grad_norm": 5.634459972381592,
      "learning_rate": 4.201002663324456e-05,
      "loss": 1.8404,
      "step": 122400
    },
    {
      "epoch": 9.595801347328843,
      "grad_norm": 4.407571792602539,
      "learning_rate": 4.2003498877225965e-05,
      "loss": 1.8024,
      "step": 122500
    },
    {
      "epoch": 9.60363465455115,
      "grad_norm": 7.915002346038818,
      "learning_rate": 4.199697112120737e-05,
      "loss": 1.8418,
      "step": 122600
    },
    {
      "epoch": 9.61146796177346,
      "grad_norm": 5.313738822937012,
      "learning_rate": 4.199044336518878e-05,
      "loss": 1.8931,
      "step": 122700
    },
    {
      "epoch": 9.61930126899577,
      "grad_norm": 4.245553016662598,
      "learning_rate": 4.1983915609170196e-05,
      "loss": 1.7789,
      "step": 122800
    },
    {
      "epoch": 9.62713457621808,
      "grad_norm": 5.222876071929932,
      "learning_rate": 4.19773878531516e-05,
      "loss": 1.8132,
      "step": 122900
    },
    {
      "epoch": 9.634967883440389,
      "grad_norm": 4.196323871612549,
      "learning_rate": 4.1970860097133014e-05,
      "loss": 1.9617,
      "step": 123000
    },
    {
      "epoch": 9.642801190662698,
      "grad_norm": 5.9204230308532715,
      "learning_rate": 4.196433234111442e-05,
      "loss": 1.9682,
      "step": 123100
    },
    {
      "epoch": 9.650634497885006,
      "grad_norm": 5.641167163848877,
      "learning_rate": 4.1957804585095826e-05,
      "loss": 1.8764,
      "step": 123200
    },
    {
      "epoch": 9.658467805107316,
      "grad_norm": 4.699087619781494,
      "learning_rate": 4.195127682907724e-05,
      "loss": 1.8312,
      "step": 123300
    },
    {
      "epoch": 9.666301112329625,
      "grad_norm": 5.640275001525879,
      "learning_rate": 4.194474907305865e-05,
      "loss": 1.8761,
      "step": 123400
    },
    {
      "epoch": 9.674134419551935,
      "grad_norm": 5.064510345458984,
      "learning_rate": 4.1938221317040057e-05,
      "loss": 1.7832,
      "step": 123500
    },
    {
      "epoch": 9.681967726774245,
      "grad_norm": 6.452486038208008,
      "learning_rate": 4.193169356102147e-05,
      "loss": 1.7533,
      "step": 123600
    },
    {
      "epoch": 9.689801033996554,
      "grad_norm": 4.861899375915527,
      "learning_rate": 4.1925165805002875e-05,
      "loss": 1.8888,
      "step": 123700
    },
    {
      "epoch": 9.697634341218862,
      "grad_norm": 5.119953155517578,
      "learning_rate": 4.191863804898428e-05,
      "loss": 1.7905,
      "step": 123800
    },
    {
      "epoch": 9.705467648441171,
      "grad_norm": 6.686440467834473,
      "learning_rate": 4.1912110292965686e-05,
      "loss": 1.8554,
      "step": 123900
    },
    {
      "epoch": 9.713300955663481,
      "grad_norm": 5.002430438995361,
      "learning_rate": 4.19055825369471e-05,
      "loss": 1.8106,
      "step": 124000
    },
    {
      "epoch": 9.72113426288579,
      "grad_norm": 4.427046298980713,
      "learning_rate": 4.189905478092851e-05,
      "loss": 1.8035,
      "step": 124100
    },
    {
      "epoch": 9.7289675701081,
      "grad_norm": 6.025758266448975,
      "learning_rate": 4.189252702490992e-05,
      "loss": 1.85,
      "step": 124200
    },
    {
      "epoch": 9.73680087733041,
      "grad_norm": 4.636564254760742,
      "learning_rate": 4.188599926889133e-05,
      "loss": 1.8641,
      "step": 124300
    },
    {
      "epoch": 9.744634184552718,
      "grad_norm": 5.298236846923828,
      "learning_rate": 4.1879471512872736e-05,
      "loss": 1.8192,
      "step": 124400
    },
    {
      "epoch": 9.752467491775027,
      "grad_norm": 3.8266050815582275,
      "learning_rate": 4.187294375685414e-05,
      "loss": 1.7969,
      "step": 124500
    },
    {
      "epoch": 9.760300798997337,
      "grad_norm": 4.641727447509766,
      "learning_rate": 4.1866416000835554e-05,
      "loss": 1.7963,
      "step": 124600
    },
    {
      "epoch": 9.768134106219646,
      "grad_norm": 5.348908424377441,
      "learning_rate": 4.1859888244816966e-05,
      "loss": 1.8138,
      "step": 124700
    },
    {
      "epoch": 9.775967413441956,
      "grad_norm": 4.851718425750732,
      "learning_rate": 4.185336048879837e-05,
      "loss": 1.8047,
      "step": 124800
    },
    {
      "epoch": 9.783800720664264,
      "grad_norm": 7.169709205627441,
      "learning_rate": 4.1846832732779785e-05,
      "loss": 1.8632,
      "step": 124900
    },
    {
      "epoch": 9.791634027886573,
      "grad_norm": 3.469620943069458,
      "learning_rate": 4.184030497676119e-05,
      "loss": 1.7823,
      "step": 125000
    },
    {
      "epoch": 9.799467335108883,
      "grad_norm": 3.702895164489746,
      "learning_rate": 4.1833777220742596e-05,
      "loss": 1.8354,
      "step": 125100
    },
    {
      "epoch": 9.807300642331192,
      "grad_norm": 6.285669326782227,
      "learning_rate": 4.182724946472401e-05,
      "loss": 1.8453,
      "step": 125200
    },
    {
      "epoch": 9.815133949553502,
      "grad_norm": 5.0523247718811035,
      "learning_rate": 4.182072170870542e-05,
      "loss": 1.848,
      "step": 125300
    },
    {
      "epoch": 9.822967256775812,
      "grad_norm": 5.000265598297119,
      "learning_rate": 4.181419395268683e-05,
      "loss": 1.8256,
      "step": 125400
    },
    {
      "epoch": 9.83080056399812,
      "grad_norm": 4.963984489440918,
      "learning_rate": 4.180766619666823e-05,
      "loss": 1.8265,
      "step": 125500
    },
    {
      "epoch": 9.838633871220429,
      "grad_norm": 5.138875484466553,
      "learning_rate": 4.1801138440649645e-05,
      "loss": 1.8448,
      "step": 125600
    },
    {
      "epoch": 9.846467178442738,
      "grad_norm": 7.430324077606201,
      "learning_rate": 4.179461068463105e-05,
      "loss": 1.8316,
      "step": 125700
    },
    {
      "epoch": 9.854300485665048,
      "grad_norm": 5.509220600128174,
      "learning_rate": 4.178808292861246e-05,
      "loss": 1.8708,
      "step": 125800
    },
    {
      "epoch": 9.862133792887358,
      "grad_norm": 4.1423468589782715,
      "learning_rate": 4.178155517259387e-05,
      "loss": 1.9515,
      "step": 125900
    },
    {
      "epoch": 9.869967100109665,
      "grad_norm": 4.366488456726074,
      "learning_rate": 4.177502741657528e-05,
      "loss": 1.8961,
      "step": 126000
    },
    {
      "epoch": 9.877800407331975,
      "grad_norm": 6.915213108062744,
      "learning_rate": 4.176849966055669e-05,
      "loss": 1.8391,
      "step": 126100
    },
    {
      "epoch": 9.885633714554285,
      "grad_norm": 5.490963935852051,
      "learning_rate": 4.17619719045381e-05,
      "loss": 1.8502,
      "step": 126200
    },
    {
      "epoch": 9.893467021776594,
      "grad_norm": 4.212085247039795,
      "learning_rate": 4.1755444148519506e-05,
      "loss": 1.8795,
      "step": 126300
    },
    {
      "epoch": 9.901300328998904,
      "grad_norm": 4.1461710929870605,
      "learning_rate": 4.174891639250091e-05,
      "loss": 1.7414,
      "step": 126400
    },
    {
      "epoch": 9.909133636221213,
      "grad_norm": 5.238288879394531,
      "learning_rate": 4.1742388636482324e-05,
      "loss": 1.8924,
      "step": 126500
    },
    {
      "epoch": 9.916966943443521,
      "grad_norm": 6.263756275177002,
      "learning_rate": 4.173586088046374e-05,
      "loss": 1.8514,
      "step": 126600
    },
    {
      "epoch": 9.92480025066583,
      "grad_norm": 6.07481050491333,
      "learning_rate": 4.172933312444514e-05,
      "loss": 1.8031,
      "step": 126700
    },
    {
      "epoch": 9.93263355788814,
      "grad_norm": 7.825135231018066,
      "learning_rate": 4.1722805368426555e-05,
      "loss": 1.7747,
      "step": 126800
    },
    {
      "epoch": 9.94046686511045,
      "grad_norm": 5.424579620361328,
      "learning_rate": 4.171627761240796e-05,
      "loss": 1.8472,
      "step": 126900
    },
    {
      "epoch": 9.94830017233276,
      "grad_norm": 5.354381084442139,
      "learning_rate": 4.170974985638937e-05,
      "loss": 1.8125,
      "step": 127000
    },
    {
      "epoch": 9.956133479555069,
      "grad_norm": 6.476437091827393,
      "learning_rate": 4.170322210037078e-05,
      "loss": 1.8648,
      "step": 127100
    },
    {
      "epoch": 9.963966786777377,
      "grad_norm": 5.337865829467773,
      "learning_rate": 4.1696694344352185e-05,
      "loss": 1.8413,
      "step": 127200
    },
    {
      "epoch": 9.971800093999686,
      "grad_norm": 4.6442341804504395,
      "learning_rate": 4.16901665883336e-05,
      "loss": 1.8774,
      "step": 127300
    },
    {
      "epoch": 9.979633401221996,
      "grad_norm": 5.602654933929443,
      "learning_rate": 4.1683638832315004e-05,
      "loss": 1.7578,
      "step": 127400
    },
    {
      "epoch": 9.987466708444305,
      "grad_norm": 5.764124393463135,
      "learning_rate": 4.1677111076296416e-05,
      "loss": 1.8169,
      "step": 127500
    },
    {
      "epoch": 9.995300015666615,
      "grad_norm": 5.524801254272461,
      "learning_rate": 4.167058332027782e-05,
      "loss": 1.9211,
      "step": 127600
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8385710716247559,
      "eval_runtime": 1.4305,
      "eval_samples_per_second": 469.756,
      "eval_steps_per_second": 469.756,
      "step": 127660
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.6072973012924194,
      "eval_runtime": 27.631,
      "eval_samples_per_second": 462.018,
      "eval_steps_per_second": 462.018,
      "step": 127660
    },
    {
      "epoch": 10.003133322888925,
      "grad_norm": 4.287569522857666,
      "learning_rate": 4.166405556425923e-05,
      "loss": 1.8063,
      "step": 127700
    },
    {
      "epoch": 10.010966630111232,
      "grad_norm": 5.963932037353516,
      "learning_rate": 4.165752780824064e-05,
      "loss": 1.7677,
      "step": 127800
    },
    {
      "epoch": 10.018799937333542,
      "grad_norm": 5.160428047180176,
      "learning_rate": 4.165100005222205e-05,
      "loss": 1.761,
      "step": 127900
    },
    {
      "epoch": 10.026633244555851,
      "grad_norm": 5.077071666717529,
      "learning_rate": 4.164447229620346e-05,
      "loss": 1.8162,
      "step": 128000
    },
    {
      "epoch": 10.034466551778161,
      "grad_norm": 4.7662577629089355,
      "learning_rate": 4.163794454018487e-05,
      "loss": 1.7327,
      "step": 128100
    },
    {
      "epoch": 10.04229985900047,
      "grad_norm": 6.177248001098633,
      "learning_rate": 4.163141678416628e-05,
      "loss": 1.8401,
      "step": 128200
    },
    {
      "epoch": 10.050133166222778,
      "grad_norm": 6.155420303344727,
      "learning_rate": 4.162488902814768e-05,
      "loss": 1.7829,
      "step": 128300
    },
    {
      "epoch": 10.057966473445088,
      "grad_norm": 5.208957195281982,
      "learning_rate": 4.1618361272129095e-05,
      "loss": 1.8465,
      "step": 128400
    },
    {
      "epoch": 10.065799780667398,
      "grad_norm": 5.868691921234131,
      "learning_rate": 4.161183351611051e-05,
      "loss": 1.7959,
      "step": 128500
    },
    {
      "epoch": 10.073633087889707,
      "grad_norm": 4.148695945739746,
      "learning_rate": 4.160530576009191e-05,
      "loss": 1.761,
      "step": 128600
    },
    {
      "epoch": 10.081466395112017,
      "grad_norm": 4.09648323059082,
      "learning_rate": 4.1598778004073326e-05,
      "loss": 1.7962,
      "step": 128700
    },
    {
      "epoch": 10.089299702334326,
      "grad_norm": 4.7675347328186035,
      "learning_rate": 4.159225024805473e-05,
      "loss": 1.7999,
      "step": 128800
    },
    {
      "epoch": 10.097133009556634,
      "grad_norm": 4.977889060974121,
      "learning_rate": 4.158572249203614e-05,
      "loss": 1.8679,
      "step": 128900
    },
    {
      "epoch": 10.104966316778944,
      "grad_norm": 3.41011381149292,
      "learning_rate": 4.157919473601754e-05,
      "loss": 1.9222,
      "step": 129000
    },
    {
      "epoch": 10.112799624001253,
      "grad_norm": 4.582361698150635,
      "learning_rate": 4.1572666979998956e-05,
      "loss": 1.8614,
      "step": 129100
    },
    {
      "epoch": 10.120632931223563,
      "grad_norm": 5.8608717918396,
      "learning_rate": 4.156613922398037e-05,
      "loss": 1.8643,
      "step": 129200
    },
    {
      "epoch": 10.128466238445872,
      "grad_norm": 3.6271378993988037,
      "learning_rate": 4.1559611467961774e-05,
      "loss": 1.7907,
      "step": 129300
    },
    {
      "epoch": 10.136299545668182,
      "grad_norm": 5.338126182556152,
      "learning_rate": 4.155308371194319e-05,
      "loss": 1.7228,
      "step": 129400
    },
    {
      "epoch": 10.14413285289049,
      "grad_norm": 4.693605422973633,
      "learning_rate": 4.154655595592459e-05,
      "loss": 1.8293,
      "step": 129500
    },
    {
      "epoch": 10.1519661601128,
      "grad_norm": 5.186053276062012,
      "learning_rate": 4.1540028199906e-05,
      "loss": 1.968,
      "step": 129600
    },
    {
      "epoch": 10.159799467335109,
      "grad_norm": 4.147787094116211,
      "learning_rate": 4.153350044388741e-05,
      "loss": 1.8016,
      "step": 129700
    },
    {
      "epoch": 10.167632774557418,
      "grad_norm": 4.111258029937744,
      "learning_rate": 4.152697268786882e-05,
      "loss": 1.8044,
      "step": 129800
    },
    {
      "epoch": 10.175466081779728,
      "grad_norm": 6.332160472869873,
      "learning_rate": 4.152044493185023e-05,
      "loss": 1.8373,
      "step": 129900
    },
    {
      "epoch": 10.183299389002036,
      "grad_norm": 4.723007678985596,
      "learning_rate": 4.151391717583164e-05,
      "loss": 1.841,
      "step": 130000
    },
    {
      "epoch": 10.191132696224345,
      "grad_norm": 6.08096170425415,
      "learning_rate": 4.150738941981305e-05,
      "loss": 1.7704,
      "step": 130100
    },
    {
      "epoch": 10.198966003446655,
      "grad_norm": 7.503873825073242,
      "learning_rate": 4.150086166379445e-05,
      "loss": 1.9086,
      "step": 130200
    },
    {
      "epoch": 10.206799310668965,
      "grad_norm": 5.534624099731445,
      "learning_rate": 4.1494333907775866e-05,
      "loss": 1.8632,
      "step": 130300
    },
    {
      "epoch": 10.214632617891274,
      "grad_norm": 5.090787410736084,
      "learning_rate": 4.148780615175727e-05,
      "loss": 1.9008,
      "step": 130400
    },
    {
      "epoch": 10.222465925113584,
      "grad_norm": 4.973941326141357,
      "learning_rate": 4.1481278395738684e-05,
      "loss": 1.7238,
      "step": 130500
    },
    {
      "epoch": 10.230299232335891,
      "grad_norm": 6.086690425872803,
      "learning_rate": 4.147475063972009e-05,
      "loss": 1.7546,
      "step": 130600
    },
    {
      "epoch": 10.238132539558201,
      "grad_norm": 7.186936855316162,
      "learning_rate": 4.14682228837015e-05,
      "loss": 1.8422,
      "step": 130700
    },
    {
      "epoch": 10.24596584678051,
      "grad_norm": 5.38337516784668,
      "learning_rate": 4.146169512768291e-05,
      "loss": 1.8779,
      "step": 130800
    },
    {
      "epoch": 10.25379915400282,
      "grad_norm": 14.359004974365234,
      "learning_rate": 4.1455167371664314e-05,
      "loss": 1.7259,
      "step": 130900
    },
    {
      "epoch": 10.26163246122513,
      "grad_norm": 5.279439926147461,
      "learning_rate": 4.1448639615645726e-05,
      "loss": 1.7929,
      "step": 131000
    },
    {
      "epoch": 10.26946576844744,
      "grad_norm": 6.164882659912109,
      "learning_rate": 4.144211185962714e-05,
      "loss": 1.7622,
      "step": 131100
    },
    {
      "epoch": 10.277299075669747,
      "grad_norm": 4.962742805480957,
      "learning_rate": 4.1435584103608545e-05,
      "loss": 1.7973,
      "step": 131200
    },
    {
      "epoch": 10.285132382892057,
      "grad_norm": 5.564085483551025,
      "learning_rate": 4.142905634758996e-05,
      "loss": 1.8667,
      "step": 131300
    },
    {
      "epoch": 10.292965690114366,
      "grad_norm": 5.692348003387451,
      "learning_rate": 4.142252859157136e-05,
      "loss": 1.784,
      "step": 131400
    },
    {
      "epoch": 10.300798997336676,
      "grad_norm": 4.047954559326172,
      "learning_rate": 4.141600083555277e-05,
      "loss": 1.8102,
      "step": 131500
    },
    {
      "epoch": 10.308632304558985,
      "grad_norm": 5.601818561553955,
      "learning_rate": 4.140947307953418e-05,
      "loss": 1.7608,
      "step": 131600
    },
    {
      "epoch": 10.316465611781293,
      "grad_norm": 3.9485039710998535,
      "learning_rate": 4.1402945323515594e-05,
      "loss": 1.7991,
      "step": 131700
    },
    {
      "epoch": 10.324298919003603,
      "grad_norm": 4.590242862701416,
      "learning_rate": 4.1396417567497e-05,
      "loss": 1.8172,
      "step": 131800
    },
    {
      "epoch": 10.332132226225912,
      "grad_norm": 5.334256649017334,
      "learning_rate": 4.138988981147841e-05,
      "loss": 1.8555,
      "step": 131900
    },
    {
      "epoch": 10.339965533448222,
      "grad_norm": 6.152012348175049,
      "learning_rate": 4.138336205545982e-05,
      "loss": 1.8522,
      "step": 132000
    },
    {
      "epoch": 10.347798840670531,
      "grad_norm": 5.0727410316467285,
      "learning_rate": 4.1376834299441224e-05,
      "loss": 1.8193,
      "step": 132100
    },
    {
      "epoch": 10.355632147892841,
      "grad_norm": 5.257081985473633,
      "learning_rate": 4.1370306543422636e-05,
      "loss": 1.8232,
      "step": 132200
    },
    {
      "epoch": 10.363465455115149,
      "grad_norm": 5.867132186889648,
      "learning_rate": 4.136377878740404e-05,
      "loss": 1.8482,
      "step": 132300
    },
    {
      "epoch": 10.371298762337458,
      "grad_norm": 4.58188009262085,
      "learning_rate": 4.1357251031385455e-05,
      "loss": 1.8912,
      "step": 132400
    },
    {
      "epoch": 10.379132069559768,
      "grad_norm": 3.719834566116333,
      "learning_rate": 4.135072327536686e-05,
      "loss": 1.8622,
      "step": 132500
    },
    {
      "epoch": 10.386965376782078,
      "grad_norm": 7.930324554443359,
      "learning_rate": 4.134419551934827e-05,
      "loss": 1.8768,
      "step": 132600
    },
    {
      "epoch": 10.394798684004387,
      "grad_norm": 4.917719841003418,
      "learning_rate": 4.133766776332968e-05,
      "loss": 1.8463,
      "step": 132700
    },
    {
      "epoch": 10.402631991226697,
      "grad_norm": 3.96905779838562,
      "learning_rate": 4.1331140007311084e-05,
      "loss": 1.8553,
      "step": 132800
    },
    {
      "epoch": 10.410465298449004,
      "grad_norm": 4.6690144538879395,
      "learning_rate": 4.13246122512925e-05,
      "loss": 1.8943,
      "step": 132900
    },
    {
      "epoch": 10.418298605671314,
      "grad_norm": 5.057079315185547,
      "learning_rate": 4.131808449527391e-05,
      "loss": 1.7894,
      "step": 133000
    },
    {
      "epoch": 10.426131912893624,
      "grad_norm": 5.596561908721924,
      "learning_rate": 4.1311556739255315e-05,
      "loss": 1.7729,
      "step": 133100
    },
    {
      "epoch": 10.433965220115933,
      "grad_norm": 5.010034084320068,
      "learning_rate": 4.130502898323673e-05,
      "loss": 1.8343,
      "step": 133200
    },
    {
      "epoch": 10.441798527338243,
      "grad_norm": 5.367262840270996,
      "learning_rate": 4.1298501227218134e-05,
      "loss": 1.7715,
      "step": 133300
    },
    {
      "epoch": 10.449631834560552,
      "grad_norm": 5.810190200805664,
      "learning_rate": 4.129197347119954e-05,
      "loss": 1.853,
      "step": 133400
    },
    {
      "epoch": 10.45746514178286,
      "grad_norm": 4.996625900268555,
      "learning_rate": 4.128544571518095e-05,
      "loss": 1.811,
      "step": 133500
    },
    {
      "epoch": 10.46529844900517,
      "grad_norm": 4.262193202972412,
      "learning_rate": 4.127891795916236e-05,
      "loss": 1.8506,
      "step": 133600
    },
    {
      "epoch": 10.47313175622748,
      "grad_norm": 5.432993412017822,
      "learning_rate": 4.127239020314377e-05,
      "loss": 1.729,
      "step": 133700
    },
    {
      "epoch": 10.480965063449789,
      "grad_norm": 4.728026866912842,
      "learning_rate": 4.126586244712518e-05,
      "loss": 1.8859,
      "step": 133800
    },
    {
      "epoch": 10.488798370672098,
      "grad_norm": 7.048706531524658,
      "learning_rate": 4.125933469110659e-05,
      "loss": 1.8358,
      "step": 133900
    },
    {
      "epoch": 10.496631677894406,
      "grad_norm": 3.2759745121002197,
      "learning_rate": 4.1252806935087994e-05,
      "loss": 1.7683,
      "step": 134000
    },
    {
      "epoch": 10.504464985116716,
      "grad_norm": 5.239082336425781,
      "learning_rate": 4.12462791790694e-05,
      "loss": 1.9055,
      "step": 134100
    },
    {
      "epoch": 10.512298292339025,
      "grad_norm": 6.725254058837891,
      "learning_rate": 4.123975142305081e-05,
      "loss": 1.8792,
      "step": 134200
    },
    {
      "epoch": 10.520131599561335,
      "grad_norm": 5.425836086273193,
      "learning_rate": 4.1233223667032225e-05,
      "loss": 1.89,
      "step": 134300
    },
    {
      "epoch": 10.527964906783644,
      "grad_norm": 4.779348850250244,
      "learning_rate": 4.122669591101363e-05,
      "loss": 1.8215,
      "step": 134400
    },
    {
      "epoch": 10.535798214005954,
      "grad_norm": 6.599587917327881,
      "learning_rate": 4.1220168154995043e-05,
      "loss": 1.8677,
      "step": 134500
    },
    {
      "epoch": 10.543631521228262,
      "grad_norm": 5.399873733520508,
      "learning_rate": 4.121364039897645e-05,
      "loss": 1.878,
      "step": 134600
    },
    {
      "epoch": 10.551464828450571,
      "grad_norm": 5.171035289764404,
      "learning_rate": 4.1207112642957855e-05,
      "loss": 1.9191,
      "step": 134700
    },
    {
      "epoch": 10.559298135672881,
      "grad_norm": 4.477651119232178,
      "learning_rate": 4.120058488693927e-05,
      "loss": 1.7717,
      "step": 134800
    },
    {
      "epoch": 10.56713144289519,
      "grad_norm": 5.607550144195557,
      "learning_rate": 4.119405713092068e-05,
      "loss": 1.784,
      "step": 134900
    },
    {
      "epoch": 10.5749647501175,
      "grad_norm": 5.800075531005859,
      "learning_rate": 4.1187529374902086e-05,
      "loss": 1.8021,
      "step": 135000
    },
    {
      "epoch": 10.582798057339808,
      "grad_norm": 4.721376419067383,
      "learning_rate": 4.11810016188835e-05,
      "loss": 1.7512,
      "step": 135100
    },
    {
      "epoch": 10.590631364562118,
      "grad_norm": 4.690079689025879,
      "learning_rate": 4.1174473862864904e-05,
      "loss": 1.7697,
      "step": 135200
    },
    {
      "epoch": 10.598464671784427,
      "grad_norm": 6.981759071350098,
      "learning_rate": 4.116794610684631e-05,
      "loss": 1.8493,
      "step": 135300
    },
    {
      "epoch": 10.606297979006737,
      "grad_norm": 4.551659107208252,
      "learning_rate": 4.116141835082772e-05,
      "loss": 1.7711,
      "step": 135400
    },
    {
      "epoch": 10.614131286229046,
      "grad_norm": 5.849189281463623,
      "learning_rate": 4.115489059480913e-05,
      "loss": 1.7874,
      "step": 135500
    },
    {
      "epoch": 10.621964593451356,
      "grad_norm": 5.964055061340332,
      "learning_rate": 4.114836283879054e-05,
      "loss": 1.7899,
      "step": 135600
    },
    {
      "epoch": 10.629797900673664,
      "grad_norm": 4.510652542114258,
      "learning_rate": 4.1141835082771947e-05,
      "loss": 1.7849,
      "step": 135700
    },
    {
      "epoch": 10.637631207895973,
      "grad_norm": 4.693610191345215,
      "learning_rate": 4.113530732675336e-05,
      "loss": 1.8683,
      "step": 135800
    },
    {
      "epoch": 10.645464515118283,
      "grad_norm": 4.66658353805542,
      "learning_rate": 4.1128779570734765e-05,
      "loss": 1.7834,
      "step": 135900
    },
    {
      "epoch": 10.653297822340592,
      "grad_norm": 5.5294671058654785,
      "learning_rate": 4.112225181471617e-05,
      "loss": 1.8778,
      "step": 136000
    },
    {
      "epoch": 10.661131129562902,
      "grad_norm": 5.062653064727783,
      "learning_rate": 4.111572405869758e-05,
      "loss": 1.9072,
      "step": 136100
    },
    {
      "epoch": 10.668964436785211,
      "grad_norm": 5.178603649139404,
      "learning_rate": 4.1109196302678996e-05,
      "loss": 1.689,
      "step": 136200
    },
    {
      "epoch": 10.67679774400752,
      "grad_norm": 5.2949090003967285,
      "learning_rate": 4.11026685466604e-05,
      "loss": 1.7471,
      "step": 136300
    },
    {
      "epoch": 10.684631051229829,
      "grad_norm": 4.358661651611328,
      "learning_rate": 4.1096140790641814e-05,
      "loss": 1.782,
      "step": 136400
    },
    {
      "epoch": 10.692464358452138,
      "grad_norm": 3.2555885314941406,
      "learning_rate": 4.108961303462322e-05,
      "loss": 1.7842,
      "step": 136500
    },
    {
      "epoch": 10.700297665674448,
      "grad_norm": 2.837944984436035,
      "learning_rate": 4.1083085278604626e-05,
      "loss": 1.673,
      "step": 136600
    },
    {
      "epoch": 10.708130972896758,
      "grad_norm": 4.713308811187744,
      "learning_rate": 4.107655752258604e-05,
      "loss": 1.8889,
      "step": 136700
    },
    {
      "epoch": 10.715964280119067,
      "grad_norm": 5.797365665435791,
      "learning_rate": 4.1070029766567444e-05,
      "loss": 1.8281,
      "step": 136800
    },
    {
      "epoch": 10.723797587341375,
      "grad_norm": 5.597569465637207,
      "learning_rate": 4.1063502010548856e-05,
      "loss": 1.8638,
      "step": 136900
    },
    {
      "epoch": 10.731630894563684,
      "grad_norm": 3.5830862522125244,
      "learning_rate": 4.105697425453027e-05,
      "loss": 1.8956,
      "step": 137000
    },
    {
      "epoch": 10.739464201785994,
      "grad_norm": 4.958897590637207,
      "learning_rate": 4.1050446498511675e-05,
      "loss": 1.8221,
      "step": 137100
    },
    {
      "epoch": 10.747297509008304,
      "grad_norm": 6.142293453216553,
      "learning_rate": 4.104391874249308e-05,
      "loss": 1.8615,
      "step": 137200
    },
    {
      "epoch": 10.755130816230613,
      "grad_norm": 4.8987202644348145,
      "learning_rate": 4.1037390986474486e-05,
      "loss": 1.8079,
      "step": 137300
    },
    {
      "epoch": 10.762964123452921,
      "grad_norm": 4.600276947021484,
      "learning_rate": 4.10308632304559e-05,
      "loss": 1.8941,
      "step": 137400
    },
    {
      "epoch": 10.77079743067523,
      "grad_norm": 4.5073957443237305,
      "learning_rate": 4.102433547443731e-05,
      "loss": 1.909,
      "step": 137500
    },
    {
      "epoch": 10.77863073789754,
      "grad_norm": 5.403568744659424,
      "learning_rate": 4.101780771841872e-05,
      "loss": 1.8335,
      "step": 137600
    },
    {
      "epoch": 10.78646404511985,
      "grad_norm": 5.746227264404297,
      "learning_rate": 4.101127996240013e-05,
      "loss": 1.8534,
      "step": 137700
    },
    {
      "epoch": 10.79429735234216,
      "grad_norm": 6.429660797119141,
      "learning_rate": 4.1004752206381535e-05,
      "loss": 1.8694,
      "step": 137800
    },
    {
      "epoch": 10.802130659564469,
      "grad_norm": 4.920832633972168,
      "learning_rate": 4.099822445036294e-05,
      "loss": 1.7975,
      "step": 137900
    },
    {
      "epoch": 10.809963966786777,
      "grad_norm": 5.3856048583984375,
      "learning_rate": 4.0991696694344354e-05,
      "loss": 1.7591,
      "step": 138000
    },
    {
      "epoch": 10.817797274009086,
      "grad_norm": 6.068603038787842,
      "learning_rate": 4.0985168938325766e-05,
      "loss": 1.8692,
      "step": 138100
    },
    {
      "epoch": 10.825630581231396,
      "grad_norm": 3.699453115463257,
      "learning_rate": 4.097864118230717e-05,
      "loss": 1.7753,
      "step": 138200
    },
    {
      "epoch": 10.833463888453705,
      "grad_norm": 5.941880226135254,
      "learning_rate": 4.0972113426288585e-05,
      "loss": 1.8157,
      "step": 138300
    },
    {
      "epoch": 10.841297195676015,
      "grad_norm": 5.950603485107422,
      "learning_rate": 4.096558567026999e-05,
      "loss": 1.8987,
      "step": 138400
    },
    {
      "epoch": 10.849130502898324,
      "grad_norm": 4.1078972816467285,
      "learning_rate": 4.0959057914251396e-05,
      "loss": 1.78,
      "step": 138500
    },
    {
      "epoch": 10.856963810120632,
      "grad_norm": 5.712967872619629,
      "learning_rate": 4.095253015823281e-05,
      "loss": 1.872,
      "step": 138600
    },
    {
      "epoch": 10.864797117342942,
      "grad_norm": 6.265463829040527,
      "learning_rate": 4.0946002402214214e-05,
      "loss": 1.7719,
      "step": 138700
    },
    {
      "epoch": 10.872630424565251,
      "grad_norm": 4.513693332672119,
      "learning_rate": 4.093947464619563e-05,
      "loss": 1.7912,
      "step": 138800
    },
    {
      "epoch": 10.880463731787561,
      "grad_norm": 5.863320827484131,
      "learning_rate": 4.093294689017704e-05,
      "loss": 1.8105,
      "step": 138900
    },
    {
      "epoch": 10.88829703900987,
      "grad_norm": 5.096554756164551,
      "learning_rate": 4.0926419134158445e-05,
      "loss": 1.8332,
      "step": 139000
    },
    {
      "epoch": 10.89613034623218,
      "grad_norm": 6.600611209869385,
      "learning_rate": 4.091989137813985e-05,
      "loss": 1.8552,
      "step": 139100
    },
    {
      "epoch": 10.903963653454488,
      "grad_norm": 5.53299617767334,
      "learning_rate": 4.091336362212126e-05,
      "loss": 1.823,
      "step": 139200
    },
    {
      "epoch": 10.911796960676797,
      "grad_norm": 4.702469348907471,
      "learning_rate": 4.090683586610267e-05,
      "loss": 1.8612,
      "step": 139300
    },
    {
      "epoch": 10.919630267899107,
      "grad_norm": 3.649394989013672,
      "learning_rate": 4.090030811008408e-05,
      "loss": 1.8151,
      "step": 139400
    },
    {
      "epoch": 10.927463575121417,
      "grad_norm": 6.890254020690918,
      "learning_rate": 4.089378035406549e-05,
      "loss": 1.9097,
      "step": 139500
    },
    {
      "epoch": 10.935296882343726,
      "grad_norm": 5.574761867523193,
      "learning_rate": 4.08872525980469e-05,
      "loss": 1.8698,
      "step": 139600
    },
    {
      "epoch": 10.943130189566034,
      "grad_norm": 6.682387351989746,
      "learning_rate": 4.0880724842028306e-05,
      "loss": 1.7621,
      "step": 139700
    },
    {
      "epoch": 10.950963496788344,
      "grad_norm": 5.583367824554443,
      "learning_rate": 4.087419708600971e-05,
      "loss": 1.8397,
      "step": 139800
    },
    {
      "epoch": 10.958796804010653,
      "grad_norm": 4.749546051025391,
      "learning_rate": 4.0867669329991124e-05,
      "loss": 1.6564,
      "step": 139900
    },
    {
      "epoch": 10.966630111232963,
      "grad_norm": 5.078197956085205,
      "learning_rate": 4.086114157397253e-05,
      "loss": 1.7355,
      "step": 140000
    },
    {
      "epoch": 10.974463418455272,
      "grad_norm": 4.3873796463012695,
      "learning_rate": 4.085461381795394e-05,
      "loss": 1.7753,
      "step": 140100
    },
    {
      "epoch": 10.982296725677582,
      "grad_norm": 5.657583236694336,
      "learning_rate": 4.0848086061935355e-05,
      "loss": 1.7916,
      "step": 140200
    },
    {
      "epoch": 10.99013003289989,
      "grad_norm": 4.700838088989258,
      "learning_rate": 4.084155830591676e-05,
      "loss": 1.7308,
      "step": 140300
    },
    {
      "epoch": 10.9979633401222,
      "grad_norm": 6.053603172302246,
      "learning_rate": 4.083503054989817e-05,
      "loss": 1.8649,
      "step": 140400
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.8222368955612183,
      "eval_runtime": 1.4319,
      "eval_samples_per_second": 469.323,
      "eval_steps_per_second": 469.323,
      "step": 140426
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.5881558656692505,
      "eval_runtime": 27.2289,
      "eval_samples_per_second": 468.841,
      "eval_steps_per_second": 468.841,
      "step": 140426
    },
    {
      "epoch": 11.005796647344509,
      "grad_norm": 6.301976203918457,
      "learning_rate": 4.082850279387958e-05,
      "loss": 1.8562,
      "step": 140500
    },
    {
      "epoch": 11.013629954566818,
      "grad_norm": 3.915038585662842,
      "learning_rate": 4.0821975037860985e-05,
      "loss": 1.7496,
      "step": 140600
    },
    {
      "epoch": 11.021463261789128,
      "grad_norm": 5.555649757385254,
      "learning_rate": 4.08154472818424e-05,
      "loss": 1.7712,
      "step": 140700
    },
    {
      "epoch": 11.029296569011438,
      "grad_norm": 3.814906358718872,
      "learning_rate": 4.08089195258238e-05,
      "loss": 1.7706,
      "step": 140800
    },
    {
      "epoch": 11.037129876233745,
      "grad_norm": 5.7679572105407715,
      "learning_rate": 4.0802391769805216e-05,
      "loss": 1.7249,
      "step": 140900
    },
    {
      "epoch": 11.044963183456055,
      "grad_norm": 4.323382377624512,
      "learning_rate": 4.079586401378662e-05,
      "loss": 1.7506,
      "step": 141000
    },
    {
      "epoch": 11.052796490678364,
      "grad_norm": 5.069875717163086,
      "learning_rate": 4.078933625776803e-05,
      "loss": 1.7188,
      "step": 141100
    },
    {
      "epoch": 11.060629797900674,
      "grad_norm": 5.585636615753174,
      "learning_rate": 4.078280850174944e-05,
      "loss": 1.8668,
      "step": 141200
    },
    {
      "epoch": 11.068463105122984,
      "grad_norm": 5.1540985107421875,
      "learning_rate": 4.077628074573085e-05,
      "loss": 1.7087,
      "step": 141300
    },
    {
      "epoch": 11.076296412345291,
      "grad_norm": 5.602571964263916,
      "learning_rate": 4.076975298971226e-05,
      "loss": 1.816,
      "step": 141400
    },
    {
      "epoch": 11.084129719567601,
      "grad_norm": 5.972549915313721,
      "learning_rate": 4.076322523369367e-05,
      "loss": 1.7502,
      "step": 141500
    },
    {
      "epoch": 11.09196302678991,
      "grad_norm": 5.9196624755859375,
      "learning_rate": 4.075669747767508e-05,
      "loss": 1.9015,
      "step": 141600
    },
    {
      "epoch": 11.09979633401222,
      "grad_norm": 3.278115749359131,
      "learning_rate": 4.075016972165648e-05,
      "loss": 1.7789,
      "step": 141700
    },
    {
      "epoch": 11.10762964123453,
      "grad_norm": 4.694962978363037,
      "learning_rate": 4.0743641965637895e-05,
      "loss": 1.7612,
      "step": 141800
    },
    {
      "epoch": 11.11546294845684,
      "grad_norm": 5.844036102294922,
      "learning_rate": 4.07371142096193e-05,
      "loss": 1.7503,
      "step": 141900
    },
    {
      "epoch": 11.123296255679147,
      "grad_norm": 5.405784606933594,
      "learning_rate": 4.073058645360071e-05,
      "loss": 1.747,
      "step": 142000
    },
    {
      "epoch": 11.131129562901457,
      "grad_norm": 4.1536173820495605,
      "learning_rate": 4.0724058697582126e-05,
      "loss": 1.8169,
      "step": 142100
    },
    {
      "epoch": 11.138962870123766,
      "grad_norm": 3.5486032962799072,
      "learning_rate": 4.071753094156353e-05,
      "loss": 1.6775,
      "step": 142200
    },
    {
      "epoch": 11.146796177346076,
      "grad_norm": 4.681095123291016,
      "learning_rate": 4.071100318554494e-05,
      "loss": 1.8235,
      "step": 142300
    },
    {
      "epoch": 11.154629484568385,
      "grad_norm": 6.927281379699707,
      "learning_rate": 4.070447542952634e-05,
      "loss": 1.867,
      "step": 142400
    },
    {
      "epoch": 11.162462791790695,
      "grad_norm": 6.301866054534912,
      "learning_rate": 4.0697947673507756e-05,
      "loss": 1.8044,
      "step": 142500
    },
    {
      "epoch": 11.170296099013003,
      "grad_norm": 4.536208629608154,
      "learning_rate": 4.069141991748917e-05,
      "loss": 1.7829,
      "step": 142600
    },
    {
      "epoch": 11.178129406235312,
      "grad_norm": 5.828163146972656,
      "learning_rate": 4.0684892161470574e-05,
      "loss": 1.8197,
      "step": 142700
    },
    {
      "epoch": 11.185962713457622,
      "grad_norm": 5.339266300201416,
      "learning_rate": 4.0678364405451986e-05,
      "loss": 1.768,
      "step": 142800
    },
    {
      "epoch": 11.193796020679931,
      "grad_norm": 5.376049518585205,
      "learning_rate": 4.067183664943339e-05,
      "loss": 1.8536,
      "step": 142900
    },
    {
      "epoch": 11.201629327902241,
      "grad_norm": 4.297596454620361,
      "learning_rate": 4.06653088934148e-05,
      "loss": 1.7942,
      "step": 143000
    },
    {
      "epoch": 11.209462635124549,
      "grad_norm": 4.919799327850342,
      "learning_rate": 4.065878113739621e-05,
      "loss": 1.929,
      "step": 143100
    },
    {
      "epoch": 11.217295942346858,
      "grad_norm": 5.163508415222168,
      "learning_rate": 4.0652253381377616e-05,
      "loss": 1.7961,
      "step": 143200
    },
    {
      "epoch": 11.225129249569168,
      "grad_norm": 6.140036582946777,
      "learning_rate": 4.064572562535903e-05,
      "loss": 1.8322,
      "step": 143300
    },
    {
      "epoch": 11.232962556791477,
      "grad_norm": 5.396933078765869,
      "learning_rate": 4.063919786934044e-05,
      "loss": 1.7409,
      "step": 143400
    },
    {
      "epoch": 11.240795864013787,
      "grad_norm": 5.799992084503174,
      "learning_rate": 4.063267011332185e-05,
      "loss": 1.7593,
      "step": 143500
    },
    {
      "epoch": 11.248629171236097,
      "grad_norm": 4.913410663604736,
      "learning_rate": 4.062614235730325e-05,
      "loss": 1.745,
      "step": 143600
    },
    {
      "epoch": 11.256462478458404,
      "grad_norm": 4.934516906738281,
      "learning_rate": 4.0619614601284666e-05,
      "loss": 1.8644,
      "step": 143700
    },
    {
      "epoch": 11.264295785680714,
      "grad_norm": 4.46231746673584,
      "learning_rate": 4.061308684526607e-05,
      "loss": 1.8037,
      "step": 143800
    },
    {
      "epoch": 11.272129092903024,
      "grad_norm": 6.203946590423584,
      "learning_rate": 4.0606559089247484e-05,
      "loss": 1.7935,
      "step": 143900
    },
    {
      "epoch": 11.279962400125333,
      "grad_norm": 5.04424524307251,
      "learning_rate": 4.060003133322889e-05,
      "loss": 1.7249,
      "step": 144000
    },
    {
      "epoch": 11.287795707347643,
      "grad_norm": 4.573604106903076,
      "learning_rate": 4.05935035772103e-05,
      "loss": 1.8112,
      "step": 144100
    },
    {
      "epoch": 11.295629014569952,
      "grad_norm": 4.352574825286865,
      "learning_rate": 4.058697582119171e-05,
      "loss": 1.7421,
      "step": 144200
    },
    {
      "epoch": 11.30346232179226,
      "grad_norm": 5.9337968826293945,
      "learning_rate": 4.0580448065173114e-05,
      "loss": 1.7982,
      "step": 144300
    },
    {
      "epoch": 11.31129562901457,
      "grad_norm": 4.50941276550293,
      "learning_rate": 4.0573920309154526e-05,
      "loss": 1.8078,
      "step": 144400
    },
    {
      "epoch": 11.31912893623688,
      "grad_norm": 4.969478130340576,
      "learning_rate": 4.056739255313594e-05,
      "loss": 1.8177,
      "step": 144500
    },
    {
      "epoch": 11.326962243459189,
      "grad_norm": 6.407554626464844,
      "learning_rate": 4.0560864797117345e-05,
      "loss": 1.8269,
      "step": 144600
    },
    {
      "epoch": 11.334795550681498,
      "grad_norm": 6.088711261749268,
      "learning_rate": 4.055433704109876e-05,
      "loss": 1.728,
      "step": 144700
    },
    {
      "epoch": 11.342628857903806,
      "grad_norm": 5.105169773101807,
      "learning_rate": 4.054780928508016e-05,
      "loss": 1.8495,
      "step": 144800
    },
    {
      "epoch": 11.350462165126116,
      "grad_norm": 3.922300338745117,
      "learning_rate": 4.054128152906157e-05,
      "loss": 1.7963,
      "step": 144900
    },
    {
      "epoch": 11.358295472348425,
      "grad_norm": 6.602329254150391,
      "learning_rate": 4.053475377304298e-05,
      "loss": 1.8969,
      "step": 145000
    },
    {
      "epoch": 11.366128779570735,
      "grad_norm": 6.884658336639404,
      "learning_rate": 4.052822601702439e-05,
      "loss": 1.7919,
      "step": 145100
    },
    {
      "epoch": 11.373962086793044,
      "grad_norm": 5.90474796295166,
      "learning_rate": 4.05216982610058e-05,
      "loss": 1.7823,
      "step": 145200
    },
    {
      "epoch": 11.381795394015354,
      "grad_norm": 6.0968828201293945,
      "learning_rate": 4.051517050498721e-05,
      "loss": 1.767,
      "step": 145300
    },
    {
      "epoch": 11.389628701237662,
      "grad_norm": 5.071339130401611,
      "learning_rate": 4.050864274896862e-05,
      "loss": 1.8015,
      "step": 145400
    },
    {
      "epoch": 11.397462008459971,
      "grad_norm": 4.942965507507324,
      "learning_rate": 4.0502114992950024e-05,
      "loss": 1.865,
      "step": 145500
    },
    {
      "epoch": 11.405295315682281,
      "grad_norm": 5.508852481842041,
      "learning_rate": 4.0495587236931436e-05,
      "loss": 1.7658,
      "step": 145600
    },
    {
      "epoch": 11.41312862290459,
      "grad_norm": 4.738806247711182,
      "learning_rate": 4.048905948091284e-05,
      "loss": 1.8464,
      "step": 145700
    },
    {
      "epoch": 11.4209619301269,
      "grad_norm": 5.617380619049072,
      "learning_rate": 4.0482531724894254e-05,
      "loss": 1.7627,
      "step": 145800
    },
    {
      "epoch": 11.42879523734921,
      "grad_norm": 6.101325035095215,
      "learning_rate": 4.047600396887566e-05,
      "loss": 1.798,
      "step": 145900
    },
    {
      "epoch": 11.436628544571517,
      "grad_norm": 6.644357681274414,
      "learning_rate": 4.046947621285707e-05,
      "loss": 1.7156,
      "step": 146000
    },
    {
      "epoch": 11.444461851793827,
      "grad_norm": 4.800540924072266,
      "learning_rate": 4.046294845683848e-05,
      "loss": 1.8863,
      "step": 146100
    },
    {
      "epoch": 11.452295159016137,
      "grad_norm": 6.6700029373168945,
      "learning_rate": 4.0456420700819884e-05,
      "loss": 1.7983,
      "step": 146200
    },
    {
      "epoch": 11.460128466238446,
      "grad_norm": 6.135544300079346,
      "learning_rate": 4.04498929448013e-05,
      "loss": 1.8275,
      "step": 146300
    },
    {
      "epoch": 11.467961773460756,
      "grad_norm": 6.492306709289551,
      "learning_rate": 4.04433651887827e-05,
      "loss": 1.8666,
      "step": 146400
    },
    {
      "epoch": 11.475795080683064,
      "grad_norm": 4.833384037017822,
      "learning_rate": 4.0436837432764115e-05,
      "loss": 1.8456,
      "step": 146500
    },
    {
      "epoch": 11.483628387905373,
      "grad_norm": 7.005275249481201,
      "learning_rate": 4.043030967674553e-05,
      "loss": 1.8409,
      "step": 146600
    },
    {
      "epoch": 11.491461695127683,
      "grad_norm": 6.013458251953125,
      "learning_rate": 4.0423781920726933e-05,
      "loss": 1.7567,
      "step": 146700
    },
    {
      "epoch": 11.499295002349992,
      "grad_norm": 4.22739839553833,
      "learning_rate": 4.041725416470834e-05,
      "loss": 1.8831,
      "step": 146800
    },
    {
      "epoch": 11.507128309572302,
      "grad_norm": 4.988680839538574,
      "learning_rate": 4.041072640868975e-05,
      "loss": 1.81,
      "step": 146900
    },
    {
      "epoch": 11.514961616794611,
      "grad_norm": 5.950304985046387,
      "learning_rate": 4.040419865267116e-05,
      "loss": 1.886,
      "step": 147000
    },
    {
      "epoch": 11.52279492401692,
      "grad_norm": 6.205716609954834,
      "learning_rate": 4.039767089665257e-05,
      "loss": 1.7305,
      "step": 147100
    },
    {
      "epoch": 11.530628231239229,
      "grad_norm": 5.795111179351807,
      "learning_rate": 4.039114314063398e-05,
      "loss": 1.7535,
      "step": 147200
    },
    {
      "epoch": 11.538461538461538,
      "grad_norm": 5.15782356262207,
      "learning_rate": 4.038461538461539e-05,
      "loss": 1.871,
      "step": 147300
    },
    {
      "epoch": 11.546294845683848,
      "grad_norm": 6.328808784484863,
      "learning_rate": 4.0378087628596794e-05,
      "loss": 1.7782,
      "step": 147400
    },
    {
      "epoch": 11.554128152906157,
      "grad_norm": 5.5858259201049805,
      "learning_rate": 4.03715598725782e-05,
      "loss": 1.788,
      "step": 147500
    },
    {
      "epoch": 11.561961460128467,
      "grad_norm": 4.839585304260254,
      "learning_rate": 4.036503211655961e-05,
      "loss": 1.7897,
      "step": 147600
    },
    {
      "epoch": 11.569794767350775,
      "grad_norm": 5.405055522918701,
      "learning_rate": 4.0358504360541025e-05,
      "loss": 1.8005,
      "step": 147700
    },
    {
      "epoch": 11.577628074573084,
      "grad_norm": 6.253679275512695,
      "learning_rate": 4.035197660452243e-05,
      "loss": 1.7842,
      "step": 147800
    },
    {
      "epoch": 11.585461381795394,
      "grad_norm": 6.077577114105225,
      "learning_rate": 4.034544884850384e-05,
      "loss": 1.9419,
      "step": 147900
    },
    {
      "epoch": 11.593294689017704,
      "grad_norm": 5.170449256896973,
      "learning_rate": 4.033892109248525e-05,
      "loss": 1.8067,
      "step": 148000
    },
    {
      "epoch": 11.601127996240013,
      "grad_norm": 6.533443927764893,
      "learning_rate": 4.0332393336466655e-05,
      "loss": 1.824,
      "step": 148100
    },
    {
      "epoch": 11.608961303462323,
      "grad_norm": 4.170341491699219,
      "learning_rate": 4.032586558044807e-05,
      "loss": 1.8249,
      "step": 148200
    },
    {
      "epoch": 11.61679461068463,
      "grad_norm": 6.054890155792236,
      "learning_rate": 4.031933782442947e-05,
      "loss": 1.7152,
      "step": 148300
    },
    {
      "epoch": 11.62462791790694,
      "grad_norm": 4.591594696044922,
      "learning_rate": 4.0312810068410886e-05,
      "loss": 1.7686,
      "step": 148400
    },
    {
      "epoch": 11.63246122512925,
      "grad_norm": 5.76455020904541,
      "learning_rate": 4.03062823123923e-05,
      "loss": 1.737,
      "step": 148500
    },
    {
      "epoch": 11.64029453235156,
      "grad_norm": 6.691446781158447,
      "learning_rate": 4.0299754556373704e-05,
      "loss": 1.7809,
      "step": 148600
    },
    {
      "epoch": 11.648127839573869,
      "grad_norm": 5.485404968261719,
      "learning_rate": 4.029322680035511e-05,
      "loss": 1.8785,
      "step": 148700
    },
    {
      "epoch": 11.655961146796177,
      "grad_norm": 4.180944919586182,
      "learning_rate": 4.028669904433652e-05,
      "loss": 1.8435,
      "step": 148800
    },
    {
      "epoch": 11.663794454018486,
      "grad_norm": 5.325989246368408,
      "learning_rate": 4.028017128831793e-05,
      "loss": 1.7127,
      "step": 148900
    },
    {
      "epoch": 11.671627761240796,
      "grad_norm": 4.650001049041748,
      "learning_rate": 4.027364353229934e-05,
      "loss": 1.7707,
      "step": 149000
    },
    {
      "epoch": 11.679461068463105,
      "grad_norm": 6.855203151702881,
      "learning_rate": 4.0267115776280746e-05,
      "loss": 1.8653,
      "step": 149100
    },
    {
      "epoch": 11.687294375685415,
      "grad_norm": 5.1392412185668945,
      "learning_rate": 4.026058802026216e-05,
      "loss": 1.7937,
      "step": 149200
    },
    {
      "epoch": 11.695127682907724,
      "grad_norm": 4.8213372230529785,
      "learning_rate": 4.0254060264243565e-05,
      "loss": 1.6795,
      "step": 149300
    },
    {
      "epoch": 11.702960990130032,
      "grad_norm": 5.372971057891846,
      "learning_rate": 4.024753250822497e-05,
      "loss": 1.7063,
      "step": 149400
    },
    {
      "epoch": 11.710794297352342,
      "grad_norm": 4.730587482452393,
      "learning_rate": 4.024100475220638e-05,
      "loss": 1.9303,
      "step": 149500
    },
    {
      "epoch": 11.718627604574651,
      "grad_norm": 4.922173976898193,
      "learning_rate": 4.023447699618779e-05,
      "loss": 1.8402,
      "step": 149600
    },
    {
      "epoch": 11.726460911796961,
      "grad_norm": 5.72368049621582,
      "learning_rate": 4.02279492401692e-05,
      "loss": 1.773,
      "step": 149700
    },
    {
      "epoch": 11.73429421901927,
      "grad_norm": 4.861461162567139,
      "learning_rate": 4.0221421484150614e-05,
      "loss": 1.8566,
      "step": 149800
    },
    {
      "epoch": 11.742127526241578,
      "grad_norm": 4.017309188842773,
      "learning_rate": 4.021489372813202e-05,
      "loss": 1.8575,
      "step": 149900
    },
    {
      "epoch": 11.749960833463888,
      "grad_norm": 6.3321685791015625,
      "learning_rate": 4.0208365972113425e-05,
      "loss": 1.8409,
      "step": 150000
    },
    {
      "epoch": 11.757794140686197,
      "grad_norm": 4.374701023101807,
      "learning_rate": 4.020183821609484e-05,
      "loss": 1.7847,
      "step": 150100
    },
    {
      "epoch": 11.765627447908507,
      "grad_norm": 4.627448081970215,
      "learning_rate": 4.0195310460076244e-05,
      "loss": 1.7683,
      "step": 150200
    },
    {
      "epoch": 11.773460755130817,
      "grad_norm": 5.9991583824157715,
      "learning_rate": 4.0188782704057656e-05,
      "loss": 1.9266,
      "step": 150300
    },
    {
      "epoch": 11.781294062353126,
      "grad_norm": 4.7335686683654785,
      "learning_rate": 4.018225494803907e-05,
      "loss": 1.857,
      "step": 150400
    },
    {
      "epoch": 11.789127369575434,
      "grad_norm": 4.05258846282959,
      "learning_rate": 4.0175727192020475e-05,
      "loss": 1.7758,
      "step": 150500
    },
    {
      "epoch": 11.796960676797744,
      "grad_norm": 5.038198947906494,
      "learning_rate": 4.016919943600188e-05,
      "loss": 1.765,
      "step": 150600
    },
    {
      "epoch": 11.804793984020053,
      "grad_norm": 4.523595809936523,
      "learning_rate": 4.016267167998329e-05,
      "loss": 1.8416,
      "step": 150700
    },
    {
      "epoch": 11.812627291242363,
      "grad_norm": 4.458621025085449,
      "learning_rate": 4.01561439239647e-05,
      "loss": 1.8625,
      "step": 150800
    },
    {
      "epoch": 11.820460598464672,
      "grad_norm": 4.51149320602417,
      "learning_rate": 4.014961616794611e-05,
      "loss": 1.8362,
      "step": 150900
    },
    {
      "epoch": 11.828293905686982,
      "grad_norm": 6.658724308013916,
      "learning_rate": 4.014308841192752e-05,
      "loss": 1.711,
      "step": 151000
    },
    {
      "epoch": 11.83612721290929,
      "grad_norm": 4.640743732452393,
      "learning_rate": 4.013656065590893e-05,
      "loss": 1.9039,
      "step": 151100
    },
    {
      "epoch": 11.8439605201316,
      "grad_norm": 4.586106777191162,
      "learning_rate": 4.0130032899890335e-05,
      "loss": 1.8071,
      "step": 151200
    },
    {
      "epoch": 11.851793827353909,
      "grad_norm": 7.75941276550293,
      "learning_rate": 4.012350514387174e-05,
      "loss": 1.8508,
      "step": 151300
    },
    {
      "epoch": 11.859627134576218,
      "grad_norm": 5.865960597991943,
      "learning_rate": 4.0116977387853154e-05,
      "loss": 1.8415,
      "step": 151400
    },
    {
      "epoch": 11.867460441798528,
      "grad_norm": 6.001175880432129,
      "learning_rate": 4.011044963183456e-05,
      "loss": 1.8197,
      "step": 151500
    },
    {
      "epoch": 11.875293749020837,
      "grad_norm": 4.901718616485596,
      "learning_rate": 4.010392187581597e-05,
      "loss": 1.9944,
      "step": 151600
    },
    {
      "epoch": 11.883127056243145,
      "grad_norm": 5.29929780960083,
      "learning_rate": 4.0097394119797384e-05,
      "loss": 1.8309,
      "step": 151700
    },
    {
      "epoch": 11.890960363465455,
      "grad_norm": 4.857419013977051,
      "learning_rate": 4.009086636377879e-05,
      "loss": 1.8077,
      "step": 151800
    },
    {
      "epoch": 11.898793670687764,
      "grad_norm": 5.023062229156494,
      "learning_rate": 4.0084338607760196e-05,
      "loss": 1.9039,
      "step": 151900
    },
    {
      "epoch": 11.906626977910074,
      "grad_norm": 6.310451507568359,
      "learning_rate": 4.007781085174161e-05,
      "loss": 1.8188,
      "step": 152000
    },
    {
      "epoch": 11.914460285132384,
      "grad_norm": 5.486985206604004,
      "learning_rate": 4.0071283095723014e-05,
      "loss": 1.844,
      "step": 152100
    },
    {
      "epoch": 11.922293592354691,
      "grad_norm": 5.068970680236816,
      "learning_rate": 4.006475533970443e-05,
      "loss": 1.874,
      "step": 152200
    },
    {
      "epoch": 11.930126899577001,
      "grad_norm": 5.087235450744629,
      "learning_rate": 4.005822758368584e-05,
      "loss": 1.7704,
      "step": 152300
    },
    {
      "epoch": 11.93796020679931,
      "grad_norm": 4.630747318267822,
      "learning_rate": 4.0051699827667245e-05,
      "loss": 1.9136,
      "step": 152400
    },
    {
      "epoch": 11.94579351402162,
      "grad_norm": 5.174354553222656,
      "learning_rate": 4.004517207164865e-05,
      "loss": 1.8084,
      "step": 152500
    },
    {
      "epoch": 11.95362682124393,
      "grad_norm": 5.3466033935546875,
      "learning_rate": 4.003864431563006e-05,
      "loss": 1.7904,
      "step": 152600
    },
    {
      "epoch": 11.96146012846624,
      "grad_norm": 5.021913051605225,
      "learning_rate": 4.003211655961147e-05,
      "loss": 1.8463,
      "step": 152700
    },
    {
      "epoch": 11.969293435688547,
      "grad_norm": 5.1377997398376465,
      "learning_rate": 4.0025588803592875e-05,
      "loss": 1.7489,
      "step": 152800
    },
    {
      "epoch": 11.977126742910857,
      "grad_norm": 3.8794326782226562,
      "learning_rate": 4.001906104757429e-05,
      "loss": 1.7877,
      "step": 152900
    },
    {
      "epoch": 11.984960050133166,
      "grad_norm": 4.108494758605957,
      "learning_rate": 4.00125332915557e-05,
      "loss": 1.8337,
      "step": 153000
    },
    {
      "epoch": 11.992793357355476,
      "grad_norm": 4.639400005340576,
      "learning_rate": 4.0006005535537106e-05,
      "loss": 1.8679,
      "step": 153100
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.8253122568130493,
      "eval_runtime": 1.4306,
      "eval_samples_per_second": 469.742,
      "eval_steps_per_second": 469.742,
      "step": 153192
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.5812053680419922,
      "eval_runtime": 27.1205,
      "eval_samples_per_second": 470.714,
      "eval_steps_per_second": 470.714,
      "step": 153192
    },
    {
      "epoch": 12.000626664577785,
      "grad_norm": 7.276236534118652,
      "learning_rate": 3.999947777951851e-05,
      "loss": 1.8199,
      "step": 153200
    },
    {
      "epoch": 12.008459971800095,
      "grad_norm": 8.750696182250977,
      "learning_rate": 3.9992950023499924e-05,
      "loss": 1.7982,
      "step": 153300
    },
    {
      "epoch": 12.016293279022403,
      "grad_norm": 6.368273735046387,
      "learning_rate": 3.998642226748133e-05,
      "loss": 1.7806,
      "step": 153400
    },
    {
      "epoch": 12.024126586244712,
      "grad_norm": 4.810441493988037,
      "learning_rate": 3.997989451146274e-05,
      "loss": 1.7395,
      "step": 153500
    },
    {
      "epoch": 12.031959893467022,
      "grad_norm": 3.903978109359741,
      "learning_rate": 3.9973366755444155e-05,
      "loss": 1.7738,
      "step": 153600
    },
    {
      "epoch": 12.039793200689331,
      "grad_norm": 5.028501510620117,
      "learning_rate": 3.996683899942556e-05,
      "loss": 1.8225,
      "step": 153700
    },
    {
      "epoch": 12.047626507911641,
      "grad_norm": 4.45176362991333,
      "learning_rate": 3.9960311243406967e-05,
      "loss": 1.8068,
      "step": 153800
    },
    {
      "epoch": 12.055459815133949,
      "grad_norm": 4.762037754058838,
      "learning_rate": 3.995378348738838e-05,
      "loss": 1.8302,
      "step": 153900
    },
    {
      "epoch": 12.063293122356258,
      "grad_norm": 4.872426986694336,
      "learning_rate": 3.9947255731369785e-05,
      "loss": 1.8101,
      "step": 154000
    },
    {
      "epoch": 12.071126429578568,
      "grad_norm": 4.702608108520508,
      "learning_rate": 3.99407279753512e-05,
      "loss": 1.809,
      "step": 154100
    },
    {
      "epoch": 12.078959736800877,
      "grad_norm": 5.442201614379883,
      "learning_rate": 3.99342002193326e-05,
      "loss": 1.7994,
      "step": 154200
    },
    {
      "epoch": 12.086793044023187,
      "grad_norm": 4.429525375366211,
      "learning_rate": 3.9927672463314016e-05,
      "loss": 1.7499,
      "step": 154300
    },
    {
      "epoch": 12.094626351245497,
      "grad_norm": 7.3475542068481445,
      "learning_rate": 3.992114470729542e-05,
      "loss": 1.7903,
      "step": 154400
    },
    {
      "epoch": 12.102459658467804,
      "grad_norm": 6.142049312591553,
      "learning_rate": 3.991461695127683e-05,
      "loss": 1.807,
      "step": 154500
    },
    {
      "epoch": 12.110292965690114,
      "grad_norm": 5.589120864868164,
      "learning_rate": 3.990808919525824e-05,
      "loss": 1.7612,
      "step": 154600
    },
    {
      "epoch": 12.118126272912424,
      "grad_norm": 4.447899341583252,
      "learning_rate": 3.9901561439239646e-05,
      "loss": 1.8252,
      "step": 154700
    },
    {
      "epoch": 12.125959580134733,
      "grad_norm": 5.318755626678467,
      "learning_rate": 3.989503368322106e-05,
      "loss": 1.8376,
      "step": 154800
    },
    {
      "epoch": 12.133792887357043,
      "grad_norm": 4.998591899871826,
      "learning_rate": 3.988850592720247e-05,
      "loss": 1.8297,
      "step": 154900
    },
    {
      "epoch": 12.141626194579352,
      "grad_norm": 4.598895072937012,
      "learning_rate": 3.9881978171183876e-05,
      "loss": 1.8184,
      "step": 155000
    },
    {
      "epoch": 12.14945950180166,
      "grad_norm": 5.710958003997803,
      "learning_rate": 3.987545041516528e-05,
      "loss": 1.8189,
      "step": 155100
    },
    {
      "epoch": 12.15729280902397,
      "grad_norm": 6.4614787101745605,
      "learning_rate": 3.9868922659146695e-05,
      "loss": 1.7041,
      "step": 155200
    },
    {
      "epoch": 12.16512611624628,
      "grad_norm": 4.758566379547119,
      "learning_rate": 3.98623949031281e-05,
      "loss": 1.8124,
      "step": 155300
    },
    {
      "epoch": 12.172959423468589,
      "grad_norm": 4.7446513175964355,
      "learning_rate": 3.985586714710951e-05,
      "loss": 1.7791,
      "step": 155400
    },
    {
      "epoch": 12.180792730690898,
      "grad_norm": 4.712378978729248,
      "learning_rate": 3.9849339391090926e-05,
      "loss": 1.8108,
      "step": 155500
    },
    {
      "epoch": 12.188626037913206,
      "grad_norm": 4.848465919494629,
      "learning_rate": 3.984281163507233e-05,
      "loss": 1.7817,
      "step": 155600
    },
    {
      "epoch": 12.196459345135516,
      "grad_norm": 5.123757362365723,
      "learning_rate": 3.983628387905374e-05,
      "loss": 1.8598,
      "step": 155700
    },
    {
      "epoch": 12.204292652357825,
      "grad_norm": 5.36911153793335,
      "learning_rate": 3.982975612303514e-05,
      "loss": 1.8312,
      "step": 155800
    },
    {
      "epoch": 12.212125959580135,
      "grad_norm": 5.582035064697266,
      "learning_rate": 3.9823228367016556e-05,
      "loss": 1.8432,
      "step": 155900
    },
    {
      "epoch": 12.219959266802444,
      "grad_norm": 4.396231651306152,
      "learning_rate": 3.981670061099796e-05,
      "loss": 1.81,
      "step": 156000
    },
    {
      "epoch": 12.227792574024754,
      "grad_norm": 5.176718235015869,
      "learning_rate": 3.9810172854979374e-05,
      "loss": 1.7852,
      "step": 156100
    },
    {
      "epoch": 12.235625881247062,
      "grad_norm": 6.441323757171631,
      "learning_rate": 3.9803645098960786e-05,
      "loss": 1.739,
      "step": 156200
    },
    {
      "epoch": 12.243459188469371,
      "grad_norm": 4.820942401885986,
      "learning_rate": 3.979711734294219e-05,
      "loss": 1.7874,
      "step": 156300
    },
    {
      "epoch": 12.25129249569168,
      "grad_norm": 5.085225582122803,
      "learning_rate": 3.97905895869236e-05,
      "loss": 1.8434,
      "step": 156400
    },
    {
      "epoch": 12.25912580291399,
      "grad_norm": 5.795144081115723,
      "learning_rate": 3.978406183090501e-05,
      "loss": 1.806,
      "step": 156500
    },
    {
      "epoch": 12.2669591101363,
      "grad_norm": 4.789124011993408,
      "learning_rate": 3.9777534074886416e-05,
      "loss": 1.7299,
      "step": 156600
    },
    {
      "epoch": 12.27479241735861,
      "grad_norm": 5.923069000244141,
      "learning_rate": 3.977100631886783e-05,
      "loss": 1.8186,
      "step": 156700
    },
    {
      "epoch": 12.282625724580917,
      "grad_norm": 4.8528852462768555,
      "learning_rate": 3.976447856284924e-05,
      "loss": 1.8199,
      "step": 156800
    },
    {
      "epoch": 12.290459031803227,
      "grad_norm": 5.031863689422607,
      "learning_rate": 3.975795080683065e-05,
      "loss": 1.824,
      "step": 156900
    },
    {
      "epoch": 12.298292339025537,
      "grad_norm": 4.154623985290527,
      "learning_rate": 3.975142305081205e-05,
      "loss": 1.8804,
      "step": 157000
    },
    {
      "epoch": 12.306125646247846,
      "grad_norm": 8.741153717041016,
      "learning_rate": 3.9744895294793465e-05,
      "loss": 1.7499,
      "step": 157100
    },
    {
      "epoch": 12.313958953470156,
      "grad_norm": 4.353488922119141,
      "learning_rate": 3.973836753877487e-05,
      "loss": 1.7244,
      "step": 157200
    },
    {
      "epoch": 12.321792260692465,
      "grad_norm": 5.102248191833496,
      "learning_rate": 3.9731839782756284e-05,
      "loss": 1.8306,
      "step": 157300
    },
    {
      "epoch": 12.329625567914773,
      "grad_norm": 2.9310498237609863,
      "learning_rate": 3.9725312026737696e-05,
      "loss": 1.7421,
      "step": 157400
    },
    {
      "epoch": 12.337458875137083,
      "grad_norm": 4.712825298309326,
      "learning_rate": 3.97187842707191e-05,
      "loss": 1.8398,
      "step": 157500
    },
    {
      "epoch": 12.345292182359392,
      "grad_norm": 5.566307544708252,
      "learning_rate": 3.971225651470051e-05,
      "loss": 1.7767,
      "step": 157600
    },
    {
      "epoch": 12.353125489581702,
      "grad_norm": 5.019011497497559,
      "learning_rate": 3.9705728758681914e-05,
      "loss": 1.7251,
      "step": 157700
    },
    {
      "epoch": 12.360958796804011,
      "grad_norm": 4.741562366485596,
      "learning_rate": 3.9699201002663326e-05,
      "loss": 1.7961,
      "step": 157800
    },
    {
      "epoch": 12.36879210402632,
      "grad_norm": 4.427722454071045,
      "learning_rate": 3.969267324664473e-05,
      "loss": 1.8044,
      "step": 157900
    },
    {
      "epoch": 12.376625411248629,
      "grad_norm": 5.153570175170898,
      "learning_rate": 3.9686145490626144e-05,
      "loss": 1.7052,
      "step": 158000
    },
    {
      "epoch": 12.384458718470938,
      "grad_norm": 5.722478866577148,
      "learning_rate": 3.967961773460756e-05,
      "loss": 1.8083,
      "step": 158100
    },
    {
      "epoch": 12.392292025693248,
      "grad_norm": 5.105972766876221,
      "learning_rate": 3.967308997858896e-05,
      "loss": 1.8547,
      "step": 158200
    },
    {
      "epoch": 12.400125332915557,
      "grad_norm": 5.433505535125732,
      "learning_rate": 3.966656222257037e-05,
      "loss": 1.7229,
      "step": 158300
    },
    {
      "epoch": 12.407958640137867,
      "grad_norm": 3.9523143768310547,
      "learning_rate": 3.966003446655178e-05,
      "loss": 1.8604,
      "step": 158400
    },
    {
      "epoch": 12.415791947360175,
      "grad_norm": 4.841176509857178,
      "learning_rate": 3.965350671053319e-05,
      "loss": 1.7778,
      "step": 158500
    },
    {
      "epoch": 12.423625254582484,
      "grad_norm": 3.8985466957092285,
      "learning_rate": 3.96469789545146e-05,
      "loss": 1.7247,
      "step": 158600
    },
    {
      "epoch": 12.431458561804794,
      "grad_norm": 7.259186744689941,
      "learning_rate": 3.964045119849601e-05,
      "loss": 1.8073,
      "step": 158700
    },
    {
      "epoch": 12.439291869027103,
      "grad_norm": 4.960779666900635,
      "learning_rate": 3.963392344247742e-05,
      "loss": 1.8423,
      "step": 158800
    },
    {
      "epoch": 12.447125176249413,
      "grad_norm": 5.186026096343994,
      "learning_rate": 3.9627395686458823e-05,
      "loss": 1.7765,
      "step": 158900
    },
    {
      "epoch": 12.45495848347172,
      "grad_norm": 5.462690830230713,
      "learning_rate": 3.9620867930440236e-05,
      "loss": 1.7465,
      "step": 159000
    },
    {
      "epoch": 12.46279179069403,
      "grad_norm": 4.290689945220947,
      "learning_rate": 3.961434017442164e-05,
      "loss": 1.7312,
      "step": 159100
    },
    {
      "epoch": 12.47062509791634,
      "grad_norm": 6.3530073165893555,
      "learning_rate": 3.960781241840305e-05,
      "loss": 1.7997,
      "step": 159200
    },
    {
      "epoch": 12.47845840513865,
      "grad_norm": 5.160086631774902,
      "learning_rate": 3.960128466238446e-05,
      "loss": 1.7872,
      "step": 159300
    },
    {
      "epoch": 12.48629171236096,
      "grad_norm": 4.631466388702393,
      "learning_rate": 3.959475690636587e-05,
      "loss": 1.8459,
      "step": 159400
    },
    {
      "epoch": 12.494125019583269,
      "grad_norm": 5.161085605621338,
      "learning_rate": 3.958822915034728e-05,
      "loss": 1.7984,
      "step": 159500
    },
    {
      "epoch": 12.501958326805577,
      "grad_norm": 5.902651309967041,
      "learning_rate": 3.9581701394328684e-05,
      "loss": 1.8736,
      "step": 159600
    },
    {
      "epoch": 12.509791634027886,
      "grad_norm": 4.529541492462158,
      "learning_rate": 3.95751736383101e-05,
      "loss": 1.8025,
      "step": 159700
    },
    {
      "epoch": 12.517624941250196,
      "grad_norm": 6.3499274253845215,
      "learning_rate": 3.95686458822915e-05,
      "loss": 1.7385,
      "step": 159800
    },
    {
      "epoch": 12.525458248472505,
      "grad_norm": 6.5704169273376465,
      "learning_rate": 3.9562118126272915e-05,
      "loss": 1.7772,
      "step": 159900
    },
    {
      "epoch": 12.533291555694815,
      "grad_norm": 4.042911529541016,
      "learning_rate": 3.955559037025433e-05,
      "loss": 1.8216,
      "step": 160000
    },
    {
      "epoch": 12.541124862917124,
      "grad_norm": 5.864465236663818,
      "learning_rate": 3.954906261423573e-05,
      "loss": 1.8562,
      "step": 160100
    },
    {
      "epoch": 12.548958170139432,
      "grad_norm": 3.323845386505127,
      "learning_rate": 3.954253485821714e-05,
      "loss": 1.8454,
      "step": 160200
    },
    {
      "epoch": 12.556791477361742,
      "grad_norm": 5.605738162994385,
      "learning_rate": 3.953600710219855e-05,
      "loss": 1.8141,
      "step": 160300
    },
    {
      "epoch": 12.564624784584051,
      "grad_norm": 4.278053283691406,
      "learning_rate": 3.952947934617996e-05,
      "loss": 1.7479,
      "step": 160400
    },
    {
      "epoch": 12.57245809180636,
      "grad_norm": 5.009651184082031,
      "learning_rate": 3.952295159016137e-05,
      "loss": 1.8151,
      "step": 160500
    },
    {
      "epoch": 12.58029139902867,
      "grad_norm": 3.8945181369781494,
      "learning_rate": 3.951642383414278e-05,
      "loss": 1.7418,
      "step": 160600
    },
    {
      "epoch": 12.58812470625098,
      "grad_norm": 5.398610591888428,
      "learning_rate": 3.950989607812419e-05,
      "loss": 1.7423,
      "step": 160700
    },
    {
      "epoch": 12.595958013473288,
      "grad_norm": 4.963168144226074,
      "learning_rate": 3.9503368322105594e-05,
      "loss": 1.809,
      "step": 160800
    },
    {
      "epoch": 12.603791320695597,
      "grad_norm": 7.885236740112305,
      "learning_rate": 3.9496840566087e-05,
      "loss": 1.854,
      "step": 160900
    },
    {
      "epoch": 12.611624627917907,
      "grad_norm": 5.002671718597412,
      "learning_rate": 3.949031281006841e-05,
      "loss": 1.7478,
      "step": 161000
    },
    {
      "epoch": 12.619457935140217,
      "grad_norm": 6.323652267456055,
      "learning_rate": 3.948378505404982e-05,
      "loss": 1.734,
      "step": 161100
    },
    {
      "epoch": 12.627291242362526,
      "grad_norm": 5.721584320068359,
      "learning_rate": 3.947725729803123e-05,
      "loss": 1.7409,
      "step": 161200
    },
    {
      "epoch": 12.635124549584834,
      "grad_norm": 6.922741413116455,
      "learning_rate": 3.947072954201264e-05,
      "loss": 1.8189,
      "step": 161300
    },
    {
      "epoch": 12.642957856807143,
      "grad_norm": 4.835982799530029,
      "learning_rate": 3.946420178599405e-05,
      "loss": 1.7582,
      "step": 161400
    },
    {
      "epoch": 12.650791164029453,
      "grad_norm": 3.896951913833618,
      "learning_rate": 3.9457674029975455e-05,
      "loss": 1.7746,
      "step": 161500
    },
    {
      "epoch": 12.658624471251763,
      "grad_norm": 5.279387474060059,
      "learning_rate": 3.945114627395687e-05,
      "loss": 1.8516,
      "step": 161600
    },
    {
      "epoch": 12.666457778474072,
      "grad_norm": 5.457810401916504,
      "learning_rate": 3.944461851793827e-05,
      "loss": 1.8771,
      "step": 161700
    },
    {
      "epoch": 12.674291085696382,
      "grad_norm": 5.237711429595947,
      "learning_rate": 3.9438090761919686e-05,
      "loss": 1.8109,
      "step": 161800
    },
    {
      "epoch": 12.68212439291869,
      "grad_norm": 5.025452613830566,
      "learning_rate": 3.94315630059011e-05,
      "loss": 1.7657,
      "step": 161900
    },
    {
      "epoch": 12.689957700140999,
      "grad_norm": 5.775849342346191,
      "learning_rate": 3.9425035249882504e-05,
      "loss": 1.6932,
      "step": 162000
    },
    {
      "epoch": 12.697791007363309,
      "grad_norm": 4.174179553985596,
      "learning_rate": 3.941850749386391e-05,
      "loss": 1.7766,
      "step": 162100
    },
    {
      "epoch": 12.705624314585618,
      "grad_norm": 4.428360939025879,
      "learning_rate": 3.941197973784532e-05,
      "loss": 1.7845,
      "step": 162200
    },
    {
      "epoch": 12.713457621807928,
      "grad_norm": 7.183592796325684,
      "learning_rate": 3.940545198182673e-05,
      "loss": 1.8218,
      "step": 162300
    },
    {
      "epoch": 12.721290929030237,
      "grad_norm": 5.416407108306885,
      "learning_rate": 3.9398924225808134e-05,
      "loss": 1.7428,
      "step": 162400
    },
    {
      "epoch": 12.729124236252545,
      "grad_norm": 4.000406265258789,
      "learning_rate": 3.939239646978955e-05,
      "loss": 1.8399,
      "step": 162500
    },
    {
      "epoch": 12.736957543474855,
      "grad_norm": 4.357480049133301,
      "learning_rate": 3.938586871377096e-05,
      "loss": 1.7993,
      "step": 162600
    },
    {
      "epoch": 12.744790850697164,
      "grad_norm": 5.380331516265869,
      "learning_rate": 3.9379340957752365e-05,
      "loss": 1.7836,
      "step": 162700
    },
    {
      "epoch": 12.752624157919474,
      "grad_norm": 6.054409027099609,
      "learning_rate": 3.937281320173377e-05,
      "loss": 1.7952,
      "step": 162800
    },
    {
      "epoch": 12.760457465141783,
      "grad_norm": 6.316432952880859,
      "learning_rate": 3.936628544571518e-05,
      "loss": 1.7247,
      "step": 162900
    },
    {
      "epoch": 12.768290772364093,
      "grad_norm": 5.766672611236572,
      "learning_rate": 3.935975768969659e-05,
      "loss": 1.7824,
      "step": 163000
    },
    {
      "epoch": 12.7761240795864,
      "grad_norm": 3.4858756065368652,
      "learning_rate": 3.9353229933678e-05,
      "loss": 1.7692,
      "step": 163100
    },
    {
      "epoch": 12.78395738680871,
      "grad_norm": 5.866402626037598,
      "learning_rate": 3.9346702177659414e-05,
      "loss": 1.818,
      "step": 163200
    },
    {
      "epoch": 12.79179069403102,
      "grad_norm": 5.928481578826904,
      "learning_rate": 3.934017442164082e-05,
      "loss": 1.7432,
      "step": 163300
    },
    {
      "epoch": 12.79962400125333,
      "grad_norm": 4.660841941833496,
      "learning_rate": 3.9333646665622225e-05,
      "loss": 1.8295,
      "step": 163400
    },
    {
      "epoch": 12.80745730847564,
      "grad_norm": 4.846434593200684,
      "learning_rate": 3.932711890960364e-05,
      "loss": 1.908,
      "step": 163500
    },
    {
      "epoch": 12.815290615697947,
      "grad_norm": 5.293313026428223,
      "learning_rate": 3.9320591153585044e-05,
      "loss": 1.7636,
      "step": 163600
    },
    {
      "epoch": 12.823123922920256,
      "grad_norm": 6.076146602630615,
      "learning_rate": 3.9314063397566456e-05,
      "loss": 1.7437,
      "step": 163700
    },
    {
      "epoch": 12.830957230142566,
      "grad_norm": 3.732880115509033,
      "learning_rate": 3.930753564154787e-05,
      "loss": 1.8587,
      "step": 163800
    },
    {
      "epoch": 12.838790537364876,
      "grad_norm": 4.025707244873047,
      "learning_rate": 3.9301007885529274e-05,
      "loss": 1.7829,
      "step": 163900
    },
    {
      "epoch": 12.846623844587185,
      "grad_norm": 4.288510322570801,
      "learning_rate": 3.929448012951068e-05,
      "loss": 1.8606,
      "step": 164000
    },
    {
      "epoch": 12.854457151809495,
      "grad_norm": 5.309770107269287,
      "learning_rate": 3.928795237349209e-05,
      "loss": 1.782,
      "step": 164100
    },
    {
      "epoch": 12.862290459031803,
      "grad_norm": 5.884365558624268,
      "learning_rate": 3.92814246174735e-05,
      "loss": 1.7086,
      "step": 164200
    },
    {
      "epoch": 12.870123766254112,
      "grad_norm": 4.636316299438477,
      "learning_rate": 3.9274896861454904e-05,
      "loss": 1.8368,
      "step": 164300
    },
    {
      "epoch": 12.877957073476422,
      "grad_norm": 3.8738350868225098,
      "learning_rate": 3.926836910543632e-05,
      "loss": 1.8231,
      "step": 164400
    },
    {
      "epoch": 12.885790380698731,
      "grad_norm": 4.6752190589904785,
      "learning_rate": 3.926184134941773e-05,
      "loss": 1.7859,
      "step": 164500
    },
    {
      "epoch": 12.89362368792104,
      "grad_norm": 4.302687168121338,
      "learning_rate": 3.9255313593399135e-05,
      "loss": 1.8216,
      "step": 164600
    },
    {
      "epoch": 12.901456995143349,
      "grad_norm": 7.044744491577148,
      "learning_rate": 3.924878583738054e-05,
      "loss": 1.809,
      "step": 164700
    },
    {
      "epoch": 12.909290302365658,
      "grad_norm": 2.811765670776367,
      "learning_rate": 3.9242258081361953e-05,
      "loss": 1.8061,
      "step": 164800
    },
    {
      "epoch": 12.917123609587968,
      "grad_norm": 4.507182598114014,
      "learning_rate": 3.923573032534336e-05,
      "loss": 1.8812,
      "step": 164900
    },
    {
      "epoch": 12.924956916810277,
      "grad_norm": 4.827368259429932,
      "learning_rate": 3.922920256932477e-05,
      "loss": 1.9017,
      "step": 165000
    },
    {
      "epoch": 12.932790224032587,
      "grad_norm": 5.392820835113525,
      "learning_rate": 3.9222674813306184e-05,
      "loss": 1.7551,
      "step": 165100
    },
    {
      "epoch": 12.940623531254897,
      "grad_norm": 4.6800618171691895,
      "learning_rate": 3.921614705728759e-05,
      "loss": 1.8637,
      "step": 165200
    },
    {
      "epoch": 12.948456838477204,
      "grad_norm": 4.898895740509033,
      "learning_rate": 3.9209619301268996e-05,
      "loss": 1.7938,
      "step": 165300
    },
    {
      "epoch": 12.956290145699514,
      "grad_norm": 5.764939308166504,
      "learning_rate": 3.920309154525041e-05,
      "loss": 1.8181,
      "step": 165400
    },
    {
      "epoch": 12.964123452921823,
      "grad_norm": 5.185505390167236,
      "learning_rate": 3.9196563789231814e-05,
      "loss": 1.8553,
      "step": 165500
    },
    {
      "epoch": 12.971956760144133,
      "grad_norm": 6.355406761169434,
      "learning_rate": 3.919003603321322e-05,
      "loss": 1.885,
      "step": 165600
    },
    {
      "epoch": 12.979790067366443,
      "grad_norm": 5.9627556800842285,
      "learning_rate": 3.918350827719464e-05,
      "loss": 1.8693,
      "step": 165700
    },
    {
      "epoch": 12.987623374588752,
      "grad_norm": 7.318894386291504,
      "learning_rate": 3.9176980521176045e-05,
      "loss": 1.8265,
      "step": 165800
    },
    {
      "epoch": 12.99545668181106,
      "grad_norm": 5.141191005706787,
      "learning_rate": 3.917045276515745e-05,
      "loss": 1.8115,
      "step": 165900
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.8162627220153809,
      "eval_runtime": 1.4291,
      "eval_samples_per_second": 470.241,
      "eval_steps_per_second": 470.241,
      "step": 165958
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.5667507648468018,
      "eval_runtime": 27.1245,
      "eval_samples_per_second": 470.644,
      "eval_steps_per_second": 470.644,
      "step": 165958
    },
    {
      "epoch": 13.00328998903337,
      "grad_norm": 4.748952865600586,
      "learning_rate": 3.9163925009138857e-05,
      "loss": 1.7812,
      "step": 166000
    },
    {
      "epoch": 13.011123296255679,
      "grad_norm": 5.351701736450195,
      "learning_rate": 3.915739725312027e-05,
      "loss": 1.7597,
      "step": 166100
    },
    {
      "epoch": 13.018956603477989,
      "grad_norm": 4.699431896209717,
      "learning_rate": 3.9150869497101675e-05,
      "loss": 1.8249,
      "step": 166200
    },
    {
      "epoch": 13.026789910700298,
      "grad_norm": 5.419631004333496,
      "learning_rate": 3.914434174108309e-05,
      "loss": 1.7512,
      "step": 166300
    },
    {
      "epoch": 13.034623217922608,
      "grad_norm": 5.079813480377197,
      "learning_rate": 3.91378139850645e-05,
      "loss": 1.7695,
      "step": 166400
    },
    {
      "epoch": 13.042456525144916,
      "grad_norm": 5.80999231338501,
      "learning_rate": 3.9131286229045906e-05,
      "loss": 1.6669,
      "step": 166500
    },
    {
      "epoch": 13.050289832367225,
      "grad_norm": 4.4878315925598145,
      "learning_rate": 3.912475847302731e-05,
      "loss": 1.7612,
      "step": 166600
    },
    {
      "epoch": 13.058123139589535,
      "grad_norm": 4.484317779541016,
      "learning_rate": 3.9118230717008724e-05,
      "loss": 1.6558,
      "step": 166700
    },
    {
      "epoch": 13.065956446811844,
      "grad_norm": 6.1390299797058105,
      "learning_rate": 3.911170296099013e-05,
      "loss": 1.7959,
      "step": 166800
    },
    {
      "epoch": 13.073789754034154,
      "grad_norm": 5.672947406768799,
      "learning_rate": 3.910517520497154e-05,
      "loss": 1.74,
      "step": 166900
    },
    {
      "epoch": 13.081623061256462,
      "grad_norm": 5.468817234039307,
      "learning_rate": 3.9098647448952955e-05,
      "loss": 1.8165,
      "step": 167000
    },
    {
      "epoch": 13.089456368478771,
      "grad_norm": 6.114367485046387,
      "learning_rate": 3.909211969293436e-05,
      "loss": 1.7904,
      "step": 167100
    },
    {
      "epoch": 13.09728967570108,
      "grad_norm": 5.90901517868042,
      "learning_rate": 3.9085591936915766e-05,
      "loss": 1.8456,
      "step": 167200
    },
    {
      "epoch": 13.10512298292339,
      "grad_norm": 5.909564018249512,
      "learning_rate": 3.907906418089718e-05,
      "loss": 1.7641,
      "step": 167300
    },
    {
      "epoch": 13.1129562901457,
      "grad_norm": 4.22572135925293,
      "learning_rate": 3.9072536424878585e-05,
      "loss": 1.8832,
      "step": 167400
    },
    {
      "epoch": 13.12078959736801,
      "grad_norm": 4.286782741546631,
      "learning_rate": 3.906600866885999e-05,
      "loss": 1.8234,
      "step": 167500
    },
    {
      "epoch": 13.128622904590317,
      "grad_norm": 4.63918399810791,
      "learning_rate": 3.90594809128414e-05,
      "loss": 1.7063,
      "step": 167600
    },
    {
      "epoch": 13.136456211812627,
      "grad_norm": 5.599717140197754,
      "learning_rate": 3.9052953156822816e-05,
      "loss": 1.7526,
      "step": 167700
    },
    {
      "epoch": 13.144289519034936,
      "grad_norm": 4.3128790855407715,
      "learning_rate": 3.904642540080422e-05,
      "loss": 1.8439,
      "step": 167800
    },
    {
      "epoch": 13.152122826257246,
      "grad_norm": 5.839547634124756,
      "learning_rate": 3.903989764478563e-05,
      "loss": 1.7575,
      "step": 167900
    },
    {
      "epoch": 13.159956133479556,
      "grad_norm": 5.308355331420898,
      "learning_rate": 3.903336988876704e-05,
      "loss": 1.7949,
      "step": 168000
    },
    {
      "epoch": 13.167789440701865,
      "grad_norm": 4.824971675872803,
      "learning_rate": 3.9026842132748445e-05,
      "loss": 1.7841,
      "step": 168100
    },
    {
      "epoch": 13.175622747924173,
      "grad_norm": 4.367224216461182,
      "learning_rate": 3.902031437672986e-05,
      "loss": 1.8004,
      "step": 168200
    },
    {
      "epoch": 13.183456055146483,
      "grad_norm": 6.134756088256836,
      "learning_rate": 3.901378662071127e-05,
      "loss": 1.8291,
      "step": 168300
    },
    {
      "epoch": 13.191289362368792,
      "grad_norm": 6.022414684295654,
      "learning_rate": 3.9007258864692676e-05,
      "loss": 1.7577,
      "step": 168400
    },
    {
      "epoch": 13.199122669591102,
      "grad_norm": 5.800997734069824,
      "learning_rate": 3.900073110867408e-05,
      "loss": 1.7859,
      "step": 168500
    },
    {
      "epoch": 13.206955976813411,
      "grad_norm": 5.786701202392578,
      "learning_rate": 3.8994203352655495e-05,
      "loss": 1.826,
      "step": 168600
    },
    {
      "epoch": 13.214789284035719,
      "grad_norm": 4.842444896697998,
      "learning_rate": 3.89876755966369e-05,
      "loss": 1.8814,
      "step": 168700
    },
    {
      "epoch": 13.222622591258029,
      "grad_norm": 5.143456935882568,
      "learning_rate": 3.8981147840618306e-05,
      "loss": 1.7577,
      "step": 168800
    },
    {
      "epoch": 13.230455898480338,
      "grad_norm": 4.184808254241943,
      "learning_rate": 3.8974620084599726e-05,
      "loss": 1.7502,
      "step": 168900
    },
    {
      "epoch": 13.238289205702648,
      "grad_norm": 3.7506062984466553,
      "learning_rate": 3.896809232858113e-05,
      "loss": 1.67,
      "step": 169000
    },
    {
      "epoch": 13.246122512924957,
      "grad_norm": 4.712923049926758,
      "learning_rate": 3.896156457256254e-05,
      "loss": 1.7724,
      "step": 169100
    },
    {
      "epoch": 13.253955820147267,
      "grad_norm": 5.035167217254639,
      "learning_rate": 3.895503681654395e-05,
      "loss": 1.7855,
      "step": 169200
    },
    {
      "epoch": 13.261789127369575,
      "grad_norm": 3.9650747776031494,
      "learning_rate": 3.8948509060525355e-05,
      "loss": 1.8317,
      "step": 169300
    },
    {
      "epoch": 13.269622434591884,
      "grad_norm": 5.115106105804443,
      "learning_rate": 3.894198130450676e-05,
      "loss": 1.8623,
      "step": 169400
    },
    {
      "epoch": 13.277455741814194,
      "grad_norm": 5.421906471252441,
      "learning_rate": 3.8935453548488174e-05,
      "loss": 1.8643,
      "step": 169500
    },
    {
      "epoch": 13.285289049036503,
      "grad_norm": 5.668488502502441,
      "learning_rate": 3.8928925792469586e-05,
      "loss": 1.8547,
      "step": 169600
    },
    {
      "epoch": 13.293122356258813,
      "grad_norm": 6.488804340362549,
      "learning_rate": 3.892239803645099e-05,
      "loss": 1.9048,
      "step": 169700
    },
    {
      "epoch": 13.300955663481123,
      "grad_norm": 4.513278007507324,
      "learning_rate": 3.89158702804324e-05,
      "loss": 1.6864,
      "step": 169800
    },
    {
      "epoch": 13.30878897070343,
      "grad_norm": 4.215249538421631,
      "learning_rate": 3.890934252441381e-05,
      "loss": 1.883,
      "step": 169900
    },
    {
      "epoch": 13.31662227792574,
      "grad_norm": 5.4843292236328125,
      "learning_rate": 3.8902814768395216e-05,
      "loss": 1.8157,
      "step": 170000
    },
    {
      "epoch": 13.32445558514805,
      "grad_norm": 3.7768354415893555,
      "learning_rate": 3.889628701237663e-05,
      "loss": 1.9395,
      "step": 170100
    },
    {
      "epoch": 13.332288892370359,
      "grad_norm": 4.4986677169799805,
      "learning_rate": 3.888975925635804e-05,
      "loss": 1.7946,
      "step": 170200
    },
    {
      "epoch": 13.340122199592669,
      "grad_norm": 6.105264663696289,
      "learning_rate": 3.888323150033945e-05,
      "loss": 1.7726,
      "step": 170300
    },
    {
      "epoch": 13.347955506814976,
      "grad_norm": 5.674715042114258,
      "learning_rate": 3.887670374432085e-05,
      "loss": 1.7726,
      "step": 170400
    },
    {
      "epoch": 13.355788814037286,
      "grad_norm": 5.374125003814697,
      "learning_rate": 3.8870175988302265e-05,
      "loss": 1.9068,
      "step": 170500
    },
    {
      "epoch": 13.363622121259596,
      "grad_norm": 5.131375789642334,
      "learning_rate": 3.886364823228367e-05,
      "loss": 1.7193,
      "step": 170600
    },
    {
      "epoch": 13.371455428481905,
      "grad_norm": 4.270883560180664,
      "learning_rate": 3.885712047626508e-05,
      "loss": 1.7622,
      "step": 170700
    },
    {
      "epoch": 13.379288735704215,
      "grad_norm": 5.72641134262085,
      "learning_rate": 3.885059272024649e-05,
      "loss": 1.8562,
      "step": 170800
    },
    {
      "epoch": 13.387122042926524,
      "grad_norm": 5.170114517211914,
      "learning_rate": 3.88440649642279e-05,
      "loss": 1.885,
      "step": 170900
    },
    {
      "epoch": 13.394955350148832,
      "grad_norm": 4.704384803771973,
      "learning_rate": 3.883753720820931e-05,
      "loss": 1.7262,
      "step": 171000
    },
    {
      "epoch": 13.402788657371142,
      "grad_norm": 5.407165050506592,
      "learning_rate": 3.8831009452190713e-05,
      "loss": 1.7303,
      "step": 171100
    },
    {
      "epoch": 13.410621964593451,
      "grad_norm": 3.8841941356658936,
      "learning_rate": 3.8824481696172126e-05,
      "loss": 1.7687,
      "step": 171200
    },
    {
      "epoch": 13.41845527181576,
      "grad_norm": 6.856179714202881,
      "learning_rate": 3.881795394015353e-05,
      "loss": 1.7268,
      "step": 171300
    },
    {
      "epoch": 13.42628857903807,
      "grad_norm": 3.9292304515838623,
      "learning_rate": 3.8811426184134944e-05,
      "loss": 1.7605,
      "step": 171400
    },
    {
      "epoch": 13.43412188626038,
      "grad_norm": 5.143089771270752,
      "learning_rate": 3.880489842811636e-05,
      "loss": 1.8887,
      "step": 171500
    },
    {
      "epoch": 13.441955193482688,
      "grad_norm": 4.697793483734131,
      "learning_rate": 3.879837067209776e-05,
      "loss": 1.7835,
      "step": 171600
    },
    {
      "epoch": 13.449788500704997,
      "grad_norm": 6.087093353271484,
      "learning_rate": 3.879184291607917e-05,
      "loss": 1.7289,
      "step": 171700
    },
    {
      "epoch": 13.457621807927307,
      "grad_norm": 4.972575664520264,
      "learning_rate": 3.878531516006058e-05,
      "loss": 1.875,
      "step": 171800
    },
    {
      "epoch": 13.465455115149616,
      "grad_norm": 5.990908145904541,
      "learning_rate": 3.877878740404199e-05,
      "loss": 1.7641,
      "step": 171900
    },
    {
      "epoch": 13.473288422371926,
      "grad_norm": 4.016525745391846,
      "learning_rate": 3.877225964802339e-05,
      "loss": 1.7807,
      "step": 172000
    },
    {
      "epoch": 13.481121729594234,
      "grad_norm": 5.055201053619385,
      "learning_rate": 3.876573189200481e-05,
      "loss": 1.8511,
      "step": 172100
    },
    {
      "epoch": 13.488955036816543,
      "grad_norm": 3.7479469776153564,
      "learning_rate": 3.875920413598622e-05,
      "loss": 1.8089,
      "step": 172200
    },
    {
      "epoch": 13.496788344038853,
      "grad_norm": 5.204622268676758,
      "learning_rate": 3.875267637996762e-05,
      "loss": 1.7593,
      "step": 172300
    },
    {
      "epoch": 13.504621651261163,
      "grad_norm": 5.622824192047119,
      "learning_rate": 3.8746148623949036e-05,
      "loss": 1.7101,
      "step": 172400
    },
    {
      "epoch": 13.512454958483472,
      "grad_norm": 6.019144058227539,
      "learning_rate": 3.873962086793044e-05,
      "loss": 1.7234,
      "step": 172500
    },
    {
      "epoch": 13.520288265705782,
      "grad_norm": 6.770431995391846,
      "learning_rate": 3.873309311191185e-05,
      "loss": 1.7945,
      "step": 172600
    },
    {
      "epoch": 13.52812157292809,
      "grad_norm": 6.565593719482422,
      "learning_rate": 3.872656535589326e-05,
      "loss": 1.8654,
      "step": 172700
    },
    {
      "epoch": 13.535954880150399,
      "grad_norm": 5.388768196105957,
      "learning_rate": 3.872003759987467e-05,
      "loss": 1.794,
      "step": 172800
    },
    {
      "epoch": 13.543788187372709,
      "grad_norm": 4.576908111572266,
      "learning_rate": 3.871350984385608e-05,
      "loss": 1.839,
      "step": 172900
    },
    {
      "epoch": 13.551621494595018,
      "grad_norm": 4.622446060180664,
      "learning_rate": 3.8706982087837484e-05,
      "loss": 1.7679,
      "step": 173000
    },
    {
      "epoch": 13.559454801817328,
      "grad_norm": 5.170119762420654,
      "learning_rate": 3.8700454331818897e-05,
      "loss": 1.8691,
      "step": 173100
    },
    {
      "epoch": 13.567288109039637,
      "grad_norm": 6.466555118560791,
      "learning_rate": 3.86939265758003e-05,
      "loss": 1.7896,
      "step": 173200
    },
    {
      "epoch": 13.575121416261945,
      "grad_norm": 5.118459701538086,
      "learning_rate": 3.8687398819781715e-05,
      "loss": 1.8387,
      "step": 173300
    },
    {
      "epoch": 13.582954723484255,
      "grad_norm": 5.654580593109131,
      "learning_rate": 3.868087106376313e-05,
      "loss": 1.6949,
      "step": 173400
    },
    {
      "epoch": 13.590788030706564,
      "grad_norm": 4.859306335449219,
      "learning_rate": 3.867434330774453e-05,
      "loss": 1.8358,
      "step": 173500
    },
    {
      "epoch": 13.598621337928874,
      "grad_norm": 6.994357109069824,
      "learning_rate": 3.866781555172594e-05,
      "loss": 1.7042,
      "step": 173600
    },
    {
      "epoch": 13.606454645151183,
      "grad_norm": 5.360662937164307,
      "learning_rate": 3.866128779570735e-05,
      "loss": 1.7512,
      "step": 173700
    },
    {
      "epoch": 13.614287952373491,
      "grad_norm": 5.107349872589111,
      "learning_rate": 3.865476003968876e-05,
      "loss": 1.7285,
      "step": 173800
    },
    {
      "epoch": 13.6221212595958,
      "grad_norm": 4.155874729156494,
      "learning_rate": 3.864823228367016e-05,
      "loss": 1.6599,
      "step": 173900
    },
    {
      "epoch": 13.62995456681811,
      "grad_norm": 5.416489124298096,
      "learning_rate": 3.8641704527651576e-05,
      "loss": 1.7972,
      "step": 174000
    },
    {
      "epoch": 13.63778787404042,
      "grad_norm": 6.216955661773682,
      "learning_rate": 3.863517677163299e-05,
      "loss": 1.8656,
      "step": 174100
    },
    {
      "epoch": 13.64562118126273,
      "grad_norm": 4.471798419952393,
      "learning_rate": 3.8628649015614394e-05,
      "loss": 1.7675,
      "step": 174200
    },
    {
      "epoch": 13.653454488485039,
      "grad_norm": 4.554576396942139,
      "learning_rate": 3.8622121259595806e-05,
      "loss": 1.7933,
      "step": 174300
    },
    {
      "epoch": 13.661287795707347,
      "grad_norm": 5.696105480194092,
      "learning_rate": 3.861559350357721e-05,
      "loss": 1.7092,
      "step": 174400
    },
    {
      "epoch": 13.669121102929656,
      "grad_norm": 8.281743049621582,
      "learning_rate": 3.860906574755862e-05,
      "loss": 1.7823,
      "step": 174500
    },
    {
      "epoch": 13.676954410151966,
      "grad_norm": 5.339241027832031,
      "learning_rate": 3.860253799154003e-05,
      "loss": 1.7108,
      "step": 174600
    },
    {
      "epoch": 13.684787717374276,
      "grad_norm": 4.310434341430664,
      "learning_rate": 3.859601023552144e-05,
      "loss": 1.7561,
      "step": 174700
    },
    {
      "epoch": 13.692621024596585,
      "grad_norm": 6.103058815002441,
      "learning_rate": 3.858948247950285e-05,
      "loss": 1.676,
      "step": 174800
    },
    {
      "epoch": 13.700454331818895,
      "grad_norm": 4.925529956817627,
      "learning_rate": 3.8582954723484255e-05,
      "loss": 1.772,
      "step": 174900
    },
    {
      "epoch": 13.708287639041203,
      "grad_norm": 6.124400615692139,
      "learning_rate": 3.857642696746567e-05,
      "loss": 1.9023,
      "step": 175000
    },
    {
      "epoch": 13.716120946263512,
      "grad_norm": 5.308252811431885,
      "learning_rate": 3.856989921144707e-05,
      "loss": 1.7865,
      "step": 175100
    },
    {
      "epoch": 13.723954253485822,
      "grad_norm": 3.9073402881622314,
      "learning_rate": 3.856337145542848e-05,
      "loss": 1.8111,
      "step": 175200
    },
    {
      "epoch": 13.731787560708131,
      "grad_norm": 4.736090183258057,
      "learning_rate": 3.85568436994099e-05,
      "loss": 1.7782,
      "step": 175300
    },
    {
      "epoch": 13.73962086793044,
      "grad_norm": 5.57640266418457,
      "learning_rate": 3.8550315943391304e-05,
      "loss": 1.7966,
      "step": 175400
    },
    {
      "epoch": 13.74745417515275,
      "grad_norm": 5.677250385284424,
      "learning_rate": 3.854378818737271e-05,
      "loss": 1.8723,
      "step": 175500
    },
    {
      "epoch": 13.755287482375058,
      "grad_norm": 4.0294508934021,
      "learning_rate": 3.853726043135412e-05,
      "loss": 1.8029,
      "step": 175600
    },
    {
      "epoch": 13.763120789597368,
      "grad_norm": 6.152425765991211,
      "learning_rate": 3.853073267533553e-05,
      "loss": 1.8739,
      "step": 175700
    },
    {
      "epoch": 13.770954096819677,
      "grad_norm": 3.844874382019043,
      "learning_rate": 3.8524204919316934e-05,
      "loss": 1.8546,
      "step": 175800
    },
    {
      "epoch": 13.778787404041987,
      "grad_norm": 4.383488178253174,
      "learning_rate": 3.8517677163298346e-05,
      "loss": 1.757,
      "step": 175900
    },
    {
      "epoch": 13.786620711264296,
      "grad_norm": 5.812398433685303,
      "learning_rate": 3.851114940727976e-05,
      "loss": 1.8513,
      "step": 176000
    },
    {
      "epoch": 13.794454018486604,
      "grad_norm": 4.879055500030518,
      "learning_rate": 3.8504621651261164e-05,
      "loss": 1.7493,
      "step": 176100
    },
    {
      "epoch": 13.802287325708914,
      "grad_norm": 5.23547887802124,
      "learning_rate": 3.849809389524257e-05,
      "loss": 1.7227,
      "step": 176200
    },
    {
      "epoch": 13.810120632931223,
      "grad_norm": 6.209515571594238,
      "learning_rate": 3.849156613922398e-05,
      "loss": 1.7191,
      "step": 176300
    },
    {
      "epoch": 13.817953940153533,
      "grad_norm": 3.5534517765045166,
      "learning_rate": 3.848503838320539e-05,
      "loss": 1.8036,
      "step": 176400
    },
    {
      "epoch": 13.825787247375843,
      "grad_norm": 4.275642395019531,
      "learning_rate": 3.84785106271868e-05,
      "loss": 1.8161,
      "step": 176500
    },
    {
      "epoch": 13.833620554598152,
      "grad_norm": 5.082499980926514,
      "learning_rate": 3.8471982871168214e-05,
      "loss": 1.8186,
      "step": 176600
    },
    {
      "epoch": 13.84145386182046,
      "grad_norm": 7.2834625244140625,
      "learning_rate": 3.846545511514962e-05,
      "loss": 1.6222,
      "step": 176700
    },
    {
      "epoch": 13.84928716904277,
      "grad_norm": 4.814328670501709,
      "learning_rate": 3.8458927359131025e-05,
      "loss": 1.7639,
      "step": 176800
    },
    {
      "epoch": 13.857120476265079,
      "grad_norm": 5.253847122192383,
      "learning_rate": 3.845239960311244e-05,
      "loss": 1.8272,
      "step": 176900
    },
    {
      "epoch": 13.864953783487389,
      "grad_norm": 5.763682842254639,
      "learning_rate": 3.8445871847093843e-05,
      "loss": 1.7998,
      "step": 177000
    },
    {
      "epoch": 13.872787090709698,
      "grad_norm": 5.210670471191406,
      "learning_rate": 3.843934409107525e-05,
      "loss": 1.7895,
      "step": 177100
    },
    {
      "epoch": 13.880620397932006,
      "grad_norm": 5.298028945922852,
      "learning_rate": 3.843281633505666e-05,
      "loss": 1.7891,
      "step": 177200
    },
    {
      "epoch": 13.888453705154316,
      "grad_norm": 5.39259672164917,
      "learning_rate": 3.8426288579038074e-05,
      "loss": 1.7234,
      "step": 177300
    },
    {
      "epoch": 13.896287012376625,
      "grad_norm": 4.67884635925293,
      "learning_rate": 3.841976082301948e-05,
      "loss": 1.7321,
      "step": 177400
    },
    {
      "epoch": 13.904120319598935,
      "grad_norm": 6.006432056427002,
      "learning_rate": 3.841323306700089e-05,
      "loss": 1.7305,
      "step": 177500
    },
    {
      "epoch": 13.911953626821244,
      "grad_norm": 3.907721519470215,
      "learning_rate": 3.84067053109823e-05,
      "loss": 1.9322,
      "step": 177600
    },
    {
      "epoch": 13.919786934043554,
      "grad_norm": 5.366817951202393,
      "learning_rate": 3.8400177554963704e-05,
      "loss": 1.7982,
      "step": 177700
    },
    {
      "epoch": 13.927620241265862,
      "grad_norm": 5.110345840454102,
      "learning_rate": 3.839364979894512e-05,
      "loss": 1.7712,
      "step": 177800
    },
    {
      "epoch": 13.935453548488171,
      "grad_norm": 5.180009365081787,
      "learning_rate": 3.838712204292653e-05,
      "loss": 1.7009,
      "step": 177900
    },
    {
      "epoch": 13.94328685571048,
      "grad_norm": 4.378355503082275,
      "learning_rate": 3.8380594286907935e-05,
      "loss": 1.7478,
      "step": 178000
    },
    {
      "epoch": 13.95112016293279,
      "grad_norm": 4.562031269073486,
      "learning_rate": 3.837406653088934e-05,
      "loss": 1.8294,
      "step": 178100
    },
    {
      "epoch": 13.9589534701551,
      "grad_norm": 4.424117565155029,
      "learning_rate": 3.836753877487075e-05,
      "loss": 1.8526,
      "step": 178200
    },
    {
      "epoch": 13.96678677737741,
      "grad_norm": 5.864861965179443,
      "learning_rate": 3.836101101885216e-05,
      "loss": 1.7815,
      "step": 178300
    },
    {
      "epoch": 13.974620084599717,
      "grad_norm": 4.616199970245361,
      "learning_rate": 3.8354483262833565e-05,
      "loss": 1.7689,
      "step": 178400
    },
    {
      "epoch": 13.982453391822027,
      "grad_norm": 4.94639778137207,
      "learning_rate": 3.8347955506814984e-05,
      "loss": 1.8173,
      "step": 178500
    },
    {
      "epoch": 13.990286699044336,
      "grad_norm": 6.659199237823486,
      "learning_rate": 3.834142775079639e-05,
      "loss": 1.8135,
      "step": 178600
    },
    {
      "epoch": 13.998120006266646,
      "grad_norm": 5.676453590393066,
      "learning_rate": 3.8334899994777796e-05,
      "loss": 1.7533,
      "step": 178700
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8160549402236938,
      "eval_runtime": 1.4215,
      "eval_samples_per_second": 472.736,
      "eval_steps_per_second": 472.736,
      "step": 178724
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.5511199235916138,
      "eval_runtime": 27.0056,
      "eval_samples_per_second": 472.716,
      "eval_steps_per_second": 472.716,
      "step": 178724
    },
    {
      "epoch": 14.005953313488956,
      "grad_norm": 5.271119117736816,
      "learning_rate": 3.832837223875921e-05,
      "loss": 1.7605,
      "step": 178800
    },
    {
      "epoch": 14.013786620711265,
      "grad_norm": 4.3070526123046875,
      "learning_rate": 3.8321844482740614e-05,
      "loss": 1.7768,
      "step": 178900
    },
    {
      "epoch": 14.021619927933573,
      "grad_norm": 6.846783638000488,
      "learning_rate": 3.831531672672202e-05,
      "loss": 1.7396,
      "step": 179000
    },
    {
      "epoch": 14.029453235155882,
      "grad_norm": 7.133199214935303,
      "learning_rate": 3.830878897070343e-05,
      "loss": 1.8143,
      "step": 179100
    },
    {
      "epoch": 14.037286542378192,
      "grad_norm": 4.649750709533691,
      "learning_rate": 3.8302261214684845e-05,
      "loss": 1.7955,
      "step": 179200
    },
    {
      "epoch": 14.045119849600502,
      "grad_norm": 6.194532871246338,
      "learning_rate": 3.829573345866625e-05,
      "loss": 1.7629,
      "step": 179300
    },
    {
      "epoch": 14.052953156822811,
      "grad_norm": 3.9730396270751953,
      "learning_rate": 3.8289205702647656e-05,
      "loss": 1.8894,
      "step": 179400
    },
    {
      "epoch": 14.060786464045119,
      "grad_norm": 6.506685733795166,
      "learning_rate": 3.828267794662907e-05,
      "loss": 1.7262,
      "step": 179500
    },
    {
      "epoch": 14.068619771267429,
      "grad_norm": 4.339197635650635,
      "learning_rate": 3.8276150190610475e-05,
      "loss": 1.7986,
      "step": 179600
    },
    {
      "epoch": 14.076453078489738,
      "grad_norm": 4.2108154296875,
      "learning_rate": 3.826962243459189e-05,
      "loss": 1.6776,
      "step": 179700
    },
    {
      "epoch": 14.084286385712048,
      "grad_norm": 4.928173065185547,
      "learning_rate": 3.82630946785733e-05,
      "loss": 1.8126,
      "step": 179800
    },
    {
      "epoch": 14.092119692934357,
      "grad_norm": 4.977842330932617,
      "learning_rate": 3.8256566922554706e-05,
      "loss": 1.813,
      "step": 179900
    },
    {
      "epoch": 14.099953000156667,
      "grad_norm": 5.332741737365723,
      "learning_rate": 3.825003916653611e-05,
      "loss": 1.7376,
      "step": 180000
    },
    {
      "epoch": 14.107786307378975,
      "grad_norm": 5.456336975097656,
      "learning_rate": 3.8243511410517524e-05,
      "loss": 1.7508,
      "step": 180100
    },
    {
      "epoch": 14.115619614601284,
      "grad_norm": 4.154376983642578,
      "learning_rate": 3.823698365449893e-05,
      "loss": 1.8321,
      "step": 180200
    },
    {
      "epoch": 14.123452921823594,
      "grad_norm": 6.068188667297363,
      "learning_rate": 3.8230455898480335e-05,
      "loss": 1.798,
      "step": 180300
    },
    {
      "epoch": 14.131286229045903,
      "grad_norm": 5.249013423919678,
      "learning_rate": 3.822392814246175e-05,
      "loss": 1.8335,
      "step": 180400
    },
    {
      "epoch": 14.139119536268213,
      "grad_norm": 5.0737833976745605,
      "learning_rate": 3.821740038644316e-05,
      "loss": 1.8131,
      "step": 180500
    },
    {
      "epoch": 14.146952843490523,
      "grad_norm": 6.480734825134277,
      "learning_rate": 3.8210872630424566e-05,
      "loss": 1.7985,
      "step": 180600
    },
    {
      "epoch": 14.15478615071283,
      "grad_norm": 4.990756511688232,
      "learning_rate": 3.820434487440598e-05,
      "loss": 1.7629,
      "step": 180700
    },
    {
      "epoch": 14.16261945793514,
      "grad_norm": 3.9523141384124756,
      "learning_rate": 3.8197817118387385e-05,
      "loss": 1.8149,
      "step": 180800
    },
    {
      "epoch": 14.17045276515745,
      "grad_norm": 4.970745086669922,
      "learning_rate": 3.819128936236879e-05,
      "loss": 1.7719,
      "step": 180900
    },
    {
      "epoch": 14.178286072379759,
      "grad_norm": 5.252925395965576,
      "learning_rate": 3.81847616063502e-05,
      "loss": 1.7264,
      "step": 181000
    },
    {
      "epoch": 14.186119379602069,
      "grad_norm": 5.192651748657227,
      "learning_rate": 3.8178233850331616e-05,
      "loss": 1.7455,
      "step": 181100
    },
    {
      "epoch": 14.193952686824376,
      "grad_norm": 6.069540977478027,
      "learning_rate": 3.817170609431302e-05,
      "loss": 1.7702,
      "step": 181200
    },
    {
      "epoch": 14.201785994046686,
      "grad_norm": 5.89685583114624,
      "learning_rate": 3.816517833829443e-05,
      "loss": 1.7024,
      "step": 181300
    },
    {
      "epoch": 14.209619301268996,
      "grad_norm": 5.318909645080566,
      "learning_rate": 3.815865058227584e-05,
      "loss": 1.8899,
      "step": 181400
    },
    {
      "epoch": 14.217452608491305,
      "grad_norm": 3.474240779876709,
      "learning_rate": 3.8152122826257245e-05,
      "loss": 1.7764,
      "step": 181500
    },
    {
      "epoch": 14.225285915713615,
      "grad_norm": 5.0746989250183105,
      "learning_rate": 3.814559507023865e-05,
      "loss": 1.664,
      "step": 181600
    },
    {
      "epoch": 14.233119222935924,
      "grad_norm": 7.184903144836426,
      "learning_rate": 3.813906731422007e-05,
      "loss": 1.7216,
      "step": 181700
    },
    {
      "epoch": 14.240952530158232,
      "grad_norm": 5.392742156982422,
      "learning_rate": 3.8132539558201476e-05,
      "loss": 1.6557,
      "step": 181800
    },
    {
      "epoch": 14.248785837380542,
      "grad_norm": 4.761483669281006,
      "learning_rate": 3.812601180218288e-05,
      "loss": 1.7706,
      "step": 181900
    },
    {
      "epoch": 14.256619144602851,
      "grad_norm": 4.383246421813965,
      "learning_rate": 3.8119484046164295e-05,
      "loss": 1.8006,
      "step": 182000
    },
    {
      "epoch": 14.26445245182516,
      "grad_norm": 4.45457124710083,
      "learning_rate": 3.81129562901457e-05,
      "loss": 1.7134,
      "step": 182100
    },
    {
      "epoch": 14.27228575904747,
      "grad_norm": 4.949073314666748,
      "learning_rate": 3.8106428534127106e-05,
      "loss": 1.7536,
      "step": 182200
    },
    {
      "epoch": 14.28011906626978,
      "grad_norm": 4.787013530731201,
      "learning_rate": 3.809990077810852e-05,
      "loss": 1.8019,
      "step": 182300
    },
    {
      "epoch": 14.287952373492088,
      "grad_norm": 4.970616817474365,
      "learning_rate": 3.809337302208993e-05,
      "loss": 1.8458,
      "step": 182400
    },
    {
      "epoch": 14.295785680714397,
      "grad_norm": 8.446898460388184,
      "learning_rate": 3.808684526607134e-05,
      "loss": 1.7152,
      "step": 182500
    },
    {
      "epoch": 14.303618987936707,
      "grad_norm": 3.975152015686035,
      "learning_rate": 3.808031751005275e-05,
      "loss": 1.7045,
      "step": 182600
    },
    {
      "epoch": 14.311452295159016,
      "grad_norm": 7.344936370849609,
      "learning_rate": 3.8073789754034155e-05,
      "loss": 1.8074,
      "step": 182700
    },
    {
      "epoch": 14.319285602381326,
      "grad_norm": 7.075960636138916,
      "learning_rate": 3.806726199801556e-05,
      "loss": 1.6674,
      "step": 182800
    },
    {
      "epoch": 14.327118909603634,
      "grad_norm": 5.518989086151123,
      "learning_rate": 3.8060734241996974e-05,
      "loss": 1.7478,
      "step": 182900
    },
    {
      "epoch": 14.334952216825943,
      "grad_norm": 5.305008411407471,
      "learning_rate": 3.8054206485978386e-05,
      "loss": 1.7425,
      "step": 183000
    },
    {
      "epoch": 14.342785524048253,
      "grad_norm": 4.436294078826904,
      "learning_rate": 3.804767872995979e-05,
      "loss": 1.7011,
      "step": 183100
    },
    {
      "epoch": 14.350618831270562,
      "grad_norm": 3.437472343444824,
      "learning_rate": 3.80411509739412e-05,
      "loss": 1.8021,
      "step": 183200
    },
    {
      "epoch": 14.358452138492872,
      "grad_norm": 5.661198616027832,
      "learning_rate": 3.803462321792261e-05,
      "loss": 1.7474,
      "step": 183300
    },
    {
      "epoch": 14.366285445715182,
      "grad_norm": 4.250112533569336,
      "learning_rate": 3.8028095461904016e-05,
      "loss": 1.7769,
      "step": 183400
    },
    {
      "epoch": 14.37411875293749,
      "grad_norm": 5.055974960327148,
      "learning_rate": 3.802156770588542e-05,
      "loss": 1.7904,
      "step": 183500
    },
    {
      "epoch": 14.381952060159799,
      "grad_norm": 3.3177435398101807,
      "learning_rate": 3.8015039949866834e-05,
      "loss": 1.7172,
      "step": 183600
    },
    {
      "epoch": 14.389785367382109,
      "grad_norm": 5.3993449211120605,
      "learning_rate": 3.800851219384825e-05,
      "loss": 1.8047,
      "step": 183700
    },
    {
      "epoch": 14.397618674604418,
      "grad_norm": 3.7711963653564453,
      "learning_rate": 3.800198443782965e-05,
      "loss": 1.7698,
      "step": 183800
    },
    {
      "epoch": 14.405451981826728,
      "grad_norm": 4.314960956573486,
      "learning_rate": 3.7995456681811065e-05,
      "loss": 1.8181,
      "step": 183900
    },
    {
      "epoch": 14.413285289049037,
      "grad_norm": 4.095893859863281,
      "learning_rate": 3.798892892579247e-05,
      "loss": 1.758,
      "step": 184000
    },
    {
      "epoch": 14.421118596271345,
      "grad_norm": 4.741422176361084,
      "learning_rate": 3.798240116977388e-05,
      "loss": 1.8708,
      "step": 184100
    },
    {
      "epoch": 14.428951903493655,
      "grad_norm": 6.1657633781433105,
      "learning_rate": 3.797587341375529e-05,
      "loss": 1.6976,
      "step": 184200
    },
    {
      "epoch": 14.436785210715964,
      "grad_norm": 4.547234058380127,
      "learning_rate": 3.79693456577367e-05,
      "loss": 1.8018,
      "step": 184300
    },
    {
      "epoch": 14.444618517938274,
      "grad_norm": 4.752221584320068,
      "learning_rate": 3.796281790171811e-05,
      "loss": 1.8042,
      "step": 184400
    },
    {
      "epoch": 14.452451825160583,
      "grad_norm": 6.8755974769592285,
      "learning_rate": 3.795629014569951e-05,
      "loss": 1.7658,
      "step": 184500
    },
    {
      "epoch": 14.460285132382893,
      "grad_norm": 5.3890533447265625,
      "learning_rate": 3.7949762389680926e-05,
      "loss": 1.7457,
      "step": 184600
    },
    {
      "epoch": 14.4681184396052,
      "grad_norm": 7.513927936553955,
      "learning_rate": 3.794323463366233e-05,
      "loss": 1.7825,
      "step": 184700
    },
    {
      "epoch": 14.47595174682751,
      "grad_norm": 4.733928680419922,
      "learning_rate": 3.793670687764374e-05,
      "loss": 1.8134,
      "step": 184800
    },
    {
      "epoch": 14.48378505404982,
      "grad_norm": 6.727248668670654,
      "learning_rate": 3.793017912162516e-05,
      "loss": 1.7983,
      "step": 184900
    },
    {
      "epoch": 14.49161836127213,
      "grad_norm": 5.537807464599609,
      "learning_rate": 3.792365136560656e-05,
      "loss": 1.7128,
      "step": 185000
    },
    {
      "epoch": 14.499451668494439,
      "grad_norm": 7.866410255432129,
      "learning_rate": 3.791712360958797e-05,
      "loss": 1.8175,
      "step": 185100
    },
    {
      "epoch": 14.507284975716747,
      "grad_norm": 5.056429862976074,
      "learning_rate": 3.791059585356938e-05,
      "loss": 1.6415,
      "step": 185200
    },
    {
      "epoch": 14.515118282939056,
      "grad_norm": 4.6611833572387695,
      "learning_rate": 3.7904068097550787e-05,
      "loss": 1.7866,
      "step": 185300
    },
    {
      "epoch": 14.522951590161366,
      "grad_norm": 5.45473575592041,
      "learning_rate": 3.789754034153219e-05,
      "loss": 1.7757,
      "step": 185400
    },
    {
      "epoch": 14.530784897383676,
      "grad_norm": 5.7817864418029785,
      "learning_rate": 3.7891012585513605e-05,
      "loss": 1.7286,
      "step": 185500
    },
    {
      "epoch": 14.538618204605985,
      "grad_norm": 4.535912990570068,
      "learning_rate": 3.788448482949502e-05,
      "loss": 1.7893,
      "step": 185600
    },
    {
      "epoch": 14.546451511828295,
      "grad_norm": 4.111808776855469,
      "learning_rate": 3.787795707347642e-05,
      "loss": 1.7897,
      "step": 185700
    },
    {
      "epoch": 14.554284819050602,
      "grad_norm": 3.997199296951294,
      "learning_rate": 3.7871429317457836e-05,
      "loss": 1.7194,
      "step": 185800
    },
    {
      "epoch": 14.562118126272912,
      "grad_norm": 5.685548782348633,
      "learning_rate": 3.786490156143924e-05,
      "loss": 1.8082,
      "step": 185900
    },
    {
      "epoch": 14.569951433495222,
      "grad_norm": 6.209965705871582,
      "learning_rate": 3.785837380542065e-05,
      "loss": 1.7504,
      "step": 186000
    },
    {
      "epoch": 14.577784740717531,
      "grad_norm": 5.792181015014648,
      "learning_rate": 3.785184604940206e-05,
      "loss": 1.7614,
      "step": 186100
    },
    {
      "epoch": 14.58561804793984,
      "grad_norm": 5.807362079620361,
      "learning_rate": 3.784531829338347e-05,
      "loss": 1.8278,
      "step": 186200
    },
    {
      "epoch": 14.593451355162149,
      "grad_norm": 5.533743381500244,
      "learning_rate": 3.783879053736488e-05,
      "loss": 1.8265,
      "step": 186300
    },
    {
      "epoch": 14.601284662384458,
      "grad_norm": 4.696156978607178,
      "learning_rate": 3.7832262781346284e-05,
      "loss": 1.8126,
      "step": 186400
    },
    {
      "epoch": 14.609117969606768,
      "grad_norm": 5.158867359161377,
      "learning_rate": 3.7825735025327696e-05,
      "loss": 1.8163,
      "step": 186500
    },
    {
      "epoch": 14.616951276829077,
      "grad_norm": 6.636077880859375,
      "learning_rate": 3.78192072693091e-05,
      "loss": 1.7041,
      "step": 186600
    },
    {
      "epoch": 14.624784584051387,
      "grad_norm": 6.753129959106445,
      "learning_rate": 3.781267951329051e-05,
      "loss": 1.8159,
      "step": 186700
    },
    {
      "epoch": 14.632617891273696,
      "grad_norm": 3.965667724609375,
      "learning_rate": 3.780615175727192e-05,
      "loss": 1.77,
      "step": 186800
    },
    {
      "epoch": 14.640451198496004,
      "grad_norm": 4.7188334465026855,
      "learning_rate": 3.779962400125333e-05,
      "loss": 1.7625,
      "step": 186900
    },
    {
      "epoch": 14.648284505718314,
      "grad_norm": 4.949457168579102,
      "learning_rate": 3.779309624523474e-05,
      "loss": 1.6941,
      "step": 187000
    },
    {
      "epoch": 14.656117812940623,
      "grad_norm": 7.498340129852295,
      "learning_rate": 3.778656848921615e-05,
      "loss": 1.8134,
      "step": 187100
    },
    {
      "epoch": 14.663951120162933,
      "grad_norm": 5.282736301422119,
      "learning_rate": 3.778004073319756e-05,
      "loss": 1.7587,
      "step": 187200
    },
    {
      "epoch": 14.671784427385242,
      "grad_norm": 4.604700088500977,
      "learning_rate": 3.777351297717896e-05,
      "loss": 1.8652,
      "step": 187300
    },
    {
      "epoch": 14.679617734607552,
      "grad_norm": 3.936269760131836,
      "learning_rate": 3.7766985221160375e-05,
      "loss": 1.8135,
      "step": 187400
    },
    {
      "epoch": 14.68745104182986,
      "grad_norm": 5.924862384796143,
      "learning_rate": 3.776045746514179e-05,
      "loss": 1.8116,
      "step": 187500
    },
    {
      "epoch": 14.69528434905217,
      "grad_norm": 5.4311017990112305,
      "learning_rate": 3.7753929709123194e-05,
      "loss": 1.7491,
      "step": 187600
    },
    {
      "epoch": 14.703117656274479,
      "grad_norm": 5.770432472229004,
      "learning_rate": 3.7747401953104606e-05,
      "loss": 1.7864,
      "step": 187700
    },
    {
      "epoch": 14.710950963496789,
      "grad_norm": 5.893024921417236,
      "learning_rate": 3.774087419708601e-05,
      "loss": 1.7753,
      "step": 187800
    },
    {
      "epoch": 14.718784270719098,
      "grad_norm": 5.692975044250488,
      "learning_rate": 3.773434644106742e-05,
      "loss": 1.7743,
      "step": 187900
    },
    {
      "epoch": 14.726617577941408,
      "grad_norm": 5.324222564697266,
      "learning_rate": 3.7727818685048824e-05,
      "loss": 1.6693,
      "step": 188000
    },
    {
      "epoch": 14.734450885163715,
      "grad_norm": 5.326952934265137,
      "learning_rate": 3.772129092903024e-05,
      "loss": 1.8425,
      "step": 188100
    },
    {
      "epoch": 14.742284192386025,
      "grad_norm": 4.265522003173828,
      "learning_rate": 3.771476317301165e-05,
      "loss": 1.7337,
      "step": 188200
    },
    {
      "epoch": 14.750117499608335,
      "grad_norm": 5.529812335968018,
      "learning_rate": 3.7708235416993054e-05,
      "loss": 1.8333,
      "step": 188300
    },
    {
      "epoch": 14.757950806830644,
      "grad_norm": 5.963261127471924,
      "learning_rate": 3.770170766097447e-05,
      "loss": 1.8043,
      "step": 188400
    },
    {
      "epoch": 14.765784114052954,
      "grad_norm": 5.2266058921813965,
      "learning_rate": 3.769517990495587e-05,
      "loss": 1.852,
      "step": 188500
    },
    {
      "epoch": 14.773617421275262,
      "grad_norm": 6.5124359130859375,
      "learning_rate": 3.768865214893728e-05,
      "loss": 1.8251,
      "step": 188600
    },
    {
      "epoch": 14.781450728497571,
      "grad_norm": 5.567023277282715,
      "learning_rate": 3.768212439291869e-05,
      "loss": 1.7545,
      "step": 188700
    },
    {
      "epoch": 14.78928403571988,
      "grad_norm": 4.980406761169434,
      "learning_rate": 3.7675596636900104e-05,
      "loss": 1.8205,
      "step": 188800
    },
    {
      "epoch": 14.79711734294219,
      "grad_norm": 4.356911659240723,
      "learning_rate": 3.766906888088151e-05,
      "loss": 1.8502,
      "step": 188900
    },
    {
      "epoch": 14.8049506501645,
      "grad_norm": 9.074459075927734,
      "learning_rate": 3.766254112486292e-05,
      "loss": 1.7134,
      "step": 189000
    },
    {
      "epoch": 14.81278395738681,
      "grad_norm": 5.43626594543457,
      "learning_rate": 3.765601336884433e-05,
      "loss": 1.778,
      "step": 189100
    },
    {
      "epoch": 14.820617264609117,
      "grad_norm": 4.670224666595459,
      "learning_rate": 3.7649485612825733e-05,
      "loss": 1.7416,
      "step": 189200
    },
    {
      "epoch": 14.828450571831427,
      "grad_norm": 5.805469036102295,
      "learning_rate": 3.7642957856807146e-05,
      "loss": 1.8119,
      "step": 189300
    },
    {
      "epoch": 14.836283879053736,
      "grad_norm": 3.765143871307373,
      "learning_rate": 3.763643010078856e-05,
      "loss": 1.7909,
      "step": 189400
    },
    {
      "epoch": 14.844117186276046,
      "grad_norm": 4.392791748046875,
      "learning_rate": 3.7629902344769964e-05,
      "loss": 1.8637,
      "step": 189500
    },
    {
      "epoch": 14.851950493498355,
      "grad_norm": 5.27096700668335,
      "learning_rate": 3.762337458875137e-05,
      "loss": 1.7653,
      "step": 189600
    },
    {
      "epoch": 14.859783800720665,
      "grad_norm": 5.694072723388672,
      "learning_rate": 3.761684683273278e-05,
      "loss": 1.7764,
      "step": 189700
    },
    {
      "epoch": 14.867617107942973,
      "grad_norm": 5.380573749542236,
      "learning_rate": 3.761031907671419e-05,
      "loss": 1.7698,
      "step": 189800
    },
    {
      "epoch": 14.875450415165282,
      "grad_norm": 6.541125774383545,
      "learning_rate": 3.7603791320695594e-05,
      "loss": 1.8481,
      "step": 189900
    },
    {
      "epoch": 14.883283722387592,
      "grad_norm": 6.290549278259277,
      "learning_rate": 3.759726356467701e-05,
      "loss": 1.7947,
      "step": 190000
    },
    {
      "epoch": 14.891117029609902,
      "grad_norm": 5.985670566558838,
      "learning_rate": 3.759073580865842e-05,
      "loss": 1.8033,
      "step": 190100
    },
    {
      "epoch": 14.898950336832211,
      "grad_norm": 4.952131271362305,
      "learning_rate": 3.7584208052639825e-05,
      "loss": 1.7327,
      "step": 190200
    },
    {
      "epoch": 14.90678364405452,
      "grad_norm": 5.331470489501953,
      "learning_rate": 3.757768029662124e-05,
      "loss": 1.8236,
      "step": 190300
    },
    {
      "epoch": 14.914616951276829,
      "grad_norm": 4.8639655113220215,
      "learning_rate": 3.757115254060264e-05,
      "loss": 1.7877,
      "step": 190400
    },
    {
      "epoch": 14.922450258499138,
      "grad_norm": 7.5524139404296875,
      "learning_rate": 3.756462478458405e-05,
      "loss": 1.7471,
      "step": 190500
    },
    {
      "epoch": 14.930283565721448,
      "grad_norm": 4.512792587280273,
      "learning_rate": 3.755809702856546e-05,
      "loss": 1.9305,
      "step": 190600
    },
    {
      "epoch": 14.938116872943757,
      "grad_norm": 5.308772087097168,
      "learning_rate": 3.7551569272546874e-05,
      "loss": 1.797,
      "step": 190700
    },
    {
      "epoch": 14.945950180166067,
      "grad_norm": 5.753274917602539,
      "learning_rate": 3.754504151652828e-05,
      "loss": 1.788,
      "step": 190800
    },
    {
      "epoch": 14.953783487388375,
      "grad_norm": 4.937185764312744,
      "learning_rate": 3.753851376050969e-05,
      "loss": 1.7981,
      "step": 190900
    },
    {
      "epoch": 14.961616794610684,
      "grad_norm": 4.508702278137207,
      "learning_rate": 3.75319860044911e-05,
      "loss": 1.6573,
      "step": 191000
    },
    {
      "epoch": 14.969450101832994,
      "grad_norm": 4.569511890411377,
      "learning_rate": 3.7525458248472504e-05,
      "loss": 1.8742,
      "step": 191100
    },
    {
      "epoch": 14.977283409055303,
      "grad_norm": 5.165747165679932,
      "learning_rate": 3.751893049245391e-05,
      "loss": 1.7514,
      "step": 191200
    },
    {
      "epoch": 14.985116716277613,
      "grad_norm": 7.754812717437744,
      "learning_rate": 3.751240273643533e-05,
      "loss": 1.7758,
      "step": 191300
    },
    {
      "epoch": 14.992950023499922,
      "grad_norm": 5.113643646240234,
      "learning_rate": 3.7505874980416735e-05,
      "loss": 1.8868,
      "step": 191400
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.8172777891159058,
      "eval_runtime": 1.4344,
      "eval_samples_per_second": 468.499,
      "eval_steps_per_second": 468.499,
      "step": 191490
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.5490705966949463,
      "eval_runtime": 26.9968,
      "eval_samples_per_second": 472.871,
      "eval_steps_per_second": 472.871,
      "step": 191490
    },
    {
      "epoch": 15.00078333072223,
      "grad_norm": 6.234717845916748,
      "learning_rate": 3.749934722439814e-05,
      "loss": 1.74,
      "step": 191500
    },
    {
      "epoch": 15.00861663794454,
      "grad_norm": 5.4733381271362305,
      "learning_rate": 3.749281946837955e-05,
      "loss": 1.7812,
      "step": 191600
    },
    {
      "epoch": 15.01644994516685,
      "grad_norm": 5.58595085144043,
      "learning_rate": 3.748629171236096e-05,
      "loss": 1.753,
      "step": 191700
    },
    {
      "epoch": 15.024283252389159,
      "grad_norm": 5.871858596801758,
      "learning_rate": 3.7479763956342365e-05,
      "loss": 1.7266,
      "step": 191800
    },
    {
      "epoch": 15.032116559611469,
      "grad_norm": 5.164860248565674,
      "learning_rate": 3.747323620032378e-05,
      "loss": 1.7188,
      "step": 191900
    },
    {
      "epoch": 15.039949866833778,
      "grad_norm": 3.822608232498169,
      "learning_rate": 3.746670844430519e-05,
      "loss": 1.8062,
      "step": 192000
    },
    {
      "epoch": 15.047783174056086,
      "grad_norm": 4.114007949829102,
      "learning_rate": 3.7460180688286596e-05,
      "loss": 1.7437,
      "step": 192100
    },
    {
      "epoch": 15.055616481278395,
      "grad_norm": 6.170637607574463,
      "learning_rate": 3.745365293226801e-05,
      "loss": 1.8116,
      "step": 192200
    },
    {
      "epoch": 15.063449788500705,
      "grad_norm": 5.117210865020752,
      "learning_rate": 3.7447125176249414e-05,
      "loss": 1.738,
      "step": 192300
    },
    {
      "epoch": 15.071283095723015,
      "grad_norm": 5.361043930053711,
      "learning_rate": 3.744059742023082e-05,
      "loss": 1.7495,
      "step": 192400
    },
    {
      "epoch": 15.079116402945324,
      "grad_norm": 5.759314060211182,
      "learning_rate": 3.743406966421223e-05,
      "loss": 1.7582,
      "step": 192500
    },
    {
      "epoch": 15.086949710167632,
      "grad_norm": 5.5003981590271,
      "learning_rate": 3.7427541908193645e-05,
      "loss": 1.809,
      "step": 192600
    },
    {
      "epoch": 15.094783017389942,
      "grad_norm": 4.793831825256348,
      "learning_rate": 3.742101415217505e-05,
      "loss": 1.7749,
      "step": 192700
    },
    {
      "epoch": 15.102616324612251,
      "grad_norm": 6.670733451843262,
      "learning_rate": 3.741448639615646e-05,
      "loss": 1.8032,
      "step": 192800
    },
    {
      "epoch": 15.11044963183456,
      "grad_norm": 4.141787052154541,
      "learning_rate": 3.740795864013787e-05,
      "loss": 1.6834,
      "step": 192900
    },
    {
      "epoch": 15.11828293905687,
      "grad_norm": 5.006453990936279,
      "learning_rate": 3.7401430884119275e-05,
      "loss": 1.7403,
      "step": 193000
    },
    {
      "epoch": 15.12611624627918,
      "grad_norm": 3.957455635070801,
      "learning_rate": 3.739490312810068e-05,
      "loss": 1.7397,
      "step": 193100
    },
    {
      "epoch": 15.133949553501488,
      "grad_norm": 5.446768283843994,
      "learning_rate": 3.738837537208209e-05,
      "loss": 1.703,
      "step": 193200
    },
    {
      "epoch": 15.141782860723797,
      "grad_norm": 5.372135162353516,
      "learning_rate": 3.7381847616063505e-05,
      "loss": 1.75,
      "step": 193300
    },
    {
      "epoch": 15.149616167946107,
      "grad_norm": 4.42007303237915,
      "learning_rate": 3.737531986004491e-05,
      "loss": 1.7206,
      "step": 193400
    },
    {
      "epoch": 15.157449475168416,
      "grad_norm": 4.33490514755249,
      "learning_rate": 3.7368792104026324e-05,
      "loss": 1.782,
      "step": 193500
    },
    {
      "epoch": 15.165282782390726,
      "grad_norm": 4.972123622894287,
      "learning_rate": 3.736226434800773e-05,
      "loss": 1.7973,
      "step": 193600
    },
    {
      "epoch": 15.173116089613035,
      "grad_norm": 3.5294253826141357,
      "learning_rate": 3.7355736591989135e-05,
      "loss": 1.7089,
      "step": 193700
    },
    {
      "epoch": 15.180949396835343,
      "grad_norm": 5.654325485229492,
      "learning_rate": 3.734920883597055e-05,
      "loss": 1.7756,
      "step": 193800
    },
    {
      "epoch": 15.188782704057653,
      "grad_norm": 5.367505073547363,
      "learning_rate": 3.734268107995196e-05,
      "loss": 1.7249,
      "step": 193900
    },
    {
      "epoch": 15.196616011279962,
      "grad_norm": 5.920363903045654,
      "learning_rate": 3.7336153323933366e-05,
      "loss": 1.7815,
      "step": 194000
    },
    {
      "epoch": 15.204449318502272,
      "grad_norm": 5.00240421295166,
      "learning_rate": 3.732962556791478e-05,
      "loss": 1.8234,
      "step": 194100
    },
    {
      "epoch": 15.212282625724582,
      "grad_norm": 5.3689799308776855,
      "learning_rate": 3.7323097811896185e-05,
      "loss": 1.7866,
      "step": 194200
    },
    {
      "epoch": 15.22011593294689,
      "grad_norm": 5.211946487426758,
      "learning_rate": 3.731657005587759e-05,
      "loss": 1.7597,
      "step": 194300
    },
    {
      "epoch": 15.227949240169199,
      "grad_norm": 5.425889015197754,
      "learning_rate": 3.7310042299859e-05,
      "loss": 1.7659,
      "step": 194400
    },
    {
      "epoch": 15.235782547391509,
      "grad_norm": 6.4313435554504395,
      "learning_rate": 3.7303514543840415e-05,
      "loss": 1.6862,
      "step": 194500
    },
    {
      "epoch": 15.243615854613818,
      "grad_norm": 7.646506309509277,
      "learning_rate": 3.729698678782182e-05,
      "loss": 1.8787,
      "step": 194600
    },
    {
      "epoch": 15.251449161836128,
      "grad_norm": 5.69332218170166,
      "learning_rate": 3.729045903180323e-05,
      "loss": 1.8114,
      "step": 194700
    },
    {
      "epoch": 15.259282469058437,
      "grad_norm": 4.5331807136535645,
      "learning_rate": 3.728393127578464e-05,
      "loss": 1.8318,
      "step": 194800
    },
    {
      "epoch": 15.267115776280745,
      "grad_norm": 4.448832988739014,
      "learning_rate": 3.7277403519766045e-05,
      "loss": 1.7916,
      "step": 194900
    },
    {
      "epoch": 15.274949083503055,
      "grad_norm": 6.326939582824707,
      "learning_rate": 3.727087576374745e-05,
      "loss": 1.7379,
      "step": 195000
    },
    {
      "epoch": 15.282782390725364,
      "grad_norm": 5.295870304107666,
      "learning_rate": 3.7264348007728864e-05,
      "loss": 1.8683,
      "step": 195100
    },
    {
      "epoch": 15.290615697947674,
      "grad_norm": 5.068219184875488,
      "learning_rate": 3.7257820251710276e-05,
      "loss": 1.7775,
      "step": 195200
    },
    {
      "epoch": 15.298449005169983,
      "grad_norm": 4.603160858154297,
      "learning_rate": 3.725129249569168e-05,
      "loss": 1.8328,
      "step": 195300
    },
    {
      "epoch": 15.306282312392293,
      "grad_norm": 5.014862537384033,
      "learning_rate": 3.7244764739673094e-05,
      "loss": 1.6969,
      "step": 195400
    },
    {
      "epoch": 15.3141156196146,
      "grad_norm": 5.0626020431518555,
      "learning_rate": 3.72382369836545e-05,
      "loss": 1.7491,
      "step": 195500
    },
    {
      "epoch": 15.32194892683691,
      "grad_norm": 4.628351211547852,
      "learning_rate": 3.7231709227635906e-05,
      "loss": 1.7892,
      "step": 195600
    },
    {
      "epoch": 15.32978223405922,
      "grad_norm": 6.573256492614746,
      "learning_rate": 3.722518147161732e-05,
      "loss": 1.7401,
      "step": 195700
    },
    {
      "epoch": 15.33761554128153,
      "grad_norm": 4.608582496643066,
      "learning_rate": 3.721865371559873e-05,
      "loss": 1.8132,
      "step": 195800
    },
    {
      "epoch": 15.345448848503839,
      "grad_norm": 4.856846809387207,
      "learning_rate": 3.721212595958014e-05,
      "loss": 1.7495,
      "step": 195900
    },
    {
      "epoch": 15.353282155726147,
      "grad_norm": 6.095139026641846,
      "learning_rate": 3.720559820356155e-05,
      "loss": 1.7864,
      "step": 196000
    },
    {
      "epoch": 15.361115462948456,
      "grad_norm": 4.340642929077148,
      "learning_rate": 3.7199070447542955e-05,
      "loss": 1.8248,
      "step": 196100
    },
    {
      "epoch": 15.368948770170766,
      "grad_norm": 5.92629861831665,
      "learning_rate": 3.719254269152436e-05,
      "loss": 1.8061,
      "step": 196200
    },
    {
      "epoch": 15.376782077393075,
      "grad_norm": 2.6751136779785156,
      "learning_rate": 3.718601493550577e-05,
      "loss": 1.7351,
      "step": 196300
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 6.066078186035156,
      "learning_rate": 3.717948717948718e-05,
      "loss": 1.8203,
      "step": 196400
    },
    {
      "epoch": 15.392448691837695,
      "grad_norm": 3.453449010848999,
      "learning_rate": 3.717295942346859e-05,
      "loss": 1.7032,
      "step": 196500
    },
    {
      "epoch": 15.400281999060002,
      "grad_norm": 6.237548828125,
      "learning_rate": 3.716643166745e-05,
      "loss": 1.7165,
      "step": 196600
    },
    {
      "epoch": 15.408115306282312,
      "grad_norm": 5.525651931762695,
      "learning_rate": 3.715990391143141e-05,
      "loss": 1.8047,
      "step": 196700
    },
    {
      "epoch": 15.415948613504622,
      "grad_norm": 4.71232271194458,
      "learning_rate": 3.7153376155412816e-05,
      "loss": 1.7361,
      "step": 196800
    },
    {
      "epoch": 15.423781920726931,
      "grad_norm": 4.236047744750977,
      "learning_rate": 3.714684839939422e-05,
      "loss": 1.8596,
      "step": 196900
    },
    {
      "epoch": 15.43161522794924,
      "grad_norm": 5.273618698120117,
      "learning_rate": 3.7140320643375634e-05,
      "loss": 1.799,
      "step": 197000
    },
    {
      "epoch": 15.43944853517155,
      "grad_norm": 4.199822425842285,
      "learning_rate": 3.713379288735705e-05,
      "loss": 1.745,
      "step": 197100
    },
    {
      "epoch": 15.447281842393858,
      "grad_norm": 5.688235759735107,
      "learning_rate": 3.712726513133845e-05,
      "loss": 1.8224,
      "step": 197200
    },
    {
      "epoch": 15.455115149616168,
      "grad_norm": 5.540764331817627,
      "learning_rate": 3.7120737375319865e-05,
      "loss": 1.772,
      "step": 197300
    },
    {
      "epoch": 15.462948456838477,
      "grad_norm": 5.970554828643799,
      "learning_rate": 3.711420961930127e-05,
      "loss": 1.7574,
      "step": 197400
    },
    {
      "epoch": 15.470781764060787,
      "grad_norm": 4.540746688842773,
      "learning_rate": 3.7107681863282677e-05,
      "loss": 1.8003,
      "step": 197500
    },
    {
      "epoch": 15.478615071283096,
      "grad_norm": 6.778101921081543,
      "learning_rate": 3.710115410726409e-05,
      "loss": 1.7592,
      "step": 197600
    },
    {
      "epoch": 15.486448378505404,
      "grad_norm": 4.729541301727295,
      "learning_rate": 3.70946263512455e-05,
      "loss": 1.7573,
      "step": 197700
    },
    {
      "epoch": 15.494281685727714,
      "grad_norm": 4.379539966583252,
      "learning_rate": 3.708809859522691e-05,
      "loss": 1.7846,
      "step": 197800
    },
    {
      "epoch": 15.502114992950023,
      "grad_norm": 5.63214635848999,
      "learning_rate": 3.708157083920831e-05,
      "loss": 1.7526,
      "step": 197900
    },
    {
      "epoch": 15.509948300172333,
      "grad_norm": 7.461489200592041,
      "learning_rate": 3.7075043083189726e-05,
      "loss": 1.7044,
      "step": 198000
    },
    {
      "epoch": 15.517781607394642,
      "grad_norm": 4.994558334350586,
      "learning_rate": 3.706851532717113e-05,
      "loss": 1.7877,
      "step": 198100
    },
    {
      "epoch": 15.525614914616952,
      "grad_norm": 4.20977783203125,
      "learning_rate": 3.706198757115254e-05,
      "loss": 1.8833,
      "step": 198200
    },
    {
      "epoch": 15.53344822183926,
      "grad_norm": 4.481426239013672,
      "learning_rate": 3.705545981513395e-05,
      "loss": 1.6748,
      "step": 198300
    },
    {
      "epoch": 15.54128152906157,
      "grad_norm": 5.293956279754639,
      "learning_rate": 3.704893205911536e-05,
      "loss": 1.7731,
      "step": 198400
    },
    {
      "epoch": 15.549114836283879,
      "grad_norm": 5.763190746307373,
      "learning_rate": 3.704240430309677e-05,
      "loss": 1.6882,
      "step": 198500
    },
    {
      "epoch": 15.556948143506188,
      "grad_norm": 6.252686977386475,
      "learning_rate": 3.703587654707818e-05,
      "loss": 1.7337,
      "step": 198600
    },
    {
      "epoch": 15.564781450728498,
      "grad_norm": 5.500396251678467,
      "learning_rate": 3.7029348791059586e-05,
      "loss": 1.7543,
      "step": 198700
    },
    {
      "epoch": 15.572614757950808,
      "grad_norm": 5.065734386444092,
      "learning_rate": 3.702282103504099e-05,
      "loss": 1.7983,
      "step": 198800
    },
    {
      "epoch": 15.580448065173115,
      "grad_norm": 5.315433979034424,
      "learning_rate": 3.7016293279022405e-05,
      "loss": 1.6727,
      "step": 198900
    },
    {
      "epoch": 15.588281372395425,
      "grad_norm": 4.571102142333984,
      "learning_rate": 3.700976552300382e-05,
      "loss": 1.6987,
      "step": 199000
    },
    {
      "epoch": 15.596114679617735,
      "grad_norm": 4.974903106689453,
      "learning_rate": 3.700323776698522e-05,
      "loss": 1.7681,
      "step": 199100
    },
    {
      "epoch": 15.603947986840044,
      "grad_norm": 4.608024597167969,
      "learning_rate": 3.6996710010966636e-05,
      "loss": 1.798,
      "step": 199200
    },
    {
      "epoch": 15.611781294062354,
      "grad_norm": 4.041510581970215,
      "learning_rate": 3.699018225494804e-05,
      "loss": 1.7983,
      "step": 199300
    },
    {
      "epoch": 15.619614601284663,
      "grad_norm": 6.280375003814697,
      "learning_rate": 3.698365449892945e-05,
      "loss": 1.7804,
      "step": 199400
    },
    {
      "epoch": 15.627447908506971,
      "grad_norm": 4.839295864105225,
      "learning_rate": 3.697712674291086e-05,
      "loss": 1.8064,
      "step": 199500
    },
    {
      "epoch": 15.63528121572928,
      "grad_norm": 5.926759719848633,
      "learning_rate": 3.6970598986892265e-05,
      "loss": 1.7774,
      "step": 199600
    },
    {
      "epoch": 15.64311452295159,
      "grad_norm": 4.998183250427246,
      "learning_rate": 3.696407123087368e-05,
      "loss": 1.9111,
      "step": 199700
    },
    {
      "epoch": 15.6509478301739,
      "grad_norm": 6.332402229309082,
      "learning_rate": 3.6957543474855084e-05,
      "loss": 1.8218,
      "step": 199800
    },
    {
      "epoch": 15.65878113739621,
      "grad_norm": 5.920125961303711,
      "learning_rate": 3.6951015718836496e-05,
      "loss": 1.7141,
      "step": 199900
    },
    {
      "epoch": 15.666614444618517,
      "grad_norm": 5.551675796508789,
      "learning_rate": 3.69444879628179e-05,
      "loss": 1.6924,
      "step": 200000
    },
    {
      "epoch": 15.674447751840827,
      "grad_norm": 5.302107334136963,
      "learning_rate": 3.693796020679931e-05,
      "loss": 1.767,
      "step": 200100
    },
    {
      "epoch": 15.682281059063136,
      "grad_norm": 4.720101356506348,
      "learning_rate": 3.693143245078072e-05,
      "loss": 1.8022,
      "step": 200200
    },
    {
      "epoch": 15.690114366285446,
      "grad_norm": 4.152097225189209,
      "learning_rate": 3.692490469476213e-05,
      "loss": 1.7577,
      "step": 200300
    },
    {
      "epoch": 15.697947673507755,
      "grad_norm": 4.083327770233154,
      "learning_rate": 3.691837693874354e-05,
      "loss": 1.7881,
      "step": 200400
    },
    {
      "epoch": 15.705780980730065,
      "grad_norm": 8.851053237915039,
      "learning_rate": 3.691184918272495e-05,
      "loss": 1.8359,
      "step": 200500
    },
    {
      "epoch": 15.713614287952373,
      "grad_norm": 5.305042266845703,
      "learning_rate": 3.690532142670636e-05,
      "loss": 1.8334,
      "step": 200600
    },
    {
      "epoch": 15.721447595174682,
      "grad_norm": 4.847792148590088,
      "learning_rate": 3.689879367068776e-05,
      "loss": 1.8034,
      "step": 200700
    },
    {
      "epoch": 15.729280902396992,
      "grad_norm": 3.9845190048217773,
      "learning_rate": 3.6892265914669175e-05,
      "loss": 1.7878,
      "step": 200800
    },
    {
      "epoch": 15.737114209619302,
      "grad_norm": 5.8487162590026855,
      "learning_rate": 3.688573815865059e-05,
      "loss": 1.73,
      "step": 200900
    },
    {
      "epoch": 15.744947516841611,
      "grad_norm": 5.087048053741455,
      "learning_rate": 3.6879210402631994e-05,
      "loss": 1.8293,
      "step": 201000
    },
    {
      "epoch": 15.752780824063919,
      "grad_norm": 4.464767932891846,
      "learning_rate": 3.6872682646613406e-05,
      "loss": 1.7467,
      "step": 201100
    },
    {
      "epoch": 15.760614131286228,
      "grad_norm": 3.5272810459136963,
      "learning_rate": 3.686615489059481e-05,
      "loss": 1.776,
      "step": 201200
    },
    {
      "epoch": 15.768447438508538,
      "grad_norm": 6.449268817901611,
      "learning_rate": 3.685962713457622e-05,
      "loss": 1.7536,
      "step": 201300
    },
    {
      "epoch": 15.776280745730848,
      "grad_norm": 5.398101329803467,
      "learning_rate": 3.6853099378557623e-05,
      "loss": 1.7706,
      "step": 201400
    },
    {
      "epoch": 15.784114052953157,
      "grad_norm": 5.148498058319092,
      "learning_rate": 3.6846571622539036e-05,
      "loss": 1.8729,
      "step": 201500
    },
    {
      "epoch": 15.791947360175467,
      "grad_norm": 4.907670974731445,
      "learning_rate": 3.684004386652045e-05,
      "loss": 1.7149,
      "step": 201600
    },
    {
      "epoch": 15.799780667397775,
      "grad_norm": 4.831201553344727,
      "learning_rate": 3.6833516110501854e-05,
      "loss": 1.7293,
      "step": 201700
    },
    {
      "epoch": 15.807613974620084,
      "grad_norm": 4.594746112823486,
      "learning_rate": 3.682698835448327e-05,
      "loss": 1.7237,
      "step": 201800
    },
    {
      "epoch": 15.815447281842394,
      "grad_norm": 5.978193283081055,
      "learning_rate": 3.682046059846467e-05,
      "loss": 1.7363,
      "step": 201900
    },
    {
      "epoch": 15.823280589064703,
      "grad_norm": 6.063981533050537,
      "learning_rate": 3.681393284244608e-05,
      "loss": 1.8161,
      "step": 202000
    },
    {
      "epoch": 15.831113896287013,
      "grad_norm": 4.758950710296631,
      "learning_rate": 3.680740508642749e-05,
      "loss": 1.7935,
      "step": 202100
    },
    {
      "epoch": 15.838947203509322,
      "grad_norm": 5.999532699584961,
      "learning_rate": 3.6800877330408903e-05,
      "loss": 1.748,
      "step": 202200
    },
    {
      "epoch": 15.84678051073163,
      "grad_norm": 6.073871612548828,
      "learning_rate": 3.679434957439031e-05,
      "loss": 1.742,
      "step": 202300
    },
    {
      "epoch": 15.85461381795394,
      "grad_norm": 6.296602725982666,
      "learning_rate": 3.678782181837172e-05,
      "loss": 1.812,
      "step": 202400
    },
    {
      "epoch": 15.86244712517625,
      "grad_norm": 4.271735668182373,
      "learning_rate": 3.678129406235313e-05,
      "loss": 1.8095,
      "step": 202500
    },
    {
      "epoch": 15.870280432398559,
      "grad_norm": 5.103944301605225,
      "learning_rate": 3.677476630633453e-05,
      "loss": 1.7409,
      "step": 202600
    },
    {
      "epoch": 15.878113739620868,
      "grad_norm": 6.980304718017578,
      "learning_rate": 3.6768238550315946e-05,
      "loss": 1.7461,
      "step": 202700
    },
    {
      "epoch": 15.885947046843178,
      "grad_norm": 4.6734299659729,
      "learning_rate": 3.676171079429735e-05,
      "loss": 1.7431,
      "step": 202800
    },
    {
      "epoch": 15.893780354065486,
      "grad_norm": 5.219124794006348,
      "learning_rate": 3.6755183038278764e-05,
      "loss": 1.7954,
      "step": 202900
    },
    {
      "epoch": 15.901613661287795,
      "grad_norm": 7.484925270080566,
      "learning_rate": 3.674865528226017e-05,
      "loss": 1.7863,
      "step": 203000
    },
    {
      "epoch": 15.909446968510105,
      "grad_norm": 5.164222717285156,
      "learning_rate": 3.674212752624158e-05,
      "loss": 1.804,
      "step": 203100
    },
    {
      "epoch": 15.917280275732415,
      "grad_norm": 5.174248695373535,
      "learning_rate": 3.673559977022299e-05,
      "loss": 1.7255,
      "step": 203200
    },
    {
      "epoch": 15.925113582954724,
      "grad_norm": 6.959413528442383,
      "learning_rate": 3.6729072014204394e-05,
      "loss": 1.7965,
      "step": 203300
    },
    {
      "epoch": 15.932946890177032,
      "grad_norm": 5.8462347984313965,
      "learning_rate": 3.6722544258185807e-05,
      "loss": 1.7761,
      "step": 203400
    },
    {
      "epoch": 15.940780197399341,
      "grad_norm": 5.288654327392578,
      "learning_rate": 3.671601650216722e-05,
      "loss": 1.7188,
      "step": 203500
    },
    {
      "epoch": 15.948613504621651,
      "grad_norm": 5.842166900634766,
      "learning_rate": 3.6709488746148625e-05,
      "loss": 1.7093,
      "step": 203600
    },
    {
      "epoch": 15.95644681184396,
      "grad_norm": 5.068965911865234,
      "learning_rate": 3.670296099013004e-05,
      "loss": 1.7215,
      "step": 203700
    },
    {
      "epoch": 15.96428011906627,
      "grad_norm": 4.800003528594971,
      "learning_rate": 3.669643323411144e-05,
      "loss": 1.7921,
      "step": 203800
    },
    {
      "epoch": 15.97211342628858,
      "grad_norm": 5.578462600708008,
      "learning_rate": 3.668990547809285e-05,
      "loss": 1.8312,
      "step": 203900
    },
    {
      "epoch": 15.979946733510888,
      "grad_norm": 5.038451194763184,
      "learning_rate": 3.668337772207426e-05,
      "loss": 1.7665,
      "step": 204000
    },
    {
      "epoch": 15.987780040733197,
      "grad_norm": 6.931435585021973,
      "learning_rate": 3.6676849966055674e-05,
      "loss": 1.7593,
      "step": 204100
    },
    {
      "epoch": 15.995613347955507,
      "grad_norm": 5.494816303253174,
      "learning_rate": 3.667032221003708e-05,
      "loss": 1.7614,
      "step": 204200
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.8108323812484741,
      "eval_runtime": 1.4366,
      "eval_samples_per_second": 467.773,
      "eval_steps_per_second": 467.773,
      "step": 204256
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.5401787757873535,
      "eval_runtime": 27.179,
      "eval_samples_per_second": 469.7,
      "eval_steps_per_second": 469.7,
      "step": 204256
    },
    {
      "epoch": 16.003446655177815,
      "grad_norm": 4.598220348358154,
      "learning_rate": 3.666379445401849e-05,
      "loss": 1.8237,
      "step": 204300
    },
    {
      "epoch": 16.011279962400124,
      "grad_norm": 5.38232946395874,
      "learning_rate": 3.66572666979999e-05,
      "loss": 1.8217,
      "step": 204400
    },
    {
      "epoch": 16.019113269622434,
      "grad_norm": 5.561832427978516,
      "learning_rate": 3.6650738941981304e-05,
      "loss": 1.6746,
      "step": 204500
    },
    {
      "epoch": 16.026946576844743,
      "grad_norm": 4.9473466873168945,
      "learning_rate": 3.6644211185962716e-05,
      "loss": 1.7493,
      "step": 204600
    },
    {
      "epoch": 16.034779884067053,
      "grad_norm": 3.889193296432495,
      "learning_rate": 3.663768342994412e-05,
      "loss": 1.7946,
      "step": 204700
    },
    {
      "epoch": 16.042613191289362,
      "grad_norm": 5.11060094833374,
      "learning_rate": 3.6631155673925535e-05,
      "loss": 1.6945,
      "step": 204800
    },
    {
      "epoch": 16.050446498511672,
      "grad_norm": 6.505221843719482,
      "learning_rate": 3.662462791790694e-05,
      "loss": 1.7571,
      "step": 204900
    },
    {
      "epoch": 16.05827980573398,
      "grad_norm": 5.131584644317627,
      "learning_rate": 3.661810016188835e-05,
      "loss": 1.7052,
      "step": 205000
    },
    {
      "epoch": 16.06611311295629,
      "grad_norm": 6.20350456237793,
      "learning_rate": 3.661157240586976e-05,
      "loss": 1.6891,
      "step": 205100
    },
    {
      "epoch": 16.0739464201786,
      "grad_norm": 7.491498947143555,
      "learning_rate": 3.6605044649851165e-05,
      "loss": 1.766,
      "step": 205200
    },
    {
      "epoch": 16.08177972740091,
      "grad_norm": 6.759005069732666,
      "learning_rate": 3.659851689383258e-05,
      "loss": 1.7204,
      "step": 205300
    },
    {
      "epoch": 16.089613034623216,
      "grad_norm": 7.068594455718994,
      "learning_rate": 3.659198913781399e-05,
      "loss": 1.7588,
      "step": 205400
    },
    {
      "epoch": 16.097446341845526,
      "grad_norm": 4.433161735534668,
      "learning_rate": 3.6585461381795395e-05,
      "loss": 1.7418,
      "step": 205500
    },
    {
      "epoch": 16.105279649067835,
      "grad_norm": 4.781456470489502,
      "learning_rate": 3.657893362577681e-05,
      "loss": 1.6986,
      "step": 205600
    },
    {
      "epoch": 16.113112956290145,
      "grad_norm": 6.544692516326904,
      "learning_rate": 3.6572405869758214e-05,
      "loss": 1.7092,
      "step": 205700
    },
    {
      "epoch": 16.120946263512455,
      "grad_norm": 4.772083759307861,
      "learning_rate": 3.656587811373962e-05,
      "loss": 1.731,
      "step": 205800
    },
    {
      "epoch": 16.128779570734764,
      "grad_norm": 3.8843040466308594,
      "learning_rate": 3.655935035772103e-05,
      "loss": 1.7955,
      "step": 205900
    },
    {
      "epoch": 16.136612877957074,
      "grad_norm": 5.160353660583496,
      "learning_rate": 3.655282260170244e-05,
      "loss": 1.8174,
      "step": 206000
    },
    {
      "epoch": 16.144446185179383,
      "grad_norm": 4.165118217468262,
      "learning_rate": 3.654629484568385e-05,
      "loss": 1.8022,
      "step": 206100
    },
    {
      "epoch": 16.152279492401693,
      "grad_norm": 5.195791244506836,
      "learning_rate": 3.653976708966526e-05,
      "loss": 1.8639,
      "step": 206200
    },
    {
      "epoch": 16.160112799624002,
      "grad_norm": 5.524160385131836,
      "learning_rate": 3.653323933364667e-05,
      "loss": 1.736,
      "step": 206300
    },
    {
      "epoch": 16.167946106846312,
      "grad_norm": 7.477412700653076,
      "learning_rate": 3.6526711577628075e-05,
      "loss": 1.7859,
      "step": 206400
    },
    {
      "epoch": 16.17577941406862,
      "grad_norm": 4.453889846801758,
      "learning_rate": 3.652018382160948e-05,
      "loss": 1.8464,
      "step": 206500
    },
    {
      "epoch": 16.183612721290928,
      "grad_norm": 4.264494895935059,
      "learning_rate": 3.651365606559089e-05,
      "loss": 1.81,
      "step": 206600
    },
    {
      "epoch": 16.191446028513237,
      "grad_norm": 4.529977798461914,
      "learning_rate": 3.6507128309572305e-05,
      "loss": 1.7882,
      "step": 206700
    },
    {
      "epoch": 16.199279335735547,
      "grad_norm": 4.202321529388428,
      "learning_rate": 3.650060055355371e-05,
      "loss": 1.7296,
      "step": 206800
    },
    {
      "epoch": 16.207112642957856,
      "grad_norm": 6.851582050323486,
      "learning_rate": 3.6494072797535124e-05,
      "loss": 1.7714,
      "step": 206900
    },
    {
      "epoch": 16.214945950180166,
      "grad_norm": 4.25796365737915,
      "learning_rate": 3.648754504151653e-05,
      "loss": 1.7164,
      "step": 207000
    },
    {
      "epoch": 16.222779257402475,
      "grad_norm": 8.495193481445312,
      "learning_rate": 3.6481017285497935e-05,
      "loss": 1.7913,
      "step": 207100
    },
    {
      "epoch": 16.230612564624785,
      "grad_norm": 4.688594341278076,
      "learning_rate": 3.647448952947935e-05,
      "loss": 1.8338,
      "step": 207200
    },
    {
      "epoch": 16.238445871847095,
      "grad_norm": 4.83063268661499,
      "learning_rate": 3.646796177346076e-05,
      "loss": 1.7612,
      "step": 207300
    },
    {
      "epoch": 16.246279179069404,
      "grad_norm": 6.041172981262207,
      "learning_rate": 3.6461434017442166e-05,
      "loss": 1.8084,
      "step": 207400
    },
    {
      "epoch": 16.254112486291714,
      "grad_norm": 4.829526424407959,
      "learning_rate": 3.645490626142358e-05,
      "loss": 1.7322,
      "step": 207500
    },
    {
      "epoch": 16.261945793514023,
      "grad_norm": 5.8868184089660645,
      "learning_rate": 3.6448378505404984e-05,
      "loss": 1.7906,
      "step": 207600
    },
    {
      "epoch": 16.26977910073633,
      "grad_norm": 7.160767555236816,
      "learning_rate": 3.644185074938639e-05,
      "loss": 1.7406,
      "step": 207700
    },
    {
      "epoch": 16.27761240795864,
      "grad_norm": 4.492410182952881,
      "learning_rate": 3.64353229933678e-05,
      "loss": 1.7519,
      "step": 207800
    },
    {
      "epoch": 16.28544571518095,
      "grad_norm": 4.931197643280029,
      "learning_rate": 3.642879523734921e-05,
      "loss": 1.8885,
      "step": 207900
    },
    {
      "epoch": 16.293279022403258,
      "grad_norm": 5.928357124328613,
      "learning_rate": 3.642226748133062e-05,
      "loss": 1.8078,
      "step": 208000
    },
    {
      "epoch": 16.301112329625568,
      "grad_norm": 5.7253642082214355,
      "learning_rate": 3.641573972531203e-05,
      "loss": 1.7293,
      "step": 208100
    },
    {
      "epoch": 16.308945636847877,
      "grad_norm": 6.793992519378662,
      "learning_rate": 3.640921196929344e-05,
      "loss": 1.8002,
      "step": 208200
    },
    {
      "epoch": 16.316778944070187,
      "grad_norm": 5.43430757522583,
      "learning_rate": 3.6402684213274845e-05,
      "loss": 1.7563,
      "step": 208300
    },
    {
      "epoch": 16.324612251292496,
      "grad_norm": 6.08176326751709,
      "learning_rate": 3.639615645725625e-05,
      "loss": 1.7172,
      "step": 208400
    },
    {
      "epoch": 16.332445558514806,
      "grad_norm": 6.065235614776611,
      "learning_rate": 3.6389628701237663e-05,
      "loss": 1.7902,
      "step": 208500
    },
    {
      "epoch": 16.340278865737115,
      "grad_norm": 6.144933700561523,
      "learning_rate": 3.6383100945219076e-05,
      "loss": 1.8108,
      "step": 208600
    },
    {
      "epoch": 16.348112172959425,
      "grad_norm": 4.134304046630859,
      "learning_rate": 3.637657318920048e-05,
      "loss": 1.6913,
      "step": 208700
    },
    {
      "epoch": 16.35594548018173,
      "grad_norm": 2.6209940910339355,
      "learning_rate": 3.6370045433181894e-05,
      "loss": 1.7185,
      "step": 208800
    },
    {
      "epoch": 16.36377878740404,
      "grad_norm": 4.962623596191406,
      "learning_rate": 3.63635176771633e-05,
      "loss": 1.8283,
      "step": 208900
    },
    {
      "epoch": 16.37161209462635,
      "grad_norm": 5.213534832000732,
      "learning_rate": 3.6356989921144706e-05,
      "loss": 1.7273,
      "step": 209000
    },
    {
      "epoch": 16.37944540184866,
      "grad_norm": 5.33690071105957,
      "learning_rate": 3.635046216512612e-05,
      "loss": 1.8283,
      "step": 209100
    },
    {
      "epoch": 16.38727870907097,
      "grad_norm": 5.264028549194336,
      "learning_rate": 3.6343934409107524e-05,
      "loss": 1.7564,
      "step": 209200
    },
    {
      "epoch": 16.39511201629328,
      "grad_norm": 8.826525688171387,
      "learning_rate": 3.633740665308894e-05,
      "loss": 1.7537,
      "step": 209300
    },
    {
      "epoch": 16.40294532351559,
      "grad_norm": 6.325276851654053,
      "learning_rate": 3.633087889707035e-05,
      "loss": 1.7834,
      "step": 209400
    },
    {
      "epoch": 16.410778630737898,
      "grad_norm": 6.4898271560668945,
      "learning_rate": 3.6324351141051755e-05,
      "loss": 1.8069,
      "step": 209500
    },
    {
      "epoch": 16.418611937960208,
      "grad_norm": 4.7087907791137695,
      "learning_rate": 3.631782338503316e-05,
      "loss": 1.7803,
      "step": 209600
    },
    {
      "epoch": 16.426445245182517,
      "grad_norm": 5.4518513679504395,
      "learning_rate": 3.6311295629014567e-05,
      "loss": 1.7834,
      "step": 209700
    },
    {
      "epoch": 16.434278552404827,
      "grad_norm": 5.263149738311768,
      "learning_rate": 3.630476787299598e-05,
      "loss": 1.795,
      "step": 209800
    },
    {
      "epoch": 16.442111859627136,
      "grad_norm": 5.636702060699463,
      "learning_rate": 3.629824011697739e-05,
      "loss": 1.7982,
      "step": 209900
    },
    {
      "epoch": 16.449945166849442,
      "grad_norm": 5.5254225730896,
      "learning_rate": 3.62917123609588e-05,
      "loss": 1.8507,
      "step": 210000
    },
    {
      "epoch": 16.457778474071752,
      "grad_norm": 5.094542980194092,
      "learning_rate": 3.628518460494021e-05,
      "loss": 1.7184,
      "step": 210100
    },
    {
      "epoch": 16.46561178129406,
      "grad_norm": 5.064923286437988,
      "learning_rate": 3.6278656848921616e-05,
      "loss": 1.7422,
      "step": 210200
    },
    {
      "epoch": 16.47344508851637,
      "grad_norm": 4.986728668212891,
      "learning_rate": 3.627212909290302e-05,
      "loss": 1.7838,
      "step": 210300
    },
    {
      "epoch": 16.48127839573868,
      "grad_norm": 5.895050048828125,
      "learning_rate": 3.6265601336884434e-05,
      "loss": 1.7669,
      "step": 210400
    },
    {
      "epoch": 16.48911170296099,
      "grad_norm": 5.742918014526367,
      "learning_rate": 3.6259073580865847e-05,
      "loss": 1.7721,
      "step": 210500
    },
    {
      "epoch": 16.4969450101833,
      "grad_norm": 7.040675163269043,
      "learning_rate": 3.625254582484725e-05,
      "loss": 1.7501,
      "step": 210600
    },
    {
      "epoch": 16.50477831740561,
      "grad_norm": 4.868465900421143,
      "learning_rate": 3.6246018068828665e-05,
      "loss": 1.7629,
      "step": 210700
    },
    {
      "epoch": 16.51261162462792,
      "grad_norm": 6.39569091796875,
      "learning_rate": 3.623949031281007e-05,
      "loss": 1.7035,
      "step": 210800
    },
    {
      "epoch": 16.52044493185023,
      "grad_norm": 5.19428825378418,
      "learning_rate": 3.6232962556791476e-05,
      "loss": 1.7748,
      "step": 210900
    },
    {
      "epoch": 16.528278239072538,
      "grad_norm": 5.979430675506592,
      "learning_rate": 3.622643480077289e-05,
      "loss": 1.7085,
      "step": 211000
    },
    {
      "epoch": 16.536111546294844,
      "grad_norm": 7.804104328155518,
      "learning_rate": 3.6219907044754295e-05,
      "loss": 1.7167,
      "step": 211100
    },
    {
      "epoch": 16.543944853517154,
      "grad_norm": 9.034956932067871,
      "learning_rate": 3.621337928873571e-05,
      "loss": 1.6995,
      "step": 211200
    },
    {
      "epoch": 16.551778160739463,
      "grad_norm": 3.8664727210998535,
      "learning_rate": 3.620685153271712e-05,
      "loss": 1.757,
      "step": 211300
    },
    {
      "epoch": 16.559611467961773,
      "grad_norm": 6.101513862609863,
      "learning_rate": 3.6200323776698526e-05,
      "loss": 1.7755,
      "step": 211400
    },
    {
      "epoch": 16.567444775184082,
      "grad_norm": 4.83343505859375,
      "learning_rate": 3.619379602067993e-05,
      "loss": 1.7861,
      "step": 211500
    },
    {
      "epoch": 16.575278082406392,
      "grad_norm": 5.901455879211426,
      "learning_rate": 3.618726826466134e-05,
      "loss": 1.7035,
      "step": 211600
    },
    {
      "epoch": 16.5831113896287,
      "grad_norm": 5.407230854034424,
      "learning_rate": 3.618074050864275e-05,
      "loss": 1.751,
      "step": 211700
    },
    {
      "epoch": 16.59094469685101,
      "grad_norm": 6.2401957511901855,
      "learning_rate": 3.617421275262416e-05,
      "loss": 1.7834,
      "step": 211800
    },
    {
      "epoch": 16.59877800407332,
      "grad_norm": 7.267906188964844,
      "learning_rate": 3.616768499660557e-05,
      "loss": 1.7833,
      "step": 211900
    },
    {
      "epoch": 16.60661131129563,
      "grad_norm": 5.421351432800293,
      "learning_rate": 3.616115724058698e-05,
      "loss": 1.7127,
      "step": 212000
    },
    {
      "epoch": 16.61444461851794,
      "grad_norm": 5.853125095367432,
      "learning_rate": 3.6154629484568386e-05,
      "loss": 1.8281,
      "step": 212100
    },
    {
      "epoch": 16.62227792574025,
      "grad_norm": 5.170389652252197,
      "learning_rate": 3.614810172854979e-05,
      "loss": 1.6787,
      "step": 212200
    },
    {
      "epoch": 16.630111232962555,
      "grad_norm": 4.633978366851807,
      "learning_rate": 3.6141573972531205e-05,
      "loss": 1.6784,
      "step": 212300
    },
    {
      "epoch": 16.637944540184865,
      "grad_norm": 5.773545742034912,
      "learning_rate": 3.613504621651261e-05,
      "loss": 1.8256,
      "step": 212400
    },
    {
      "epoch": 16.645777847407174,
      "grad_norm": 5.5071940422058105,
      "learning_rate": 3.612851846049402e-05,
      "loss": 1.7928,
      "step": 212500
    },
    {
      "epoch": 16.653611154629484,
      "grad_norm": 5.6705756187438965,
      "learning_rate": 3.6121990704475435e-05,
      "loss": 1.7081,
      "step": 212600
    },
    {
      "epoch": 16.661444461851794,
      "grad_norm": 4.445174694061279,
      "learning_rate": 3.611546294845684e-05,
      "loss": 1.774,
      "step": 212700
    },
    {
      "epoch": 16.669277769074103,
      "grad_norm": 7.419712543487549,
      "learning_rate": 3.610893519243825e-05,
      "loss": 1.7076,
      "step": 212800
    },
    {
      "epoch": 16.677111076296413,
      "grad_norm": 4.719988822937012,
      "learning_rate": 3.610240743641966e-05,
      "loss": 1.8455,
      "step": 212900
    },
    {
      "epoch": 16.684944383518722,
      "grad_norm": 4.6163763999938965,
      "learning_rate": 3.6095879680401065e-05,
      "loss": 1.7834,
      "step": 213000
    },
    {
      "epoch": 16.692777690741032,
      "grad_norm": 5.293985366821289,
      "learning_rate": 3.608935192438248e-05,
      "loss": 1.8178,
      "step": 213100
    },
    {
      "epoch": 16.70061099796334,
      "grad_norm": 4.982847690582275,
      "learning_rate": 3.6082824168363884e-05,
      "loss": 1.7261,
      "step": 213200
    },
    {
      "epoch": 16.70844430518565,
      "grad_norm": 5.412184238433838,
      "learning_rate": 3.6076296412345296e-05,
      "loss": 1.8507,
      "step": 213300
    },
    {
      "epoch": 16.716277612407957,
      "grad_norm": 4.804986953735352,
      "learning_rate": 3.60697686563267e-05,
      "loss": 1.8029,
      "step": 213400
    },
    {
      "epoch": 16.724110919630267,
      "grad_norm": 5.1490044593811035,
      "learning_rate": 3.606324090030811e-05,
      "loss": 1.821,
      "step": 213500
    },
    {
      "epoch": 16.731944226852576,
      "grad_norm": 5.879875183105469,
      "learning_rate": 3.605671314428952e-05,
      "loss": 1.7218,
      "step": 213600
    },
    {
      "epoch": 16.739777534074886,
      "grad_norm": 4.733367919921875,
      "learning_rate": 3.605018538827093e-05,
      "loss": 1.7973,
      "step": 213700
    },
    {
      "epoch": 16.747610841297195,
      "grad_norm": 4.561358451843262,
      "learning_rate": 3.604365763225234e-05,
      "loss": 1.6672,
      "step": 213800
    },
    {
      "epoch": 16.755444148519505,
      "grad_norm": 5.845532417297363,
      "learning_rate": 3.603712987623375e-05,
      "loss": 1.7627,
      "step": 213900
    },
    {
      "epoch": 16.763277455741814,
      "grad_norm": 5.134249687194824,
      "learning_rate": 3.603060212021516e-05,
      "loss": 1.7114,
      "step": 214000
    },
    {
      "epoch": 16.771110762964124,
      "grad_norm": 5.385976791381836,
      "learning_rate": 3.602407436419656e-05,
      "loss": 1.7166,
      "step": 214100
    },
    {
      "epoch": 16.778944070186434,
      "grad_norm": 5.729296684265137,
      "learning_rate": 3.6017546608177975e-05,
      "loss": 1.7667,
      "step": 214200
    },
    {
      "epoch": 16.786777377408743,
      "grad_norm": 5.736466407775879,
      "learning_rate": 3.601101885215938e-05,
      "loss": 1.6426,
      "step": 214300
    },
    {
      "epoch": 16.794610684631053,
      "grad_norm": 8.23654556274414,
      "learning_rate": 3.6004491096140793e-05,
      "loss": 1.7387,
      "step": 214400
    },
    {
      "epoch": 16.80244399185336,
      "grad_norm": 5.387964725494385,
      "learning_rate": 3.5997963340122206e-05,
      "loss": 1.6371,
      "step": 214500
    },
    {
      "epoch": 16.81027729907567,
      "grad_norm": 6.5128984451293945,
      "learning_rate": 3.599143558410361e-05,
      "loss": 1.8483,
      "step": 214600
    },
    {
      "epoch": 16.818110606297978,
      "grad_norm": 3.922833204269409,
      "learning_rate": 3.598490782808502e-05,
      "loss": 1.7757,
      "step": 214700
    },
    {
      "epoch": 16.825943913520288,
      "grad_norm": 4.315032482147217,
      "learning_rate": 3.597838007206642e-05,
      "loss": 1.7605,
      "step": 214800
    },
    {
      "epoch": 16.833777220742597,
      "grad_norm": 7.901282787322998,
      "learning_rate": 3.5971852316047836e-05,
      "loss": 1.8033,
      "step": 214900
    },
    {
      "epoch": 16.841610527964907,
      "grad_norm": 5.314873695373535,
      "learning_rate": 3.596532456002925e-05,
      "loss": 1.7868,
      "step": 215000
    },
    {
      "epoch": 16.849443835187216,
      "grad_norm": 6.940103530883789,
      "learning_rate": 3.5958796804010654e-05,
      "loss": 1.8234,
      "step": 215100
    },
    {
      "epoch": 16.857277142409526,
      "grad_norm": 5.3962016105651855,
      "learning_rate": 3.595226904799207e-05,
      "loss": 1.757,
      "step": 215200
    },
    {
      "epoch": 16.865110449631835,
      "grad_norm": 5.48961067199707,
      "learning_rate": 3.594574129197347e-05,
      "loss": 1.7507,
      "step": 215300
    },
    {
      "epoch": 16.872943756854145,
      "grad_norm": 4.779481410980225,
      "learning_rate": 3.593921353595488e-05,
      "loss": 1.7441,
      "step": 215400
    },
    {
      "epoch": 16.880777064076455,
      "grad_norm": 4.828400135040283,
      "learning_rate": 3.593268577993629e-05,
      "loss": 1.7277,
      "step": 215500
    },
    {
      "epoch": 16.88861037129876,
      "grad_norm": 4.662358283996582,
      "learning_rate": 3.5926158023917697e-05,
      "loss": 1.7417,
      "step": 215600
    },
    {
      "epoch": 16.89644367852107,
      "grad_norm": 5.3719282150268555,
      "learning_rate": 3.591963026789911e-05,
      "loss": 1.8107,
      "step": 215700
    },
    {
      "epoch": 16.90427698574338,
      "grad_norm": 7.1382341384887695,
      "learning_rate": 3.591310251188052e-05,
      "loss": 1.8053,
      "step": 215800
    },
    {
      "epoch": 16.91211029296569,
      "grad_norm": 5.324184417724609,
      "learning_rate": 3.590657475586193e-05,
      "loss": 1.736,
      "step": 215900
    },
    {
      "epoch": 16.919943600188,
      "grad_norm": 5.871600151062012,
      "learning_rate": 3.590004699984333e-05,
      "loss": 1.8174,
      "step": 216000
    },
    {
      "epoch": 16.92777690741031,
      "grad_norm": 4.932738304138184,
      "learning_rate": 3.5893519243824746e-05,
      "loss": 1.6879,
      "step": 216100
    },
    {
      "epoch": 16.935610214632618,
      "grad_norm": 5.1253132820129395,
      "learning_rate": 3.588699148780615e-05,
      "loss": 1.7733,
      "step": 216200
    },
    {
      "epoch": 16.943443521854928,
      "grad_norm": 4.982131481170654,
      "learning_rate": 3.5880463731787564e-05,
      "loss": 1.7758,
      "step": 216300
    },
    {
      "epoch": 16.951276829077237,
      "grad_norm": 6.536248207092285,
      "learning_rate": 3.5873935975768977e-05,
      "loss": 1.7122,
      "step": 216400
    },
    {
      "epoch": 16.959110136299547,
      "grad_norm": 6.174797058105469,
      "learning_rate": 3.586740821975038e-05,
      "loss": 1.7346,
      "step": 216500
    },
    {
      "epoch": 16.966943443521856,
      "grad_norm": 5.565299987792969,
      "learning_rate": 3.586088046373179e-05,
      "loss": 1.7541,
      "step": 216600
    },
    {
      "epoch": 16.974776750744166,
      "grad_norm": 5.248706340789795,
      "learning_rate": 3.5854352707713194e-05,
      "loss": 1.6683,
      "step": 216700
    },
    {
      "epoch": 16.982610057966472,
      "grad_norm": 5.263193130493164,
      "learning_rate": 3.5847824951694606e-05,
      "loss": 1.7688,
      "step": 216800
    },
    {
      "epoch": 16.99044336518878,
      "grad_norm": 4.349100112915039,
      "learning_rate": 3.584129719567602e-05,
      "loss": 1.8157,
      "step": 216900
    },
    {
      "epoch": 16.99827667241109,
      "grad_norm": 6.618422985076904,
      "learning_rate": 3.5834769439657425e-05,
      "loss": 1.8268,
      "step": 217000
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.808953046798706,
      "eval_runtime": 2.9401,
      "eval_samples_per_second": 228.56,
      "eval_steps_per_second": 228.56,
      "step": 217022
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.532071590423584,
      "eval_runtime": 56.2349,
      "eval_samples_per_second": 227.012,
      "eval_steps_per_second": 227.012,
      "step": 217022
    },
    {
      "epoch": 17.0061099796334,
      "grad_norm": 5.500391006469727,
      "learning_rate": 3.582824168363884e-05,
      "loss": 1.7474,
      "step": 217100
    },
    {
      "epoch": 17.01394328685571,
      "grad_norm": 8.71727466583252,
      "learning_rate": 3.582171392762024e-05,
      "loss": 1.7798,
      "step": 217200
    },
    {
      "epoch": 17.02177659407802,
      "grad_norm": 7.320802211761475,
      "learning_rate": 3.581518617160165e-05,
      "loss": 1.7616,
      "step": 217300
    },
    {
      "epoch": 17.02960990130033,
      "grad_norm": 5.803774356842041,
      "learning_rate": 3.580865841558306e-05,
      "loss": 1.6889,
      "step": 217400
    },
    {
      "epoch": 17.03744320852264,
      "grad_norm": 7.31018590927124,
      "learning_rate": 3.580213065956447e-05,
      "loss": 1.6753,
      "step": 217500
    },
    {
      "epoch": 17.04527651574495,
      "grad_norm": 3.9329023361206055,
      "learning_rate": 3.579560290354588e-05,
      "loss": 1.7802,
      "step": 217600
    },
    {
      "epoch": 17.053109822967258,
      "grad_norm": 4.054136276245117,
      "learning_rate": 3.578907514752729e-05,
      "loss": 1.717,
      "step": 217700
    },
    {
      "epoch": 17.060943130189568,
      "grad_norm": 4.838926792144775,
      "learning_rate": 3.57825473915087e-05,
      "loss": 1.7677,
      "step": 217800
    },
    {
      "epoch": 17.068776437411874,
      "grad_norm": 4.296998023986816,
      "learning_rate": 3.5776019635490104e-05,
      "loss": 1.7093,
      "step": 217900
    },
    {
      "epoch": 17.076609744634183,
      "grad_norm": 5.404440879821777,
      "learning_rate": 3.5769491879471516e-05,
      "loss": 1.8348,
      "step": 218000
    },
    {
      "epoch": 17.084443051856493,
      "grad_norm": 6.139613628387451,
      "learning_rate": 3.576296412345292e-05,
      "loss": 1.6537,
      "step": 218100
    },
    {
      "epoch": 17.092276359078802,
      "grad_norm": 5.381930351257324,
      "learning_rate": 3.5756436367434335e-05,
      "loss": 1.7604,
      "step": 218200
    },
    {
      "epoch": 17.100109666301112,
      "grad_norm": 5.107881546020508,
      "learning_rate": 3.574990861141574e-05,
      "loss": 1.6786,
      "step": 218300
    },
    {
      "epoch": 17.10794297352342,
      "grad_norm": 7.467442989349365,
      "learning_rate": 3.574338085539715e-05,
      "loss": 1.6912,
      "step": 218400
    },
    {
      "epoch": 17.11577628074573,
      "grad_norm": 5.375411510467529,
      "learning_rate": 3.573685309937856e-05,
      "loss": 1.8325,
      "step": 218500
    },
    {
      "epoch": 17.12360958796804,
      "grad_norm": 5.436591625213623,
      "learning_rate": 3.5730325343359964e-05,
      "loss": 1.7127,
      "step": 218600
    },
    {
      "epoch": 17.13144289519035,
      "grad_norm": 5.572061538696289,
      "learning_rate": 3.572379758734138e-05,
      "loss": 1.7269,
      "step": 218700
    },
    {
      "epoch": 17.13927620241266,
      "grad_norm": 3.5195231437683105,
      "learning_rate": 3.571726983132278e-05,
      "loss": 1.7583,
      "step": 218800
    },
    {
      "epoch": 17.14710950963497,
      "grad_norm": 4.080060958862305,
      "learning_rate": 3.5710742075304195e-05,
      "loss": 1.7258,
      "step": 218900
    },
    {
      "epoch": 17.15494281685728,
      "grad_norm": 4.618427276611328,
      "learning_rate": 3.570421431928561e-05,
      "loss": 1.8216,
      "step": 219000
    },
    {
      "epoch": 17.162776124079585,
      "grad_norm": 6.22703742980957,
      "learning_rate": 3.5697686563267014e-05,
      "loss": 1.7562,
      "step": 219100
    },
    {
      "epoch": 17.170609431301894,
      "grad_norm": 5.3920111656188965,
      "learning_rate": 3.569115880724842e-05,
      "loss": 1.7303,
      "step": 219200
    },
    {
      "epoch": 17.178442738524204,
      "grad_norm": 3.9315528869628906,
      "learning_rate": 3.568463105122983e-05,
      "loss": 1.7589,
      "step": 219300
    },
    {
      "epoch": 17.186276045746514,
      "grad_norm": 5.318977355957031,
      "learning_rate": 3.567810329521124e-05,
      "loss": 1.7743,
      "step": 219400
    },
    {
      "epoch": 17.194109352968823,
      "grad_norm": 5.453790187835693,
      "learning_rate": 3.567157553919265e-05,
      "loss": 1.7226,
      "step": 219500
    },
    {
      "epoch": 17.201942660191133,
      "grad_norm": 5.590092182159424,
      "learning_rate": 3.566504778317406e-05,
      "loss": 1.7376,
      "step": 219600
    },
    {
      "epoch": 17.209775967413442,
      "grad_norm": 5.7418212890625,
      "learning_rate": 3.565852002715547e-05,
      "loss": 1.704,
      "step": 219700
    },
    {
      "epoch": 17.217609274635752,
      "grad_norm": 5.16671895980835,
      "learning_rate": 3.5651992271136874e-05,
      "loss": 1.745,
      "step": 219800
    },
    {
      "epoch": 17.22544258185806,
      "grad_norm": 7.0385966300964355,
      "learning_rate": 3.564546451511828e-05,
      "loss": 1.8079,
      "step": 219900
    },
    {
      "epoch": 17.23327588908037,
      "grad_norm": 5.046268939971924,
      "learning_rate": 3.563893675909969e-05,
      "loss": 1.7781,
      "step": 220000
    },
    {
      "epoch": 17.24110919630268,
      "grad_norm": 5.908335208892822,
      "learning_rate": 3.5632409003081105e-05,
      "loss": 1.8021,
      "step": 220100
    },
    {
      "epoch": 17.248942503524987,
      "grad_norm": 4.225519180297852,
      "learning_rate": 3.562588124706251e-05,
      "loss": 1.7384,
      "step": 220200
    },
    {
      "epoch": 17.256775810747296,
      "grad_norm": 6.1607890129089355,
      "learning_rate": 3.5619353491043924e-05,
      "loss": 1.7346,
      "step": 220300
    },
    {
      "epoch": 17.264609117969606,
      "grad_norm": 5.739657878875732,
      "learning_rate": 3.561282573502533e-05,
      "loss": 1.6423,
      "step": 220400
    },
    {
      "epoch": 17.272442425191915,
      "grad_norm": 5.448159694671631,
      "learning_rate": 3.5606297979006735e-05,
      "loss": 1.7283,
      "step": 220500
    },
    {
      "epoch": 17.280275732414225,
      "grad_norm": 4.9373931884765625,
      "learning_rate": 3.559977022298815e-05,
      "loss": 1.6544,
      "step": 220600
    },
    {
      "epoch": 17.288109039636534,
      "grad_norm": 4.363663196563721,
      "learning_rate": 3.5593242466969553e-05,
      "loss": 1.7373,
      "step": 220700
    },
    {
      "epoch": 17.295942346858844,
      "grad_norm": 4.0997748374938965,
      "learning_rate": 3.5586714710950966e-05,
      "loss": 1.7695,
      "step": 220800
    },
    {
      "epoch": 17.303775654081154,
      "grad_norm": 5.134222984313965,
      "learning_rate": 3.558018695493238e-05,
      "loss": 1.6654,
      "step": 220900
    },
    {
      "epoch": 17.311608961303463,
      "grad_norm": 4.143840312957764,
      "learning_rate": 3.5573659198913784e-05,
      "loss": 1.7857,
      "step": 221000
    },
    {
      "epoch": 17.319442268525773,
      "grad_norm": 5.518462181091309,
      "learning_rate": 3.556713144289519e-05,
      "loss": 1.6716,
      "step": 221100
    },
    {
      "epoch": 17.327275575748082,
      "grad_norm": 3.6766176223754883,
      "learning_rate": 3.55606036868766e-05,
      "loss": 1.7068,
      "step": 221200
    },
    {
      "epoch": 17.33510888297039,
      "grad_norm": 7.901398181915283,
      "learning_rate": 3.555407593085801e-05,
      "loss": 1.773,
      "step": 221300
    },
    {
      "epoch": 17.342942190192698,
      "grad_norm": 5.999179840087891,
      "learning_rate": 3.554754817483942e-05,
      "loss": 1.7864,
      "step": 221400
    },
    {
      "epoch": 17.350775497415007,
      "grad_norm": 5.794987678527832,
      "learning_rate": 3.554102041882083e-05,
      "loss": 1.6524,
      "step": 221500
    },
    {
      "epoch": 17.358608804637317,
      "grad_norm": 4.472372531890869,
      "learning_rate": 3.553449266280224e-05,
      "loss": 1.724,
      "step": 221600
    },
    {
      "epoch": 17.366442111859627,
      "grad_norm": 5.602108001708984,
      "learning_rate": 3.5527964906783645e-05,
      "loss": 1.6909,
      "step": 221700
    },
    {
      "epoch": 17.374275419081936,
      "grad_norm": 6.424257755279541,
      "learning_rate": 3.552143715076505e-05,
      "loss": 1.7643,
      "step": 221800
    },
    {
      "epoch": 17.382108726304246,
      "grad_norm": 5.804378032684326,
      "learning_rate": 3.551490939474646e-05,
      "loss": 1.7777,
      "step": 221900
    },
    {
      "epoch": 17.389942033526555,
      "grad_norm": 7.320661544799805,
      "learning_rate": 3.550838163872787e-05,
      "loss": 1.75,
      "step": 222000
    },
    {
      "epoch": 17.397775340748865,
      "grad_norm": 4.949387550354004,
      "learning_rate": 3.550185388270928e-05,
      "loss": 1.8015,
      "step": 222100
    },
    {
      "epoch": 17.405608647971174,
      "grad_norm": 4.919185638427734,
      "learning_rate": 3.5495326126690694e-05,
      "loss": 1.7267,
      "step": 222200
    },
    {
      "epoch": 17.413441955193484,
      "grad_norm": 4.458053112030029,
      "learning_rate": 3.54887983706721e-05,
      "loss": 1.7931,
      "step": 222300
    },
    {
      "epoch": 17.421275262415794,
      "grad_norm": 5.338703632354736,
      "learning_rate": 3.5482270614653506e-05,
      "loss": 1.7271,
      "step": 222400
    },
    {
      "epoch": 17.4291085696381,
      "grad_norm": 6.908034801483154,
      "learning_rate": 3.547574285863492e-05,
      "loss": 1.7429,
      "step": 222500
    },
    {
      "epoch": 17.43694187686041,
      "grad_norm": 5.4195122718811035,
      "learning_rate": 3.5469215102616324e-05,
      "loss": 1.7965,
      "step": 222600
    },
    {
      "epoch": 17.44477518408272,
      "grad_norm": 5.986292362213135,
      "learning_rate": 3.5462687346597737e-05,
      "loss": 1.7952,
      "step": 222700
    },
    {
      "epoch": 17.45260849130503,
      "grad_norm": 6.654728889465332,
      "learning_rate": 3.545615959057915e-05,
      "loss": 1.7448,
      "step": 222800
    },
    {
      "epoch": 17.460441798527338,
      "grad_norm": 5.23868989944458,
      "learning_rate": 3.5449631834560555e-05,
      "loss": 1.6986,
      "step": 222900
    },
    {
      "epoch": 17.468275105749647,
      "grad_norm": 6.051875591278076,
      "learning_rate": 3.544310407854196e-05,
      "loss": 1.792,
      "step": 223000
    },
    {
      "epoch": 17.476108412971957,
      "grad_norm": 5.293931007385254,
      "learning_rate": 3.543657632252337e-05,
      "loss": 1.8487,
      "step": 223100
    },
    {
      "epoch": 17.483941720194267,
      "grad_norm": 6.312282562255859,
      "learning_rate": 3.543004856650478e-05,
      "loss": 1.7927,
      "step": 223200
    },
    {
      "epoch": 17.491775027416576,
      "grad_norm": 4.520009994506836,
      "learning_rate": 3.542352081048619e-05,
      "loss": 1.7967,
      "step": 223300
    },
    {
      "epoch": 17.499608334638886,
      "grad_norm": 6.951230049133301,
      "learning_rate": 3.54169930544676e-05,
      "loss": 1.7115,
      "step": 223400
    },
    {
      "epoch": 17.507441641861195,
      "grad_norm": 4.997302532196045,
      "learning_rate": 3.541046529844901e-05,
      "loss": 1.833,
      "step": 223500
    },
    {
      "epoch": 17.5152749490835,
      "grad_norm": 4.677977085113525,
      "learning_rate": 3.5403937542430416e-05,
      "loss": 1.7182,
      "step": 223600
    },
    {
      "epoch": 17.52310825630581,
      "grad_norm": 5.2805399894714355,
      "learning_rate": 3.539740978641182e-05,
      "loss": 1.6593,
      "step": 223700
    },
    {
      "epoch": 17.53094156352812,
      "grad_norm": 7.786140441894531,
      "learning_rate": 3.5390882030393234e-05,
      "loss": 1.8117,
      "step": 223800
    },
    {
      "epoch": 17.53877487075043,
      "grad_norm": 5.329115390777588,
      "learning_rate": 3.538435427437464e-05,
      "loss": 1.7595,
      "step": 223900
    },
    {
      "epoch": 17.54660817797274,
      "grad_norm": 10.054994583129883,
      "learning_rate": 3.537782651835605e-05,
      "loss": 1.7721,
      "step": 224000
    },
    {
      "epoch": 17.55444148519505,
      "grad_norm": 5.13079833984375,
      "learning_rate": 3.5371298762337465e-05,
      "loss": 1.7592,
      "step": 224100
    },
    {
      "epoch": 17.56227479241736,
      "grad_norm": 5.751967430114746,
      "learning_rate": 3.536477100631887e-05,
      "loss": 1.6971,
      "step": 224200
    },
    {
      "epoch": 17.57010809963967,
      "grad_norm": 5.212663650512695,
      "learning_rate": 3.5358243250300276e-05,
      "loss": 1.7368,
      "step": 224300
    },
    {
      "epoch": 17.577941406861978,
      "grad_norm": 4.10975456237793,
      "learning_rate": 3.535171549428169e-05,
      "loss": 1.8073,
      "step": 224400
    },
    {
      "epoch": 17.585774714084287,
      "grad_norm": 6.141500949859619,
      "learning_rate": 3.5345187738263095e-05,
      "loss": 1.8005,
      "step": 224500
    },
    {
      "epoch": 17.593608021306597,
      "grad_norm": 6.393099784851074,
      "learning_rate": 3.533865998224451e-05,
      "loss": 1.7504,
      "step": 224600
    },
    {
      "epoch": 17.601441328528907,
      "grad_norm": 6.168729782104492,
      "learning_rate": 3.533213222622592e-05,
      "loss": 1.7891,
      "step": 224700
    },
    {
      "epoch": 17.609274635751213,
      "grad_norm": 4.701919078826904,
      "learning_rate": 3.5325604470207325e-05,
      "loss": 1.8137,
      "step": 224800
    },
    {
      "epoch": 17.617107942973522,
      "grad_norm": 5.3783135414123535,
      "learning_rate": 3.531907671418873e-05,
      "loss": 1.6992,
      "step": 224900
    },
    {
      "epoch": 17.624941250195832,
      "grad_norm": 5.6182732582092285,
      "learning_rate": 3.531254895817014e-05,
      "loss": 1.7877,
      "step": 225000
    },
    {
      "epoch": 17.63277455741814,
      "grad_norm": 5.55918550491333,
      "learning_rate": 3.530602120215155e-05,
      "loss": 1.705,
      "step": 225100
    },
    {
      "epoch": 17.64060786464045,
      "grad_norm": 3.698554277420044,
      "learning_rate": 3.5299493446132955e-05,
      "loss": 1.8369,
      "step": 225200
    },
    {
      "epoch": 17.64844117186276,
      "grad_norm": 5.2920684814453125,
      "learning_rate": 3.529296569011437e-05,
      "loss": 1.7224,
      "step": 225300
    },
    {
      "epoch": 17.65627447908507,
      "grad_norm": 6.132623195648193,
      "learning_rate": 3.528643793409578e-05,
      "loss": 1.768,
      "step": 225400
    },
    {
      "epoch": 17.66410778630738,
      "grad_norm": 2.167112350463867,
      "learning_rate": 3.5279910178077186e-05,
      "loss": 1.7685,
      "step": 225500
    },
    {
      "epoch": 17.67194109352969,
      "grad_norm": 6.689836502075195,
      "learning_rate": 3.527338242205859e-05,
      "loss": 1.7687,
      "step": 225600
    },
    {
      "epoch": 17.679774400752,
      "grad_norm": 6.172458171844482,
      "learning_rate": 3.5266854666040004e-05,
      "loss": 1.7619,
      "step": 225700
    },
    {
      "epoch": 17.68760770797431,
      "grad_norm": 5.803749084472656,
      "learning_rate": 3.526032691002141e-05,
      "loss": 1.7135,
      "step": 225800
    },
    {
      "epoch": 17.695441015196614,
      "grad_norm": 5.653705596923828,
      "learning_rate": 3.525379915400282e-05,
      "loss": 1.7943,
      "step": 225900
    },
    {
      "epoch": 17.703274322418924,
      "grad_norm": 5.031309604644775,
      "learning_rate": 3.5247271397984235e-05,
      "loss": 1.7055,
      "step": 226000
    },
    {
      "epoch": 17.711107629641234,
      "grad_norm": 5.787615776062012,
      "learning_rate": 3.524074364196564e-05,
      "loss": 1.713,
      "step": 226100
    },
    {
      "epoch": 17.718940936863543,
      "grad_norm": 5.44447660446167,
      "learning_rate": 3.523421588594705e-05,
      "loss": 1.7535,
      "step": 226200
    },
    {
      "epoch": 17.726774244085853,
      "grad_norm": 7.202000617980957,
      "learning_rate": 3.522768812992846e-05,
      "loss": 1.7676,
      "step": 226300
    },
    {
      "epoch": 17.734607551308162,
      "grad_norm": 4.776876449584961,
      "learning_rate": 3.5221160373909865e-05,
      "loss": 1.8636,
      "step": 226400
    },
    {
      "epoch": 17.742440858530472,
      "grad_norm": 4.989073753356934,
      "learning_rate": 3.521463261789128e-05,
      "loss": 1.7769,
      "step": 226500
    },
    {
      "epoch": 17.75027416575278,
      "grad_norm": 4.902114391326904,
      "learning_rate": 3.5208104861872683e-05,
      "loss": 1.7406,
      "step": 226600
    },
    {
      "epoch": 17.75810747297509,
      "grad_norm": 3.352287530899048,
      "learning_rate": 3.5201577105854096e-05,
      "loss": 1.8675,
      "step": 226700
    },
    {
      "epoch": 17.7659407801974,
      "grad_norm": 6.11626672744751,
      "learning_rate": 3.51950493498355e-05,
      "loss": 1.8426,
      "step": 226800
    },
    {
      "epoch": 17.77377408741971,
      "grad_norm": 5.753328323364258,
      "learning_rate": 3.518852159381691e-05,
      "loss": 1.769,
      "step": 226900
    },
    {
      "epoch": 17.781607394642016,
      "grad_norm": 5.60890007019043,
      "learning_rate": 3.518199383779832e-05,
      "loss": 1.7935,
      "step": 227000
    },
    {
      "epoch": 17.789440701864326,
      "grad_norm": 5.466099739074707,
      "learning_rate": 3.5175466081779726e-05,
      "loss": 1.7275,
      "step": 227100
    },
    {
      "epoch": 17.797274009086635,
      "grad_norm": 4.890838623046875,
      "learning_rate": 3.516893832576114e-05,
      "loss": 1.7445,
      "step": 227200
    },
    {
      "epoch": 17.805107316308945,
      "grad_norm": 5.131200790405273,
      "learning_rate": 3.516241056974255e-05,
      "loss": 1.6978,
      "step": 227300
    },
    {
      "epoch": 17.812940623531254,
      "grad_norm": 6.051722049713135,
      "learning_rate": 3.515588281372396e-05,
      "loss": 1.7921,
      "step": 227400
    },
    {
      "epoch": 17.820773930753564,
      "grad_norm": 5.5584211349487305,
      "learning_rate": 3.514935505770536e-05,
      "loss": 1.7149,
      "step": 227500
    },
    {
      "epoch": 17.828607237975874,
      "grad_norm": 4.588625431060791,
      "learning_rate": 3.5142827301686775e-05,
      "loss": 1.7836,
      "step": 227600
    },
    {
      "epoch": 17.836440545198183,
      "grad_norm": 5.595667839050293,
      "learning_rate": 3.513629954566818e-05,
      "loss": 1.7919,
      "step": 227700
    },
    {
      "epoch": 17.844273852420493,
      "grad_norm": 4.444714069366455,
      "learning_rate": 3.512977178964959e-05,
      "loss": 1.721,
      "step": 227800
    },
    {
      "epoch": 17.852107159642802,
      "grad_norm": 6.403124809265137,
      "learning_rate": 3.5123244033631006e-05,
      "loss": 1.7986,
      "step": 227900
    },
    {
      "epoch": 17.859940466865112,
      "grad_norm": 3.8577873706817627,
      "learning_rate": 3.511671627761241e-05,
      "loss": 1.7131,
      "step": 228000
    },
    {
      "epoch": 17.86777377408742,
      "grad_norm": 5.5987629890441895,
      "learning_rate": 3.511018852159382e-05,
      "loss": 1.7705,
      "step": 228100
    },
    {
      "epoch": 17.875607081309727,
      "grad_norm": 4.83943510055542,
      "learning_rate": 3.510366076557523e-05,
      "loss": 1.8231,
      "step": 228200
    },
    {
      "epoch": 17.883440388532037,
      "grad_norm": 4.124549388885498,
      "learning_rate": 3.5097133009556636e-05,
      "loss": 1.7513,
      "step": 228300
    },
    {
      "epoch": 17.891273695754347,
      "grad_norm": 6.454588890075684,
      "learning_rate": 3.509060525353804e-05,
      "loss": 1.679,
      "step": 228400
    },
    {
      "epoch": 17.899107002976656,
      "grad_norm": 4.518318176269531,
      "learning_rate": 3.5084077497519454e-05,
      "loss": 1.7154,
      "step": 228500
    },
    {
      "epoch": 17.906940310198966,
      "grad_norm": 4.314413070678711,
      "learning_rate": 3.5077549741500867e-05,
      "loss": 1.7633,
      "step": 228600
    },
    {
      "epoch": 17.914773617421275,
      "grad_norm": 8.046538352966309,
      "learning_rate": 3.507102198548227e-05,
      "loss": 1.7574,
      "step": 228700
    },
    {
      "epoch": 17.922606924643585,
      "grad_norm": 5.386385917663574,
      "learning_rate": 3.506449422946368e-05,
      "loss": 1.7688,
      "step": 228800
    },
    {
      "epoch": 17.930440231865894,
      "grad_norm": 4.385447025299072,
      "learning_rate": 3.505796647344509e-05,
      "loss": 1.749,
      "step": 228900
    },
    {
      "epoch": 17.938273539088204,
      "grad_norm": 4.540524482727051,
      "learning_rate": 3.5051438717426496e-05,
      "loss": 1.8012,
      "step": 229000
    },
    {
      "epoch": 17.946106846310514,
      "grad_norm": 3.662327527999878,
      "learning_rate": 3.504491096140791e-05,
      "loss": 1.7175,
      "step": 229100
    },
    {
      "epoch": 17.953940153532823,
      "grad_norm": 6.093521595001221,
      "learning_rate": 3.503838320538932e-05,
      "loss": 1.7793,
      "step": 229200
    },
    {
      "epoch": 17.96177346075513,
      "grad_norm": 4.675536632537842,
      "learning_rate": 3.503185544937073e-05,
      "loss": 1.6888,
      "step": 229300
    },
    {
      "epoch": 17.96960676797744,
      "grad_norm": 6.236936569213867,
      "learning_rate": 3.502532769335213e-05,
      "loss": 1.7698,
      "step": 229400
    },
    {
      "epoch": 17.97744007519975,
      "grad_norm": 6.304960250854492,
      "learning_rate": 3.5018799937333546e-05,
      "loss": 1.8334,
      "step": 229500
    },
    {
      "epoch": 17.985273382422058,
      "grad_norm": 4.885039329528809,
      "learning_rate": 3.501227218131495e-05,
      "loss": 1.7019,
      "step": 229600
    },
    {
      "epoch": 17.993106689644367,
      "grad_norm": 5.777518272399902,
      "learning_rate": 3.5005744425296364e-05,
      "loss": 1.7707,
      "step": 229700
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.7998498678207397,
      "eval_runtime": 2.9894,
      "eval_samples_per_second": 224.794,
      "eval_steps_per_second": 224.794,
      "step": 229788
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.5191295146942139,
      "eval_runtime": 57.2559,
      "eval_samples_per_second": 222.964,
      "eval_steps_per_second": 222.964,
      "step": 229788
    },
    {
      "epoch": 18.000939996866677,
      "grad_norm": 5.285122871398926,
      "learning_rate": 3.4999216669277776e-05,
      "loss": 1.7848,
      "step": 229800
    },
    {
      "epoch": 18.008773304088987,
      "grad_norm": 5.7864460945129395,
      "learning_rate": 3.499268891325918e-05,
      "loss": 1.764,
      "step": 229900
    },
    {
      "epoch": 18.016606611311296,
      "grad_norm": 4.515341758728027,
      "learning_rate": 3.498616115724059e-05,
      "loss": 1.7369,
      "step": 230000
    },
    {
      "epoch": 18.024439918533606,
      "grad_norm": 4.669499397277832,
      "learning_rate": 3.4979633401221994e-05,
      "loss": 1.7216,
      "step": 230100
    },
    {
      "epoch": 18.032273225755915,
      "grad_norm": 5.512998104095459,
      "learning_rate": 3.4973105645203406e-05,
      "loss": 1.6341,
      "step": 230200
    },
    {
      "epoch": 18.040106532978225,
      "grad_norm": 4.8738226890563965,
      "learning_rate": 3.496657788918481e-05,
      "loss": 1.7857,
      "step": 230300
    },
    {
      "epoch": 18.047939840200534,
      "grad_norm": 4.705188274383545,
      "learning_rate": 3.4960050133166225e-05,
      "loss": 1.7351,
      "step": 230400
    },
    {
      "epoch": 18.05577314742284,
      "grad_norm": 6.559483528137207,
      "learning_rate": 3.495352237714764e-05,
      "loss": 1.699,
      "step": 230500
    },
    {
      "epoch": 18.06360645464515,
      "grad_norm": 6.038263320922852,
      "learning_rate": 3.494699462112904e-05,
      "loss": 1.735,
      "step": 230600
    },
    {
      "epoch": 18.07143976186746,
      "grad_norm": 5.833034992218018,
      "learning_rate": 3.494046686511045e-05,
      "loss": 1.8577,
      "step": 230700
    },
    {
      "epoch": 18.07927306908977,
      "grad_norm": 7.188844680786133,
      "learning_rate": 3.493393910909186e-05,
      "loss": 1.7307,
      "step": 230800
    },
    {
      "epoch": 18.08710637631208,
      "grad_norm": 5.4612555503845215,
      "learning_rate": 3.492741135307327e-05,
      "loss": 1.7155,
      "step": 230900
    },
    {
      "epoch": 18.09493968353439,
      "grad_norm": 5.8673095703125,
      "learning_rate": 3.492088359705468e-05,
      "loss": 1.6675,
      "step": 231000
    },
    {
      "epoch": 18.102772990756698,
      "grad_norm": 5.434812068939209,
      "learning_rate": 3.491435584103609e-05,
      "loss": 1.7207,
      "step": 231100
    },
    {
      "epoch": 18.110606297979007,
      "grad_norm": 5.104983806610107,
      "learning_rate": 3.49078280850175e-05,
      "loss": 1.7514,
      "step": 231200
    },
    {
      "epoch": 18.118439605201317,
      "grad_norm": 5.371586799621582,
      "learning_rate": 3.4901300328998904e-05,
      "loss": 1.6838,
      "step": 231300
    },
    {
      "epoch": 18.126272912423627,
      "grad_norm": 5.168045520782471,
      "learning_rate": 3.4894772572980316e-05,
      "loss": 1.7094,
      "step": 231400
    },
    {
      "epoch": 18.134106219645936,
      "grad_norm": 4.072323322296143,
      "learning_rate": 3.488824481696172e-05,
      "loss": 1.6661,
      "step": 231500
    },
    {
      "epoch": 18.141939526868242,
      "grad_norm": 4.411445617675781,
      "learning_rate": 3.488171706094313e-05,
      "loss": 1.6939,
      "step": 231600
    },
    {
      "epoch": 18.14977283409055,
      "grad_norm": 4.465157508850098,
      "learning_rate": 3.487518930492454e-05,
      "loss": 1.698,
      "step": 231700
    },
    {
      "epoch": 18.15760614131286,
      "grad_norm": 4.530336380004883,
      "learning_rate": 3.486866154890595e-05,
      "loss": 1.7604,
      "step": 231800
    },
    {
      "epoch": 18.16543944853517,
      "grad_norm": 4.464118480682373,
      "learning_rate": 3.486213379288736e-05,
      "loss": 1.7917,
      "step": 231900
    },
    {
      "epoch": 18.17327275575748,
      "grad_norm": 5.196279048919678,
      "learning_rate": 3.4855606036868764e-05,
      "loss": 1.7506,
      "step": 232000
    },
    {
      "epoch": 18.18110606297979,
      "grad_norm": 5.755962371826172,
      "learning_rate": 3.484907828085018e-05,
      "loss": 1.8576,
      "step": 232100
    },
    {
      "epoch": 18.1889393702021,
      "grad_norm": 4.982206344604492,
      "learning_rate": 3.484255052483158e-05,
      "loss": 1.7765,
      "step": 232200
    },
    {
      "epoch": 18.19677267742441,
      "grad_norm": 7.536870956420898,
      "learning_rate": 3.4836022768812995e-05,
      "loss": 1.6918,
      "step": 232300
    },
    {
      "epoch": 18.20460598464672,
      "grad_norm": 3.7017934322357178,
      "learning_rate": 3.482949501279441e-05,
      "loss": 1.721,
      "step": 232400
    },
    {
      "epoch": 18.21243929186903,
      "grad_norm": 4.739131927490234,
      "learning_rate": 3.4822967256775814e-05,
      "loss": 1.6667,
      "step": 232500
    },
    {
      "epoch": 18.220272599091338,
      "grad_norm": 6.010350704193115,
      "learning_rate": 3.481643950075722e-05,
      "loss": 1.7102,
      "step": 232600
    },
    {
      "epoch": 18.228105906313644,
      "grad_norm": 4.474090576171875,
      "learning_rate": 3.480991174473863e-05,
      "loss": 1.729,
      "step": 232700
    },
    {
      "epoch": 18.235939213535953,
      "grad_norm": 4.453984260559082,
      "learning_rate": 3.480338398872004e-05,
      "loss": 1.7688,
      "step": 232800
    },
    {
      "epoch": 18.243772520758263,
      "grad_norm": 5.925853252410889,
      "learning_rate": 3.479685623270145e-05,
      "loss": 1.7808,
      "step": 232900
    },
    {
      "epoch": 18.251605827980573,
      "grad_norm": 6.04968786239624,
      "learning_rate": 3.479032847668286e-05,
      "loss": 1.7174,
      "step": 233000
    },
    {
      "epoch": 18.259439135202882,
      "grad_norm": 7.985805988311768,
      "learning_rate": 3.478380072066427e-05,
      "loss": 1.7506,
      "step": 233100
    },
    {
      "epoch": 18.26727244242519,
      "grad_norm": 6.231112957000732,
      "learning_rate": 3.4777272964645674e-05,
      "loss": 1.7208,
      "step": 233200
    },
    {
      "epoch": 18.2751057496475,
      "grad_norm": 5.2236409187316895,
      "learning_rate": 3.477074520862708e-05,
      "loss": 1.7921,
      "step": 233300
    },
    {
      "epoch": 18.28293905686981,
      "grad_norm": 7.5534443855285645,
      "learning_rate": 3.476421745260849e-05,
      "loss": 1.6843,
      "step": 233400
    },
    {
      "epoch": 18.29077236409212,
      "grad_norm": 4.428615570068359,
      "learning_rate": 3.47576896965899e-05,
      "loss": 1.662,
      "step": 233500
    },
    {
      "epoch": 18.29860567131443,
      "grad_norm": 4.371819019317627,
      "learning_rate": 3.475116194057131e-05,
      "loss": 1.7556,
      "step": 233600
    },
    {
      "epoch": 18.30643897853674,
      "grad_norm": 6.285713195800781,
      "learning_rate": 3.4744634184552723e-05,
      "loss": 1.7893,
      "step": 233700
    },
    {
      "epoch": 18.31427228575905,
      "grad_norm": 5.80160665512085,
      "learning_rate": 3.473810642853413e-05,
      "loss": 1.8047,
      "step": 233800
    },
    {
      "epoch": 18.322105592981355,
      "grad_norm": 3.7350332736968994,
      "learning_rate": 3.4731578672515535e-05,
      "loss": 1.7062,
      "step": 233900
    },
    {
      "epoch": 18.329938900203665,
      "grad_norm": 4.2761549949646,
      "learning_rate": 3.472505091649695e-05,
      "loss": 1.7378,
      "step": 234000
    },
    {
      "epoch": 18.337772207425974,
      "grad_norm": 6.770364761352539,
      "learning_rate": 3.471852316047835e-05,
      "loss": 1.8093,
      "step": 234100
    },
    {
      "epoch": 18.345605514648284,
      "grad_norm": 5.25692081451416,
      "learning_rate": 3.4711995404459766e-05,
      "loss": 1.6936,
      "step": 234200
    },
    {
      "epoch": 18.353438821870594,
      "grad_norm": 7.4238362312316895,
      "learning_rate": 3.470546764844118e-05,
      "loss": 1.7144,
      "step": 234300
    },
    {
      "epoch": 18.361272129092903,
      "grad_norm": 4.5481061935424805,
      "learning_rate": 3.4698939892422584e-05,
      "loss": 1.8197,
      "step": 234400
    },
    {
      "epoch": 18.369105436315213,
      "grad_norm": 5.323039531707764,
      "learning_rate": 3.469241213640399e-05,
      "loss": 1.7396,
      "step": 234500
    },
    {
      "epoch": 18.376938743537522,
      "grad_norm": 5.414144992828369,
      "learning_rate": 3.46858843803854e-05,
      "loss": 1.6864,
      "step": 234600
    },
    {
      "epoch": 18.384772050759832,
      "grad_norm": 7.908783912658691,
      "learning_rate": 3.467935662436681e-05,
      "loss": 1.6958,
      "step": 234700
    },
    {
      "epoch": 18.39260535798214,
      "grad_norm": 5.122724533081055,
      "learning_rate": 3.4672828868348214e-05,
      "loss": 1.8522,
      "step": 234800
    },
    {
      "epoch": 18.40043866520445,
      "grad_norm": 5.546050548553467,
      "learning_rate": 3.466630111232963e-05,
      "loss": 1.7913,
      "step": 234900
    },
    {
      "epoch": 18.408271972426757,
      "grad_norm": 4.854353904724121,
      "learning_rate": 3.465977335631104e-05,
      "loss": 1.7597,
      "step": 235000
    },
    {
      "epoch": 18.416105279649067,
      "grad_norm": 4.756610870361328,
      "learning_rate": 3.4653245600292445e-05,
      "loss": 1.7869,
      "step": 235100
    },
    {
      "epoch": 18.423938586871376,
      "grad_norm": 5.211430072784424,
      "learning_rate": 3.464671784427385e-05,
      "loss": 1.6414,
      "step": 235200
    },
    {
      "epoch": 18.431771894093686,
      "grad_norm": 5.257385730743408,
      "learning_rate": 3.464019008825526e-05,
      "loss": 1.6819,
      "step": 235300
    },
    {
      "epoch": 18.439605201315995,
      "grad_norm": 6.591749668121338,
      "learning_rate": 3.463366233223667e-05,
      "loss": 1.7893,
      "step": 235400
    },
    {
      "epoch": 18.447438508538305,
      "grad_norm": 3.5311975479125977,
      "learning_rate": 3.462713457621808e-05,
      "loss": 1.8237,
      "step": 235500
    },
    {
      "epoch": 18.455271815760614,
      "grad_norm": 6.083911418914795,
      "learning_rate": 3.4620606820199494e-05,
      "loss": 1.7776,
      "step": 235600
    },
    {
      "epoch": 18.463105122982924,
      "grad_norm": 6.532848834991455,
      "learning_rate": 3.46140790641809e-05,
      "loss": 1.7412,
      "step": 235700
    },
    {
      "epoch": 18.470938430205234,
      "grad_norm": 4.308034420013428,
      "learning_rate": 3.4607551308162306e-05,
      "loss": 1.7675,
      "step": 235800
    },
    {
      "epoch": 18.478771737427543,
      "grad_norm": 5.389368057250977,
      "learning_rate": 3.460102355214372e-05,
      "loss": 1.76,
      "step": 235900
    },
    {
      "epoch": 18.486605044649853,
      "grad_norm": 4.6336588859558105,
      "learning_rate": 3.4594495796125124e-05,
      "loss": 1.7335,
      "step": 236000
    },
    {
      "epoch": 18.494438351872162,
      "grad_norm": 5.7185540199279785,
      "learning_rate": 3.4587968040106536e-05,
      "loss": 1.6929,
      "step": 236100
    },
    {
      "epoch": 18.50227165909447,
      "grad_norm": 6.273105144500732,
      "learning_rate": 3.458144028408795e-05,
      "loss": 1.897,
      "step": 236200
    },
    {
      "epoch": 18.510104966316778,
      "grad_norm": 6.186746597290039,
      "learning_rate": 3.4574912528069355e-05,
      "loss": 1.7265,
      "step": 236300
    },
    {
      "epoch": 18.517938273539087,
      "grad_norm": 5.924357891082764,
      "learning_rate": 3.456838477205076e-05,
      "loss": 1.7583,
      "step": 236400
    },
    {
      "epoch": 18.525771580761397,
      "grad_norm": 5.508255958557129,
      "learning_rate": 3.456185701603217e-05,
      "loss": 1.6713,
      "step": 236500
    },
    {
      "epoch": 18.533604887983707,
      "grad_norm": 5.169567108154297,
      "learning_rate": 3.455532926001358e-05,
      "loss": 1.8485,
      "step": 236600
    },
    {
      "epoch": 18.541438195206016,
      "grad_norm": 5.361072063446045,
      "learning_rate": 3.4548801503994985e-05,
      "loss": 1.7707,
      "step": 236700
    },
    {
      "epoch": 18.549271502428326,
      "grad_norm": 6.00400972366333,
      "learning_rate": 3.45422737479764e-05,
      "loss": 1.7371,
      "step": 236800
    },
    {
      "epoch": 18.557104809650635,
      "grad_norm": 6.75986909866333,
      "learning_rate": 3.453574599195781e-05,
      "loss": 1.8404,
      "step": 236900
    },
    {
      "epoch": 18.564938116872945,
      "grad_norm": 4.153865337371826,
      "learning_rate": 3.4529218235939215e-05,
      "loss": 1.7201,
      "step": 237000
    },
    {
      "epoch": 18.572771424095254,
      "grad_norm": 5.749295711517334,
      "learning_rate": 3.452269047992062e-05,
      "loss": 1.6859,
      "step": 237100
    },
    {
      "epoch": 18.580604731317564,
      "grad_norm": 6.4944844245910645,
      "learning_rate": 3.4516162723902034e-05,
      "loss": 1.7154,
      "step": 237200
    },
    {
      "epoch": 18.58843803853987,
      "grad_norm": 5.912265300750732,
      "learning_rate": 3.450963496788344e-05,
      "loss": 1.7679,
      "step": 237300
    },
    {
      "epoch": 18.59627134576218,
      "grad_norm": 4.894385814666748,
      "learning_rate": 3.450310721186485e-05,
      "loss": 1.8249,
      "step": 237400
    },
    {
      "epoch": 18.60410465298449,
      "grad_norm": 4.843627452850342,
      "learning_rate": 3.4496579455846265e-05,
      "loss": 1.7316,
      "step": 237500
    },
    {
      "epoch": 18.6119379602068,
      "grad_norm": 6.1104230880737305,
      "learning_rate": 3.449005169982767e-05,
      "loss": 1.8165,
      "step": 237600
    },
    {
      "epoch": 18.61977126742911,
      "grad_norm": 4.246814250946045,
      "learning_rate": 3.4483523943809076e-05,
      "loss": 1.8978,
      "step": 237700
    },
    {
      "epoch": 18.627604574651418,
      "grad_norm": 5.809471607208252,
      "learning_rate": 3.447699618779049e-05,
      "loss": 1.6882,
      "step": 237800
    },
    {
      "epoch": 18.635437881873727,
      "grad_norm": 5.594640254974365,
      "learning_rate": 3.4470468431771894e-05,
      "loss": 1.7523,
      "step": 237900
    },
    {
      "epoch": 18.643271189096037,
      "grad_norm": 4.257898330688477,
      "learning_rate": 3.44639406757533e-05,
      "loss": 1.7318,
      "step": 238000
    },
    {
      "epoch": 18.651104496318347,
      "grad_norm": 4.261225700378418,
      "learning_rate": 3.445741291973472e-05,
      "loss": 1.6587,
      "step": 238100
    },
    {
      "epoch": 18.658937803540656,
      "grad_norm": 6.310374736785889,
      "learning_rate": 3.4450885163716125e-05,
      "loss": 1.8394,
      "step": 238200
    },
    {
      "epoch": 18.666771110762966,
      "grad_norm": 3.5195298194885254,
      "learning_rate": 3.444435740769753e-05,
      "loss": 1.7482,
      "step": 238300
    },
    {
      "epoch": 18.67460441798527,
      "grad_norm": 6.075938701629639,
      "learning_rate": 3.443782965167894e-05,
      "loss": 1.6617,
      "step": 238400
    },
    {
      "epoch": 18.68243772520758,
      "grad_norm": 5.163079261779785,
      "learning_rate": 3.443130189566035e-05,
      "loss": 1.7266,
      "step": 238500
    },
    {
      "epoch": 18.69027103242989,
      "grad_norm": 5.53345251083374,
      "learning_rate": 3.4424774139641755e-05,
      "loss": 1.7293,
      "step": 238600
    },
    {
      "epoch": 18.6981043396522,
      "grad_norm": 4.738762378692627,
      "learning_rate": 3.441824638362317e-05,
      "loss": 1.7478,
      "step": 238700
    },
    {
      "epoch": 18.70593764687451,
      "grad_norm": 5.214503765106201,
      "learning_rate": 3.441171862760458e-05,
      "loss": 1.8485,
      "step": 238800
    },
    {
      "epoch": 18.71377095409682,
      "grad_norm": 8.31553840637207,
      "learning_rate": 3.4405190871585986e-05,
      "loss": 1.7564,
      "step": 238900
    },
    {
      "epoch": 18.72160426131913,
      "grad_norm": 5.743299961090088,
      "learning_rate": 3.439866311556739e-05,
      "loss": 1.7192,
      "step": 239000
    },
    {
      "epoch": 18.72943756854144,
      "grad_norm": 2.1595959663391113,
      "learning_rate": 3.4392135359548804e-05,
      "loss": 1.7707,
      "step": 239100
    },
    {
      "epoch": 18.73727087576375,
      "grad_norm": 4.595539093017578,
      "learning_rate": 3.438560760353021e-05,
      "loss": 1.7505,
      "step": 239200
    },
    {
      "epoch": 18.745104182986058,
      "grad_norm": 6.683590412139893,
      "learning_rate": 3.437907984751162e-05,
      "loss": 1.7742,
      "step": 239300
    },
    {
      "epoch": 18.752937490208367,
      "grad_norm": 3.6409685611724854,
      "learning_rate": 3.4372552091493035e-05,
      "loss": 1.6225,
      "step": 239400
    },
    {
      "epoch": 18.760770797430673,
      "grad_norm": 4.633524417877197,
      "learning_rate": 3.436602433547444e-05,
      "loss": 1.7605,
      "step": 239500
    },
    {
      "epoch": 18.768604104652983,
      "grad_norm": 7.366899013519287,
      "learning_rate": 3.435949657945585e-05,
      "loss": 1.743,
      "step": 239600
    },
    {
      "epoch": 18.776437411875293,
      "grad_norm": 5.829844951629639,
      "learning_rate": 3.435296882343726e-05,
      "loss": 1.7688,
      "step": 239700
    },
    {
      "epoch": 18.784270719097602,
      "grad_norm": 4.0360260009765625,
      "learning_rate": 3.4346441067418665e-05,
      "loss": 1.8354,
      "step": 239800
    },
    {
      "epoch": 18.79210402631991,
      "grad_norm": 5.542727470397949,
      "learning_rate": 3.433991331140007e-05,
      "loss": 1.8267,
      "step": 239900
    },
    {
      "epoch": 18.79993733354222,
      "grad_norm": 4.924705505371094,
      "learning_rate": 3.433338555538148e-05,
      "loss": 1.7796,
      "step": 240000
    },
    {
      "epoch": 18.80777064076453,
      "grad_norm": 6.00018835067749,
      "learning_rate": 3.4326857799362896e-05,
      "loss": 1.7614,
      "step": 240100
    },
    {
      "epoch": 18.81560394798684,
      "grad_norm": 4.962367534637451,
      "learning_rate": 3.43203300433443e-05,
      "loss": 1.7569,
      "step": 240200
    },
    {
      "epoch": 18.82343725520915,
      "grad_norm": 5.822216510772705,
      "learning_rate": 3.431380228732571e-05,
      "loss": 1.7499,
      "step": 240300
    },
    {
      "epoch": 18.83127056243146,
      "grad_norm": 5.736032009124756,
      "learning_rate": 3.430727453130712e-05,
      "loss": 1.7782,
      "step": 240400
    },
    {
      "epoch": 18.83910386965377,
      "grad_norm": 4.44241189956665,
      "learning_rate": 3.4300746775288526e-05,
      "loss": 1.8583,
      "step": 240500
    },
    {
      "epoch": 18.84693717687608,
      "grad_norm": 5.466540813446045,
      "learning_rate": 3.429421901926994e-05,
      "loss": 1.6888,
      "step": 240600
    },
    {
      "epoch": 18.854770484098385,
      "grad_norm": 5.647725582122803,
      "learning_rate": 3.428769126325135e-05,
      "loss": 1.7379,
      "step": 240700
    },
    {
      "epoch": 18.862603791320694,
      "grad_norm": 8.147762298583984,
      "learning_rate": 3.4281163507232757e-05,
      "loss": 1.6662,
      "step": 240800
    },
    {
      "epoch": 18.870437098543004,
      "grad_norm": 5.280120372772217,
      "learning_rate": 3.427463575121416e-05,
      "loss": 1.8921,
      "step": 240900
    },
    {
      "epoch": 18.878270405765313,
      "grad_norm": 5.876079559326172,
      "learning_rate": 3.4268107995195575e-05,
      "loss": 1.7466,
      "step": 241000
    },
    {
      "epoch": 18.886103712987623,
      "grad_norm": 5.491352558135986,
      "learning_rate": 3.426158023917698e-05,
      "loss": 1.8258,
      "step": 241100
    },
    {
      "epoch": 18.893937020209933,
      "grad_norm": 5.522162914276123,
      "learning_rate": 3.4255052483158386e-05,
      "loss": 1.7112,
      "step": 241200
    },
    {
      "epoch": 18.901770327432242,
      "grad_norm": 4.885463714599609,
      "learning_rate": 3.4248524727139806e-05,
      "loss": 1.7073,
      "step": 241300
    },
    {
      "epoch": 18.90960363465455,
      "grad_norm": 5.608529567718506,
      "learning_rate": 3.424199697112121e-05,
      "loss": 1.8023,
      "step": 241400
    },
    {
      "epoch": 18.91743694187686,
      "grad_norm": 5.272202968597412,
      "learning_rate": 3.423546921510262e-05,
      "loss": 1.7737,
      "step": 241500
    },
    {
      "epoch": 18.92527024909917,
      "grad_norm": 4.187513828277588,
      "learning_rate": 3.422894145908403e-05,
      "loss": 1.6951,
      "step": 241600
    },
    {
      "epoch": 18.93310355632148,
      "grad_norm": 4.486360549926758,
      "learning_rate": 3.4222413703065436e-05,
      "loss": 1.7688,
      "step": 241700
    },
    {
      "epoch": 18.940936863543786,
      "grad_norm": 6.824182033538818,
      "learning_rate": 3.421588594704684e-05,
      "loss": 1.7565,
      "step": 241800
    },
    {
      "epoch": 18.948770170766096,
      "grad_norm": 3.6620349884033203,
      "learning_rate": 3.4209358191028254e-05,
      "loss": 1.6312,
      "step": 241900
    },
    {
      "epoch": 18.956603477988406,
      "grad_norm": 4.143131256103516,
      "learning_rate": 3.4202830435009666e-05,
      "loss": 1.7109,
      "step": 242000
    },
    {
      "epoch": 18.964436785210715,
      "grad_norm": 4.018932819366455,
      "learning_rate": 3.419630267899107e-05,
      "loss": 1.7574,
      "step": 242100
    },
    {
      "epoch": 18.972270092433025,
      "grad_norm": 5.836492538452148,
      "learning_rate": 3.418977492297248e-05,
      "loss": 1.7735,
      "step": 242200
    },
    {
      "epoch": 18.980103399655334,
      "grad_norm": 4.230889320373535,
      "learning_rate": 3.418324716695389e-05,
      "loss": 1.8254,
      "step": 242300
    },
    {
      "epoch": 18.987936706877644,
      "grad_norm": 5.226894378662109,
      "learning_rate": 3.4176719410935296e-05,
      "loss": 1.6558,
      "step": 242400
    },
    {
      "epoch": 18.995770014099953,
      "grad_norm": 5.279845237731934,
      "learning_rate": 3.417019165491671e-05,
      "loss": 1.6089,
      "step": 242500
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.802006721496582,
      "eval_runtime": 1.8016,
      "eval_samples_per_second": 372.999,
      "eval_steps_per_second": 372.999,
      "step": 242554
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.510530948638916,
      "eval_runtime": 34.8152,
      "eval_samples_per_second": 366.678,
      "eval_steps_per_second": 366.678,
      "step": 242554
    },
    {
      "epoch": 19.003603321322263,
      "grad_norm": 5.152630805969238,
      "learning_rate": 3.416366389889812e-05,
      "loss": 1.6648,
      "step": 242600
    },
    {
      "epoch": 19.011436628544573,
      "grad_norm": 6.056123733520508,
      "learning_rate": 3.415713614287953e-05,
      "loss": 1.7315,
      "step": 242700
    },
    {
      "epoch": 19.019269935766882,
      "grad_norm": 6.309383392333984,
      "learning_rate": 3.415060838686093e-05,
      "loss": 1.7628,
      "step": 242800
    },
    {
      "epoch": 19.02710324298919,
      "grad_norm": 5.939711093902588,
      "learning_rate": 3.4144080630842345e-05,
      "loss": 1.7383,
      "step": 242900
    },
    {
      "epoch": 19.034936550211498,
      "grad_norm": 4.825427532196045,
      "learning_rate": 3.413755287482375e-05,
      "loss": 1.7422,
      "step": 243000
    },
    {
      "epoch": 19.042769857433807,
      "grad_norm": 7.465604305267334,
      "learning_rate": 3.413102511880516e-05,
      "loss": 1.7655,
      "step": 243100
    },
    {
      "epoch": 19.050603164656117,
      "grad_norm": 7.092587947845459,
      "learning_rate": 3.412449736278657e-05,
      "loss": 1.7089,
      "step": 243200
    },
    {
      "epoch": 19.058436471878426,
      "grad_norm": 4.6348185539245605,
      "learning_rate": 3.411796960676798e-05,
      "loss": 1.7581,
      "step": 243300
    },
    {
      "epoch": 19.066269779100736,
      "grad_norm": 6.076801776885986,
      "learning_rate": 3.411144185074939e-05,
      "loss": 1.7396,
      "step": 243400
    },
    {
      "epoch": 19.074103086323046,
      "grad_norm": 5.815659046173096,
      "learning_rate": 3.4104914094730794e-05,
      "loss": 1.7109,
      "step": 243500
    },
    {
      "epoch": 19.081936393545355,
      "grad_norm": 6.163923263549805,
      "learning_rate": 3.4098386338712206e-05,
      "loss": 1.7371,
      "step": 243600
    },
    {
      "epoch": 19.089769700767665,
      "grad_norm": 4.727201461791992,
      "learning_rate": 3.409185858269361e-05,
      "loss": 1.6952,
      "step": 243700
    },
    {
      "epoch": 19.097603007989974,
      "grad_norm": 5.2263054847717285,
      "learning_rate": 3.4085330826675024e-05,
      "loss": 1.7524,
      "step": 243800
    },
    {
      "epoch": 19.105436315212284,
      "grad_norm": 5.742881774902344,
      "learning_rate": 3.407880307065644e-05,
      "loss": 1.7034,
      "step": 243900
    },
    {
      "epoch": 19.113269622434593,
      "grad_norm": 5.594286918640137,
      "learning_rate": 3.407227531463784e-05,
      "loss": 1.7232,
      "step": 244000
    },
    {
      "epoch": 19.1211029296569,
      "grad_norm": 6.251829624176025,
      "learning_rate": 3.406574755861925e-05,
      "loss": 1.8079,
      "step": 244100
    },
    {
      "epoch": 19.12893623687921,
      "grad_norm": 5.84804630279541,
      "learning_rate": 3.405921980260066e-05,
      "loss": 1.7245,
      "step": 244200
    },
    {
      "epoch": 19.13676954410152,
      "grad_norm": 6.397113800048828,
      "learning_rate": 3.405269204658207e-05,
      "loss": 1.6477,
      "step": 244300
    },
    {
      "epoch": 19.144602851323828,
      "grad_norm": 4.7162275314331055,
      "learning_rate": 3.404616429056347e-05,
      "loss": 1.6969,
      "step": 244400
    },
    {
      "epoch": 19.152436158546138,
      "grad_norm": 5.180833339691162,
      "learning_rate": 3.403963653454489e-05,
      "loss": 1.7737,
      "step": 244500
    },
    {
      "epoch": 19.160269465768447,
      "grad_norm": 7.075132846832275,
      "learning_rate": 3.40331087785263e-05,
      "loss": 1.6542,
      "step": 244600
    },
    {
      "epoch": 19.168102772990757,
      "grad_norm": 4.00411319732666,
      "learning_rate": 3.4026581022507704e-05,
      "loss": 1.7608,
      "step": 244700
    },
    {
      "epoch": 19.175936080213067,
      "grad_norm": 4.519414901733398,
      "learning_rate": 3.4020053266489116e-05,
      "loss": 1.7888,
      "step": 244800
    },
    {
      "epoch": 19.183769387435376,
      "grad_norm": 5.536751747131348,
      "learning_rate": 3.401352551047052e-05,
      "loss": 1.6935,
      "step": 244900
    },
    {
      "epoch": 19.191602694657686,
      "grad_norm": 4.179839134216309,
      "learning_rate": 3.400699775445193e-05,
      "loss": 1.7309,
      "step": 245000
    },
    {
      "epoch": 19.199436001879995,
      "grad_norm": 6.153616428375244,
      "learning_rate": 3.400046999843334e-05,
      "loss": 1.7198,
      "step": 245100
    },
    {
      "epoch": 19.2072693091023,
      "grad_norm": 4.5806169509887695,
      "learning_rate": 3.399394224241475e-05,
      "loss": 1.7265,
      "step": 245200
    },
    {
      "epoch": 19.21510261632461,
      "grad_norm": 4.843899726867676,
      "learning_rate": 3.398741448639616e-05,
      "loss": 1.8163,
      "step": 245300
    },
    {
      "epoch": 19.22293592354692,
      "grad_norm": 4.8464035987854,
      "learning_rate": 3.3980886730377564e-05,
      "loss": 1.8276,
      "step": 245400
    },
    {
      "epoch": 19.23076923076923,
      "grad_norm": 3.854318857192993,
      "learning_rate": 3.397435897435898e-05,
      "loss": 1.7196,
      "step": 245500
    },
    {
      "epoch": 19.23860253799154,
      "grad_norm": 6.003282070159912,
      "learning_rate": 3.396783121834038e-05,
      "loss": 1.7235,
      "step": 245600
    },
    {
      "epoch": 19.24643584521385,
      "grad_norm": 5.114321708679199,
      "learning_rate": 3.3961303462321795e-05,
      "loss": 1.7906,
      "step": 245700
    },
    {
      "epoch": 19.25426915243616,
      "grad_norm": 5.530184745788574,
      "learning_rate": 3.395477570630321e-05,
      "loss": 1.7131,
      "step": 245800
    },
    {
      "epoch": 19.26210245965847,
      "grad_norm": 6.222625732421875,
      "learning_rate": 3.3948247950284613e-05,
      "loss": 1.72,
      "step": 245900
    },
    {
      "epoch": 19.269935766880778,
      "grad_norm": 5.113473415374756,
      "learning_rate": 3.394172019426602e-05,
      "loss": 1.7267,
      "step": 246000
    },
    {
      "epoch": 19.277769074103087,
      "grad_norm": 5.583507061004639,
      "learning_rate": 3.393519243824743e-05,
      "loss": 1.7021,
      "step": 246100
    },
    {
      "epoch": 19.285602381325397,
      "grad_norm": 5.290441513061523,
      "learning_rate": 3.392866468222884e-05,
      "loss": 1.7972,
      "step": 246200
    },
    {
      "epoch": 19.293435688547707,
      "grad_norm": 5.718385219573975,
      "learning_rate": 3.392213692621024e-05,
      "loss": 1.7462,
      "step": 246300
    },
    {
      "epoch": 19.301268995770013,
      "grad_norm": 4.613076210021973,
      "learning_rate": 3.3915609170191656e-05,
      "loss": 1.7073,
      "step": 246400
    },
    {
      "epoch": 19.309102302992322,
      "grad_norm": 6.7790985107421875,
      "learning_rate": 3.390908141417307e-05,
      "loss": 1.7255,
      "step": 246500
    },
    {
      "epoch": 19.31693561021463,
      "grad_norm": 8.826014518737793,
      "learning_rate": 3.3902553658154474e-05,
      "loss": 1.7122,
      "step": 246600
    },
    {
      "epoch": 19.32476891743694,
      "grad_norm": 4.900052070617676,
      "learning_rate": 3.389602590213589e-05,
      "loss": 1.7348,
      "step": 246700
    },
    {
      "epoch": 19.33260222465925,
      "grad_norm": 6.590669631958008,
      "learning_rate": 3.388949814611729e-05,
      "loss": 1.7088,
      "step": 246800
    },
    {
      "epoch": 19.34043553188156,
      "grad_norm": 4.655740261077881,
      "learning_rate": 3.38829703900987e-05,
      "loss": 1.6885,
      "step": 246900
    },
    {
      "epoch": 19.34826883910387,
      "grad_norm": 5.050217151641846,
      "learning_rate": 3.387644263408011e-05,
      "loss": 1.7789,
      "step": 247000
    },
    {
      "epoch": 19.35610214632618,
      "grad_norm": 5.768696308135986,
      "learning_rate": 3.386991487806152e-05,
      "loss": 1.7652,
      "step": 247100
    },
    {
      "epoch": 19.36393545354849,
      "grad_norm": 5.052178859710693,
      "learning_rate": 3.386338712204293e-05,
      "loss": 1.7082,
      "step": 247200
    },
    {
      "epoch": 19.3717687607708,
      "grad_norm": 4.480762958526611,
      "learning_rate": 3.3856859366024335e-05,
      "loss": 1.7723,
      "step": 247300
    },
    {
      "epoch": 19.37960206799311,
      "grad_norm": 4.991272926330566,
      "learning_rate": 3.385033161000575e-05,
      "loss": 1.7732,
      "step": 247400
    },
    {
      "epoch": 19.387435375215414,
      "grad_norm": 5.935246467590332,
      "learning_rate": 3.384380385398715e-05,
      "loss": 1.6412,
      "step": 247500
    },
    {
      "epoch": 19.395268682437724,
      "grad_norm": 6.554347991943359,
      "learning_rate": 3.383727609796856e-05,
      "loss": 1.7879,
      "step": 247600
    },
    {
      "epoch": 19.403101989660033,
      "grad_norm": 4.571323871612549,
      "learning_rate": 3.383074834194998e-05,
      "loss": 1.6603,
      "step": 247700
    },
    {
      "epoch": 19.410935296882343,
      "grad_norm": 4.6370344161987305,
      "learning_rate": 3.3824220585931384e-05,
      "loss": 1.72,
      "step": 247800
    },
    {
      "epoch": 19.418768604104653,
      "grad_norm": 6.5440802574157715,
      "learning_rate": 3.381769282991279e-05,
      "loss": 1.7156,
      "step": 247900
    },
    {
      "epoch": 19.426601911326962,
      "grad_norm": 5.459475994110107,
      "learning_rate": 3.38111650738942e-05,
      "loss": 1.7428,
      "step": 248000
    },
    {
      "epoch": 19.43443521854927,
      "grad_norm": 6.836058616638184,
      "learning_rate": 3.380463731787561e-05,
      "loss": 1.7563,
      "step": 248100
    },
    {
      "epoch": 19.44226852577158,
      "grad_norm": 5.4113850593566895,
      "learning_rate": 3.3798109561857014e-05,
      "loss": 1.9178,
      "step": 248200
    },
    {
      "epoch": 19.45010183299389,
      "grad_norm": 5.126969337463379,
      "learning_rate": 3.3791581805838426e-05,
      "loss": 1.7002,
      "step": 248300
    },
    {
      "epoch": 19.4579351402162,
      "grad_norm": 5.970591068267822,
      "learning_rate": 3.378505404981984e-05,
      "loss": 1.7191,
      "step": 248400
    },
    {
      "epoch": 19.46576844743851,
      "grad_norm": 6.055588722229004,
      "learning_rate": 3.3778526293801245e-05,
      "loss": 1.7491,
      "step": 248500
    },
    {
      "epoch": 19.47360175466082,
      "grad_norm": 12.343399047851562,
      "learning_rate": 3.377199853778265e-05,
      "loss": 1.85,
      "step": 248600
    },
    {
      "epoch": 19.481435061883126,
      "grad_norm": 5.513978481292725,
      "learning_rate": 3.376547078176406e-05,
      "loss": 1.74,
      "step": 248700
    },
    {
      "epoch": 19.489268369105435,
      "grad_norm": 5.122888565063477,
      "learning_rate": 3.375894302574547e-05,
      "loss": 1.7872,
      "step": 248800
    },
    {
      "epoch": 19.497101676327745,
      "grad_norm": 7.325355052947998,
      "learning_rate": 3.375241526972688e-05,
      "loss": 1.713,
      "step": 248900
    },
    {
      "epoch": 19.504934983550054,
      "grad_norm": 7.357900619506836,
      "learning_rate": 3.3745887513708294e-05,
      "loss": 1.6549,
      "step": 249000
    },
    {
      "epoch": 19.512768290772364,
      "grad_norm": 5.155677795410156,
      "learning_rate": 3.37393597576897e-05,
      "loss": 1.7314,
      "step": 249100
    },
    {
      "epoch": 19.520601597994673,
      "grad_norm": 7.046156883239746,
      "learning_rate": 3.3732832001671105e-05,
      "loss": 1.6345,
      "step": 249200
    },
    {
      "epoch": 19.528434905216983,
      "grad_norm": 4.802130699157715,
      "learning_rate": 3.372630424565252e-05,
      "loss": 1.7426,
      "step": 249300
    },
    {
      "epoch": 19.536268212439293,
      "grad_norm": 4.518631458282471,
      "learning_rate": 3.3719776489633924e-05,
      "loss": 1.8396,
      "step": 249400
    },
    {
      "epoch": 19.544101519661602,
      "grad_norm": 5.339874267578125,
      "learning_rate": 3.371324873361533e-05,
      "loss": 1.6822,
      "step": 249500
    },
    {
      "epoch": 19.55193482688391,
      "grad_norm": 4.473441123962402,
      "learning_rate": 3.370672097759674e-05,
      "loss": 1.7345,
      "step": 249600
    },
    {
      "epoch": 19.55976813410622,
      "grad_norm": 5.862194538116455,
      "learning_rate": 3.3700193221578155e-05,
      "loss": 1.6356,
      "step": 249700
    },
    {
      "epoch": 19.567601441328527,
      "grad_norm": 5.180520534515381,
      "learning_rate": 3.369366546555956e-05,
      "loss": 1.8115,
      "step": 249800
    },
    {
      "epoch": 19.575434748550837,
      "grad_norm": 5.9505295753479,
      "learning_rate": 3.368713770954097e-05,
      "loss": 1.7902,
      "step": 249900
    },
    {
      "epoch": 19.583268055773146,
      "grad_norm": 5.14935827255249,
      "learning_rate": 3.368060995352238e-05,
      "loss": 1.8558,
      "step": 250000
    },
    {
      "epoch": 19.591101362995456,
      "grad_norm": 4.657191753387451,
      "learning_rate": 3.3674082197503784e-05,
      "loss": 1.6682,
      "step": 250100
    },
    {
      "epoch": 19.598934670217766,
      "grad_norm": 4.826613903045654,
      "learning_rate": 3.36675544414852e-05,
      "loss": 1.7761,
      "step": 250200
    },
    {
      "epoch": 19.606767977440075,
      "grad_norm": 7.420471668243408,
      "learning_rate": 3.366102668546661e-05,
      "loss": 1.7016,
      "step": 250300
    },
    {
      "epoch": 19.614601284662385,
      "grad_norm": 4.960168361663818,
      "learning_rate": 3.3654498929448015e-05,
      "loss": 1.8115,
      "step": 250400
    },
    {
      "epoch": 19.622434591884694,
      "grad_norm": 5.583457946777344,
      "learning_rate": 3.364797117342942e-05,
      "loss": 1.7361,
      "step": 250500
    },
    {
      "epoch": 19.630267899107004,
      "grad_norm": 4.0606689453125,
      "learning_rate": 3.3641443417410834e-05,
      "loss": 1.7957,
      "step": 250600
    },
    {
      "epoch": 19.638101206329313,
      "grad_norm": 6.50840950012207,
      "learning_rate": 3.363491566139224e-05,
      "loss": 1.7665,
      "step": 250700
    },
    {
      "epoch": 19.645934513551623,
      "grad_norm": 6.3407087326049805,
      "learning_rate": 3.3628387905373645e-05,
      "loss": 1.7198,
      "step": 250800
    },
    {
      "epoch": 19.65376782077393,
      "grad_norm": 3.8610072135925293,
      "learning_rate": 3.3621860149355064e-05,
      "loss": 1.7557,
      "step": 250900
    },
    {
      "epoch": 19.66160112799624,
      "grad_norm": 6.229483127593994,
      "learning_rate": 3.361533239333647e-05,
      "loss": 1.7964,
      "step": 251000
    },
    {
      "epoch": 19.669434435218548,
      "grad_norm": 4.27925968170166,
      "learning_rate": 3.3608804637317876e-05,
      "loss": 1.7622,
      "step": 251100
    },
    {
      "epoch": 19.677267742440858,
      "grad_norm": 5.054651260375977,
      "learning_rate": 3.360227688129929e-05,
      "loss": 1.7401,
      "step": 251200
    },
    {
      "epoch": 19.685101049663167,
      "grad_norm": 5.305995941162109,
      "learning_rate": 3.3595749125280694e-05,
      "loss": 1.6781,
      "step": 251300
    },
    {
      "epoch": 19.692934356885477,
      "grad_norm": 2.3074564933776855,
      "learning_rate": 3.35892213692621e-05,
      "loss": 1.7826,
      "step": 251400
    },
    {
      "epoch": 19.700767664107786,
      "grad_norm": 5.904114246368408,
      "learning_rate": 3.358269361324351e-05,
      "loss": 1.7868,
      "step": 251500
    },
    {
      "epoch": 19.708600971330096,
      "grad_norm": 5.174170970916748,
      "learning_rate": 3.3576165857224925e-05,
      "loss": 1.8178,
      "step": 251600
    },
    {
      "epoch": 19.716434278552406,
      "grad_norm": 4.736686706542969,
      "learning_rate": 3.356963810120633e-05,
      "loss": 1.8073,
      "step": 251700
    },
    {
      "epoch": 19.724267585774715,
      "grad_norm": 5.455540657043457,
      "learning_rate": 3.356311034518774e-05,
      "loss": 1.7117,
      "step": 251800
    },
    {
      "epoch": 19.732100892997025,
      "grad_norm": 5.496097087860107,
      "learning_rate": 3.355658258916915e-05,
      "loss": 1.6817,
      "step": 251900
    },
    {
      "epoch": 19.739934200219334,
      "grad_norm": 4.410638332366943,
      "learning_rate": 3.3550054833150555e-05,
      "loss": 1.7996,
      "step": 252000
    },
    {
      "epoch": 19.74776750744164,
      "grad_norm": 6.797187805175781,
      "learning_rate": 3.354352707713197e-05,
      "loss": 1.786,
      "step": 252100
    },
    {
      "epoch": 19.75560081466395,
      "grad_norm": 4.252767562866211,
      "learning_rate": 3.353699932111338e-05,
      "loss": 1.8174,
      "step": 252200
    },
    {
      "epoch": 19.76343412188626,
      "grad_norm": 5.485382556915283,
      "learning_rate": 3.3530471565094786e-05,
      "loss": 1.7623,
      "step": 252300
    },
    {
      "epoch": 19.77126742910857,
      "grad_norm": 4.388828277587891,
      "learning_rate": 3.352394380907619e-05,
      "loss": 1.6802,
      "step": 252400
    },
    {
      "epoch": 19.77910073633088,
      "grad_norm": 5.89089822769165,
      "learning_rate": 3.3517416053057604e-05,
      "loss": 1.7242,
      "step": 252500
    },
    {
      "epoch": 19.786934043553188,
      "grad_norm": 6.232696533203125,
      "learning_rate": 3.351088829703901e-05,
      "loss": 1.6719,
      "step": 252600
    },
    {
      "epoch": 19.794767350775498,
      "grad_norm": 6.293489933013916,
      "learning_rate": 3.3504360541020416e-05,
      "loss": 1.7522,
      "step": 252700
    },
    {
      "epoch": 19.802600657997807,
      "grad_norm": 5.0980987548828125,
      "learning_rate": 3.349783278500183e-05,
      "loss": 1.7716,
      "step": 252800
    },
    {
      "epoch": 19.810433965220117,
      "grad_norm": 5.5011067390441895,
      "learning_rate": 3.349130502898324e-05,
      "loss": 1.803,
      "step": 252900
    },
    {
      "epoch": 19.818267272442426,
      "grad_norm": 4.861245632171631,
      "learning_rate": 3.3484777272964647e-05,
      "loss": 1.6831,
      "step": 253000
    },
    {
      "epoch": 19.826100579664736,
      "grad_norm": 4.9030866622924805,
      "learning_rate": 3.347824951694606e-05,
      "loss": 1.7526,
      "step": 253100
    },
    {
      "epoch": 19.833933886887042,
      "grad_norm": 4.461179733276367,
      "learning_rate": 3.3471721760927465e-05,
      "loss": 1.756,
      "step": 253200
    },
    {
      "epoch": 19.84176719410935,
      "grad_norm": 5.5067338943481445,
      "learning_rate": 3.346519400490887e-05,
      "loss": 1.7397,
      "step": 253300
    },
    {
      "epoch": 19.84960050133166,
      "grad_norm": 6.997286796569824,
      "learning_rate": 3.345866624889028e-05,
      "loss": 1.7042,
      "step": 253400
    },
    {
      "epoch": 19.85743380855397,
      "grad_norm": 5.435263633728027,
      "learning_rate": 3.3452138492871696e-05,
      "loss": 1.7647,
      "step": 253500
    },
    {
      "epoch": 19.86526711577628,
      "grad_norm": 5.359297275543213,
      "learning_rate": 3.34456107368531e-05,
      "loss": 1.7847,
      "step": 253600
    },
    {
      "epoch": 19.87310042299859,
      "grad_norm": 6.209731101989746,
      "learning_rate": 3.343908298083451e-05,
      "loss": 1.7174,
      "step": 253700
    },
    {
      "epoch": 19.8809337302209,
      "grad_norm": 4.754078388214111,
      "learning_rate": 3.343255522481592e-05,
      "loss": 1.7681,
      "step": 253800
    },
    {
      "epoch": 19.88876703744321,
      "grad_norm": 5.592331409454346,
      "learning_rate": 3.3426027468797326e-05,
      "loss": 1.6802,
      "step": 253900
    },
    {
      "epoch": 19.89660034466552,
      "grad_norm": 5.36270809173584,
      "learning_rate": 3.341949971277873e-05,
      "loss": 1.7021,
      "step": 254000
    },
    {
      "epoch": 19.904433651887828,
      "grad_norm": 5.489862442016602,
      "learning_rate": 3.341297195676015e-05,
      "loss": 1.7392,
      "step": 254100
    },
    {
      "epoch": 19.912266959110138,
      "grad_norm": 6.789144039154053,
      "learning_rate": 3.3406444200741556e-05,
      "loss": 1.7225,
      "step": 254200
    },
    {
      "epoch": 19.920100266332447,
      "grad_norm": 5.370378494262695,
      "learning_rate": 3.339991644472296e-05,
      "loss": 1.7504,
      "step": 254300
    },
    {
      "epoch": 19.927933573554753,
      "grad_norm": 3.5247011184692383,
      "learning_rate": 3.3393388688704375e-05,
      "loss": 1.7589,
      "step": 254400
    },
    {
      "epoch": 19.935766880777063,
      "grad_norm": 6.009626865386963,
      "learning_rate": 3.338686093268578e-05,
      "loss": 1.7333,
      "step": 254500
    },
    {
      "epoch": 19.943600187999373,
      "grad_norm": 3.5067389011383057,
      "learning_rate": 3.3380333176667186e-05,
      "loss": 1.713,
      "step": 254600
    },
    {
      "epoch": 19.951433495221682,
      "grad_norm": 3.5377228260040283,
      "learning_rate": 3.33738054206486e-05,
      "loss": 1.7187,
      "step": 254700
    },
    {
      "epoch": 19.95926680244399,
      "grad_norm": 4.752649307250977,
      "learning_rate": 3.336727766463001e-05,
      "loss": 1.7547,
      "step": 254800
    },
    {
      "epoch": 19.9671001096663,
      "grad_norm": 6.5922722816467285,
      "learning_rate": 3.336074990861142e-05,
      "loss": 1.6995,
      "step": 254900
    },
    {
      "epoch": 19.97493341688861,
      "grad_norm": 6.9837117195129395,
      "learning_rate": 3.335422215259283e-05,
      "loss": 1.7152,
      "step": 255000
    },
    {
      "epoch": 19.98276672411092,
      "grad_norm": 5.3836588859558105,
      "learning_rate": 3.3347694396574235e-05,
      "loss": 1.6747,
      "step": 255100
    },
    {
      "epoch": 19.99060003133323,
      "grad_norm": 4.318220138549805,
      "learning_rate": 3.334116664055564e-05,
      "loss": 1.7969,
      "step": 255200
    },
    {
      "epoch": 19.99843333855554,
      "grad_norm": 6.213420867919922,
      "learning_rate": 3.3334638884537054e-05,
      "loss": 1.7313,
      "step": 255300
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.7973260879516602,
      "eval_runtime": 2.9726,
      "eval_samples_per_second": 226.066,
      "eval_steps_per_second": 226.066,
      "step": 255320
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.506354808807373,
      "eval_runtime": 56.4888,
      "eval_samples_per_second": 225.992,
      "eval_steps_per_second": 225.992,
      "step": 255320
    },
    {
      "epoch": 20.00626664577785,
      "grad_norm": 7.435888290405273,
      "learning_rate": 3.3328111128518466e-05,
      "loss": 1.6834,
      "step": 255400
    },
    {
      "epoch": 20.014099953000155,
      "grad_norm": 5.557739734649658,
      "learning_rate": 3.332158337249987e-05,
      "loss": 1.8037,
      "step": 255500
    },
    {
      "epoch": 20.021933260222465,
      "grad_norm": 4.406541347503662,
      "learning_rate": 3.331505561648128e-05,
      "loss": 1.6949,
      "step": 255600
    },
    {
      "epoch": 20.029766567444774,
      "grad_norm": 5.279797077178955,
      "learning_rate": 3.330852786046269e-05,
      "loss": 1.7794,
      "step": 255700
    },
    {
      "epoch": 20.037599874667084,
      "grad_norm": 5.2849225997924805,
      "learning_rate": 3.3302000104444096e-05,
      "loss": 1.7799,
      "step": 255800
    },
    {
      "epoch": 20.045433181889393,
      "grad_norm": 5.908874988555908,
      "learning_rate": 3.32954723484255e-05,
      "loss": 1.6914,
      "step": 255900
    },
    {
      "epoch": 20.053266489111703,
      "grad_norm": 4.896332263946533,
      "learning_rate": 3.3288944592406914e-05,
      "loss": 1.8064,
      "step": 256000
    },
    {
      "epoch": 20.061099796334013,
      "grad_norm": 5.881601333618164,
      "learning_rate": 3.328241683638833e-05,
      "loss": 1.7294,
      "step": 256100
    },
    {
      "epoch": 20.068933103556322,
      "grad_norm": 6.266168594360352,
      "learning_rate": 3.327588908036973e-05,
      "loss": 1.6961,
      "step": 256200
    },
    {
      "epoch": 20.07676641077863,
      "grad_norm": 4.525559902191162,
      "learning_rate": 3.3269361324351145e-05,
      "loss": 1.6998,
      "step": 256300
    },
    {
      "epoch": 20.08459971800094,
      "grad_norm": 6.381735801696777,
      "learning_rate": 3.326283356833255e-05,
      "loss": 1.6671,
      "step": 256400
    },
    {
      "epoch": 20.09243302522325,
      "grad_norm": 3.7090530395507812,
      "learning_rate": 3.325630581231396e-05,
      "loss": 1.7288,
      "step": 256500
    },
    {
      "epoch": 20.100266332445557,
      "grad_norm": 5.316845893859863,
      "learning_rate": 3.324977805629537e-05,
      "loss": 1.7623,
      "step": 256600
    },
    {
      "epoch": 20.108099639667866,
      "grad_norm": 6.5092453956604,
      "learning_rate": 3.324325030027678e-05,
      "loss": 1.7964,
      "step": 256700
    },
    {
      "epoch": 20.115932946890176,
      "grad_norm": 5.2313151359558105,
      "learning_rate": 3.323672254425819e-05,
      "loss": 1.7387,
      "step": 256800
    },
    {
      "epoch": 20.123766254112486,
      "grad_norm": 5.050793170928955,
      "learning_rate": 3.3230194788239594e-05,
      "loss": 1.7283,
      "step": 256900
    },
    {
      "epoch": 20.131599561334795,
      "grad_norm": 5.816381454467773,
      "learning_rate": 3.3223667032221006e-05,
      "loss": 1.7808,
      "step": 257000
    },
    {
      "epoch": 20.139432868557105,
      "grad_norm": 3.5190632343292236,
      "learning_rate": 3.321713927620241e-05,
      "loss": 1.6114,
      "step": 257100
    },
    {
      "epoch": 20.147266175779414,
      "grad_norm": 5.521313667297363,
      "learning_rate": 3.321061152018382e-05,
      "loss": 1.6797,
      "step": 257200
    },
    {
      "epoch": 20.155099483001724,
      "grad_norm": 5.676966190338135,
      "learning_rate": 3.320408376416524e-05,
      "loss": 1.7899,
      "step": 257300
    },
    {
      "epoch": 20.162932790224033,
      "grad_norm": 6.183404922485352,
      "learning_rate": 3.319755600814664e-05,
      "loss": 1.7368,
      "step": 257400
    },
    {
      "epoch": 20.170766097446343,
      "grad_norm": 5.57594633102417,
      "learning_rate": 3.319102825212805e-05,
      "loss": 1.763,
      "step": 257500
    },
    {
      "epoch": 20.178599404668653,
      "grad_norm": 5.508609771728516,
      "learning_rate": 3.318450049610946e-05,
      "loss": 1.6289,
      "step": 257600
    },
    {
      "epoch": 20.186432711890962,
      "grad_norm": 6.086707592010498,
      "learning_rate": 3.317797274009087e-05,
      "loss": 1.7218,
      "step": 257700
    },
    {
      "epoch": 20.194266019113268,
      "grad_norm": 5.65027379989624,
      "learning_rate": 3.317144498407227e-05,
      "loss": 1.687,
      "step": 257800
    },
    {
      "epoch": 20.202099326335578,
      "grad_norm": 4.320824146270752,
      "learning_rate": 3.3164917228053685e-05,
      "loss": 1.6883,
      "step": 257900
    },
    {
      "epoch": 20.209932633557887,
      "grad_norm": 5.2447428703308105,
      "learning_rate": 3.31583894720351e-05,
      "loss": 1.6003,
      "step": 258000
    },
    {
      "epoch": 20.217765940780197,
      "grad_norm": 5.697835922241211,
      "learning_rate": 3.31518617160165e-05,
      "loss": 1.6831,
      "step": 258100
    },
    {
      "epoch": 20.225599248002506,
      "grad_norm": 5.683125019073486,
      "learning_rate": 3.3145333959997916e-05,
      "loss": 1.7231,
      "step": 258200
    },
    {
      "epoch": 20.233432555224816,
      "grad_norm": 5.128604412078857,
      "learning_rate": 3.313880620397932e-05,
      "loss": 1.6597,
      "step": 258300
    },
    {
      "epoch": 20.241265862447126,
      "grad_norm": 7.222897052764893,
      "learning_rate": 3.313227844796073e-05,
      "loss": 1.7074,
      "step": 258400
    },
    {
      "epoch": 20.249099169669435,
      "grad_norm": 5.873612880706787,
      "learning_rate": 3.312575069194214e-05,
      "loss": 1.7524,
      "step": 258500
    },
    {
      "epoch": 20.256932476891745,
      "grad_norm": 5.819582462310791,
      "learning_rate": 3.311922293592355e-05,
      "loss": 1.6959,
      "step": 258600
    },
    {
      "epoch": 20.264765784114054,
      "grad_norm": 5.604182243347168,
      "learning_rate": 3.311269517990496e-05,
      "loss": 1.7346,
      "step": 258700
    },
    {
      "epoch": 20.272599091336364,
      "grad_norm": 5.936332702636719,
      "learning_rate": 3.3106167423886364e-05,
      "loss": 1.7098,
      "step": 258800
    },
    {
      "epoch": 20.28043239855867,
      "grad_norm": 4.940096378326416,
      "learning_rate": 3.309963966786778e-05,
      "loss": 1.7113,
      "step": 258900
    },
    {
      "epoch": 20.28826570578098,
      "grad_norm": 5.746357440948486,
      "learning_rate": 3.309311191184918e-05,
      "loss": 1.7485,
      "step": 259000
    },
    {
      "epoch": 20.29609901300329,
      "grad_norm": 6.257553577423096,
      "learning_rate": 3.308658415583059e-05,
      "loss": 1.7098,
      "step": 259100
    },
    {
      "epoch": 20.3039323202256,
      "grad_norm": 7.28766393661499,
      "learning_rate": 3.3080056399812e-05,
      "loss": 1.6892,
      "step": 259200
    },
    {
      "epoch": 20.311765627447908,
      "grad_norm": 5.3849334716796875,
      "learning_rate": 3.307352864379341e-05,
      "loss": 1.6907,
      "step": 259300
    },
    {
      "epoch": 20.319598934670218,
      "grad_norm": 4.314758777618408,
      "learning_rate": 3.306700088777482e-05,
      "loss": 1.7949,
      "step": 259400
    },
    {
      "epoch": 20.327432241892527,
      "grad_norm": 5.937405586242676,
      "learning_rate": 3.306047313175623e-05,
      "loss": 1.7722,
      "step": 259500
    },
    {
      "epoch": 20.335265549114837,
      "grad_norm": 5.343259811401367,
      "learning_rate": 3.305394537573764e-05,
      "loss": 1.7205,
      "step": 259600
    },
    {
      "epoch": 20.343098856337146,
      "grad_norm": 5.334579944610596,
      "learning_rate": 3.304741761971904e-05,
      "loss": 1.7259,
      "step": 259700
    },
    {
      "epoch": 20.350932163559456,
      "grad_norm": 7.041387557983398,
      "learning_rate": 3.3040889863700456e-05,
      "loss": 1.6849,
      "step": 259800
    },
    {
      "epoch": 20.358765470781766,
      "grad_norm": 5.898163318634033,
      "learning_rate": 3.303436210768187e-05,
      "loss": 1.6924,
      "step": 259900
    },
    {
      "epoch": 20.36659877800407,
      "grad_norm": 6.45947265625,
      "learning_rate": 3.3027834351663274e-05,
      "loss": 1.7251,
      "step": 260000
    },
    {
      "epoch": 20.37443208522638,
      "grad_norm": 7.254117488861084,
      "learning_rate": 3.3021306595644687e-05,
      "loss": 1.731,
      "step": 260100
    },
    {
      "epoch": 20.38226539244869,
      "grad_norm": 5.392463684082031,
      "learning_rate": 3.301477883962609e-05,
      "loss": 1.7047,
      "step": 260200
    },
    {
      "epoch": 20.390098699671,
      "grad_norm": 5.7661027908325195,
      "learning_rate": 3.30082510836075e-05,
      "loss": 1.6939,
      "step": 260300
    },
    {
      "epoch": 20.39793200689331,
      "grad_norm": 6.642177581787109,
      "learning_rate": 3.3001723327588904e-05,
      "loss": 1.8107,
      "step": 260400
    },
    {
      "epoch": 20.40576531411562,
      "grad_norm": 5.05372428894043,
      "learning_rate": 3.299519557157032e-05,
      "loss": 1.7322,
      "step": 260500
    },
    {
      "epoch": 20.41359862133793,
      "grad_norm": 5.931292533874512,
      "learning_rate": 3.298866781555173e-05,
      "loss": 1.7411,
      "step": 260600
    },
    {
      "epoch": 20.42143192856024,
      "grad_norm": 5.42243766784668,
      "learning_rate": 3.2982140059533135e-05,
      "loss": 1.8391,
      "step": 260700
    },
    {
      "epoch": 20.429265235782548,
      "grad_norm": 4.905848026275635,
      "learning_rate": 3.297561230351455e-05,
      "loss": 1.6928,
      "step": 260800
    },
    {
      "epoch": 20.437098543004858,
      "grad_norm": 4.537418365478516,
      "learning_rate": 3.296908454749595e-05,
      "loss": 1.7055,
      "step": 260900
    },
    {
      "epoch": 20.444931850227167,
      "grad_norm": 5.665157318115234,
      "learning_rate": 3.296255679147736e-05,
      "loss": 1.6998,
      "step": 261000
    },
    {
      "epoch": 20.452765157449477,
      "grad_norm": 4.135890007019043,
      "learning_rate": 3.295602903545877e-05,
      "loss": 1.6942,
      "step": 261100
    },
    {
      "epoch": 20.460598464671783,
      "grad_norm": 5.201457500457764,
      "learning_rate": 3.2949501279440184e-05,
      "loss": 1.7699,
      "step": 261200
    },
    {
      "epoch": 20.468431771894092,
      "grad_norm": 5.002505779266357,
      "learning_rate": 3.294297352342159e-05,
      "loss": 1.8188,
      "step": 261300
    },
    {
      "epoch": 20.476265079116402,
      "grad_norm": 4.489547252655029,
      "learning_rate": 3.2936445767403e-05,
      "loss": 1.6802,
      "step": 261400
    },
    {
      "epoch": 20.48409838633871,
      "grad_norm": 4.795833110809326,
      "learning_rate": 3.292991801138441e-05,
      "loss": 1.7822,
      "step": 261500
    },
    {
      "epoch": 20.49193169356102,
      "grad_norm": 7.846566200256348,
      "learning_rate": 3.2923390255365814e-05,
      "loss": 1.7326,
      "step": 261600
    },
    {
      "epoch": 20.49976500078333,
      "grad_norm": 5.709949493408203,
      "learning_rate": 3.2916862499347226e-05,
      "loss": 1.822,
      "step": 261700
    },
    {
      "epoch": 20.50759830800564,
      "grad_norm": 5.819818496704102,
      "learning_rate": 3.291033474332864e-05,
      "loss": 1.7354,
      "step": 261800
    },
    {
      "epoch": 20.51543161522795,
      "grad_norm": 5.242661476135254,
      "learning_rate": 3.2903806987310045e-05,
      "loss": 1.7822,
      "step": 261900
    },
    {
      "epoch": 20.52326492245026,
      "grad_norm": 6.763650894165039,
      "learning_rate": 3.289727923129145e-05,
      "loss": 1.8334,
      "step": 262000
    },
    {
      "epoch": 20.53109822967257,
      "grad_norm": 4.886420249938965,
      "learning_rate": 3.289075147527286e-05,
      "loss": 1.7229,
      "step": 262100
    },
    {
      "epoch": 20.53893153689488,
      "grad_norm": 4.197001934051514,
      "learning_rate": 3.288422371925427e-05,
      "loss": 1.7345,
      "step": 262200
    },
    {
      "epoch": 20.546764844117185,
      "grad_norm": 5.2990193367004395,
      "learning_rate": 3.2877695963235674e-05,
      "loss": 1.7347,
      "step": 262300
    },
    {
      "epoch": 20.554598151339494,
      "grad_norm": 7.762998580932617,
      "learning_rate": 3.287116820721709e-05,
      "loss": 1.7526,
      "step": 262400
    },
    {
      "epoch": 20.562431458561804,
      "grad_norm": 5.125401496887207,
      "learning_rate": 3.28646404511985e-05,
      "loss": 1.6991,
      "step": 262500
    },
    {
      "epoch": 20.570264765784113,
      "grad_norm": 6.139955997467041,
      "learning_rate": 3.2858112695179905e-05,
      "loss": 1.8156,
      "step": 262600
    },
    {
      "epoch": 20.578098073006423,
      "grad_norm": 6.648483753204346,
      "learning_rate": 3.285158493916132e-05,
      "loss": 1.8079,
      "step": 262700
    },
    {
      "epoch": 20.585931380228732,
      "grad_norm": 6.276026248931885,
      "learning_rate": 3.2845057183142724e-05,
      "loss": 1.7818,
      "step": 262800
    },
    {
      "epoch": 20.593764687451042,
      "grad_norm": 5.252725124359131,
      "learning_rate": 3.283852942712413e-05,
      "loss": 1.7528,
      "step": 262900
    },
    {
      "epoch": 20.60159799467335,
      "grad_norm": 5.8914313316345215,
      "learning_rate": 3.283200167110554e-05,
      "loss": 1.7393,
      "step": 263000
    },
    {
      "epoch": 20.60943130189566,
      "grad_norm": 5.510906219482422,
      "learning_rate": 3.2825473915086954e-05,
      "loss": 1.6828,
      "step": 263100
    },
    {
      "epoch": 20.61726460911797,
      "grad_norm": 5.906579494476318,
      "learning_rate": 3.281894615906836e-05,
      "loss": 1.7388,
      "step": 263200
    },
    {
      "epoch": 20.62509791634028,
      "grad_norm": 4.9633026123046875,
      "learning_rate": 3.281241840304977e-05,
      "loss": 1.6507,
      "step": 263300
    },
    {
      "epoch": 20.632931223562586,
      "grad_norm": 5.031734466552734,
      "learning_rate": 3.280589064703118e-05,
      "loss": 1.7251,
      "step": 263400
    },
    {
      "epoch": 20.640764530784896,
      "grad_norm": 6.885354518890381,
      "learning_rate": 3.2799362891012584e-05,
      "loss": 1.6582,
      "step": 263500
    },
    {
      "epoch": 20.648597838007205,
      "grad_norm": 4.743790626525879,
      "learning_rate": 3.279283513499399e-05,
      "loss": 1.7604,
      "step": 263600
    },
    {
      "epoch": 20.656431145229515,
      "grad_norm": 4.579655647277832,
      "learning_rate": 3.278630737897541e-05,
      "loss": 1.7753,
      "step": 263700
    },
    {
      "epoch": 20.664264452451825,
      "grad_norm": 4.169064521789551,
      "learning_rate": 3.2779779622956815e-05,
      "loss": 1.7265,
      "step": 263800
    },
    {
      "epoch": 20.672097759674134,
      "grad_norm": 5.213883399963379,
      "learning_rate": 3.277325186693822e-05,
      "loss": 1.6728,
      "step": 263900
    },
    {
      "epoch": 20.679931066896444,
      "grad_norm": 4.8706536293029785,
      "learning_rate": 3.2766724110919633e-05,
      "loss": 1.7687,
      "step": 264000
    },
    {
      "epoch": 20.687764374118753,
      "grad_norm": 4.936290740966797,
      "learning_rate": 3.276019635490104e-05,
      "loss": 1.7278,
      "step": 264100
    },
    {
      "epoch": 20.695597681341063,
      "grad_norm": 2.658195972442627,
      "learning_rate": 3.2753668598882445e-05,
      "loss": 1.6807,
      "step": 264200
    },
    {
      "epoch": 20.703430988563372,
      "grad_norm": 5.105815887451172,
      "learning_rate": 3.274714084286386e-05,
      "loss": 1.7013,
      "step": 264300
    },
    {
      "epoch": 20.711264295785682,
      "grad_norm": 4.531414985656738,
      "learning_rate": 3.274061308684527e-05,
      "loss": 1.7814,
      "step": 264400
    },
    {
      "epoch": 20.71909760300799,
      "grad_norm": 6.398532390594482,
      "learning_rate": 3.2734085330826676e-05,
      "loss": 1.6791,
      "step": 264500
    },
    {
      "epoch": 20.726930910230298,
      "grad_norm": 5.206355571746826,
      "learning_rate": 3.272755757480809e-05,
      "loss": 1.7338,
      "step": 264600
    },
    {
      "epoch": 20.734764217452607,
      "grad_norm": 7.197338581085205,
      "learning_rate": 3.2721029818789494e-05,
      "loss": 1.7151,
      "step": 264700
    },
    {
      "epoch": 20.742597524674917,
      "grad_norm": 4.636108875274658,
      "learning_rate": 3.27145020627709e-05,
      "loss": 1.7869,
      "step": 264800
    },
    {
      "epoch": 20.750430831897226,
      "grad_norm": 4.416858673095703,
      "learning_rate": 3.270797430675231e-05,
      "loss": 1.7759,
      "step": 264900
    },
    {
      "epoch": 20.758264139119536,
      "grad_norm": 3.8326046466827393,
      "learning_rate": 3.2701446550733725e-05,
      "loss": 1.7015,
      "step": 265000
    },
    {
      "epoch": 20.766097446341846,
      "grad_norm": 5.720578670501709,
      "learning_rate": 3.269491879471513e-05,
      "loss": 1.707,
      "step": 265100
    },
    {
      "epoch": 20.773930753564155,
      "grad_norm": 4.741036415100098,
      "learning_rate": 3.268839103869654e-05,
      "loss": 1.7475,
      "step": 265200
    },
    {
      "epoch": 20.781764060786465,
      "grad_norm": 6.180941581726074,
      "learning_rate": 3.268186328267795e-05,
      "loss": 1.7956,
      "step": 265300
    },
    {
      "epoch": 20.789597368008774,
      "grad_norm": 5.901320457458496,
      "learning_rate": 3.2675335526659355e-05,
      "loss": 1.6773,
      "step": 265400
    },
    {
      "epoch": 20.797430675231084,
      "grad_norm": 6.372900485992432,
      "learning_rate": 3.266880777064076e-05,
      "loss": 1.7768,
      "step": 265500
    },
    {
      "epoch": 20.805263982453393,
      "grad_norm": 5.431594371795654,
      "learning_rate": 3.266228001462217e-05,
      "loss": 1.7643,
      "step": 265600
    },
    {
      "epoch": 20.8130972896757,
      "grad_norm": 5.201633453369141,
      "learning_rate": 3.2655752258603586e-05,
      "loss": 1.6918,
      "step": 265700
    },
    {
      "epoch": 20.82093059689801,
      "grad_norm": 5.130938529968262,
      "learning_rate": 3.264922450258499e-05,
      "loss": 1.8165,
      "step": 265800
    },
    {
      "epoch": 20.82876390412032,
      "grad_norm": 4.933437347412109,
      "learning_rate": 3.2642696746566404e-05,
      "loss": 1.8257,
      "step": 265900
    },
    {
      "epoch": 20.836597211342628,
      "grad_norm": 6.742824554443359,
      "learning_rate": 3.263616899054781e-05,
      "loss": 1.777,
      "step": 266000
    },
    {
      "epoch": 20.844430518564938,
      "grad_norm": 5.457162857055664,
      "learning_rate": 3.2629641234529216e-05,
      "loss": 1.6879,
      "step": 266100
    },
    {
      "epoch": 20.852263825787247,
      "grad_norm": 5.363160133361816,
      "learning_rate": 3.262311347851063e-05,
      "loss": 1.7348,
      "step": 266200
    },
    {
      "epoch": 20.860097133009557,
      "grad_norm": 5.3738226890563965,
      "learning_rate": 3.261658572249204e-05,
      "loss": 1.7832,
      "step": 266300
    },
    {
      "epoch": 20.867930440231866,
      "grad_norm": 5.685855865478516,
      "learning_rate": 3.2610057966473446e-05,
      "loss": 1.8006,
      "step": 266400
    },
    {
      "epoch": 20.875763747454176,
      "grad_norm": 5.852370738983154,
      "learning_rate": 3.260353021045486e-05,
      "loss": 1.6275,
      "step": 266500
    },
    {
      "epoch": 20.883597054676486,
      "grad_norm": 5.132314205169678,
      "learning_rate": 3.2597002454436265e-05,
      "loss": 1.7564,
      "step": 266600
    },
    {
      "epoch": 20.891430361898795,
      "grad_norm": 5.322751045227051,
      "learning_rate": 3.259047469841767e-05,
      "loss": 1.7622,
      "step": 266700
    },
    {
      "epoch": 20.899263669121105,
      "grad_norm": 6.538908958435059,
      "learning_rate": 3.258394694239908e-05,
      "loss": 1.7254,
      "step": 266800
    },
    {
      "epoch": 20.90709697634341,
      "grad_norm": 6.040015697479248,
      "learning_rate": 3.2577419186380496e-05,
      "loss": 1.7611,
      "step": 266900
    },
    {
      "epoch": 20.91493028356572,
      "grad_norm": 5.112114906311035,
      "learning_rate": 3.25708914303619e-05,
      "loss": 1.6561,
      "step": 267000
    },
    {
      "epoch": 20.92276359078803,
      "grad_norm": 4.887202262878418,
      "learning_rate": 3.256436367434331e-05,
      "loss": 1.7898,
      "step": 267100
    },
    {
      "epoch": 20.93059689801034,
      "grad_norm": 5.852387428283691,
      "learning_rate": 3.255783591832472e-05,
      "loss": 1.7483,
      "step": 267200
    },
    {
      "epoch": 20.93843020523265,
      "grad_norm": 4.516641139984131,
      "learning_rate": 3.2551308162306125e-05,
      "loss": 1.7313,
      "step": 267300
    },
    {
      "epoch": 20.94626351245496,
      "grad_norm": 6.299520492553711,
      "learning_rate": 3.254478040628753e-05,
      "loss": 1.6859,
      "step": 267400
    },
    {
      "epoch": 20.954096819677268,
      "grad_norm": 6.5412702560424805,
      "learning_rate": 3.2538252650268944e-05,
      "loss": 1.7908,
      "step": 267500
    },
    {
      "epoch": 20.961930126899578,
      "grad_norm": 4.144240856170654,
      "learning_rate": 3.2531724894250356e-05,
      "loss": 1.7251,
      "step": 267600
    },
    {
      "epoch": 20.969763434121887,
      "grad_norm": 5.141000270843506,
      "learning_rate": 3.252519713823176e-05,
      "loss": 1.8124,
      "step": 267700
    },
    {
      "epoch": 20.977596741344197,
      "grad_norm": 3.4029154777526855,
      "learning_rate": 3.2518669382213175e-05,
      "loss": 1.7098,
      "step": 267800
    },
    {
      "epoch": 20.985430048566506,
      "grad_norm": 5.125945568084717,
      "learning_rate": 3.251214162619458e-05,
      "loss": 1.7705,
      "step": 267900
    },
    {
      "epoch": 20.993263355788812,
      "grad_norm": 6.674647331237793,
      "learning_rate": 3.2505613870175986e-05,
      "loss": 1.6624,
      "step": 268000
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.7855231761932373,
      "eval_runtime": 3.024,
      "eval_samples_per_second": 222.222,
      "eval_steps_per_second": 222.222,
      "step": 268086
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.4940764904022217,
      "eval_runtime": 56.3129,
      "eval_samples_per_second": 226.697,
      "eval_steps_per_second": 226.697,
      "step": 268086
    },
    {
      "epoch": 21.001096663011122,
      "grad_norm": 5.455316543579102,
      "learning_rate": 3.24990861141574e-05,
      "loss": 1.8202,
      "step": 268100
    },
    {
      "epoch": 21.00892997023343,
      "grad_norm": 6.16819429397583,
      "learning_rate": 3.249255835813881e-05,
      "loss": 1.7783,
      "step": 268200
    },
    {
      "epoch": 21.01676327745574,
      "grad_norm": 5.879716396331787,
      "learning_rate": 3.248603060212022e-05,
      "loss": 1.7141,
      "step": 268300
    },
    {
      "epoch": 21.02459658467805,
      "grad_norm": 8.121237754821777,
      "learning_rate": 3.247950284610163e-05,
      "loss": 1.694,
      "step": 268400
    },
    {
      "epoch": 21.03242989190036,
      "grad_norm": 5.80143928527832,
      "learning_rate": 3.2472975090083035e-05,
      "loss": 1.7181,
      "step": 268500
    },
    {
      "epoch": 21.04026319912267,
      "grad_norm": 7.678218364715576,
      "learning_rate": 3.246644733406444e-05,
      "loss": 1.7172,
      "step": 268600
    },
    {
      "epoch": 21.04809650634498,
      "grad_norm": 5.7805867195129395,
      "learning_rate": 3.245991957804585e-05,
      "loss": 1.7447,
      "step": 268700
    },
    {
      "epoch": 21.05592981356729,
      "grad_norm": 5.417957305908203,
      "learning_rate": 3.245339182202726e-05,
      "loss": 1.6766,
      "step": 268800
    },
    {
      "epoch": 21.0637631207896,
      "grad_norm": 6.051098346710205,
      "learning_rate": 3.244686406600867e-05,
      "loss": 1.723,
      "step": 268900
    },
    {
      "epoch": 21.071596428011908,
      "grad_norm": 5.608667850494385,
      "learning_rate": 3.244033630999008e-05,
      "loss": 1.7326,
      "step": 269000
    },
    {
      "epoch": 21.079429735234214,
      "grad_norm": 5.020913124084473,
      "learning_rate": 3.243380855397149e-05,
      "loss": 1.668,
      "step": 269100
    },
    {
      "epoch": 21.087263042456524,
      "grad_norm": 4.651805400848389,
      "learning_rate": 3.2427280797952896e-05,
      "loss": 1.6966,
      "step": 269200
    },
    {
      "epoch": 21.095096349678833,
      "grad_norm": 5.739140510559082,
      "learning_rate": 3.24207530419343e-05,
      "loss": 1.7539,
      "step": 269300
    },
    {
      "epoch": 21.102929656901143,
      "grad_norm": 4.835646629333496,
      "learning_rate": 3.2414225285915714e-05,
      "loss": 1.7192,
      "step": 269400
    },
    {
      "epoch": 21.110762964123452,
      "grad_norm": 6.322944164276123,
      "learning_rate": 3.240769752989713e-05,
      "loss": 1.7755,
      "step": 269500
    },
    {
      "epoch": 21.118596271345762,
      "grad_norm": 5.698612689971924,
      "learning_rate": 3.240116977387853e-05,
      "loss": 1.7946,
      "step": 269600
    },
    {
      "epoch": 21.12642957856807,
      "grad_norm": 5.232866287231445,
      "learning_rate": 3.2394642017859945e-05,
      "loss": 1.7442,
      "step": 269700
    },
    {
      "epoch": 21.13426288579038,
      "grad_norm": 5.423445224761963,
      "learning_rate": 3.238811426184135e-05,
      "loss": 1.7583,
      "step": 269800
    },
    {
      "epoch": 21.14209619301269,
      "grad_norm": 5.426389217376709,
      "learning_rate": 3.238158650582276e-05,
      "loss": 1.713,
      "step": 269900
    },
    {
      "epoch": 21.149929500235,
      "grad_norm": 3.5689656734466553,
      "learning_rate": 3.237505874980417e-05,
      "loss": 1.7254,
      "step": 270000
    },
    {
      "epoch": 21.15776280745731,
      "grad_norm": 3.9034416675567627,
      "learning_rate": 3.236853099378558e-05,
      "loss": 1.5922,
      "step": 270100
    },
    {
      "epoch": 21.16559611467962,
      "grad_norm": 5.021735191345215,
      "learning_rate": 3.236200323776699e-05,
      "loss": 1.7462,
      "step": 270200
    },
    {
      "epoch": 21.173429421901925,
      "grad_norm": 3.9575252532958984,
      "learning_rate": 3.23554754817484e-05,
      "loss": 1.748,
      "step": 270300
    },
    {
      "epoch": 21.181262729124235,
      "grad_norm": 6.108388900756836,
      "learning_rate": 3.2348947725729806e-05,
      "loss": 1.7055,
      "step": 270400
    },
    {
      "epoch": 21.189096036346545,
      "grad_norm": 5.317625999450684,
      "learning_rate": 3.234241996971121e-05,
      "loss": 1.5965,
      "step": 270500
    },
    {
      "epoch": 21.196929343568854,
      "grad_norm": 4.939490795135498,
      "learning_rate": 3.233589221369262e-05,
      "loss": 1.7201,
      "step": 270600
    },
    {
      "epoch": 21.204762650791164,
      "grad_norm": 4.9501776695251465,
      "learning_rate": 3.232936445767403e-05,
      "loss": 1.6476,
      "step": 270700
    },
    {
      "epoch": 21.212595958013473,
      "grad_norm": 5.554996490478516,
      "learning_rate": 3.232283670165544e-05,
      "loss": 1.7637,
      "step": 270800
    },
    {
      "epoch": 21.220429265235783,
      "grad_norm": 5.334503650665283,
      "learning_rate": 3.231630894563685e-05,
      "loss": 1.7488,
      "step": 270900
    },
    {
      "epoch": 21.228262572458092,
      "grad_norm": 6.328357219696045,
      "learning_rate": 3.230978118961826e-05,
      "loss": 1.6156,
      "step": 271000
    },
    {
      "epoch": 21.236095879680402,
      "grad_norm": 6.4812517166137695,
      "learning_rate": 3.230325343359967e-05,
      "loss": 1.705,
      "step": 271100
    },
    {
      "epoch": 21.24392918690271,
      "grad_norm": 5.048250675201416,
      "learning_rate": 3.229672567758107e-05,
      "loss": 1.6627,
      "step": 271200
    },
    {
      "epoch": 21.25176249412502,
      "grad_norm": 5.274087905883789,
      "learning_rate": 3.2290197921562485e-05,
      "loss": 1.7341,
      "step": 271300
    },
    {
      "epoch": 21.259595801347327,
      "grad_norm": 5.0691914558410645,
      "learning_rate": 3.22836701655439e-05,
      "loss": 1.7176,
      "step": 271400
    },
    {
      "epoch": 21.267429108569637,
      "grad_norm": 4.678524971008301,
      "learning_rate": 3.22771424095253e-05,
      "loss": 1.8145,
      "step": 271500
    },
    {
      "epoch": 21.275262415791946,
      "grad_norm": 5.934199810028076,
      "learning_rate": 3.2270614653506716e-05,
      "loss": 1.7262,
      "step": 271600
    },
    {
      "epoch": 21.283095723014256,
      "grad_norm": 6.536271572113037,
      "learning_rate": 3.226408689748812e-05,
      "loss": 1.6799,
      "step": 271700
    },
    {
      "epoch": 21.290929030236565,
      "grad_norm": 5.91333532333374,
      "learning_rate": 3.225755914146953e-05,
      "loss": 1.8504,
      "step": 271800
    },
    {
      "epoch": 21.298762337458875,
      "grad_norm": 5.82975959777832,
      "learning_rate": 3.225103138545094e-05,
      "loss": 1.7735,
      "step": 271900
    },
    {
      "epoch": 21.306595644681185,
      "grad_norm": 4.044770240783691,
      "learning_rate": 3.2244503629432346e-05,
      "loss": 1.7657,
      "step": 272000
    },
    {
      "epoch": 21.314428951903494,
      "grad_norm": 5.11740255355835,
      "learning_rate": 3.223797587341376e-05,
      "loss": 1.7163,
      "step": 272100
    },
    {
      "epoch": 21.322262259125804,
      "grad_norm": 6.671248912811279,
      "learning_rate": 3.2231448117395164e-05,
      "loss": 1.7029,
      "step": 272200
    },
    {
      "epoch": 21.330095566348113,
      "grad_norm": 4.973665714263916,
      "learning_rate": 3.2224920361376577e-05,
      "loss": 1.8408,
      "step": 272300
    },
    {
      "epoch": 21.337928873570423,
      "grad_norm": 5.854466915130615,
      "learning_rate": 3.221839260535798e-05,
      "loss": 1.7566,
      "step": 272400
    },
    {
      "epoch": 21.345762180792732,
      "grad_norm": 5.719666004180908,
      "learning_rate": 3.221186484933939e-05,
      "loss": 1.7775,
      "step": 272500
    },
    {
      "epoch": 21.35359548801504,
      "grad_norm": 6.495645046234131,
      "learning_rate": 3.22053370933208e-05,
      "loss": 1.7463,
      "step": 272600
    },
    {
      "epoch": 21.361428795237348,
      "grad_norm": 4.84332799911499,
      "learning_rate": 3.219880933730221e-05,
      "loss": 1.763,
      "step": 272700
    },
    {
      "epoch": 21.369262102459658,
      "grad_norm": 5.0293145179748535,
      "learning_rate": 3.219228158128362e-05,
      "loss": 1.8479,
      "step": 272800
    },
    {
      "epoch": 21.377095409681967,
      "grad_norm": 5.901330947875977,
      "learning_rate": 3.218575382526503e-05,
      "loss": 1.695,
      "step": 272900
    },
    {
      "epoch": 21.384928716904277,
      "grad_norm": 8.353348731994629,
      "learning_rate": 3.217922606924644e-05,
      "loss": 1.6489,
      "step": 273000
    },
    {
      "epoch": 21.392762024126586,
      "grad_norm": 5.244693279266357,
      "learning_rate": 3.217269831322784e-05,
      "loss": 1.729,
      "step": 273100
    },
    {
      "epoch": 21.400595331348896,
      "grad_norm": 5.128543376922607,
      "learning_rate": 3.2166170557209256e-05,
      "loss": 1.7524,
      "step": 273200
    },
    {
      "epoch": 21.408428638571205,
      "grad_norm": 6.023739814758301,
      "learning_rate": 3.215964280119067e-05,
      "loss": 1.6615,
      "step": 273300
    },
    {
      "epoch": 21.416261945793515,
      "grad_norm": 4.696405410766602,
      "learning_rate": 3.2153115045172074e-05,
      "loss": 1.7796,
      "step": 273400
    },
    {
      "epoch": 21.424095253015825,
      "grad_norm": 6.429832458496094,
      "learning_rate": 3.2146587289153486e-05,
      "loss": 1.6773,
      "step": 273500
    },
    {
      "epoch": 21.431928560238134,
      "grad_norm": 4.9790143966674805,
      "learning_rate": 3.214005953313489e-05,
      "loss": 1.8374,
      "step": 273600
    },
    {
      "epoch": 21.43976186746044,
      "grad_norm": 5.546313285827637,
      "learning_rate": 3.21335317771163e-05,
      "loss": 1.643,
      "step": 273700
    },
    {
      "epoch": 21.44759517468275,
      "grad_norm": 5.348499774932861,
      "learning_rate": 3.2127004021097704e-05,
      "loss": 1.7607,
      "step": 273800
    },
    {
      "epoch": 21.45542848190506,
      "grad_norm": 5.182377338409424,
      "learning_rate": 3.2120476265079116e-05,
      "loss": 1.6785,
      "step": 273900
    },
    {
      "epoch": 21.46326178912737,
      "grad_norm": 4.990668773651123,
      "learning_rate": 3.211394850906053e-05,
      "loss": 1.7314,
      "step": 274000
    },
    {
      "epoch": 21.47109509634968,
      "grad_norm": 5.583372592926025,
      "learning_rate": 3.2107420753041935e-05,
      "loss": 1.7276,
      "step": 274100
    },
    {
      "epoch": 21.478928403571988,
      "grad_norm": 5.480179309844971,
      "learning_rate": 3.210089299702335e-05,
      "loss": 1.6964,
      "step": 274200
    },
    {
      "epoch": 21.486761710794298,
      "grad_norm": 5.535803318023682,
      "learning_rate": 3.209436524100475e-05,
      "loss": 1.8025,
      "step": 274300
    },
    {
      "epoch": 21.494595018016607,
      "grad_norm": 5.904477596282959,
      "learning_rate": 3.208783748498616e-05,
      "loss": 1.7359,
      "step": 274400
    },
    {
      "epoch": 21.502428325238917,
      "grad_norm": 4.918327808380127,
      "learning_rate": 3.208130972896757e-05,
      "loss": 1.8268,
      "step": 274500
    },
    {
      "epoch": 21.510261632461226,
      "grad_norm": 5.214669227600098,
      "learning_rate": 3.2074781972948984e-05,
      "loss": 1.6654,
      "step": 274600
    },
    {
      "epoch": 21.518094939683536,
      "grad_norm": 7.948025703430176,
      "learning_rate": 3.206825421693039e-05,
      "loss": 1.7891,
      "step": 274700
    },
    {
      "epoch": 21.525928246905842,
      "grad_norm": 4.95236349105835,
      "learning_rate": 3.20617264609118e-05,
      "loss": 1.6864,
      "step": 274800
    },
    {
      "epoch": 21.53376155412815,
      "grad_norm": 7.219773769378662,
      "learning_rate": 3.205519870489321e-05,
      "loss": 1.727,
      "step": 274900
    },
    {
      "epoch": 21.54159486135046,
      "grad_norm": 5.525318622589111,
      "learning_rate": 3.2048670948874614e-05,
      "loss": 1.6574,
      "step": 275000
    },
    {
      "epoch": 21.54942816857277,
      "grad_norm": 5.368072509765625,
      "learning_rate": 3.2042143192856026e-05,
      "loss": 1.7887,
      "step": 275100
    },
    {
      "epoch": 21.55726147579508,
      "grad_norm": 4.468935012817383,
      "learning_rate": 3.203561543683743e-05,
      "loss": 1.7252,
      "step": 275200
    },
    {
      "epoch": 21.56509478301739,
      "grad_norm": 5.400574207305908,
      "learning_rate": 3.2029087680818844e-05,
      "loss": 1.6836,
      "step": 275300
    },
    {
      "epoch": 21.5729280902397,
      "grad_norm": 4.5203986167907715,
      "learning_rate": 3.202255992480025e-05,
      "loss": 1.6566,
      "step": 275400
    },
    {
      "epoch": 21.58076139746201,
      "grad_norm": 5.512347221374512,
      "learning_rate": 3.201603216878166e-05,
      "loss": 1.7226,
      "step": 275500
    },
    {
      "epoch": 21.58859470468432,
      "grad_norm": 5.499870777130127,
      "learning_rate": 3.200950441276307e-05,
      "loss": 1.7644,
      "step": 275600
    },
    {
      "epoch": 21.596428011906628,
      "grad_norm": 4.499482154846191,
      "learning_rate": 3.2002976656744474e-05,
      "loss": 1.8076,
      "step": 275700
    },
    {
      "epoch": 21.604261319128938,
      "grad_norm": 5.531407356262207,
      "learning_rate": 3.199644890072589e-05,
      "loss": 1.7786,
      "step": 275800
    },
    {
      "epoch": 21.612094626351244,
      "grad_norm": 5.9726481437683105,
      "learning_rate": 3.19899211447073e-05,
      "loss": 1.7228,
      "step": 275900
    },
    {
      "epoch": 21.619927933573553,
      "grad_norm": 4.059138298034668,
      "learning_rate": 3.1983393388688705e-05,
      "loss": 1.6973,
      "step": 276000
    },
    {
      "epoch": 21.627761240795863,
      "grad_norm": 5.444362163543701,
      "learning_rate": 3.197686563267012e-05,
      "loss": 1.6975,
      "step": 276100
    },
    {
      "epoch": 21.635594548018172,
      "grad_norm": 4.927603244781494,
      "learning_rate": 3.1970337876651523e-05,
      "loss": 1.7751,
      "step": 276200
    },
    {
      "epoch": 21.643427855240482,
      "grad_norm": 5.54964017868042,
      "learning_rate": 3.196381012063293e-05,
      "loss": 1.5657,
      "step": 276300
    },
    {
      "epoch": 21.65126116246279,
      "grad_norm": 6.432741641998291,
      "learning_rate": 3.195728236461434e-05,
      "loss": 1.8196,
      "step": 276400
    },
    {
      "epoch": 21.6590944696851,
      "grad_norm": 6.061318397521973,
      "learning_rate": 3.1950754608595754e-05,
      "loss": 1.743,
      "step": 276500
    },
    {
      "epoch": 21.66692777690741,
      "grad_norm": 5.383225917816162,
      "learning_rate": 3.194422685257716e-05,
      "loss": 1.7317,
      "step": 276600
    },
    {
      "epoch": 21.67476108412972,
      "grad_norm": 5.831097602844238,
      "learning_rate": 3.193769909655857e-05,
      "loss": 1.7777,
      "step": 276700
    },
    {
      "epoch": 21.68259439135203,
      "grad_norm": 5.979214191436768,
      "learning_rate": 3.193117134053998e-05,
      "loss": 1.7867,
      "step": 276800
    },
    {
      "epoch": 21.69042769857434,
      "grad_norm": 5.831466197967529,
      "learning_rate": 3.1924643584521384e-05,
      "loss": 1.7493,
      "step": 276900
    },
    {
      "epoch": 21.69826100579665,
      "grad_norm": 4.973071575164795,
      "learning_rate": 3.19181158285028e-05,
      "loss": 1.7125,
      "step": 277000
    },
    {
      "epoch": 21.706094313018955,
      "grad_norm": 6.405328273773193,
      "learning_rate": 3.19115880724842e-05,
      "loss": 1.6972,
      "step": 277100
    },
    {
      "epoch": 21.713927620241265,
      "grad_norm": 4.632096290588379,
      "learning_rate": 3.1905060316465615e-05,
      "loss": 1.6845,
      "step": 277200
    },
    {
      "epoch": 21.721760927463574,
      "grad_norm": 4.280832767486572,
      "learning_rate": 3.189853256044702e-05,
      "loss": 1.704,
      "step": 277300
    },
    {
      "epoch": 21.729594234685884,
      "grad_norm": 5.167923927307129,
      "learning_rate": 3.189200480442843e-05,
      "loss": 1.8083,
      "step": 277400
    },
    {
      "epoch": 21.737427541908193,
      "grad_norm": 4.355892181396484,
      "learning_rate": 3.188547704840984e-05,
      "loss": 1.7435,
      "step": 277500
    },
    {
      "epoch": 21.745260849130503,
      "grad_norm": 7.341737270355225,
      "learning_rate": 3.1878949292391245e-05,
      "loss": 1.7063,
      "step": 277600
    },
    {
      "epoch": 21.753094156352812,
      "grad_norm": 6.086036682128906,
      "learning_rate": 3.187242153637266e-05,
      "loss": 1.7387,
      "step": 277700
    },
    {
      "epoch": 21.760927463575122,
      "grad_norm": 4.083805084228516,
      "learning_rate": 3.186589378035407e-05,
      "loss": 1.7516,
      "step": 277800
    },
    {
      "epoch": 21.76876077079743,
      "grad_norm": 4.550032138824463,
      "learning_rate": 3.1859366024335476e-05,
      "loss": 1.7454,
      "step": 277900
    },
    {
      "epoch": 21.77659407801974,
      "grad_norm": 4.199700832366943,
      "learning_rate": 3.185283826831689e-05,
      "loss": 1.702,
      "step": 278000
    },
    {
      "epoch": 21.78442738524205,
      "grad_norm": 7.20166015625,
      "learning_rate": 3.1846310512298294e-05,
      "loss": 1.757,
      "step": 278100
    },
    {
      "epoch": 21.79226069246436,
      "grad_norm": 5.151463985443115,
      "learning_rate": 3.18397827562797e-05,
      "loss": 1.7119,
      "step": 278200
    },
    {
      "epoch": 21.800093999686666,
      "grad_norm": 7.399721145629883,
      "learning_rate": 3.183325500026111e-05,
      "loss": 1.8474,
      "step": 278300
    },
    {
      "epoch": 21.807927306908976,
      "grad_norm": 5.185227394104004,
      "learning_rate": 3.182672724424252e-05,
      "loss": 1.6742,
      "step": 278400
    },
    {
      "epoch": 21.815760614131285,
      "grad_norm": 5.723212718963623,
      "learning_rate": 3.182019948822393e-05,
      "loss": 1.753,
      "step": 278500
    },
    {
      "epoch": 21.823593921353595,
      "grad_norm": 5.953592300415039,
      "learning_rate": 3.181367173220534e-05,
      "loss": 1.7993,
      "step": 278600
    },
    {
      "epoch": 21.831427228575905,
      "grad_norm": 4.2409539222717285,
      "learning_rate": 3.180714397618675e-05,
      "loss": 1.6791,
      "step": 278700
    },
    {
      "epoch": 21.839260535798214,
      "grad_norm": 4.629489898681641,
      "learning_rate": 3.1800616220168155e-05,
      "loss": 1.6398,
      "step": 278800
    },
    {
      "epoch": 21.847093843020524,
      "grad_norm": 4.712653636932373,
      "learning_rate": 3.179408846414956e-05,
      "loss": 1.735,
      "step": 278900
    },
    {
      "epoch": 21.854927150242833,
      "grad_norm": 5.993380546569824,
      "learning_rate": 3.178756070813097e-05,
      "loss": 1.7585,
      "step": 279000
    },
    {
      "epoch": 21.862760457465143,
      "grad_norm": 4.993176460266113,
      "learning_rate": 3.1781032952112386e-05,
      "loss": 1.8013,
      "step": 279100
    },
    {
      "epoch": 21.870593764687452,
      "grad_norm": 5.049488067626953,
      "learning_rate": 3.177450519609379e-05,
      "loss": 1.7575,
      "step": 279200
    },
    {
      "epoch": 21.878427071909762,
      "grad_norm": 5.130029678344727,
      "learning_rate": 3.1767977440075204e-05,
      "loss": 1.6801,
      "step": 279300
    },
    {
      "epoch": 21.886260379132068,
      "grad_norm": 4.330575942993164,
      "learning_rate": 3.176144968405661e-05,
      "loss": 1.7138,
      "step": 279400
    },
    {
      "epoch": 21.894093686354378,
      "grad_norm": 5.628951549530029,
      "learning_rate": 3.1754921928038015e-05,
      "loss": 1.7225,
      "step": 279500
    },
    {
      "epoch": 21.901926993576687,
      "grad_norm": 6.118596076965332,
      "learning_rate": 3.174839417201943e-05,
      "loss": 1.7194,
      "step": 279600
    },
    {
      "epoch": 21.909760300798997,
      "grad_norm": 5.201389789581299,
      "learning_rate": 3.174186641600084e-05,
      "loss": 1.7337,
      "step": 279700
    },
    {
      "epoch": 21.917593608021306,
      "grad_norm": 6.146477222442627,
      "learning_rate": 3.1735338659982246e-05,
      "loss": 1.6783,
      "step": 279800
    },
    {
      "epoch": 21.925426915243616,
      "grad_norm": 6.224388599395752,
      "learning_rate": 3.172881090396366e-05,
      "loss": 1.7434,
      "step": 279900
    },
    {
      "epoch": 21.933260222465925,
      "grad_norm": 6.431290149688721,
      "learning_rate": 3.1722283147945065e-05,
      "loss": 1.7986,
      "step": 280000
    },
    {
      "epoch": 21.941093529688235,
      "grad_norm": 3.832590341567993,
      "learning_rate": 3.171575539192647e-05,
      "loss": 1.7067,
      "step": 280100
    },
    {
      "epoch": 21.948926836910545,
      "grad_norm": 6.561845302581787,
      "learning_rate": 3.170922763590788e-05,
      "loss": 1.741,
      "step": 280200
    },
    {
      "epoch": 21.956760144132854,
      "grad_norm": 5.590771675109863,
      "learning_rate": 3.170269987988929e-05,
      "loss": 1.7211,
      "step": 280300
    },
    {
      "epoch": 21.964593451355164,
      "grad_norm": 5.8888468742370605,
      "learning_rate": 3.16961721238707e-05,
      "loss": 1.692,
      "step": 280400
    },
    {
      "epoch": 21.97242675857747,
      "grad_norm": 4.862770080566406,
      "learning_rate": 3.168964436785211e-05,
      "loss": 1.712,
      "step": 280500
    },
    {
      "epoch": 21.98026006579978,
      "grad_norm": 6.043678283691406,
      "learning_rate": 3.168311661183352e-05,
      "loss": 1.666,
      "step": 280600
    },
    {
      "epoch": 21.98809337302209,
      "grad_norm": 6.542028427124023,
      "learning_rate": 3.1676588855814925e-05,
      "loss": 1.739,
      "step": 280700
    },
    {
      "epoch": 21.9959266802444,
      "grad_norm": 6.481015205383301,
      "learning_rate": 3.167006109979633e-05,
      "loss": 1.7175,
      "step": 280800
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.7968285083770752,
      "eval_runtime": 2.9753,
      "eval_samples_per_second": 225.86,
      "eval_steps_per_second": 225.86,
      "step": 280852
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.4927592277526855,
      "eval_runtime": 56.754,
      "eval_samples_per_second": 224.936,
      "eval_steps_per_second": 224.936,
      "step": 280852
    },
    {
      "epoch": 22.003759987466708,
      "grad_norm": 4.372936248779297,
      "learning_rate": 3.1663533343777744e-05,
      "loss": 1.6244,
      "step": 280900
    },
    {
      "epoch": 22.011593294689018,
      "grad_norm": 5.64537239074707,
      "learning_rate": 3.1657005587759156e-05,
      "loss": 1.6542,
      "step": 281000
    },
    {
      "epoch": 22.019426601911327,
      "grad_norm": 4.913946628570557,
      "learning_rate": 3.165047783174056e-05,
      "loss": 1.655,
      "step": 281100
    },
    {
      "epoch": 22.027259909133637,
      "grad_norm": 6.866063594818115,
      "learning_rate": 3.1643950075721974e-05,
      "loss": 1.7178,
      "step": 281200
    },
    {
      "epoch": 22.035093216355946,
      "grad_norm": 8.895668029785156,
      "learning_rate": 3.163742231970338e-05,
      "loss": 1.7299,
      "step": 281300
    },
    {
      "epoch": 22.042926523578256,
      "grad_norm": 6.5949015617370605,
      "learning_rate": 3.1630894563684786e-05,
      "loss": 1.773,
      "step": 281400
    },
    {
      "epoch": 22.050759830800565,
      "grad_norm": 6.3123040199279785,
      "learning_rate": 3.16243668076662e-05,
      "loss": 1.7563,
      "step": 281500
    },
    {
      "epoch": 22.058593138022875,
      "grad_norm": 5.230783462524414,
      "learning_rate": 3.1617839051647604e-05,
      "loss": 1.6914,
      "step": 281600
    },
    {
      "epoch": 22.06642644524518,
      "grad_norm": 6.169436454772949,
      "learning_rate": 3.161131129562902e-05,
      "loss": 1.6623,
      "step": 281700
    },
    {
      "epoch": 22.07425975246749,
      "grad_norm": 7.143948554992676,
      "learning_rate": 3.160478353961043e-05,
      "loss": 1.7072,
      "step": 281800
    },
    {
      "epoch": 22.0820930596898,
      "grad_norm": 3.8163087368011475,
      "learning_rate": 3.1598255783591835e-05,
      "loss": 1.6494,
      "step": 281900
    },
    {
      "epoch": 22.08992636691211,
      "grad_norm": 4.73813533782959,
      "learning_rate": 3.159172802757324e-05,
      "loss": 1.7697,
      "step": 282000
    },
    {
      "epoch": 22.09775967413442,
      "grad_norm": 4.29341983795166,
      "learning_rate": 3.158520027155465e-05,
      "loss": 1.6977,
      "step": 282100
    },
    {
      "epoch": 22.10559298135673,
      "grad_norm": 7.630998134613037,
      "learning_rate": 3.157867251553606e-05,
      "loss": 1.6502,
      "step": 282200
    },
    {
      "epoch": 22.11342628857904,
      "grad_norm": 6.900686264038086,
      "learning_rate": 3.157214475951747e-05,
      "loss": 1.7524,
      "step": 282300
    },
    {
      "epoch": 22.121259595801348,
      "grad_norm": 5.1569647789001465,
      "learning_rate": 3.156561700349888e-05,
      "loss": 1.7723,
      "step": 282400
    },
    {
      "epoch": 22.129092903023658,
      "grad_norm": 5.085183620452881,
      "learning_rate": 3.155908924748029e-05,
      "loss": 1.7281,
      "step": 282500
    },
    {
      "epoch": 22.136926210245967,
      "grad_norm": 5.042238712310791,
      "learning_rate": 3.1552561491461696e-05,
      "loss": 1.6487,
      "step": 282600
    },
    {
      "epoch": 22.144759517468277,
      "grad_norm": 5.421289920806885,
      "learning_rate": 3.15460337354431e-05,
      "loss": 1.696,
      "step": 282700
    },
    {
      "epoch": 22.152592824690583,
      "grad_norm": 5.051898956298828,
      "learning_rate": 3.1539505979424514e-05,
      "loss": 1.765,
      "step": 282800
    },
    {
      "epoch": 22.160426131912892,
      "grad_norm": 6.536805629730225,
      "learning_rate": 3.153297822340593e-05,
      "loss": 1.6691,
      "step": 282900
    },
    {
      "epoch": 22.168259439135202,
      "grad_norm": 5.000789165496826,
      "learning_rate": 3.152645046738733e-05,
      "loss": 1.7896,
      "step": 283000
    },
    {
      "epoch": 22.17609274635751,
      "grad_norm": 4.570471286773682,
      "learning_rate": 3.1519922711368745e-05,
      "loss": 1.7878,
      "step": 283100
    },
    {
      "epoch": 22.18392605357982,
      "grad_norm": 5.545615196228027,
      "learning_rate": 3.151339495535015e-05,
      "loss": 1.8567,
      "step": 283200
    },
    {
      "epoch": 22.19175936080213,
      "grad_norm": 6.163811683654785,
      "learning_rate": 3.1506867199331557e-05,
      "loss": 1.7459,
      "step": 283300
    },
    {
      "epoch": 22.19959266802444,
      "grad_norm": 4.717906475067139,
      "learning_rate": 3.150033944331297e-05,
      "loss": 1.7548,
      "step": 283400
    },
    {
      "epoch": 22.20742597524675,
      "grad_norm": 4.786890983581543,
      "learning_rate": 3.1493811687294375e-05,
      "loss": 1.7197,
      "step": 283500
    },
    {
      "epoch": 22.21525928246906,
      "grad_norm": 5.008566379547119,
      "learning_rate": 3.148728393127579e-05,
      "loss": 1.6807,
      "step": 283600
    },
    {
      "epoch": 22.22309258969137,
      "grad_norm": 7.0416178703308105,
      "learning_rate": 3.14807561752572e-05,
      "loss": 1.6254,
      "step": 283700
    },
    {
      "epoch": 22.23092589691368,
      "grad_norm": 5.574080467224121,
      "learning_rate": 3.1474228419238606e-05,
      "loss": 1.6478,
      "step": 283800
    },
    {
      "epoch": 22.238759204135985,
      "grad_norm": 6.990809440612793,
      "learning_rate": 3.146770066322001e-05,
      "loss": 1.753,
      "step": 283900
    },
    {
      "epoch": 22.246592511358294,
      "grad_norm": 4.607572078704834,
      "learning_rate": 3.146117290720142e-05,
      "loss": 1.6969,
      "step": 284000
    },
    {
      "epoch": 22.254425818580604,
      "grad_norm": 5.65228796005249,
      "learning_rate": 3.145464515118283e-05,
      "loss": 1.7276,
      "step": 284100
    },
    {
      "epoch": 22.262259125802913,
      "grad_norm": 5.349900722503662,
      "learning_rate": 3.144811739516424e-05,
      "loss": 1.6936,
      "step": 284200
    },
    {
      "epoch": 22.270092433025223,
      "grad_norm": 6.981428623199463,
      "learning_rate": 3.144158963914565e-05,
      "loss": 1.7485,
      "step": 284300
    },
    {
      "epoch": 22.277925740247532,
      "grad_norm": 6.6942877769470215,
      "learning_rate": 3.143506188312706e-05,
      "loss": 1.7339,
      "step": 284400
    },
    {
      "epoch": 22.285759047469842,
      "grad_norm": 4.937692165374756,
      "learning_rate": 3.1428534127108466e-05,
      "loss": 1.7216,
      "step": 284500
    },
    {
      "epoch": 22.29359235469215,
      "grad_norm": 4.668025016784668,
      "learning_rate": 3.142200637108987e-05,
      "loss": 1.7546,
      "step": 284600
    },
    {
      "epoch": 22.30142566191446,
      "grad_norm": 5.367407321929932,
      "learning_rate": 3.1415478615071285e-05,
      "loss": 1.6364,
      "step": 284700
    },
    {
      "epoch": 22.30925896913677,
      "grad_norm": 5.556219577789307,
      "learning_rate": 3.140895085905269e-05,
      "loss": 1.7265,
      "step": 284800
    },
    {
      "epoch": 22.31709227635908,
      "grad_norm": 5.438953399658203,
      "learning_rate": 3.14024231030341e-05,
      "loss": 1.7406,
      "step": 284900
    },
    {
      "epoch": 22.32492558358139,
      "grad_norm": 8.496389389038086,
      "learning_rate": 3.1395895347015516e-05,
      "loss": 1.776,
      "step": 285000
    },
    {
      "epoch": 22.332758890803696,
      "grad_norm": 5.604630470275879,
      "learning_rate": 3.138936759099692e-05,
      "loss": 1.723,
      "step": 285100
    },
    {
      "epoch": 22.340592198026005,
      "grad_norm": 4.992990970611572,
      "learning_rate": 3.138283983497833e-05,
      "loss": 1.7375,
      "step": 285200
    },
    {
      "epoch": 22.348425505248315,
      "grad_norm": 5.228273391723633,
      "learning_rate": 3.137631207895974e-05,
      "loss": 1.691,
      "step": 285300
    },
    {
      "epoch": 22.356258812470625,
      "grad_norm": 4.683779239654541,
      "learning_rate": 3.1369784322941146e-05,
      "loss": 1.775,
      "step": 285400
    },
    {
      "epoch": 22.364092119692934,
      "grad_norm": 5.313415050506592,
      "learning_rate": 3.136325656692256e-05,
      "loss": 1.7484,
      "step": 285500
    },
    {
      "epoch": 22.371925426915244,
      "grad_norm": 6.6290812492370605,
      "learning_rate": 3.1356728810903964e-05,
      "loss": 1.777,
      "step": 285600
    },
    {
      "epoch": 22.379758734137553,
      "grad_norm": 5.976900577545166,
      "learning_rate": 3.1350201054885376e-05,
      "loss": 1.6988,
      "step": 285700
    },
    {
      "epoch": 22.387592041359863,
      "grad_norm": 6.501711368560791,
      "learning_rate": 3.134367329886678e-05,
      "loss": 1.8046,
      "step": 285800
    },
    {
      "epoch": 22.395425348582172,
      "grad_norm": 5.099070072174072,
      "learning_rate": 3.133714554284819e-05,
      "loss": 1.6674,
      "step": 285900
    },
    {
      "epoch": 22.403258655804482,
      "grad_norm": 4.6371073722839355,
      "learning_rate": 3.13306177868296e-05,
      "loss": 1.6978,
      "step": 286000
    },
    {
      "epoch": 22.41109196302679,
      "grad_norm": 5.9257426261901855,
      "learning_rate": 3.132409003081101e-05,
      "loss": 1.7826,
      "step": 286100
    },
    {
      "epoch": 22.418925270249098,
      "grad_norm": 5.415378570556641,
      "learning_rate": 3.131756227479242e-05,
      "loss": 1.7579,
      "step": 286200
    },
    {
      "epoch": 22.426758577471407,
      "grad_norm": 4.874994277954102,
      "learning_rate": 3.131103451877383e-05,
      "loss": 1.7707,
      "step": 286300
    },
    {
      "epoch": 22.434591884693717,
      "grad_norm": 6.281242847442627,
      "learning_rate": 3.130450676275524e-05,
      "loss": 1.6381,
      "step": 286400
    },
    {
      "epoch": 22.442425191916026,
      "grad_norm": 5.244871139526367,
      "learning_rate": 3.129797900673664e-05,
      "loss": 1.7656,
      "step": 286500
    },
    {
      "epoch": 22.450258499138336,
      "grad_norm": 4.7981696128845215,
      "learning_rate": 3.1291451250718055e-05,
      "loss": 1.7582,
      "step": 286600
    },
    {
      "epoch": 22.458091806360645,
      "grad_norm": 5.249462604522705,
      "learning_rate": 3.128492349469946e-05,
      "loss": 1.7774,
      "step": 286700
    },
    {
      "epoch": 22.465925113582955,
      "grad_norm": 5.2782511711120605,
      "learning_rate": 3.1278395738680874e-05,
      "loss": 1.612,
      "step": 286800
    },
    {
      "epoch": 22.473758420805265,
      "grad_norm": 6.409922122955322,
      "learning_rate": 3.1271867982662286e-05,
      "loss": 1.7448,
      "step": 286900
    },
    {
      "epoch": 22.481591728027574,
      "grad_norm": 6.117271900177002,
      "learning_rate": 3.126534022664369e-05,
      "loss": 1.7727,
      "step": 287000
    },
    {
      "epoch": 22.489425035249884,
      "grad_norm": 5.912721157073975,
      "learning_rate": 3.12588124706251e-05,
      "loss": 1.7172,
      "step": 287100
    },
    {
      "epoch": 22.497258342472193,
      "grad_norm": 5.107463836669922,
      "learning_rate": 3.1252284714606504e-05,
      "loss": 1.6631,
      "step": 287200
    },
    {
      "epoch": 22.5050916496945,
      "grad_norm": 4.968281269073486,
      "learning_rate": 3.1245756958587916e-05,
      "loss": 1.767,
      "step": 287300
    },
    {
      "epoch": 22.51292495691681,
      "grad_norm": 5.4093194007873535,
      "learning_rate": 3.123922920256933e-05,
      "loss": 1.682,
      "step": 287400
    },
    {
      "epoch": 22.52075826413912,
      "grad_norm": 5.0057268142700195,
      "learning_rate": 3.1232701446550734e-05,
      "loss": 1.6232,
      "step": 287500
    },
    {
      "epoch": 22.528591571361428,
      "grad_norm": 4.601038455963135,
      "learning_rate": 3.122617369053215e-05,
      "loss": 1.6848,
      "step": 287600
    },
    {
      "epoch": 22.536424878583738,
      "grad_norm": 5.279336929321289,
      "learning_rate": 3.121964593451355e-05,
      "loss": 1.7477,
      "step": 287700
    },
    {
      "epoch": 22.544258185806047,
      "grad_norm": 2.2809250354766846,
      "learning_rate": 3.121311817849496e-05,
      "loss": 1.6325,
      "step": 287800
    },
    {
      "epoch": 22.552091493028357,
      "grad_norm": 4.754916191101074,
      "learning_rate": 3.120659042247637e-05,
      "loss": 1.7749,
      "step": 287900
    },
    {
      "epoch": 22.559924800250666,
      "grad_norm": 8.084689140319824,
      "learning_rate": 3.120006266645778e-05,
      "loss": 1.7262,
      "step": 288000
    },
    {
      "epoch": 22.567758107472976,
      "grad_norm": 5.43356466293335,
      "learning_rate": 3.119353491043919e-05,
      "loss": 1.7802,
      "step": 288100
    },
    {
      "epoch": 22.575591414695285,
      "grad_norm": 5.4127020835876465,
      "learning_rate": 3.11870071544206e-05,
      "loss": 1.7398,
      "step": 288200
    },
    {
      "epoch": 22.583424721917595,
      "grad_norm": 4.588994026184082,
      "learning_rate": 3.118047939840201e-05,
      "loss": 1.6759,
      "step": 288300
    },
    {
      "epoch": 22.591258029139905,
      "grad_norm": 3.1592941284179688,
      "learning_rate": 3.1173951642383413e-05,
      "loss": 1.8261,
      "step": 288400
    },
    {
      "epoch": 22.59909133636221,
      "grad_norm": 4.792333602905273,
      "learning_rate": 3.1167423886364826e-05,
      "loss": 1.7981,
      "step": 288500
    },
    {
      "epoch": 22.60692464358452,
      "grad_norm": 6.38219690322876,
      "learning_rate": 3.116089613034623e-05,
      "loss": 1.7215,
      "step": 288600
    },
    {
      "epoch": 22.61475795080683,
      "grad_norm": 6.24065637588501,
      "learning_rate": 3.1154368374327644e-05,
      "loss": 1.7548,
      "step": 288700
    },
    {
      "epoch": 22.62259125802914,
      "grad_norm": 5.804256916046143,
      "learning_rate": 3.114784061830906e-05,
      "loss": 1.8299,
      "step": 288800
    },
    {
      "epoch": 22.63042456525145,
      "grad_norm": 6.289796829223633,
      "learning_rate": 3.114131286229046e-05,
      "loss": 1.7902,
      "step": 288900
    },
    {
      "epoch": 22.63825787247376,
      "grad_norm": 6.029387474060059,
      "learning_rate": 3.113478510627187e-05,
      "loss": 1.7505,
      "step": 289000
    },
    {
      "epoch": 22.646091179696068,
      "grad_norm": 6.169242858886719,
      "learning_rate": 3.1128257350253274e-05,
      "loss": 1.7384,
      "step": 289100
    },
    {
      "epoch": 22.653924486918378,
      "grad_norm": 5.050036907196045,
      "learning_rate": 3.112172959423469e-05,
      "loss": 1.6917,
      "step": 289200
    },
    {
      "epoch": 22.661757794140687,
      "grad_norm": 4.641050338745117,
      "learning_rate": 3.11152018382161e-05,
      "loss": 1.7755,
      "step": 289300
    },
    {
      "epoch": 22.669591101362997,
      "grad_norm": 4.7233734130859375,
      "learning_rate": 3.1108674082197505e-05,
      "loss": 1.708,
      "step": 289400
    },
    {
      "epoch": 22.677424408585306,
      "grad_norm": 5.7199482917785645,
      "learning_rate": 3.110214632617892e-05,
      "loss": 1.701,
      "step": 289500
    },
    {
      "epoch": 22.685257715807612,
      "grad_norm": 4.86213493347168,
      "learning_rate": 3.109561857016032e-05,
      "loss": 1.7103,
      "step": 289600
    },
    {
      "epoch": 22.693091023029922,
      "grad_norm": 5.741167068481445,
      "learning_rate": 3.108909081414173e-05,
      "loss": 1.7705,
      "step": 289700
    },
    {
      "epoch": 22.70092433025223,
      "grad_norm": 6.1564812660217285,
      "learning_rate": 3.108256305812314e-05,
      "loss": 1.6559,
      "step": 289800
    },
    {
      "epoch": 22.70875763747454,
      "grad_norm": 5.917902946472168,
      "learning_rate": 3.107603530210455e-05,
      "loss": 1.7084,
      "step": 289900
    },
    {
      "epoch": 22.71659094469685,
      "grad_norm": 4.690139293670654,
      "learning_rate": 3.106950754608596e-05,
      "loss": 1.6654,
      "step": 290000
    },
    {
      "epoch": 22.72442425191916,
      "grad_norm": 4.924752712249756,
      "learning_rate": 3.106297979006737e-05,
      "loss": 1.6968,
      "step": 290100
    },
    {
      "epoch": 22.73225755914147,
      "grad_norm": 5.426806449890137,
      "learning_rate": 3.105645203404878e-05,
      "loss": 1.6977,
      "step": 290200
    },
    {
      "epoch": 22.74009086636378,
      "grad_norm": 6.50513219833374,
      "learning_rate": 3.1049924278030184e-05,
      "loss": 1.8113,
      "step": 290300
    },
    {
      "epoch": 22.74792417358609,
      "grad_norm": 4.529226779937744,
      "learning_rate": 3.1043396522011597e-05,
      "loss": 1.7503,
      "step": 290400
    },
    {
      "epoch": 22.7557574808084,
      "grad_norm": 4.546811103820801,
      "learning_rate": 3.1036868765993e-05,
      "loss": 1.7807,
      "step": 290500
    },
    {
      "epoch": 22.763590788030708,
      "grad_norm": 4.170319080352783,
      "learning_rate": 3.1030341009974415e-05,
      "loss": 1.7465,
      "step": 290600
    },
    {
      "epoch": 22.771424095253018,
      "grad_norm": 9.860786437988281,
      "learning_rate": 3.102381325395582e-05,
      "loss": 1.7166,
      "step": 290700
    },
    {
      "epoch": 22.779257402475324,
      "grad_norm": 5.816154479980469,
      "learning_rate": 3.101728549793723e-05,
      "loss": 1.7412,
      "step": 290800
    },
    {
      "epoch": 22.787090709697633,
      "grad_norm": 4.648619651794434,
      "learning_rate": 3.101075774191864e-05,
      "loss": 1.6801,
      "step": 290900
    },
    {
      "epoch": 22.794924016919943,
      "grad_norm": 5.521981239318848,
      "learning_rate": 3.1004229985900045e-05,
      "loss": 1.74,
      "step": 291000
    },
    {
      "epoch": 22.802757324142252,
      "grad_norm": 6.2733588218688965,
      "learning_rate": 3.099770222988146e-05,
      "loss": 1.7356,
      "step": 291100
    },
    {
      "epoch": 22.810590631364562,
      "grad_norm": 4.605215549468994,
      "learning_rate": 3.099117447386286e-05,
      "loss": 1.7161,
      "step": 291200
    },
    {
      "epoch": 22.81842393858687,
      "grad_norm": 6.709367752075195,
      "learning_rate": 3.0984646717844276e-05,
      "loss": 1.7191,
      "step": 291300
    },
    {
      "epoch": 22.82625724580918,
      "grad_norm": 5.21791410446167,
      "learning_rate": 3.097811896182569e-05,
      "loss": 1.7318,
      "step": 291400
    },
    {
      "epoch": 22.83409055303149,
      "grad_norm": 4.235527992248535,
      "learning_rate": 3.0971591205807094e-05,
      "loss": 1.6222,
      "step": 291500
    },
    {
      "epoch": 22.8419238602538,
      "grad_norm": 6.450220108032227,
      "learning_rate": 3.09650634497885e-05,
      "loss": 1.7085,
      "step": 291600
    },
    {
      "epoch": 22.84975716747611,
      "grad_norm": 7.494153022766113,
      "learning_rate": 3.095853569376991e-05,
      "loss": 1.7244,
      "step": 291700
    },
    {
      "epoch": 22.85759047469842,
      "grad_norm": 4.200376033782959,
      "learning_rate": 3.095200793775132e-05,
      "loss": 1.7378,
      "step": 291800
    },
    {
      "epoch": 22.865423781920725,
      "grad_norm": 5.720424652099609,
      "learning_rate": 3.094548018173273e-05,
      "loss": 1.674,
      "step": 291900
    },
    {
      "epoch": 22.873257089143035,
      "grad_norm": 4.775826930999756,
      "learning_rate": 3.093895242571414e-05,
      "loss": 1.7269,
      "step": 292000
    },
    {
      "epoch": 22.881090396365344,
      "grad_norm": 5.160279273986816,
      "learning_rate": 3.093242466969555e-05,
      "loss": 1.7282,
      "step": 292100
    },
    {
      "epoch": 22.888923703587654,
      "grad_norm": 5.296425819396973,
      "learning_rate": 3.0925896913676955e-05,
      "loss": 1.6574,
      "step": 292200
    },
    {
      "epoch": 22.896757010809964,
      "grad_norm": 4.677258014678955,
      "learning_rate": 3.091936915765836e-05,
      "loss": 1.7988,
      "step": 292300
    },
    {
      "epoch": 22.904590318032273,
      "grad_norm": 6.942376613616943,
      "learning_rate": 3.091284140163977e-05,
      "loss": 1.713,
      "step": 292400
    },
    {
      "epoch": 22.912423625254583,
      "grad_norm": 6.142309665679932,
      "learning_rate": 3.0906313645621185e-05,
      "loss": 1.7699,
      "step": 292500
    },
    {
      "epoch": 22.920256932476892,
      "grad_norm": 5.720016002655029,
      "learning_rate": 3.089978588960259e-05,
      "loss": 1.7711,
      "step": 292600
    },
    {
      "epoch": 22.928090239699202,
      "grad_norm": 5.166191577911377,
      "learning_rate": 3.0893258133584004e-05,
      "loss": 1.6356,
      "step": 292700
    },
    {
      "epoch": 22.93592354692151,
      "grad_norm": 5.940925121307373,
      "learning_rate": 3.088673037756541e-05,
      "loss": 1.757,
      "step": 292800
    },
    {
      "epoch": 22.94375685414382,
      "grad_norm": 4.829354286193848,
      "learning_rate": 3.0880202621546815e-05,
      "loss": 1.7356,
      "step": 292900
    },
    {
      "epoch": 22.951590161366127,
      "grad_norm": 5.019040107727051,
      "learning_rate": 3.087367486552823e-05,
      "loss": 1.799,
      "step": 293000
    },
    {
      "epoch": 22.959423468588437,
      "grad_norm": 4.629096508026123,
      "learning_rate": 3.0867147109509634e-05,
      "loss": 1.7421,
      "step": 293100
    },
    {
      "epoch": 22.967256775810746,
      "grad_norm": 5.802663803100586,
      "learning_rate": 3.0860619353491046e-05,
      "loss": 1.6437,
      "step": 293200
    },
    {
      "epoch": 22.975090083033056,
      "grad_norm": 6.6194000244140625,
      "learning_rate": 3.085409159747246e-05,
      "loss": 1.7399,
      "step": 293300
    },
    {
      "epoch": 22.982923390255365,
      "grad_norm": 7.27699089050293,
      "learning_rate": 3.0847563841453864e-05,
      "loss": 1.7504,
      "step": 293400
    },
    {
      "epoch": 22.990756697477675,
      "grad_norm": 5.099135875701904,
      "learning_rate": 3.084103608543527e-05,
      "loss": 1.7224,
      "step": 293500
    },
    {
      "epoch": 22.998590004699984,
      "grad_norm": 5.420142650604248,
      "learning_rate": 3.083450832941668e-05,
      "loss": 1.7584,
      "step": 293600
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.7894879579544067,
      "eval_runtime": 2.9464,
      "eval_samples_per_second": 228.075,
      "eval_steps_per_second": 228.075,
      "step": 293618
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.4846644401550293,
      "eval_runtime": 56.5725,
      "eval_samples_per_second": 225.657,
      "eval_steps_per_second": 225.657,
      "step": 293618
    },
    {
      "epoch": 23.006423311922294,
      "grad_norm": 6.169538974761963,
      "learning_rate": 3.082798057339809e-05,
      "loss": 1.6793,
      "step": 293700
    },
    {
      "epoch": 23.014256619144604,
      "grad_norm": 5.131904125213623,
      "learning_rate": 3.08214528173795e-05,
      "loss": 1.6381,
      "step": 293800
    },
    {
      "epoch": 23.022089926366913,
      "grad_norm": 5.773886680603027,
      "learning_rate": 3.081492506136091e-05,
      "loss": 1.733,
      "step": 293900
    },
    {
      "epoch": 23.029923233589223,
      "grad_norm": 6.718936443328857,
      "learning_rate": 3.080839730534232e-05,
      "loss": 1.7571,
      "step": 294000
    },
    {
      "epoch": 23.037756540811532,
      "grad_norm": 6.774266242980957,
      "learning_rate": 3.0801869549323725e-05,
      "loss": 1.7108,
      "step": 294100
    },
    {
      "epoch": 23.04558984803384,
      "grad_norm": 5.148423194885254,
      "learning_rate": 3.079534179330513e-05,
      "loss": 1.7257,
      "step": 294200
    },
    {
      "epoch": 23.053423155256148,
      "grad_norm": 3.898155450820923,
      "learning_rate": 3.0788814037286543e-05,
      "loss": 1.6238,
      "step": 294300
    },
    {
      "epoch": 23.061256462478458,
      "grad_norm": 4.138207912445068,
      "learning_rate": 3.078228628126795e-05,
      "loss": 1.7284,
      "step": 294400
    },
    {
      "epoch": 23.069089769700767,
      "grad_norm": 6.669604301452637,
      "learning_rate": 3.077575852524936e-05,
      "loss": 1.6572,
      "step": 294500
    },
    {
      "epoch": 23.076923076923077,
      "grad_norm": 7.277600288391113,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 1.7857,
      "step": 294600
    },
    {
      "epoch": 23.084756384145386,
      "grad_norm": 4.729305744171143,
      "learning_rate": 3.076270301321218e-05,
      "loss": 1.7347,
      "step": 294700
    },
    {
      "epoch": 23.092589691367696,
      "grad_norm": 4.048577308654785,
      "learning_rate": 3.0756175257193586e-05,
      "loss": 1.7274,
      "step": 294800
    },
    {
      "epoch": 23.100422998590005,
      "grad_norm": 5.334682941436768,
      "learning_rate": 3.0749647501175e-05,
      "loss": 1.637,
      "step": 294900
    },
    {
      "epoch": 23.108256305812315,
      "grad_norm": 7.152481555938721,
      "learning_rate": 3.0743119745156404e-05,
      "loss": 1.6866,
      "step": 295000
    },
    {
      "epoch": 23.116089613034625,
      "grad_norm": 6.144737243652344,
      "learning_rate": 3.073659198913782e-05,
      "loss": 1.6945,
      "step": 295100
    },
    {
      "epoch": 23.123922920256934,
      "grad_norm": 5.328644752502441,
      "learning_rate": 3.073006423311923e-05,
      "loss": 1.7443,
      "step": 295200
    },
    {
      "epoch": 23.13175622747924,
      "grad_norm": 4.520713806152344,
      "learning_rate": 3.0723536477100635e-05,
      "loss": 1.8088,
      "step": 295300
    },
    {
      "epoch": 23.13958953470155,
      "grad_norm": 6.014644145965576,
      "learning_rate": 3.071700872108204e-05,
      "loss": 1.7228,
      "step": 295400
    },
    {
      "epoch": 23.14742284192386,
      "grad_norm": 4.551931381225586,
      "learning_rate": 3.071048096506345e-05,
      "loss": 1.6536,
      "step": 295500
    },
    {
      "epoch": 23.15525614914617,
      "grad_norm": 4.625406742095947,
      "learning_rate": 3.070395320904486e-05,
      "loss": 1.7677,
      "step": 295600
    },
    {
      "epoch": 23.16308945636848,
      "grad_norm": 4.820557594299316,
      "learning_rate": 3.069742545302627e-05,
      "loss": 1.6995,
      "step": 295700
    },
    {
      "epoch": 23.170922763590788,
      "grad_norm": 5.709240436553955,
      "learning_rate": 3.069089769700768e-05,
      "loss": 1.7597,
      "step": 295800
    },
    {
      "epoch": 23.178756070813098,
      "grad_norm": 5.840571403503418,
      "learning_rate": 3.068436994098909e-05,
      "loss": 1.7655,
      "step": 295900
    },
    {
      "epoch": 23.186589378035407,
      "grad_norm": 5.0226874351501465,
      "learning_rate": 3.0677842184970496e-05,
      "loss": 1.7082,
      "step": 296000
    },
    {
      "epoch": 23.194422685257717,
      "grad_norm": 5.536838054656982,
      "learning_rate": 3.06713144289519e-05,
      "loss": 1.6393,
      "step": 296100
    },
    {
      "epoch": 23.202255992480026,
      "grad_norm": 5.1473517417907715,
      "learning_rate": 3.0664786672933314e-05,
      "loss": 1.6338,
      "step": 296200
    },
    {
      "epoch": 23.210089299702336,
      "grad_norm": 5.9422407150268555,
      "learning_rate": 3.065825891691472e-05,
      "loss": 1.6849,
      "step": 296300
    },
    {
      "epoch": 23.217922606924642,
      "grad_norm": 5.561523914337158,
      "learning_rate": 3.065173116089613e-05,
      "loss": 1.7217,
      "step": 296400
    },
    {
      "epoch": 23.22575591414695,
      "grad_norm": 5.108399391174316,
      "learning_rate": 3.0645203404877545e-05,
      "loss": 1.7012,
      "step": 296500
    },
    {
      "epoch": 23.23358922136926,
      "grad_norm": 6.037974834442139,
      "learning_rate": 3.063867564885895e-05,
      "loss": 1.7058,
      "step": 296600
    },
    {
      "epoch": 23.24142252859157,
      "grad_norm": 4.976009845733643,
      "learning_rate": 3.0632147892840356e-05,
      "loss": 1.7165,
      "step": 296700
    },
    {
      "epoch": 23.24925583581388,
      "grad_norm": 6.577732086181641,
      "learning_rate": 3.062562013682177e-05,
      "loss": 1.7267,
      "step": 296800
    },
    {
      "epoch": 23.25708914303619,
      "grad_norm": 4.391816139221191,
      "learning_rate": 3.0619092380803175e-05,
      "loss": 1.6738,
      "step": 296900
    },
    {
      "epoch": 23.2649224502585,
      "grad_norm": 5.409089088439941,
      "learning_rate": 3.061256462478459e-05,
      "loss": 1.6768,
      "step": 297000
    },
    {
      "epoch": 23.27275575748081,
      "grad_norm": 5.4331817626953125,
      "learning_rate": 3.0606036868766e-05,
      "loss": 1.823,
      "step": 297100
    },
    {
      "epoch": 23.28058906470312,
      "grad_norm": 5.882251262664795,
      "learning_rate": 3.0599509112747406e-05,
      "loss": 1.6131,
      "step": 297200
    },
    {
      "epoch": 23.288422371925428,
      "grad_norm": 3.87550687789917,
      "learning_rate": 3.059298135672881e-05,
      "loss": 1.6969,
      "step": 297300
    },
    {
      "epoch": 23.296255679147738,
      "grad_norm": 4.736552715301514,
      "learning_rate": 3.058645360071022e-05,
      "loss": 1.6639,
      "step": 297400
    },
    {
      "epoch": 23.304088986370047,
      "grad_norm": 5.165604591369629,
      "learning_rate": 3.057992584469163e-05,
      "loss": 1.6418,
      "step": 297500
    },
    {
      "epoch": 23.311922293592353,
      "grad_norm": 4.652923107147217,
      "learning_rate": 3.0573398088673035e-05,
      "loss": 1.7412,
      "step": 297600
    },
    {
      "epoch": 23.319755600814663,
      "grad_norm": 3.909703254699707,
      "learning_rate": 3.056687033265445e-05,
      "loss": 1.7374,
      "step": 297700
    },
    {
      "epoch": 23.327588908036972,
      "grad_norm": 4.526771545410156,
      "learning_rate": 3.056034257663586e-05,
      "loss": 1.7566,
      "step": 297800
    },
    {
      "epoch": 23.335422215259282,
      "grad_norm": 3.1141517162323,
      "learning_rate": 3.0553814820617266e-05,
      "loss": 1.6945,
      "step": 297900
    },
    {
      "epoch": 23.34325552248159,
      "grad_norm": 4.981848239898682,
      "learning_rate": 3.054728706459867e-05,
      "loss": 1.7694,
      "step": 298000
    },
    {
      "epoch": 23.3510888297039,
      "grad_norm": 5.46175479888916,
      "learning_rate": 3.0540759308580085e-05,
      "loss": 1.7355,
      "step": 298100
    },
    {
      "epoch": 23.35892213692621,
      "grad_norm": 3.9348950386047363,
      "learning_rate": 3.053423155256149e-05,
      "loss": 1.7069,
      "step": 298200
    },
    {
      "epoch": 23.36675544414852,
      "grad_norm": 5.292340278625488,
      "learning_rate": 3.05277037965429e-05,
      "loss": 1.8154,
      "step": 298300
    },
    {
      "epoch": 23.37458875137083,
      "grad_norm": 4.9051642417907715,
      "learning_rate": 3.0521176040524316e-05,
      "loss": 1.616,
      "step": 298400
    },
    {
      "epoch": 23.38242205859314,
      "grad_norm": 4.968937397003174,
      "learning_rate": 3.051464828450572e-05,
      "loss": 1.7361,
      "step": 298500
    },
    {
      "epoch": 23.39025536581545,
      "grad_norm": 4.867792129516602,
      "learning_rate": 3.0508120528487127e-05,
      "loss": 1.7315,
      "step": 298600
    },
    {
      "epoch": 23.398088673037755,
      "grad_norm": 5.672006607055664,
      "learning_rate": 3.050159277246854e-05,
      "loss": 1.7096,
      "step": 298700
    },
    {
      "epoch": 23.405921980260064,
      "grad_norm": 5.228585243225098,
      "learning_rate": 3.049506501644995e-05,
      "loss": 1.6537,
      "step": 298800
    },
    {
      "epoch": 23.413755287482374,
      "grad_norm": 5.005222797393799,
      "learning_rate": 3.0488537260431355e-05,
      "loss": 1.7937,
      "step": 298900
    },
    {
      "epoch": 23.421588594704684,
      "grad_norm": 5.751078128814697,
      "learning_rate": 3.048200950441276e-05,
      "loss": 1.6912,
      "step": 299000
    },
    {
      "epoch": 23.429421901926993,
      "grad_norm": 5.458224296569824,
      "learning_rate": 3.0475481748394176e-05,
      "loss": 1.8271,
      "step": 299100
    },
    {
      "epoch": 23.437255209149303,
      "grad_norm": 5.363770008087158,
      "learning_rate": 3.0468953992375582e-05,
      "loss": 1.7998,
      "step": 299200
    },
    {
      "epoch": 23.445088516371612,
      "grad_norm": 5.290584087371826,
      "learning_rate": 3.0462426236356988e-05,
      "loss": 1.6566,
      "step": 299300
    },
    {
      "epoch": 23.452921823593922,
      "grad_norm": 5.267543315887451,
      "learning_rate": 3.04558984803384e-05,
      "loss": 1.7478,
      "step": 299400
    },
    {
      "epoch": 23.46075513081623,
      "grad_norm": 5.279508113861084,
      "learning_rate": 3.044937072431981e-05,
      "loss": 1.7202,
      "step": 299500
    },
    {
      "epoch": 23.46858843803854,
      "grad_norm": 6.350532531738281,
      "learning_rate": 3.0442842968301215e-05,
      "loss": 1.6277,
      "step": 299600
    },
    {
      "epoch": 23.47642174526085,
      "grad_norm": 5.085296154022217,
      "learning_rate": 3.0436315212282628e-05,
      "loss": 1.7421,
      "step": 299700
    },
    {
      "epoch": 23.484255052483157,
      "grad_norm": 5.782206058502197,
      "learning_rate": 3.0429787456264037e-05,
      "loss": 1.6586,
      "step": 299800
    },
    {
      "epoch": 23.492088359705466,
      "grad_norm": 5.857326984405518,
      "learning_rate": 3.0423259700245443e-05,
      "loss": 1.7282,
      "step": 299900
    },
    {
      "epoch": 23.499921666927776,
      "grad_norm": 3.2921524047851562,
      "learning_rate": 3.0416731944226855e-05,
      "loss": 1.7497,
      "step": 300000
    },
    {
      "epoch": 23.507754974150085,
      "grad_norm": 6.431890487670898,
      "learning_rate": 3.0410204188208264e-05,
      "loss": 1.773,
      "step": 300100
    },
    {
      "epoch": 23.515588281372395,
      "grad_norm": 5.600930690765381,
      "learning_rate": 3.040367643218967e-05,
      "loss": 1.6855,
      "step": 300200
    },
    {
      "epoch": 23.523421588594704,
      "grad_norm": 7.604100704193115,
      "learning_rate": 3.0397148676171083e-05,
      "loss": 1.7801,
      "step": 300300
    },
    {
      "epoch": 23.531254895817014,
      "grad_norm": 5.394942283630371,
      "learning_rate": 3.0390620920152492e-05,
      "loss": 1.7758,
      "step": 300400
    },
    {
      "epoch": 23.539088203039324,
      "grad_norm": 6.897117614746094,
      "learning_rate": 3.0384093164133898e-05,
      "loss": 1.6648,
      "step": 300500
    },
    {
      "epoch": 23.546921510261633,
      "grad_norm": 4.30126953125,
      "learning_rate": 3.037756540811531e-05,
      "loss": 1.701,
      "step": 300600
    },
    {
      "epoch": 23.554754817483943,
      "grad_norm": 4.667685031890869,
      "learning_rate": 3.037103765209672e-05,
      "loss": 1.6067,
      "step": 300700
    },
    {
      "epoch": 23.562588124706252,
      "grad_norm": 5.994243621826172,
      "learning_rate": 3.0364509896078125e-05,
      "loss": 1.7075,
      "step": 300800
    },
    {
      "epoch": 23.570421431928562,
      "grad_norm": 8.109087944030762,
      "learning_rate": 3.035798214005953e-05,
      "loss": 1.7938,
      "step": 300900
    },
    {
      "epoch": 23.578254739150868,
      "grad_norm": 5.236028671264648,
      "learning_rate": 3.0351454384040943e-05,
      "loss": 1.687,
      "step": 301000
    },
    {
      "epoch": 23.586088046373177,
      "grad_norm": 4.564830780029297,
      "learning_rate": 3.0344926628022353e-05,
      "loss": 1.6957,
      "step": 301100
    },
    {
      "epoch": 23.593921353595487,
      "grad_norm": 4.9266676902771,
      "learning_rate": 3.033839887200376e-05,
      "loss": 1.7867,
      "step": 301200
    },
    {
      "epoch": 23.601754660817797,
      "grad_norm": 6.012948513031006,
      "learning_rate": 3.033187111598517e-05,
      "loss": 1.7101,
      "step": 301300
    },
    {
      "epoch": 23.609587968040106,
      "grad_norm": 5.22999906539917,
      "learning_rate": 3.032534335996658e-05,
      "loss": 1.766,
      "step": 301400
    },
    {
      "epoch": 23.617421275262416,
      "grad_norm": 3.368541955947876,
      "learning_rate": 3.0318815603947986e-05,
      "loss": 1.6583,
      "step": 301500
    },
    {
      "epoch": 23.625254582484725,
      "grad_norm": 4.355592250823975,
      "learning_rate": 3.03122878479294e-05,
      "loss": 1.6429,
      "step": 301600
    },
    {
      "epoch": 23.633087889707035,
      "grad_norm": 5.662996768951416,
      "learning_rate": 3.0305760091910808e-05,
      "loss": 1.7579,
      "step": 301700
    },
    {
      "epoch": 23.640921196929344,
      "grad_norm": 4.7635498046875,
      "learning_rate": 3.0299232335892213e-05,
      "loss": 1.7349,
      "step": 301800
    },
    {
      "epoch": 23.648754504151654,
      "grad_norm": 5.089869022369385,
      "learning_rate": 3.0292704579873626e-05,
      "loss": 1.7965,
      "step": 301900
    },
    {
      "epoch": 23.656587811373964,
      "grad_norm": 5.360859394073486,
      "learning_rate": 3.0286176823855035e-05,
      "loss": 1.7619,
      "step": 302000
    },
    {
      "epoch": 23.664421118596273,
      "grad_norm": 4.861874580383301,
      "learning_rate": 3.027964906783644e-05,
      "loss": 1.7638,
      "step": 302100
    },
    {
      "epoch": 23.67225442581858,
      "grad_norm": 6.776408672332764,
      "learning_rate": 3.0273121311817853e-05,
      "loss": 1.8176,
      "step": 302200
    },
    {
      "epoch": 23.68008773304089,
      "grad_norm": 5.106059551239014,
      "learning_rate": 3.0266593555799262e-05,
      "loss": 1.7244,
      "step": 302300
    },
    {
      "epoch": 23.6879210402632,
      "grad_norm": 7.005305290222168,
      "learning_rate": 3.0260065799780668e-05,
      "loss": 1.7613,
      "step": 302400
    },
    {
      "epoch": 23.695754347485508,
      "grad_norm": 5.927325248718262,
      "learning_rate": 3.0253538043762074e-05,
      "loss": 1.7727,
      "step": 302500
    },
    {
      "epoch": 23.703587654707817,
      "grad_norm": 5.945533752441406,
      "learning_rate": 3.0247010287743487e-05,
      "loss": 1.6595,
      "step": 302600
    },
    {
      "epoch": 23.711420961930127,
      "grad_norm": 5.0145416259765625,
      "learning_rate": 3.0240482531724896e-05,
      "loss": 1.7394,
      "step": 302700
    },
    {
      "epoch": 23.719254269152437,
      "grad_norm": 4.659457206726074,
      "learning_rate": 3.02339547757063e-05,
      "loss": 1.7423,
      "step": 302800
    },
    {
      "epoch": 23.727087576374746,
      "grad_norm": 4.358913898468018,
      "learning_rate": 3.0227427019687714e-05,
      "loss": 1.7165,
      "step": 302900
    },
    {
      "epoch": 23.734920883597056,
      "grad_norm": 5.466214179992676,
      "learning_rate": 3.0220899263669123e-05,
      "loss": 1.7308,
      "step": 303000
    },
    {
      "epoch": 23.742754190819365,
      "grad_norm": 5.661168575286865,
      "learning_rate": 3.021437150765053e-05,
      "loss": 1.7784,
      "step": 303100
    },
    {
      "epoch": 23.750587498041675,
      "grad_norm": 3.3352434635162354,
      "learning_rate": 3.020784375163194e-05,
      "loss": 1.7006,
      "step": 303200
    },
    {
      "epoch": 23.75842080526398,
      "grad_norm": 5.261358261108398,
      "learning_rate": 3.020131599561335e-05,
      "loss": 1.6986,
      "step": 303300
    },
    {
      "epoch": 23.76625411248629,
      "grad_norm": 6.862226486206055,
      "learning_rate": 3.0194788239594756e-05,
      "loss": 1.6924,
      "step": 303400
    },
    {
      "epoch": 23.7740874197086,
      "grad_norm": 5.878227710723877,
      "learning_rate": 3.018826048357617e-05,
      "loss": 1.6432,
      "step": 303500
    },
    {
      "epoch": 23.78192072693091,
      "grad_norm": 7.461545944213867,
      "learning_rate": 3.0181732727557578e-05,
      "loss": 1.6524,
      "step": 303600
    },
    {
      "epoch": 23.78975403415322,
      "grad_norm": 5.064303398132324,
      "learning_rate": 3.0175204971538984e-05,
      "loss": 1.7466,
      "step": 303700
    },
    {
      "epoch": 23.79758734137553,
      "grad_norm": 5.965860366821289,
      "learning_rate": 3.0168677215520396e-05,
      "loss": 1.7052,
      "step": 303800
    },
    {
      "epoch": 23.80542064859784,
      "grad_norm": 5.784107685089111,
      "learning_rate": 3.0162149459501806e-05,
      "loss": 1.6859,
      "step": 303900
    },
    {
      "epoch": 23.813253955820148,
      "grad_norm": 6.663691997528076,
      "learning_rate": 3.015562170348321e-05,
      "loss": 1.6369,
      "step": 304000
    },
    {
      "epoch": 23.821087263042457,
      "grad_norm": 5.948809623718262,
      "learning_rate": 3.0149093947464617e-05,
      "loss": 1.7117,
      "step": 304100
    },
    {
      "epoch": 23.828920570264767,
      "grad_norm": 5.006211280822754,
      "learning_rate": 3.014256619144603e-05,
      "loss": 1.6718,
      "step": 304200
    },
    {
      "epoch": 23.836753877487077,
      "grad_norm": 4.737772464752197,
      "learning_rate": 3.013603843542744e-05,
      "loss": 1.7647,
      "step": 304300
    },
    {
      "epoch": 23.844587184709383,
      "grad_norm": 5.942655563354492,
      "learning_rate": 3.0129510679408845e-05,
      "loss": 1.789,
      "step": 304400
    },
    {
      "epoch": 23.852420491931692,
      "grad_norm": 6.216699123382568,
      "learning_rate": 3.0122982923390257e-05,
      "loss": 1.6937,
      "step": 304500
    },
    {
      "epoch": 23.860253799154002,
      "grad_norm": 38.84293746948242,
      "learning_rate": 3.0116455167371666e-05,
      "loss": 1.7344,
      "step": 304600
    },
    {
      "epoch": 23.86808710637631,
      "grad_norm": 5.40496826171875,
      "learning_rate": 3.0109927411353072e-05,
      "loss": 1.6464,
      "step": 304700
    },
    {
      "epoch": 23.87592041359862,
      "grad_norm": 5.454063892364502,
      "learning_rate": 3.0103399655334485e-05,
      "loss": 1.7576,
      "step": 304800
    },
    {
      "epoch": 23.88375372082093,
      "grad_norm": 7.05671501159668,
      "learning_rate": 3.0096871899315894e-05,
      "loss": 1.6672,
      "step": 304900
    },
    {
      "epoch": 23.89158702804324,
      "grad_norm": 7.760739803314209,
      "learning_rate": 3.00903441432973e-05,
      "loss": 1.7347,
      "step": 305000
    },
    {
      "epoch": 23.89942033526555,
      "grad_norm": 5.069332599639893,
      "learning_rate": 3.0083816387278712e-05,
      "loss": 1.7909,
      "step": 305100
    },
    {
      "epoch": 23.90725364248786,
      "grad_norm": 7.294741630554199,
      "learning_rate": 3.007728863126012e-05,
      "loss": 1.6722,
      "step": 305200
    },
    {
      "epoch": 23.91508694971017,
      "grad_norm": 4.874174118041992,
      "learning_rate": 3.0070760875241527e-05,
      "loss": 1.7612,
      "step": 305300
    },
    {
      "epoch": 23.92292025693248,
      "grad_norm": 5.193948268890381,
      "learning_rate": 3.006423311922294e-05,
      "loss": 1.7615,
      "step": 305400
    },
    {
      "epoch": 23.930753564154784,
      "grad_norm": 4.033324718475342,
      "learning_rate": 3.005770536320435e-05,
      "loss": 1.7079,
      "step": 305500
    },
    {
      "epoch": 23.938586871377094,
      "grad_norm": 5.5857625007629395,
      "learning_rate": 3.0051177607185754e-05,
      "loss": 1.7991,
      "step": 305600
    },
    {
      "epoch": 23.946420178599404,
      "grad_norm": 4.518848419189453,
      "learning_rate": 3.004464985116716e-05,
      "loss": 1.712,
      "step": 305700
    },
    {
      "epoch": 23.954253485821713,
      "grad_norm": 7.089290142059326,
      "learning_rate": 3.0038122095148573e-05,
      "loss": 1.8213,
      "step": 305800
    },
    {
      "epoch": 23.962086793044023,
      "grad_norm": 4.834380626678467,
      "learning_rate": 3.0031594339129982e-05,
      "loss": 1.6711,
      "step": 305900
    },
    {
      "epoch": 23.969920100266332,
      "grad_norm": 6.523028373718262,
      "learning_rate": 3.0025066583111388e-05,
      "loss": 1.6985,
      "step": 306000
    },
    {
      "epoch": 23.977753407488642,
      "grad_norm": 6.026276111602783,
      "learning_rate": 3.00185388270928e-05,
      "loss": 1.7161,
      "step": 306100
    },
    {
      "epoch": 23.98558671471095,
      "grad_norm": 4.657932758331299,
      "learning_rate": 3.001201107107421e-05,
      "loss": 1.6879,
      "step": 306200
    },
    {
      "epoch": 23.99342002193326,
      "grad_norm": 5.558953285217285,
      "learning_rate": 3.0005483315055615e-05,
      "loss": 1.6552,
      "step": 306300
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.7918704748153687,
      "eval_runtime": 2.9379,
      "eval_samples_per_second": 228.731,
      "eval_steps_per_second": 228.731,
      "step": 306384
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.4813449382781982,
      "eval_runtime": 56.3664,
      "eval_samples_per_second": 226.482,
      "eval_steps_per_second": 226.482,
      "step": 306384
    },
    {
      "epoch": 24.00125332915557,
      "grad_norm": 5.2587432861328125,
      "learning_rate": 2.9998955559037028e-05,
      "loss": 1.7557,
      "step": 306400
    },
    {
      "epoch": 24.00908663637788,
      "grad_norm": 3.7544543743133545,
      "learning_rate": 2.9992427803018437e-05,
      "loss": 1.7587,
      "step": 306500
    },
    {
      "epoch": 24.01691994360019,
      "grad_norm": 5.179539203643799,
      "learning_rate": 2.9985900046999843e-05,
      "loss": 1.7732,
      "step": 306600
    },
    {
      "epoch": 24.024753250822496,
      "grad_norm": 4.754140377044678,
      "learning_rate": 2.9979372290981255e-05,
      "loss": 1.5851,
      "step": 306700
    },
    {
      "epoch": 24.032586558044805,
      "grad_norm": 4.8166890144348145,
      "learning_rate": 2.9972844534962664e-05,
      "loss": 1.7477,
      "step": 306800
    },
    {
      "epoch": 24.040419865267115,
      "grad_norm": 4.474637985229492,
      "learning_rate": 2.996631677894407e-05,
      "loss": 1.6141,
      "step": 306900
    },
    {
      "epoch": 24.048253172489424,
      "grad_norm": 6.053308010101318,
      "learning_rate": 2.9959789022925483e-05,
      "loss": 1.7493,
      "step": 307000
    },
    {
      "epoch": 24.056086479711734,
      "grad_norm": 4.595617771148682,
      "learning_rate": 2.9953261266906892e-05,
      "loss": 1.7176,
      "step": 307100
    },
    {
      "epoch": 24.063919786934044,
      "grad_norm": 4.978404998779297,
      "learning_rate": 2.9946733510888298e-05,
      "loss": 1.6115,
      "step": 307200
    },
    {
      "epoch": 24.071753094156353,
      "grad_norm": 5.408664226531982,
      "learning_rate": 2.994020575486971e-05,
      "loss": 1.7104,
      "step": 307300
    },
    {
      "epoch": 24.079586401378663,
      "grad_norm": 6.7846879959106445,
      "learning_rate": 2.9933677998851116e-05,
      "loss": 1.746,
      "step": 307400
    },
    {
      "epoch": 24.087419708600972,
      "grad_norm": 4.229884624481201,
      "learning_rate": 2.9927150242832525e-05,
      "loss": 1.811,
      "step": 307500
    },
    {
      "epoch": 24.095253015823282,
      "grad_norm": 5.727955341339111,
      "learning_rate": 2.992062248681393e-05,
      "loss": 1.7836,
      "step": 307600
    },
    {
      "epoch": 24.10308632304559,
      "grad_norm": 7.142014980316162,
      "learning_rate": 2.9914094730795343e-05,
      "loss": 1.6515,
      "step": 307700
    },
    {
      "epoch": 24.110919630267897,
      "grad_norm": 5.825381278991699,
      "learning_rate": 2.9907566974776753e-05,
      "loss": 1.6857,
      "step": 307800
    },
    {
      "epoch": 24.118752937490207,
      "grad_norm": 4.666062831878662,
      "learning_rate": 2.9901039218758158e-05,
      "loss": 1.6449,
      "step": 307900
    },
    {
      "epoch": 24.126586244712517,
      "grad_norm": 4.551063060760498,
      "learning_rate": 2.989451146273957e-05,
      "loss": 1.6214,
      "step": 308000
    },
    {
      "epoch": 24.134419551934826,
      "grad_norm": 5.334242343902588,
      "learning_rate": 2.988798370672098e-05,
      "loss": 1.7315,
      "step": 308100
    },
    {
      "epoch": 24.142252859157136,
      "grad_norm": 5.218428611755371,
      "learning_rate": 2.9881455950702386e-05,
      "loss": 1.6533,
      "step": 308200
    },
    {
      "epoch": 24.150086166379445,
      "grad_norm": 4.996431827545166,
      "learning_rate": 2.9874928194683798e-05,
      "loss": 1.6895,
      "step": 308300
    },
    {
      "epoch": 24.157919473601755,
      "grad_norm": 4.223329067230225,
      "learning_rate": 2.9868400438665207e-05,
      "loss": 1.6485,
      "step": 308400
    },
    {
      "epoch": 24.165752780824064,
      "grad_norm": 8.429047584533691,
      "learning_rate": 2.9861872682646613e-05,
      "loss": 1.715,
      "step": 308500
    },
    {
      "epoch": 24.173586088046374,
      "grad_norm": 5.206665992736816,
      "learning_rate": 2.9855344926628026e-05,
      "loss": 1.6137,
      "step": 308600
    },
    {
      "epoch": 24.181419395268684,
      "grad_norm": 6.046494483947754,
      "learning_rate": 2.9848817170609435e-05,
      "loss": 1.7485,
      "step": 308700
    },
    {
      "epoch": 24.189252702490993,
      "grad_norm": 6.461653232574463,
      "learning_rate": 2.984228941459084e-05,
      "loss": 1.8054,
      "step": 308800
    },
    {
      "epoch": 24.197086009713303,
      "grad_norm": 5.005069732666016,
      "learning_rate": 2.9835761658572253e-05,
      "loss": 1.7435,
      "step": 308900
    },
    {
      "epoch": 24.20491931693561,
      "grad_norm": 4.662295818328857,
      "learning_rate": 2.982923390255366e-05,
      "loss": 1.8383,
      "step": 309000
    },
    {
      "epoch": 24.21275262415792,
      "grad_norm": 5.5690412521362305,
      "learning_rate": 2.9822706146535068e-05,
      "loss": 1.7076,
      "step": 309100
    },
    {
      "epoch": 24.220585931380228,
      "grad_norm": 6.045414924621582,
      "learning_rate": 2.9816178390516474e-05,
      "loss": 1.689,
      "step": 309200
    },
    {
      "epoch": 24.228419238602537,
      "grad_norm": 5.356842041015625,
      "learning_rate": 2.9809650634497886e-05,
      "loss": 1.7739,
      "step": 309300
    },
    {
      "epoch": 24.236252545824847,
      "grad_norm": 5.878595352172852,
      "learning_rate": 2.9803122878479296e-05,
      "loss": 1.6373,
      "step": 309400
    },
    {
      "epoch": 24.244085853047157,
      "grad_norm": 4.792840003967285,
      "learning_rate": 2.97965951224607e-05,
      "loss": 1.7207,
      "step": 309500
    },
    {
      "epoch": 24.251919160269466,
      "grad_norm": 5.329564094543457,
      "learning_rate": 2.9790067366442114e-05,
      "loss": 1.6429,
      "step": 309600
    },
    {
      "epoch": 24.259752467491776,
      "grad_norm": 4.369822025299072,
      "learning_rate": 2.9783539610423523e-05,
      "loss": 1.731,
      "step": 309700
    },
    {
      "epoch": 24.267585774714085,
      "grad_norm": 6.609817028045654,
      "learning_rate": 2.977701185440493e-05,
      "loss": 1.7553,
      "step": 309800
    },
    {
      "epoch": 24.275419081936395,
      "grad_norm": 5.560111999511719,
      "learning_rate": 2.977048409838634e-05,
      "loss": 1.7383,
      "step": 309900
    },
    {
      "epoch": 24.283252389158704,
      "grad_norm": 4.139967441558838,
      "learning_rate": 2.976395634236775e-05,
      "loss": 1.8079,
      "step": 310000
    },
    {
      "epoch": 24.29108569638101,
      "grad_norm": 5.685317516326904,
      "learning_rate": 2.9757428586349156e-05,
      "loss": 1.6886,
      "step": 310100
    },
    {
      "epoch": 24.29891900360332,
      "grad_norm": 6.731168270111084,
      "learning_rate": 2.975090083033057e-05,
      "loss": 1.6908,
      "step": 310200
    },
    {
      "epoch": 24.30675231082563,
      "grad_norm": 5.346974849700928,
      "learning_rate": 2.9744373074311978e-05,
      "loss": 1.7081,
      "step": 310300
    },
    {
      "epoch": 24.31458561804794,
      "grad_norm": 4.743523597717285,
      "learning_rate": 2.9737845318293384e-05,
      "loss": 1.6913,
      "step": 310400
    },
    {
      "epoch": 24.32241892527025,
      "grad_norm": 7.901980876922607,
      "learning_rate": 2.9731317562274796e-05,
      "loss": 1.6015,
      "step": 310500
    },
    {
      "epoch": 24.33025223249256,
      "grad_norm": 4.3918375968933105,
      "learning_rate": 2.9724789806256202e-05,
      "loss": 1.7031,
      "step": 310600
    },
    {
      "epoch": 24.338085539714868,
      "grad_norm": 5.291867733001709,
      "learning_rate": 2.971826205023761e-05,
      "loss": 1.6843,
      "step": 310700
    },
    {
      "epoch": 24.345918846937177,
      "grad_norm": 6.1490397453308105,
      "learning_rate": 2.9711734294219017e-05,
      "loss": 1.6784,
      "step": 310800
    },
    {
      "epoch": 24.353752154159487,
      "grad_norm": 4.540661334991455,
      "learning_rate": 2.970520653820043e-05,
      "loss": 1.6276,
      "step": 310900
    },
    {
      "epoch": 24.361585461381797,
      "grad_norm": 6.234786033630371,
      "learning_rate": 2.969867878218184e-05,
      "loss": 1.7231,
      "step": 311000
    },
    {
      "epoch": 24.369418768604106,
      "grad_norm": 5.791098117828369,
      "learning_rate": 2.9692151026163245e-05,
      "loss": 1.8015,
      "step": 311100
    },
    {
      "epoch": 24.377252075826412,
      "grad_norm": 5.940860748291016,
      "learning_rate": 2.9685623270144657e-05,
      "loss": 1.6585,
      "step": 311200
    },
    {
      "epoch": 24.38508538304872,
      "grad_norm": 4.276761531829834,
      "learning_rate": 2.9679095514126066e-05,
      "loss": 1.6779,
      "step": 311300
    },
    {
      "epoch": 24.39291869027103,
      "grad_norm": 6.948615074157715,
      "learning_rate": 2.9672567758107472e-05,
      "loss": 1.6347,
      "step": 311400
    },
    {
      "epoch": 24.40075199749334,
      "grad_norm": 5.687288284301758,
      "learning_rate": 2.9666040002088885e-05,
      "loss": 1.692,
      "step": 311500
    },
    {
      "epoch": 24.40858530471565,
      "grad_norm": 4.975884437561035,
      "learning_rate": 2.9659512246070294e-05,
      "loss": 1.6998,
      "step": 311600
    },
    {
      "epoch": 24.41641861193796,
      "grad_norm": 3.9439139366149902,
      "learning_rate": 2.96529844900517e-05,
      "loss": 1.7586,
      "step": 311700
    },
    {
      "epoch": 24.42425191916027,
      "grad_norm": 7.288311004638672,
      "learning_rate": 2.9646456734033112e-05,
      "loss": 1.6884,
      "step": 311800
    },
    {
      "epoch": 24.43208522638258,
      "grad_norm": 6.387816429138184,
      "learning_rate": 2.963992897801452e-05,
      "loss": 1.733,
      "step": 311900
    },
    {
      "epoch": 24.43991853360489,
      "grad_norm": 3.8932230472564697,
      "learning_rate": 2.9633401221995927e-05,
      "loss": 1.798,
      "step": 312000
    },
    {
      "epoch": 24.4477518408272,
      "grad_norm": 5.408665657043457,
      "learning_rate": 2.962687346597734e-05,
      "loss": 1.7485,
      "step": 312100
    },
    {
      "epoch": 24.455585148049508,
      "grad_norm": 6.368244171142578,
      "learning_rate": 2.9620345709958745e-05,
      "loss": 1.6846,
      "step": 312200
    },
    {
      "epoch": 24.463418455271817,
      "grad_norm": 5.806836128234863,
      "learning_rate": 2.9613817953940154e-05,
      "loss": 1.678,
      "step": 312300
    },
    {
      "epoch": 24.471251762494123,
      "grad_norm": 5.756397247314453,
      "learning_rate": 2.9607290197921567e-05,
      "loss": 1.7463,
      "step": 312400
    },
    {
      "epoch": 24.479085069716433,
      "grad_norm": 5.651927471160889,
      "learning_rate": 2.9600762441902973e-05,
      "loss": 1.6891,
      "step": 312500
    },
    {
      "epoch": 24.486918376938743,
      "grad_norm": 4.48757791519165,
      "learning_rate": 2.9594234685884382e-05,
      "loss": 1.7322,
      "step": 312600
    },
    {
      "epoch": 24.494751684161052,
      "grad_norm": 5.437263011932373,
      "learning_rate": 2.9587706929865788e-05,
      "loss": 1.6833,
      "step": 312700
    },
    {
      "epoch": 24.50258499138336,
      "grad_norm": 4.824535369873047,
      "learning_rate": 2.95811791738472e-05,
      "loss": 1.7145,
      "step": 312800
    },
    {
      "epoch": 24.51041829860567,
      "grad_norm": 4.387228965759277,
      "learning_rate": 2.957465141782861e-05,
      "loss": 1.6468,
      "step": 312900
    },
    {
      "epoch": 24.51825160582798,
      "grad_norm": 7.742867946624756,
      "learning_rate": 2.9568123661810015e-05,
      "loss": 1.7423,
      "step": 313000
    },
    {
      "epoch": 24.52608491305029,
      "grad_norm": 6.683893203735352,
      "learning_rate": 2.9561595905791428e-05,
      "loss": 1.7679,
      "step": 313100
    },
    {
      "epoch": 24.5339182202726,
      "grad_norm": 5.36212682723999,
      "learning_rate": 2.9555068149772837e-05,
      "loss": 1.7312,
      "step": 313200
    },
    {
      "epoch": 24.54175152749491,
      "grad_norm": 5.893438816070557,
      "learning_rate": 2.9548540393754243e-05,
      "loss": 1.7531,
      "step": 313300
    },
    {
      "epoch": 24.54958483471722,
      "grad_norm": 5.124128341674805,
      "learning_rate": 2.9542012637735655e-05,
      "loss": 1.6959,
      "step": 313400
    },
    {
      "epoch": 24.557418141939525,
      "grad_norm": 5.576470375061035,
      "learning_rate": 2.9535484881717064e-05,
      "loss": 1.8359,
      "step": 313500
    },
    {
      "epoch": 24.565251449161835,
      "grad_norm": 4.928490161895752,
      "learning_rate": 2.952895712569847e-05,
      "loss": 1.7209,
      "step": 313600
    },
    {
      "epoch": 24.573084756384144,
      "grad_norm": 3.6142983436584473,
      "learning_rate": 2.9522429369679883e-05,
      "loss": 1.8311,
      "step": 313700
    },
    {
      "epoch": 24.580918063606454,
      "grad_norm": 5.885801792144775,
      "learning_rate": 2.951590161366129e-05,
      "loss": 1.6977,
      "step": 313800
    },
    {
      "epoch": 24.588751370828763,
      "grad_norm": 4.858967304229736,
      "learning_rate": 2.9509373857642698e-05,
      "loss": 1.7446,
      "step": 313900
    },
    {
      "epoch": 24.596584678051073,
      "grad_norm": 4.293241500854492,
      "learning_rate": 2.950284610162411e-05,
      "loss": 1.6485,
      "step": 314000
    },
    {
      "epoch": 24.604417985273383,
      "grad_norm": 4.994330406188965,
      "learning_rate": 2.9496318345605516e-05,
      "loss": 1.7123,
      "step": 314100
    },
    {
      "epoch": 24.612251292495692,
      "grad_norm": 5.187735080718994,
      "learning_rate": 2.9489790589586925e-05,
      "loss": 1.6675,
      "step": 314200
    },
    {
      "epoch": 24.620084599718002,
      "grad_norm": 4.33699369430542,
      "learning_rate": 2.948326283356833e-05,
      "loss": 1.688,
      "step": 314300
    },
    {
      "epoch": 24.62791790694031,
      "grad_norm": 5.213564395904541,
      "learning_rate": 2.9476735077549743e-05,
      "loss": 1.6531,
      "step": 314400
    },
    {
      "epoch": 24.63575121416262,
      "grad_norm": 5.974447250366211,
      "learning_rate": 2.9470207321531152e-05,
      "loss": 1.77,
      "step": 314500
    },
    {
      "epoch": 24.64358452138493,
      "grad_norm": 4.0895676612854,
      "learning_rate": 2.9463679565512558e-05,
      "loss": 1.6973,
      "step": 314600
    },
    {
      "epoch": 24.651417828607237,
      "grad_norm": 5.907777786254883,
      "learning_rate": 2.945715180949397e-05,
      "loss": 1.6734,
      "step": 314700
    },
    {
      "epoch": 24.659251135829546,
      "grad_norm": 6.409706115722656,
      "learning_rate": 2.945062405347538e-05,
      "loss": 1.7605,
      "step": 314800
    },
    {
      "epoch": 24.667084443051856,
      "grad_norm": 4.558133602142334,
      "learning_rate": 2.9444096297456786e-05,
      "loss": 1.716,
      "step": 314900
    },
    {
      "epoch": 24.674917750274165,
      "grad_norm": 5.139335632324219,
      "learning_rate": 2.9437568541438198e-05,
      "loss": 1.684,
      "step": 315000
    },
    {
      "epoch": 24.682751057496475,
      "grad_norm": 4.753153324127197,
      "learning_rate": 2.9431040785419607e-05,
      "loss": 1.6998,
      "step": 315100
    },
    {
      "epoch": 24.690584364718784,
      "grad_norm": 5.565465450286865,
      "learning_rate": 2.9424513029401013e-05,
      "loss": 1.747,
      "step": 315200
    },
    {
      "epoch": 24.698417671941094,
      "grad_norm": 5.194471836090088,
      "learning_rate": 2.9417985273382426e-05,
      "loss": 1.7441,
      "step": 315300
    },
    {
      "epoch": 24.706250979163404,
      "grad_norm": 5.175401210784912,
      "learning_rate": 2.941145751736383e-05,
      "loss": 1.6931,
      "step": 315400
    },
    {
      "epoch": 24.714084286385713,
      "grad_norm": 5.216566562652588,
      "learning_rate": 2.940492976134524e-05,
      "loss": 1.6336,
      "step": 315500
    },
    {
      "epoch": 24.721917593608023,
      "grad_norm": 6.638464450836182,
      "learning_rate": 2.9398402005326653e-05,
      "loss": 1.7279,
      "step": 315600
    },
    {
      "epoch": 24.729750900830332,
      "grad_norm": 6.929391860961914,
      "learning_rate": 2.939187424930806e-05,
      "loss": 1.7149,
      "step": 315700
    },
    {
      "epoch": 24.73758420805264,
      "grad_norm": 4.22559118270874,
      "learning_rate": 2.9385346493289468e-05,
      "loss": 1.8381,
      "step": 315800
    },
    {
      "epoch": 24.745417515274948,
      "grad_norm": 4.6462249755859375,
      "learning_rate": 2.9378818737270874e-05,
      "loss": 1.7407,
      "step": 315900
    },
    {
      "epoch": 24.753250822497257,
      "grad_norm": 6.297964572906494,
      "learning_rate": 2.9372290981252286e-05,
      "loss": 1.6729,
      "step": 316000
    },
    {
      "epoch": 24.761084129719567,
      "grad_norm": 5.881519317626953,
      "learning_rate": 2.9365763225233696e-05,
      "loss": 1.7584,
      "step": 316100
    },
    {
      "epoch": 24.768917436941877,
      "grad_norm": 4.174673557281494,
      "learning_rate": 2.93592354692151e-05,
      "loss": 1.7619,
      "step": 316200
    },
    {
      "epoch": 24.776750744164186,
      "grad_norm": 4.88600492477417,
      "learning_rate": 2.9352707713196514e-05,
      "loss": 1.6425,
      "step": 316300
    },
    {
      "epoch": 24.784584051386496,
      "grad_norm": 7.49539041519165,
      "learning_rate": 2.9346179957177923e-05,
      "loss": 1.7119,
      "step": 316400
    },
    {
      "epoch": 24.792417358608805,
      "grad_norm": 4.152798652648926,
      "learning_rate": 2.933965220115933e-05,
      "loss": 1.6606,
      "step": 316500
    },
    {
      "epoch": 24.800250665831115,
      "grad_norm": 5.3738017082214355,
      "learning_rate": 2.933312444514074e-05,
      "loss": 1.7296,
      "step": 316600
    },
    {
      "epoch": 24.808083973053424,
      "grad_norm": 4.905267715454102,
      "learning_rate": 2.932659668912215e-05,
      "loss": 1.7077,
      "step": 316700
    },
    {
      "epoch": 24.815917280275734,
      "grad_norm": 4.327413082122803,
      "learning_rate": 2.9320068933103556e-05,
      "loss": 1.7602,
      "step": 316800
    },
    {
      "epoch": 24.82375058749804,
      "grad_norm": 4.350488662719727,
      "learning_rate": 2.931354117708497e-05,
      "loss": 1.7951,
      "step": 316900
    },
    {
      "epoch": 24.83158389472035,
      "grad_norm": 3.655817747116089,
      "learning_rate": 2.9307013421066375e-05,
      "loss": 1.6484,
      "step": 317000
    },
    {
      "epoch": 24.83941720194266,
      "grad_norm": 5.018797874450684,
      "learning_rate": 2.9300485665047784e-05,
      "loss": 1.7861,
      "step": 317100
    },
    {
      "epoch": 24.84725050916497,
      "grad_norm": 3.600446939468384,
      "learning_rate": 2.9293957909029196e-05,
      "loss": 1.7152,
      "step": 317200
    },
    {
      "epoch": 24.85508381638728,
      "grad_norm": 7.092061519622803,
      "learning_rate": 2.9287430153010602e-05,
      "loss": 1.6809,
      "step": 317300
    },
    {
      "epoch": 24.862917123609588,
      "grad_norm": 6.659358501434326,
      "learning_rate": 2.928090239699201e-05,
      "loss": 1.7336,
      "step": 317400
    },
    {
      "epoch": 24.870750430831897,
      "grad_norm": 6.268185615539551,
      "learning_rate": 2.9274374640973417e-05,
      "loss": 1.7704,
      "step": 317500
    },
    {
      "epoch": 24.878583738054207,
      "grad_norm": 4.102727890014648,
      "learning_rate": 2.926784688495483e-05,
      "loss": 1.77,
      "step": 317600
    },
    {
      "epoch": 24.886417045276517,
      "grad_norm": 5.466935157775879,
      "learning_rate": 2.926131912893624e-05,
      "loss": 1.7655,
      "step": 317700
    },
    {
      "epoch": 24.894250352498826,
      "grad_norm": 6.324934482574463,
      "learning_rate": 2.9254791372917644e-05,
      "loss": 1.7259,
      "step": 317800
    },
    {
      "epoch": 24.902083659721136,
      "grad_norm": 4.923711776733398,
      "learning_rate": 2.9248263616899057e-05,
      "loss": 1.652,
      "step": 317900
    },
    {
      "epoch": 24.90991696694344,
      "grad_norm": 5.5307297706604,
      "learning_rate": 2.9241735860880466e-05,
      "loss": 1.7437,
      "step": 318000
    },
    {
      "epoch": 24.91775027416575,
      "grad_norm": 4.521862030029297,
      "learning_rate": 2.9235208104861872e-05,
      "loss": 1.7847,
      "step": 318100
    },
    {
      "epoch": 24.92558358138806,
      "grad_norm": 4.783753871917725,
      "learning_rate": 2.9228680348843284e-05,
      "loss": 1.6684,
      "step": 318200
    },
    {
      "epoch": 24.93341688861037,
      "grad_norm": 4.8104119300842285,
      "learning_rate": 2.9222152592824694e-05,
      "loss": 1.6757,
      "step": 318300
    },
    {
      "epoch": 24.94125019583268,
      "grad_norm": 5.295691013336182,
      "learning_rate": 2.92156248368061e-05,
      "loss": 1.7141,
      "step": 318400
    },
    {
      "epoch": 24.94908350305499,
      "grad_norm": 7.698193073272705,
      "learning_rate": 2.9209097080787512e-05,
      "loss": 1.7667,
      "step": 318500
    },
    {
      "epoch": 24.9569168102773,
      "grad_norm": 5.278530120849609,
      "learning_rate": 2.9202569324768918e-05,
      "loss": 1.696,
      "step": 318600
    },
    {
      "epoch": 24.96475011749961,
      "grad_norm": 5.205007076263428,
      "learning_rate": 2.9196041568750327e-05,
      "loss": 1.6917,
      "step": 318700
    },
    {
      "epoch": 24.97258342472192,
      "grad_norm": 8.104268074035645,
      "learning_rate": 2.918951381273174e-05,
      "loss": 1.7054,
      "step": 318800
    },
    {
      "epoch": 24.980416731944228,
      "grad_norm": 5.892402172088623,
      "learning_rate": 2.9182986056713145e-05,
      "loss": 1.7362,
      "step": 318900
    },
    {
      "epoch": 24.988250039166537,
      "grad_norm": 6.157017707824707,
      "learning_rate": 2.9176458300694554e-05,
      "loss": 1.6264,
      "step": 319000
    },
    {
      "epoch": 24.996083346388847,
      "grad_norm": 5.813680648803711,
      "learning_rate": 2.9169930544675967e-05,
      "loss": 1.755,
      "step": 319100
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.7813771963119507,
      "eval_runtime": 1.6485,
      "eval_samples_per_second": 407.65,
      "eval_steps_per_second": 407.65,
      "step": 319150
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.4745304584503174,
      "eval_runtime": 31.4218,
      "eval_samples_per_second": 406.278,
      "eval_steps_per_second": 406.278,
      "step": 319150
    },
    {
      "epoch": 25.003916653611153,
      "grad_norm": 5.5201921463012695,
      "learning_rate": 2.9163402788657373e-05,
      "loss": 1.6948,
      "step": 319200
    },
    {
      "epoch": 25.011749960833463,
      "grad_norm": 4.302806377410889,
      "learning_rate": 2.9156875032638782e-05,
      "loss": 1.7137,
      "step": 319300
    },
    {
      "epoch": 25.019583268055772,
      "grad_norm": 5.202274322509766,
      "learning_rate": 2.9150347276620188e-05,
      "loss": 1.7583,
      "step": 319400
    },
    {
      "epoch": 25.02741657527808,
      "grad_norm": 4.4210076332092285,
      "learning_rate": 2.91438195206016e-05,
      "loss": 1.7112,
      "step": 319500
    },
    {
      "epoch": 25.03524988250039,
      "grad_norm": 4.728264331817627,
      "learning_rate": 2.913729176458301e-05,
      "loss": 1.6761,
      "step": 319600
    },
    {
      "epoch": 25.0430831897227,
      "grad_norm": 5.371029853820801,
      "learning_rate": 2.9130764008564415e-05,
      "loss": 1.6848,
      "step": 319700
    },
    {
      "epoch": 25.05091649694501,
      "grad_norm": 6.268295764923096,
      "learning_rate": 2.9124236252545828e-05,
      "loss": 1.646,
      "step": 319800
    },
    {
      "epoch": 25.05874980416732,
      "grad_norm": 8.726417541503906,
      "learning_rate": 2.9117708496527237e-05,
      "loss": 1.7314,
      "step": 319900
    },
    {
      "epoch": 25.06658311138963,
      "grad_norm": 5.348570823669434,
      "learning_rate": 2.9111180740508643e-05,
      "loss": 1.7389,
      "step": 320000
    },
    {
      "epoch": 25.07441641861194,
      "grad_norm": 4.763315677642822,
      "learning_rate": 2.9104652984490055e-05,
      "loss": 1.7387,
      "step": 320100
    },
    {
      "epoch": 25.08224972583425,
      "grad_norm": 4.834750175476074,
      "learning_rate": 2.909812522847146e-05,
      "loss": 1.6342,
      "step": 320200
    },
    {
      "epoch": 25.090083033056555,
      "grad_norm": 5.045517444610596,
      "learning_rate": 2.909159747245287e-05,
      "loss": 1.6805,
      "step": 320300
    },
    {
      "epoch": 25.097916340278864,
      "grad_norm": 4.828822612762451,
      "learning_rate": 2.9085069716434283e-05,
      "loss": 1.6268,
      "step": 320400
    },
    {
      "epoch": 25.105749647501174,
      "grad_norm": 4.906986713409424,
      "learning_rate": 2.9078541960415688e-05,
      "loss": 1.761,
      "step": 320500
    },
    {
      "epoch": 25.113582954723483,
      "grad_norm": 5.5986151695251465,
      "learning_rate": 2.9072014204397097e-05,
      "loss": 1.6607,
      "step": 320600
    },
    {
      "epoch": 25.121416261945793,
      "grad_norm": 4.288654327392578,
      "learning_rate": 2.906548644837851e-05,
      "loss": 1.7632,
      "step": 320700
    },
    {
      "epoch": 25.129249569168103,
      "grad_norm": 5.078334808349609,
      "learning_rate": 2.9058958692359916e-05,
      "loss": 1.7889,
      "step": 320800
    },
    {
      "epoch": 25.137082876390412,
      "grad_norm": 5.008925437927246,
      "learning_rate": 2.9052430936341325e-05,
      "loss": 1.6725,
      "step": 320900
    },
    {
      "epoch": 25.14491618361272,
      "grad_norm": 5.289426326751709,
      "learning_rate": 2.904590318032273e-05,
      "loss": 1.7655,
      "step": 321000
    },
    {
      "epoch": 25.15274949083503,
      "grad_norm": 3.8423259258270264,
      "learning_rate": 2.9039375424304143e-05,
      "loss": 1.7131,
      "step": 321100
    },
    {
      "epoch": 25.16058279805734,
      "grad_norm": 4.385591506958008,
      "learning_rate": 2.9032847668285552e-05,
      "loss": 1.7477,
      "step": 321200
    },
    {
      "epoch": 25.16841610527965,
      "grad_norm": 5.174149990081787,
      "learning_rate": 2.9026319912266958e-05,
      "loss": 1.7896,
      "step": 321300
    },
    {
      "epoch": 25.17624941250196,
      "grad_norm": 5.52268123626709,
      "learning_rate": 2.901979215624837e-05,
      "loss": 1.6226,
      "step": 321400
    },
    {
      "epoch": 25.184082719724266,
      "grad_norm": 5.880278587341309,
      "learning_rate": 2.901326440022978e-05,
      "loss": 1.6388,
      "step": 321500
    },
    {
      "epoch": 25.191916026946576,
      "grad_norm": 4.142111778259277,
      "learning_rate": 2.9006736644211186e-05,
      "loss": 1.6805,
      "step": 321600
    },
    {
      "epoch": 25.199749334168885,
      "grad_norm": 5.195102691650391,
      "learning_rate": 2.9000208888192598e-05,
      "loss": 1.7472,
      "step": 321700
    },
    {
      "epoch": 25.207582641391195,
      "grad_norm": 5.757229328155518,
      "learning_rate": 2.8993681132174004e-05,
      "loss": 1.7779,
      "step": 321800
    },
    {
      "epoch": 25.215415948613504,
      "grad_norm": 7.173249244689941,
      "learning_rate": 2.8987153376155413e-05,
      "loss": 1.8011,
      "step": 321900
    },
    {
      "epoch": 25.223249255835814,
      "grad_norm": 5.787664413452148,
      "learning_rate": 2.8980625620136826e-05,
      "loss": 1.6843,
      "step": 322000
    },
    {
      "epoch": 25.231082563058123,
      "grad_norm": 4.614360809326172,
      "learning_rate": 2.897409786411823e-05,
      "loss": 1.7235,
      "step": 322100
    },
    {
      "epoch": 25.238915870280433,
      "grad_norm": 5.515944957733154,
      "learning_rate": 2.896757010809964e-05,
      "loss": 1.8203,
      "step": 322200
    },
    {
      "epoch": 25.246749177502743,
      "grad_norm": 5.085632801055908,
      "learning_rate": 2.8961042352081053e-05,
      "loss": 1.6985,
      "step": 322300
    },
    {
      "epoch": 25.254582484725052,
      "grad_norm": 6.465414524078369,
      "learning_rate": 2.895451459606246e-05,
      "loss": 1.6212,
      "step": 322400
    },
    {
      "epoch": 25.26241579194736,
      "grad_norm": 6.9606170654296875,
      "learning_rate": 2.8947986840043868e-05,
      "loss": 1.6706,
      "step": 322500
    },
    {
      "epoch": 25.270249099169668,
      "grad_norm": 5.898036956787109,
      "learning_rate": 2.8941459084025274e-05,
      "loss": 1.7788,
      "step": 322600
    },
    {
      "epoch": 25.278082406391977,
      "grad_norm": 5.978930950164795,
      "learning_rate": 2.8934931328006686e-05,
      "loss": 1.6691,
      "step": 322700
    },
    {
      "epoch": 25.285915713614287,
      "grad_norm": 6.780986309051514,
      "learning_rate": 2.8928403571988096e-05,
      "loss": 1.6846,
      "step": 322800
    },
    {
      "epoch": 25.293749020836596,
      "grad_norm": 4.409230709075928,
      "learning_rate": 2.89218758159695e-05,
      "loss": 1.7679,
      "step": 322900
    },
    {
      "epoch": 25.301582328058906,
      "grad_norm": 8.076493263244629,
      "learning_rate": 2.8915348059950914e-05,
      "loss": 1.6917,
      "step": 323000
    },
    {
      "epoch": 25.309415635281216,
      "grad_norm": 5.090090274810791,
      "learning_rate": 2.8908820303932323e-05,
      "loss": 1.7065,
      "step": 323100
    },
    {
      "epoch": 25.317248942503525,
      "grad_norm": 6.186995983123779,
      "learning_rate": 2.890229254791373e-05,
      "loss": 1.727,
      "step": 323200
    },
    {
      "epoch": 25.325082249725835,
      "grad_norm": 4.774089813232422,
      "learning_rate": 2.889576479189514e-05,
      "loss": 1.6654,
      "step": 323300
    },
    {
      "epoch": 25.332915556948144,
      "grad_norm": 7.236123085021973,
      "learning_rate": 2.8889237035876547e-05,
      "loss": 1.6451,
      "step": 323400
    },
    {
      "epoch": 25.340748864170454,
      "grad_norm": 4.809741973876953,
      "learning_rate": 2.8882709279857956e-05,
      "loss": 1.7059,
      "step": 323500
    },
    {
      "epoch": 25.348582171392763,
      "grad_norm": 5.322749614715576,
      "learning_rate": 2.887618152383937e-05,
      "loss": 1.6965,
      "step": 323600
    },
    {
      "epoch": 25.35641547861507,
      "grad_norm": 5.51898193359375,
      "learning_rate": 2.8869653767820775e-05,
      "loss": 1.6365,
      "step": 323700
    },
    {
      "epoch": 25.36424878583738,
      "grad_norm": 5.7621026039123535,
      "learning_rate": 2.8863126011802184e-05,
      "loss": 1.7397,
      "step": 323800
    },
    {
      "epoch": 25.37208209305969,
      "grad_norm": 7.776345252990723,
      "learning_rate": 2.8856598255783596e-05,
      "loss": 1.6815,
      "step": 323900
    },
    {
      "epoch": 25.379915400281998,
      "grad_norm": 6.849848747253418,
      "learning_rate": 2.8850070499765002e-05,
      "loss": 1.7104,
      "step": 324000
    },
    {
      "epoch": 25.387748707504308,
      "grad_norm": 6.486185073852539,
      "learning_rate": 2.884354274374641e-05,
      "loss": 1.7826,
      "step": 324100
    },
    {
      "epoch": 25.395582014726617,
      "grad_norm": 4.6079277992248535,
      "learning_rate": 2.8837014987727824e-05,
      "loss": 1.7048,
      "step": 324200
    },
    {
      "epoch": 25.403415321948927,
      "grad_norm": 7.095053195953369,
      "learning_rate": 2.883048723170923e-05,
      "loss": 1.6882,
      "step": 324300
    },
    {
      "epoch": 25.411248629171237,
      "grad_norm": 5.134738922119141,
      "learning_rate": 2.882395947569064e-05,
      "loss": 1.6697,
      "step": 324400
    },
    {
      "epoch": 25.419081936393546,
      "grad_norm": 5.558394908905029,
      "learning_rate": 2.8817431719672044e-05,
      "loss": 1.6636,
      "step": 324500
    },
    {
      "epoch": 25.426915243615856,
      "grad_norm": 6.412192344665527,
      "learning_rate": 2.8810903963653457e-05,
      "loss": 1.6439,
      "step": 324600
    },
    {
      "epoch": 25.434748550838165,
      "grad_norm": 6.065520763397217,
      "learning_rate": 2.8804376207634866e-05,
      "loss": 1.634,
      "step": 324700
    },
    {
      "epoch": 25.442581858060475,
      "grad_norm": 4.783096790313721,
      "learning_rate": 2.8797848451616272e-05,
      "loss": 1.7801,
      "step": 324800
    },
    {
      "epoch": 25.45041516528278,
      "grad_norm": 6.486597061157227,
      "learning_rate": 2.8791320695597684e-05,
      "loss": 1.6963,
      "step": 324900
    },
    {
      "epoch": 25.45824847250509,
      "grad_norm": 4.0139970779418945,
      "learning_rate": 2.878479293957909e-05,
      "loss": 1.6392,
      "step": 325000
    },
    {
      "epoch": 25.4660817797274,
      "grad_norm": 6.189126014709473,
      "learning_rate": 2.87782651835605e-05,
      "loss": 1.7503,
      "step": 325100
    },
    {
      "epoch": 25.47391508694971,
      "grad_norm": 5.452335357666016,
      "learning_rate": 2.8771737427541912e-05,
      "loss": 1.6575,
      "step": 325200
    },
    {
      "epoch": 25.48174839417202,
      "grad_norm": 6.614272594451904,
      "learning_rate": 2.8765209671523318e-05,
      "loss": 1.7477,
      "step": 325300
    },
    {
      "epoch": 25.48958170139433,
      "grad_norm": 5.805622577667236,
      "learning_rate": 2.8758681915504727e-05,
      "loss": 1.7263,
      "step": 325400
    },
    {
      "epoch": 25.49741500861664,
      "grad_norm": 4.41494083404541,
      "learning_rate": 2.875215415948614e-05,
      "loss": 1.6609,
      "step": 325500
    },
    {
      "epoch": 25.505248315838948,
      "grad_norm": 5.357730388641357,
      "learning_rate": 2.8745626403467545e-05,
      "loss": 1.7664,
      "step": 325600
    },
    {
      "epoch": 25.513081623061257,
      "grad_norm": 4.440552234649658,
      "learning_rate": 2.8739098647448954e-05,
      "loss": 1.6812,
      "step": 325700
    },
    {
      "epoch": 25.520914930283567,
      "grad_norm": 4.8349995613098145,
      "learning_rate": 2.8732570891430367e-05,
      "loss": 1.7897,
      "step": 325800
    },
    {
      "epoch": 25.528748237505877,
      "grad_norm": 3.6463840007781982,
      "learning_rate": 2.8726043135411773e-05,
      "loss": 1.8006,
      "step": 325900
    },
    {
      "epoch": 25.536581544728183,
      "grad_norm": 5.738084316253662,
      "learning_rate": 2.8719515379393182e-05,
      "loss": 1.6678,
      "step": 326000
    },
    {
      "epoch": 25.544414851950492,
      "grad_norm": 6.337672233581543,
      "learning_rate": 2.8712987623374588e-05,
      "loss": 1.6317,
      "step": 326100
    },
    {
      "epoch": 25.5522481591728,
      "grad_norm": 7.787787437438965,
      "learning_rate": 2.8706459867356e-05,
      "loss": 1.7778,
      "step": 326200
    },
    {
      "epoch": 25.56008146639511,
      "grad_norm": 7.80955696105957,
      "learning_rate": 2.869993211133741e-05,
      "loss": 1.7275,
      "step": 326300
    },
    {
      "epoch": 25.56791477361742,
      "grad_norm": 5.614684581756592,
      "learning_rate": 2.8693404355318815e-05,
      "loss": 1.727,
      "step": 326400
    },
    {
      "epoch": 25.57574808083973,
      "grad_norm": 7.137882232666016,
      "learning_rate": 2.8686876599300228e-05,
      "loss": 1.7047,
      "step": 326500
    },
    {
      "epoch": 25.58358138806204,
      "grad_norm": 6.741751194000244,
      "learning_rate": 2.8680348843281633e-05,
      "loss": 1.6865,
      "step": 326600
    },
    {
      "epoch": 25.59141469528435,
      "grad_norm": 4.965021133422852,
      "learning_rate": 2.8673821087263042e-05,
      "loss": 1.6275,
      "step": 326700
    },
    {
      "epoch": 25.59924800250666,
      "grad_norm": 5.36521053314209,
      "learning_rate": 2.8667293331244455e-05,
      "loss": 1.6492,
      "step": 326800
    },
    {
      "epoch": 25.60708130972897,
      "grad_norm": 4.409821510314941,
      "learning_rate": 2.866076557522586e-05,
      "loss": 1.7042,
      "step": 326900
    },
    {
      "epoch": 25.61491461695128,
      "grad_norm": 6.476794719696045,
      "learning_rate": 2.865423781920727e-05,
      "loss": 1.6185,
      "step": 327000
    },
    {
      "epoch": 25.622747924173588,
      "grad_norm": 5.414855003356934,
      "learning_rate": 2.8647710063188682e-05,
      "loss": 1.6742,
      "step": 327100
    },
    {
      "epoch": 25.630581231395894,
      "grad_norm": 5.9294610023498535,
      "learning_rate": 2.8641182307170088e-05,
      "loss": 1.6918,
      "step": 327200
    },
    {
      "epoch": 25.638414538618203,
      "grad_norm": 5.125105857849121,
      "learning_rate": 2.8634654551151497e-05,
      "loss": 1.6591,
      "step": 327300
    },
    {
      "epoch": 25.646247845840513,
      "grad_norm": 5.712153911590576,
      "learning_rate": 2.862812679513291e-05,
      "loss": 1.7251,
      "step": 327400
    },
    {
      "epoch": 25.654081153062823,
      "grad_norm": 5.030544757843018,
      "learning_rate": 2.8621599039114316e-05,
      "loss": 1.8222,
      "step": 327500
    },
    {
      "epoch": 25.661914460285132,
      "grad_norm": 4.976857662200928,
      "learning_rate": 2.8615071283095725e-05,
      "loss": 1.7,
      "step": 327600
    },
    {
      "epoch": 25.66974776750744,
      "grad_norm": 5.296298980712891,
      "learning_rate": 2.860854352707713e-05,
      "loss": 1.7309,
      "step": 327700
    },
    {
      "epoch": 25.67758107472975,
      "grad_norm": 6.0697808265686035,
      "learning_rate": 2.8602015771058543e-05,
      "loss": 1.6604,
      "step": 327800
    },
    {
      "epoch": 25.68541438195206,
      "grad_norm": 5.869356632232666,
      "learning_rate": 2.8595488015039952e-05,
      "loss": 1.7095,
      "step": 327900
    },
    {
      "epoch": 25.69324768917437,
      "grad_norm": 4.486534118652344,
      "learning_rate": 2.8588960259021358e-05,
      "loss": 1.7395,
      "step": 328000
    },
    {
      "epoch": 25.70108099639668,
      "grad_norm": 6.034687042236328,
      "learning_rate": 2.858243250300277e-05,
      "loss": 1.726,
      "step": 328100
    },
    {
      "epoch": 25.70891430361899,
      "grad_norm": 6.263412952423096,
      "learning_rate": 2.8575904746984176e-05,
      "loss": 1.6326,
      "step": 328200
    },
    {
      "epoch": 25.716747610841296,
      "grad_norm": 7.178332805633545,
      "learning_rate": 2.8569376990965586e-05,
      "loss": 1.6792,
      "step": 328300
    },
    {
      "epoch": 25.724580918063605,
      "grad_norm": 5.118071556091309,
      "learning_rate": 2.8562849234946998e-05,
      "loss": 1.6617,
      "step": 328400
    },
    {
      "epoch": 25.732414225285915,
      "grad_norm": 8.52672290802002,
      "learning_rate": 2.8556321478928404e-05,
      "loss": 1.7161,
      "step": 328500
    },
    {
      "epoch": 25.740247532508224,
      "grad_norm": 5.4005208015441895,
      "learning_rate": 2.8549793722909813e-05,
      "loss": 1.765,
      "step": 328600
    },
    {
      "epoch": 25.748080839730534,
      "grad_norm": 5.356586933135986,
      "learning_rate": 2.8543265966891226e-05,
      "loss": 1.779,
      "step": 328700
    },
    {
      "epoch": 25.755914146952843,
      "grad_norm": 5.226175785064697,
      "learning_rate": 2.853673821087263e-05,
      "loss": 1.6522,
      "step": 328800
    },
    {
      "epoch": 25.763747454175153,
      "grad_norm": 7.0380024909973145,
      "learning_rate": 2.853021045485404e-05,
      "loss": 1.6943,
      "step": 328900
    },
    {
      "epoch": 25.771580761397463,
      "grad_norm": 6.943988800048828,
      "learning_rate": 2.8523682698835453e-05,
      "loss": 1.6734,
      "step": 329000
    },
    {
      "epoch": 25.779414068619772,
      "grad_norm": 5.311885833740234,
      "learning_rate": 2.851715494281686e-05,
      "loss": 1.7506,
      "step": 329100
    },
    {
      "epoch": 25.78724737584208,
      "grad_norm": 5.631796836853027,
      "learning_rate": 2.8510627186798268e-05,
      "loss": 1.718,
      "step": 329200
    },
    {
      "epoch": 25.79508068306439,
      "grad_norm": 6.0314202308654785,
      "learning_rate": 2.8504099430779674e-05,
      "loss": 1.7563,
      "step": 329300
    },
    {
      "epoch": 25.802913990286697,
      "grad_norm": 5.053502082824707,
      "learning_rate": 2.8497571674761086e-05,
      "loss": 1.7319,
      "step": 329400
    },
    {
      "epoch": 25.810747297509007,
      "grad_norm": 10.843780517578125,
      "learning_rate": 2.8491043918742495e-05,
      "loss": 1.8185,
      "step": 329500
    },
    {
      "epoch": 25.818580604731316,
      "grad_norm": 6.72520112991333,
      "learning_rate": 2.84845161627239e-05,
      "loss": 1.6729,
      "step": 329600
    },
    {
      "epoch": 25.826413911953626,
      "grad_norm": 6.880868434906006,
      "learning_rate": 2.8477988406705314e-05,
      "loss": 1.8066,
      "step": 329700
    },
    {
      "epoch": 25.834247219175936,
      "grad_norm": 5.244695663452148,
      "learning_rate": 2.847146065068672e-05,
      "loss": 1.6372,
      "step": 329800
    },
    {
      "epoch": 25.842080526398245,
      "grad_norm": 6.1311140060424805,
      "learning_rate": 2.846493289466813e-05,
      "loss": 1.6668,
      "step": 329900
    },
    {
      "epoch": 25.849913833620555,
      "grad_norm": 3.644523859024048,
      "learning_rate": 2.845840513864954e-05,
      "loss": 1.6534,
      "step": 330000
    },
    {
      "epoch": 25.857747140842864,
      "grad_norm": 4.808420181274414,
      "learning_rate": 2.8451877382630947e-05,
      "loss": 1.6199,
      "step": 330100
    },
    {
      "epoch": 25.865580448065174,
      "grad_norm": 5.162083148956299,
      "learning_rate": 2.8445349626612356e-05,
      "loss": 1.7074,
      "step": 330200
    },
    {
      "epoch": 25.873413755287483,
      "grad_norm": 4.89473295211792,
      "learning_rate": 2.843882187059377e-05,
      "loss": 1.6506,
      "step": 330300
    },
    {
      "epoch": 25.881247062509793,
      "grad_norm": 5.75184965133667,
      "learning_rate": 2.8432294114575174e-05,
      "loss": 1.7022,
      "step": 330400
    },
    {
      "epoch": 25.889080369732103,
      "grad_norm": 5.655361175537109,
      "learning_rate": 2.8425766358556584e-05,
      "loss": 1.6775,
      "step": 330500
    },
    {
      "epoch": 25.89691367695441,
      "grad_norm": 5.586222171783447,
      "learning_rate": 2.8419238602537996e-05,
      "loss": 1.719,
      "step": 330600
    },
    {
      "epoch": 25.904746984176718,
      "grad_norm": 3.6848325729370117,
      "learning_rate": 2.8412710846519402e-05,
      "loss": 1.7915,
      "step": 330700
    },
    {
      "epoch": 25.912580291399028,
      "grad_norm": 7.765470027923584,
      "learning_rate": 2.840618309050081e-05,
      "loss": 1.6823,
      "step": 330800
    },
    {
      "epoch": 25.920413598621337,
      "grad_norm": 5.445738315582275,
      "learning_rate": 2.8399655334482224e-05,
      "loss": 1.7047,
      "step": 330900
    },
    {
      "epoch": 25.928246905843647,
      "grad_norm": 6.177545547485352,
      "learning_rate": 2.839312757846363e-05,
      "loss": 1.7756,
      "step": 331000
    },
    {
      "epoch": 25.936080213065956,
      "grad_norm": 4.894641876220703,
      "learning_rate": 2.838659982244504e-05,
      "loss": 1.7295,
      "step": 331100
    },
    {
      "epoch": 25.943913520288266,
      "grad_norm": 8.177895545959473,
      "learning_rate": 2.8380072066426444e-05,
      "loss": 1.7007,
      "step": 331200
    },
    {
      "epoch": 25.951746827510576,
      "grad_norm": 5.966296672821045,
      "learning_rate": 2.8373544310407857e-05,
      "loss": 1.7535,
      "step": 331300
    },
    {
      "epoch": 25.959580134732885,
      "grad_norm": 4.510082244873047,
      "learning_rate": 2.8367016554389263e-05,
      "loss": 1.6964,
      "step": 331400
    },
    {
      "epoch": 25.967413441955195,
      "grad_norm": 5.700084209442139,
      "learning_rate": 2.8360488798370672e-05,
      "loss": 1.6805,
      "step": 331500
    },
    {
      "epoch": 25.975246749177504,
      "grad_norm": 4.503076076507568,
      "learning_rate": 2.8353961042352084e-05,
      "loss": 1.7821,
      "step": 331600
    },
    {
      "epoch": 25.98308005639981,
      "grad_norm": 5.243635177612305,
      "learning_rate": 2.834743328633349e-05,
      "loss": 1.8009,
      "step": 331700
    },
    {
      "epoch": 25.99091336362212,
      "grad_norm": 4.602653980255127,
      "learning_rate": 2.83409055303149e-05,
      "loss": 1.74,
      "step": 331800
    },
    {
      "epoch": 25.99874667084443,
      "grad_norm": 6.183723449707031,
      "learning_rate": 2.8334377774296312e-05,
      "loss": 1.6697,
      "step": 331900
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.7825853824615479,
      "eval_runtime": 1.6367,
      "eval_samples_per_second": 410.576,
      "eval_steps_per_second": 410.576,
      "step": 331916
    },
    {
      "epoch": 26.0,
      "eval_loss": 1.4707365036010742,
      "eval_runtime": 31.2679,
      "eval_samples_per_second": 408.279,
      "eval_steps_per_second": 408.279,
      "step": 331916
    },
    {
      "epoch": 26.00657997806674,
      "grad_norm": 4.990206241607666,
      "learning_rate": 2.8327850018277718e-05,
      "loss": 1.6805,
      "step": 332000
    },
    {
      "epoch": 26.01441328528905,
      "grad_norm": 5.006283760070801,
      "learning_rate": 2.8321322262259127e-05,
      "loss": 1.707,
      "step": 332100
    },
    {
      "epoch": 26.022246592511358,
      "grad_norm": 5.258411407470703,
      "learning_rate": 2.831479450624054e-05,
      "loss": 1.771,
      "step": 332200
    },
    {
      "epoch": 26.030079899733668,
      "grad_norm": 5.98682165145874,
      "learning_rate": 2.8308266750221945e-05,
      "loss": 1.6683,
      "step": 332300
    },
    {
      "epoch": 26.037913206955977,
      "grad_norm": 5.778068542480469,
      "learning_rate": 2.8301738994203354e-05,
      "loss": 1.7152,
      "step": 332400
    },
    {
      "epoch": 26.045746514178287,
      "grad_norm": 4.552109718322754,
      "learning_rate": 2.8295211238184767e-05,
      "loss": 1.6769,
      "step": 332500
    },
    {
      "epoch": 26.053579821400596,
      "grad_norm": 4.863180637359619,
      "learning_rate": 2.8288683482166173e-05,
      "loss": 1.6309,
      "step": 332600
    },
    {
      "epoch": 26.061413128622906,
      "grad_norm": 7.806748390197754,
      "learning_rate": 2.828215572614758e-05,
      "loss": 1.7459,
      "step": 332700
    },
    {
      "epoch": 26.069246435845216,
      "grad_norm": 7.672415733337402,
      "learning_rate": 2.8275627970128987e-05,
      "loss": 1.6595,
      "step": 332800
    },
    {
      "epoch": 26.07707974306752,
      "grad_norm": 4.80922794342041,
      "learning_rate": 2.82691002141104e-05,
      "loss": 1.5966,
      "step": 332900
    },
    {
      "epoch": 26.08491305028983,
      "grad_norm": 4.965594291687012,
      "learning_rate": 2.8262572458091806e-05,
      "loss": 1.6632,
      "step": 333000
    },
    {
      "epoch": 26.09274635751214,
      "grad_norm": 4.532486915588379,
      "learning_rate": 2.8256044702073215e-05,
      "loss": 1.6706,
      "step": 333100
    },
    {
      "epoch": 26.10057966473445,
      "grad_norm": 7.029836654663086,
      "learning_rate": 2.8249516946054627e-05,
      "loss": 1.7079,
      "step": 333200
    },
    {
      "epoch": 26.10841297195676,
      "grad_norm": 4.799263954162598,
      "learning_rate": 2.8242989190036033e-05,
      "loss": 1.6968,
      "step": 333300
    },
    {
      "epoch": 26.11624627917907,
      "grad_norm": 4.932565689086914,
      "learning_rate": 2.8236461434017442e-05,
      "loss": 1.7265,
      "step": 333400
    },
    {
      "epoch": 26.12407958640138,
      "grad_norm": 5.880257606506348,
      "learning_rate": 2.8229933677998855e-05,
      "loss": 1.8112,
      "step": 333500
    },
    {
      "epoch": 26.13191289362369,
      "grad_norm": 4.879427909851074,
      "learning_rate": 2.822340592198026e-05,
      "loss": 1.6819,
      "step": 333600
    },
    {
      "epoch": 26.139746200845998,
      "grad_norm": 5.542495250701904,
      "learning_rate": 2.821687816596167e-05,
      "loss": 1.6918,
      "step": 333700
    },
    {
      "epoch": 26.147579508068308,
      "grad_norm": 5.597409725189209,
      "learning_rate": 2.8210350409943082e-05,
      "loss": 1.6591,
      "step": 333800
    },
    {
      "epoch": 26.155412815290617,
      "grad_norm": 4.059043884277344,
      "learning_rate": 2.8203822653924488e-05,
      "loss": 1.7159,
      "step": 333900
    },
    {
      "epoch": 26.163246122512923,
      "grad_norm": 5.025992393493652,
      "learning_rate": 2.8197294897905897e-05,
      "loss": 1.6941,
      "step": 334000
    },
    {
      "epoch": 26.171079429735233,
      "grad_norm": 6.179440021514893,
      "learning_rate": 2.819076714188731e-05,
      "loss": 1.7315,
      "step": 334100
    },
    {
      "epoch": 26.178912736957543,
      "grad_norm": 4.319448947906494,
      "learning_rate": 2.8184239385868716e-05,
      "loss": 1.677,
      "step": 334200
    },
    {
      "epoch": 26.186746044179852,
      "grad_norm": 6.590382099151611,
      "learning_rate": 2.8177711629850125e-05,
      "loss": 1.7032,
      "step": 334300
    },
    {
      "epoch": 26.19457935140216,
      "grad_norm": 6.732388019561768,
      "learning_rate": 2.817118387383153e-05,
      "loss": 1.719,
      "step": 334400
    },
    {
      "epoch": 26.20241265862447,
      "grad_norm": 8.276823997497559,
      "learning_rate": 2.8164656117812943e-05,
      "loss": 1.6834,
      "step": 334500
    },
    {
      "epoch": 26.21024596584678,
      "grad_norm": 6.922515869140625,
      "learning_rate": 2.815812836179435e-05,
      "loss": 1.7481,
      "step": 334600
    },
    {
      "epoch": 26.21807927306909,
      "grad_norm": 7.360750198364258,
      "learning_rate": 2.8151600605775758e-05,
      "loss": 1.6431,
      "step": 334700
    },
    {
      "epoch": 26.2259125802914,
      "grad_norm": 5.407820224761963,
      "learning_rate": 2.814507284975717e-05,
      "loss": 1.7071,
      "step": 334800
    },
    {
      "epoch": 26.23374588751371,
      "grad_norm": 7.794459819793701,
      "learning_rate": 2.8138545093738576e-05,
      "loss": 1.6779,
      "step": 334900
    },
    {
      "epoch": 26.24157919473602,
      "grad_norm": 5.945863246917725,
      "learning_rate": 2.8132017337719985e-05,
      "loss": 1.5809,
      "step": 335000
    },
    {
      "epoch": 26.249412501958325,
      "grad_norm": 6.429967403411865,
      "learning_rate": 2.8125489581701398e-05,
      "loss": 1.7536,
      "step": 335100
    },
    {
      "epoch": 26.257245809180635,
      "grad_norm": 7.162869453430176,
      "learning_rate": 2.8118961825682804e-05,
      "loss": 1.6471,
      "step": 335200
    },
    {
      "epoch": 26.265079116402944,
      "grad_norm": 5.804153919219971,
      "learning_rate": 2.8112434069664213e-05,
      "loss": 1.8114,
      "step": 335300
    },
    {
      "epoch": 26.272912423625254,
      "grad_norm": 6.7973456382751465,
      "learning_rate": 2.8105906313645626e-05,
      "loss": 1.6472,
      "step": 335400
    },
    {
      "epoch": 26.280745730847563,
      "grad_norm": 5.035698890686035,
      "learning_rate": 2.809937855762703e-05,
      "loss": 1.7167,
      "step": 335500
    },
    {
      "epoch": 26.288579038069873,
      "grad_norm": 4.991462707519531,
      "learning_rate": 2.809285080160844e-05,
      "loss": 1.6973,
      "step": 335600
    },
    {
      "epoch": 26.296412345292183,
      "grad_norm": 4.787649154663086,
      "learning_rate": 2.8086323045589853e-05,
      "loss": 1.6846,
      "step": 335700
    },
    {
      "epoch": 26.304245652514492,
      "grad_norm": 4.867260932922363,
      "learning_rate": 2.807979528957126e-05,
      "loss": 1.7512,
      "step": 335800
    },
    {
      "epoch": 26.3120789597368,
      "grad_norm": 5.2879462242126465,
      "learning_rate": 2.8073267533552668e-05,
      "loss": 1.6946,
      "step": 335900
    },
    {
      "epoch": 26.31991226695911,
      "grad_norm": 5.543440341949463,
      "learning_rate": 2.8066739777534074e-05,
      "loss": 1.6778,
      "step": 336000
    },
    {
      "epoch": 26.32774557418142,
      "grad_norm": 4.333633899688721,
      "learning_rate": 2.8060212021515486e-05,
      "loss": 1.7486,
      "step": 336100
    },
    {
      "epoch": 26.33557888140373,
      "grad_norm": 7.894827842712402,
      "learning_rate": 2.8053684265496892e-05,
      "loss": 1.7747,
      "step": 336200
    },
    {
      "epoch": 26.343412188626036,
      "grad_norm": 5.457526683807373,
      "learning_rate": 2.80471565094783e-05,
      "loss": 1.708,
      "step": 336300
    },
    {
      "epoch": 26.351245495848346,
      "grad_norm": 5.390895366668701,
      "learning_rate": 2.8040628753459714e-05,
      "loss": 1.803,
      "step": 336400
    },
    {
      "epoch": 26.359078803070656,
      "grad_norm": 5.63268518447876,
      "learning_rate": 2.803410099744112e-05,
      "loss": 1.7389,
      "step": 336500
    },
    {
      "epoch": 26.366912110292965,
      "grad_norm": 5.653897285461426,
      "learning_rate": 2.802757324142253e-05,
      "loss": 1.7526,
      "step": 336600
    },
    {
      "epoch": 26.374745417515275,
      "grad_norm": 5.172877788543701,
      "learning_rate": 2.802104548540394e-05,
      "loss": 1.7159,
      "step": 336700
    },
    {
      "epoch": 26.382578724737584,
      "grad_norm": 4.365217685699463,
      "learning_rate": 2.8014517729385347e-05,
      "loss": 1.7261,
      "step": 336800
    },
    {
      "epoch": 26.390412031959894,
      "grad_norm": 5.003610610961914,
      "learning_rate": 2.8007989973366756e-05,
      "loss": 1.8178,
      "step": 336900
    },
    {
      "epoch": 26.398245339182203,
      "grad_norm": 7.0313005447387695,
      "learning_rate": 2.800146221734817e-05,
      "loss": 1.6979,
      "step": 337000
    },
    {
      "epoch": 26.406078646404513,
      "grad_norm": 7.1399736404418945,
      "learning_rate": 2.7994934461329574e-05,
      "loss": 1.6248,
      "step": 337100
    },
    {
      "epoch": 26.413911953626823,
      "grad_norm": 5.712208271026611,
      "learning_rate": 2.7988406705310984e-05,
      "loss": 1.7337,
      "step": 337200
    },
    {
      "epoch": 26.421745260849132,
      "grad_norm": 5.149562835693359,
      "learning_rate": 2.7981878949292396e-05,
      "loss": 1.7098,
      "step": 337300
    },
    {
      "epoch": 26.429578568071438,
      "grad_norm": 5.629619121551514,
      "learning_rate": 2.7975351193273802e-05,
      "loss": 1.7467,
      "step": 337400
    },
    {
      "epoch": 26.437411875293748,
      "grad_norm": 5.5398664474487305,
      "learning_rate": 2.796882343725521e-05,
      "loss": 1.6826,
      "step": 337500
    },
    {
      "epoch": 26.445245182516057,
      "grad_norm": 7.009023189544678,
      "learning_rate": 2.7962295681236624e-05,
      "loss": 1.7117,
      "step": 337600
    },
    {
      "epoch": 26.453078489738367,
      "grad_norm": 5.961008548736572,
      "learning_rate": 2.795576792521803e-05,
      "loss": 1.7461,
      "step": 337700
    },
    {
      "epoch": 26.460911796960676,
      "grad_norm": 7.241573810577393,
      "learning_rate": 2.7949240169199435e-05,
      "loss": 1.6688,
      "step": 337800
    },
    {
      "epoch": 26.468745104182986,
      "grad_norm": 4.878724575042725,
      "learning_rate": 2.7942712413180844e-05,
      "loss": 1.667,
      "step": 337900
    },
    {
      "epoch": 26.476578411405296,
      "grad_norm": 5.231388092041016,
      "learning_rate": 2.7936184657162257e-05,
      "loss": 1.6219,
      "step": 338000
    },
    {
      "epoch": 26.484411718627605,
      "grad_norm": 5.1295294761657715,
      "learning_rate": 2.7929656901143663e-05,
      "loss": 1.7698,
      "step": 338100
    },
    {
      "epoch": 26.492245025849915,
      "grad_norm": 4.0431132316589355,
      "learning_rate": 2.7923129145125072e-05,
      "loss": 1.7395,
      "step": 338200
    },
    {
      "epoch": 26.500078333072224,
      "grad_norm": 5.970359802246094,
      "learning_rate": 2.7916601389106484e-05,
      "loss": 1.7418,
      "step": 338300
    },
    {
      "epoch": 26.507911640294534,
      "grad_norm": 7.195934295654297,
      "learning_rate": 2.791007363308789e-05,
      "loss": 1.6297,
      "step": 338400
    },
    {
      "epoch": 26.515744947516843,
      "grad_norm": 5.994510173797607,
      "learning_rate": 2.79035458770693e-05,
      "loss": 1.6416,
      "step": 338500
    },
    {
      "epoch": 26.52357825473915,
      "grad_norm": 6.459031581878662,
      "learning_rate": 2.7897018121050712e-05,
      "loss": 1.6986,
      "step": 338600
    },
    {
      "epoch": 26.53141156196146,
      "grad_norm": 9.91554069519043,
      "learning_rate": 2.7890490365032118e-05,
      "loss": 1.5987,
      "step": 338700
    },
    {
      "epoch": 26.53924486918377,
      "grad_norm": 7.3870086669921875,
      "learning_rate": 2.7883962609013527e-05,
      "loss": 1.6836,
      "step": 338800
    },
    {
      "epoch": 26.547078176406078,
      "grad_norm": 4.869016647338867,
      "learning_rate": 2.787743485299494e-05,
      "loss": 1.6477,
      "step": 338900
    },
    {
      "epoch": 26.554911483628388,
      "grad_norm": 6.101232528686523,
      "learning_rate": 2.7870907096976345e-05,
      "loss": 1.6881,
      "step": 339000
    },
    {
      "epoch": 26.562744790850697,
      "grad_norm": 5.6947526931762695,
      "learning_rate": 2.7864379340957754e-05,
      "loss": 1.7109,
      "step": 339100
    },
    {
      "epoch": 26.570578098073007,
      "grad_norm": 5.093760013580322,
      "learning_rate": 2.7857851584939167e-05,
      "loss": 1.7261,
      "step": 339200
    },
    {
      "epoch": 26.578411405295316,
      "grad_norm": 5.186361312866211,
      "learning_rate": 2.7851323828920572e-05,
      "loss": 1.7318,
      "step": 339300
    },
    {
      "epoch": 26.586244712517626,
      "grad_norm": 5.3078436851501465,
      "learning_rate": 2.7844796072901978e-05,
      "loss": 1.6638,
      "step": 339400
    },
    {
      "epoch": 26.594078019739936,
      "grad_norm": 5.64108943939209,
      "learning_rate": 2.7838268316883387e-05,
      "loss": 1.6834,
      "step": 339500
    },
    {
      "epoch": 26.601911326962245,
      "grad_norm": 5.064760684967041,
      "learning_rate": 2.78317405608648e-05,
      "loss": 1.7873,
      "step": 339600
    },
    {
      "epoch": 26.60974463418455,
      "grad_norm": 4.7274603843688965,
      "learning_rate": 2.7825212804846206e-05,
      "loss": 1.7578,
      "step": 339700
    },
    {
      "epoch": 26.61757794140686,
      "grad_norm": 6.423643589019775,
      "learning_rate": 2.7818685048827615e-05,
      "loss": 1.7184,
      "step": 339800
    },
    {
      "epoch": 26.62541124862917,
      "grad_norm": 6.6253132820129395,
      "learning_rate": 2.7812157292809027e-05,
      "loss": 1.7707,
      "step": 339900
    },
    {
      "epoch": 26.63324455585148,
      "grad_norm": 7.300400733947754,
      "learning_rate": 2.7805629536790433e-05,
      "loss": 1.6648,
      "step": 340000
    },
    {
      "epoch": 26.64107786307379,
      "grad_norm": 5.652677536010742,
      "learning_rate": 2.7799101780771842e-05,
      "loss": 1.6435,
      "step": 340100
    },
    {
      "epoch": 26.6489111702961,
      "grad_norm": 4.391208171844482,
      "learning_rate": 2.7792574024753255e-05,
      "loss": 1.7222,
      "step": 340200
    },
    {
      "epoch": 26.65674447751841,
      "grad_norm": 6.186180591583252,
      "learning_rate": 2.778604626873466e-05,
      "loss": 1.7435,
      "step": 340300
    },
    {
      "epoch": 26.664577784740718,
      "grad_norm": 5.3868021965026855,
      "learning_rate": 2.777951851271607e-05,
      "loss": 1.7223,
      "step": 340400
    },
    {
      "epoch": 26.672411091963028,
      "grad_norm": 5.387665271759033,
      "learning_rate": 2.7772990756697482e-05,
      "loss": 1.7067,
      "step": 340500
    },
    {
      "epoch": 26.680244399185337,
      "grad_norm": 4.987640857696533,
      "learning_rate": 2.7766463000678888e-05,
      "loss": 1.7046,
      "step": 340600
    },
    {
      "epoch": 26.688077706407647,
      "grad_norm": 3.8323729038238525,
      "learning_rate": 2.7759935244660297e-05,
      "loss": 1.7091,
      "step": 340700
    },
    {
      "epoch": 26.695911013629953,
      "grad_norm": 6.090749263763428,
      "learning_rate": 2.775340748864171e-05,
      "loss": 1.6545,
      "step": 340800
    },
    {
      "epoch": 26.703744320852262,
      "grad_norm": 5.403656959533691,
      "learning_rate": 2.7746879732623116e-05,
      "loss": 1.6619,
      "step": 340900
    },
    {
      "epoch": 26.711577628074572,
      "grad_norm": 8.189419746398926,
      "learning_rate": 2.774035197660452e-05,
      "loss": 1.7118,
      "step": 341000
    },
    {
      "epoch": 26.71941093529688,
      "grad_norm": 6.616286754608154,
      "learning_rate": 2.773382422058593e-05,
      "loss": 1.6374,
      "step": 341100
    },
    {
      "epoch": 26.72724424251919,
      "grad_norm": 4.105850696563721,
      "learning_rate": 2.7727296464567343e-05,
      "loss": 1.7591,
      "step": 341200
    },
    {
      "epoch": 26.7350775497415,
      "grad_norm": 7.900105953216553,
      "learning_rate": 2.772076870854875e-05,
      "loss": 1.7006,
      "step": 341300
    },
    {
      "epoch": 26.74291085696381,
      "grad_norm": 5.126512050628662,
      "learning_rate": 2.7714240952530158e-05,
      "loss": 1.6646,
      "step": 341400
    },
    {
      "epoch": 26.75074416418612,
      "grad_norm": 4.227035045623779,
      "learning_rate": 2.770771319651157e-05,
      "loss": 1.7293,
      "step": 341500
    },
    {
      "epoch": 26.75857747140843,
      "grad_norm": 5.051473140716553,
      "learning_rate": 2.7701185440492976e-05,
      "loss": 1.682,
      "step": 341600
    },
    {
      "epoch": 26.76641077863074,
      "grad_norm": 5.340269565582275,
      "learning_rate": 2.7694657684474385e-05,
      "loss": 1.5882,
      "step": 341700
    },
    {
      "epoch": 26.77424408585305,
      "grad_norm": 4.842848777770996,
      "learning_rate": 2.7688129928455798e-05,
      "loss": 1.671,
      "step": 341800
    },
    {
      "epoch": 26.782077393075355,
      "grad_norm": 5.32833194732666,
      "learning_rate": 2.7681602172437204e-05,
      "loss": 1.7233,
      "step": 341900
    },
    {
      "epoch": 26.789910700297664,
      "grad_norm": 6.011891841888428,
      "learning_rate": 2.7675074416418613e-05,
      "loss": 1.7788,
      "step": 342000
    },
    {
      "epoch": 26.797744007519974,
      "grad_norm": 5.6921563148498535,
      "learning_rate": 2.7668546660400025e-05,
      "loss": 1.7021,
      "step": 342100
    },
    {
      "epoch": 26.805577314742283,
      "grad_norm": 5.232210636138916,
      "learning_rate": 2.766201890438143e-05,
      "loss": 1.7481,
      "step": 342200
    },
    {
      "epoch": 26.813410621964593,
      "grad_norm": 6.034036636352539,
      "learning_rate": 2.765549114836284e-05,
      "loss": 1.6882,
      "step": 342300
    },
    {
      "epoch": 26.821243929186902,
      "grad_norm": 4.65726900100708,
      "learning_rate": 2.7648963392344253e-05,
      "loss": 1.6957,
      "step": 342400
    },
    {
      "epoch": 26.829077236409212,
      "grad_norm": 7.559910774230957,
      "learning_rate": 2.764243563632566e-05,
      "loss": 1.71,
      "step": 342500
    },
    {
      "epoch": 26.83691054363152,
      "grad_norm": 4.429887294769287,
      "learning_rate": 2.7635907880307064e-05,
      "loss": 1.6733,
      "step": 342600
    },
    {
      "epoch": 26.84474385085383,
      "grad_norm": 3.7494242191314697,
      "learning_rate": 2.7629380124288477e-05,
      "loss": 1.6732,
      "step": 342700
    },
    {
      "epoch": 26.85257715807614,
      "grad_norm": 5.645668029785156,
      "learning_rate": 2.7622852368269886e-05,
      "loss": 1.657,
      "step": 342800
    },
    {
      "epoch": 26.86041046529845,
      "grad_norm": 6.121654510498047,
      "learning_rate": 2.7616324612251292e-05,
      "loss": 1.7229,
      "step": 342900
    },
    {
      "epoch": 26.86824377252076,
      "grad_norm": 5.699758529663086,
      "learning_rate": 2.76097968562327e-05,
      "loss": 1.7637,
      "step": 343000
    },
    {
      "epoch": 26.876077079743066,
      "grad_norm": 6.204553604125977,
      "learning_rate": 2.7603269100214114e-05,
      "loss": 1.6658,
      "step": 343100
    },
    {
      "epoch": 26.883910386965375,
      "grad_norm": 3.5983197689056396,
      "learning_rate": 2.759674134419552e-05,
      "loss": 1.6574,
      "step": 343200
    },
    {
      "epoch": 26.891743694187685,
      "grad_norm": 6.405057907104492,
      "learning_rate": 2.759021358817693e-05,
      "loss": 1.7458,
      "step": 343300
    },
    {
      "epoch": 26.899577001409995,
      "grad_norm": 5.070817470550537,
      "learning_rate": 2.758368583215834e-05,
      "loss": 1.724,
      "step": 343400
    },
    {
      "epoch": 26.907410308632304,
      "grad_norm": 4.939265251159668,
      "learning_rate": 2.7577158076139747e-05,
      "loss": 1.6677,
      "step": 343500
    },
    {
      "epoch": 26.915243615854614,
      "grad_norm": 4.89586877822876,
      "learning_rate": 2.7570630320121156e-05,
      "loss": 1.7288,
      "step": 343600
    },
    {
      "epoch": 26.923076923076923,
      "grad_norm": 5.424709796905518,
      "learning_rate": 2.756410256410257e-05,
      "loss": 1.6901,
      "step": 343700
    },
    {
      "epoch": 26.930910230299233,
      "grad_norm": 5.277371406555176,
      "learning_rate": 2.7557574808083974e-05,
      "loss": 1.6499,
      "step": 343800
    },
    {
      "epoch": 26.938743537521542,
      "grad_norm": 5.680613040924072,
      "learning_rate": 2.7551047052065383e-05,
      "loss": 1.7857,
      "step": 343900
    },
    {
      "epoch": 26.946576844743852,
      "grad_norm": 5.255155086517334,
      "learning_rate": 2.7544519296046796e-05,
      "loss": 1.8021,
      "step": 344000
    },
    {
      "epoch": 26.95441015196616,
      "grad_norm": 4.59852933883667,
      "learning_rate": 2.7537991540028202e-05,
      "loss": 1.7401,
      "step": 344100
    },
    {
      "epoch": 26.962243459188468,
      "grad_norm": 8.11181926727295,
      "learning_rate": 2.7531463784009608e-05,
      "loss": 1.6927,
      "step": 344200
    },
    {
      "epoch": 26.970076766410777,
      "grad_norm": 4.072417259216309,
      "learning_rate": 2.752493602799102e-05,
      "loss": 1.7302,
      "step": 344300
    },
    {
      "epoch": 26.977910073633087,
      "grad_norm": 5.458509922027588,
      "learning_rate": 2.751840827197243e-05,
      "loss": 1.7081,
      "step": 344400
    },
    {
      "epoch": 26.985743380855396,
      "grad_norm": 6.1587395668029785,
      "learning_rate": 2.7511880515953835e-05,
      "loss": 1.7059,
      "step": 344500
    },
    {
      "epoch": 26.993576688077706,
      "grad_norm": 6.3352766036987305,
      "learning_rate": 2.7505352759935244e-05,
      "loss": 1.7563,
      "step": 344600
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.7753347158432007,
      "eval_runtime": 1.6184,
      "eval_samples_per_second": 415.233,
      "eval_steps_per_second": 415.233,
      "step": 344682
    },
    {
      "epoch": 27.0,
      "eval_loss": 1.4618293046951294,
      "eval_runtime": 31.1532,
      "eval_samples_per_second": 409.781,
      "eval_steps_per_second": 409.781,
      "step": 344682
    },
    {
      "epoch": 27.001409995300016,
      "grad_norm": 6.764384746551514,
      "learning_rate": 2.7498825003916657e-05,
      "loss": 1.6812,
      "step": 344700
    },
    {
      "epoch": 27.009243302522325,
      "grad_norm": 7.567992210388184,
      "learning_rate": 2.7492297247898062e-05,
      "loss": 1.713,
      "step": 344800
    },
    {
      "epoch": 27.017076609744635,
      "grad_norm": 4.543952465057373,
      "learning_rate": 2.748576949187947e-05,
      "loss": 1.6649,
      "step": 344900
    },
    {
      "epoch": 27.024909916966944,
      "grad_norm": 5.516236305236816,
      "learning_rate": 2.7479241735860884e-05,
      "loss": 1.6378,
      "step": 345000
    },
    {
      "epoch": 27.032743224189254,
      "grad_norm": 5.1168413162231445,
      "learning_rate": 2.747271397984229e-05,
      "loss": 1.6745,
      "step": 345100
    },
    {
      "epoch": 27.040576531411563,
      "grad_norm": 5.307172775268555,
      "learning_rate": 2.74661862238237e-05,
      "loss": 1.7259,
      "step": 345200
    },
    {
      "epoch": 27.048409838633873,
      "grad_norm": 4.903384685516357,
      "learning_rate": 2.745965846780511e-05,
      "loss": 1.6692,
      "step": 345300
    },
    {
      "epoch": 27.05624314585618,
      "grad_norm": 4.574060916900635,
      "learning_rate": 2.7453130711786517e-05,
      "loss": 1.7222,
      "step": 345400
    },
    {
      "epoch": 27.06407645307849,
      "grad_norm": 5.769471168518066,
      "learning_rate": 2.7446602955767927e-05,
      "loss": 1.7188,
      "step": 345500
    },
    {
      "epoch": 27.071909760300798,
      "grad_norm": 4.885201454162598,
      "learning_rate": 2.744007519974934e-05,
      "loss": 1.6976,
      "step": 345600
    },
    {
      "epoch": 27.079743067523108,
      "grad_norm": 7.79248046875,
      "learning_rate": 2.7433547443730745e-05,
      "loss": 1.7485,
      "step": 345700
    },
    {
      "epoch": 27.087576374745417,
      "grad_norm": 4.666546821594238,
      "learning_rate": 2.742701968771215e-05,
      "loss": 1.7256,
      "step": 345800
    },
    {
      "epoch": 27.095409681967727,
      "grad_norm": 6.366685390472412,
      "learning_rate": 2.7420491931693563e-05,
      "loss": 1.7096,
      "step": 345900
    },
    {
      "epoch": 27.103242989190036,
      "grad_norm": 4.149971008300781,
      "learning_rate": 2.7413964175674972e-05,
      "loss": 1.7242,
      "step": 346000
    },
    {
      "epoch": 27.111076296412346,
      "grad_norm": 5.861364841461182,
      "learning_rate": 2.7407436419656378e-05,
      "loss": 1.7002,
      "step": 346100
    },
    {
      "epoch": 27.118909603634656,
      "grad_norm": 5.447376251220703,
      "learning_rate": 2.7400908663637787e-05,
      "loss": 1.6454,
      "step": 346200
    },
    {
      "epoch": 27.126742910856965,
      "grad_norm": 6.911892890930176,
      "learning_rate": 2.73943809076192e-05,
      "loss": 1.7087,
      "step": 346300
    },
    {
      "epoch": 27.134576218079275,
      "grad_norm": 4.907909870147705,
      "learning_rate": 2.7387853151600606e-05,
      "loss": 1.6228,
      "step": 346400
    },
    {
      "epoch": 27.14240952530158,
      "grad_norm": 4.43391752243042,
      "learning_rate": 2.7381325395582015e-05,
      "loss": 1.6573,
      "step": 346500
    },
    {
      "epoch": 27.15024283252389,
      "grad_norm": 5.033351898193359,
      "learning_rate": 2.7374797639563427e-05,
      "loss": 1.6537,
      "step": 346600
    },
    {
      "epoch": 27.1580761397462,
      "grad_norm": 5.790465354919434,
      "learning_rate": 2.7368269883544833e-05,
      "loss": 1.6785,
      "step": 346700
    },
    {
      "epoch": 27.16590944696851,
      "grad_norm": 5.898204326629639,
      "learning_rate": 2.7361742127526242e-05,
      "loss": 1.7462,
      "step": 346800
    },
    {
      "epoch": 27.17374275419082,
      "grad_norm": 5.785309791564941,
      "learning_rate": 2.7355214371507655e-05,
      "loss": 1.7115,
      "step": 346900
    },
    {
      "epoch": 27.18157606141313,
      "grad_norm": 5.365190505981445,
      "learning_rate": 2.734868661548906e-05,
      "loss": 1.7164,
      "step": 347000
    },
    {
      "epoch": 27.189409368635438,
      "grad_norm": 5.397495746612549,
      "learning_rate": 2.734215885947047e-05,
      "loss": 1.7164,
      "step": 347100
    },
    {
      "epoch": 27.197242675857748,
      "grad_norm": 5.493373394012451,
      "learning_rate": 2.7335631103451882e-05,
      "loss": 1.7134,
      "step": 347200
    },
    {
      "epoch": 27.205075983080057,
      "grad_norm": 4.683586597442627,
      "learning_rate": 2.7329103347433288e-05,
      "loss": 1.681,
      "step": 347300
    },
    {
      "epoch": 27.212909290302367,
      "grad_norm": 2.4985454082489014,
      "learning_rate": 2.7322575591414694e-05,
      "loss": 1.7144,
      "step": 347400
    },
    {
      "epoch": 27.220742597524676,
      "grad_norm": 6.6902337074279785,
      "learning_rate": 2.7316047835396106e-05,
      "loss": 1.6477,
      "step": 347500
    },
    {
      "epoch": 27.228575904746982,
      "grad_norm": 4.212608814239502,
      "learning_rate": 2.7309520079377515e-05,
      "loss": 1.6104,
      "step": 347600
    },
    {
      "epoch": 27.236409211969292,
      "grad_norm": 6.669397830963135,
      "learning_rate": 2.730299232335892e-05,
      "loss": 1.7283,
      "step": 347700
    },
    {
      "epoch": 27.2442425191916,
      "grad_norm": 6.458008766174316,
      "learning_rate": 2.729646456734033e-05,
      "loss": 1.7201,
      "step": 347800
    },
    {
      "epoch": 27.25207582641391,
      "grad_norm": 4.886904716491699,
      "learning_rate": 2.7289936811321743e-05,
      "loss": 1.6698,
      "step": 347900
    },
    {
      "epoch": 27.25990913363622,
      "grad_norm": 6.021799087524414,
      "learning_rate": 2.728340905530315e-05,
      "loss": 1.8046,
      "step": 348000
    },
    {
      "epoch": 27.26774244085853,
      "grad_norm": 5.9532575607299805,
      "learning_rate": 2.7276881299284558e-05,
      "loss": 1.7581,
      "step": 348100
    },
    {
      "epoch": 27.27557574808084,
      "grad_norm": 4.204619884490967,
      "learning_rate": 2.727035354326597e-05,
      "loss": 1.7382,
      "step": 348200
    },
    {
      "epoch": 27.28340905530315,
      "grad_norm": 4.786262035369873,
      "learning_rate": 2.7263825787247376e-05,
      "loss": 1.7786,
      "step": 348300
    },
    {
      "epoch": 27.29124236252546,
      "grad_norm": 6.973668098449707,
      "learning_rate": 2.7257298031228785e-05,
      "loss": 1.7151,
      "step": 348400
    },
    {
      "epoch": 27.29907566974777,
      "grad_norm": 4.972959995269775,
      "learning_rate": 2.7250770275210198e-05,
      "loss": 1.724,
      "step": 348500
    },
    {
      "epoch": 27.306908976970078,
      "grad_norm": 4.753907203674316,
      "learning_rate": 2.7244242519191604e-05,
      "loss": 1.6278,
      "step": 348600
    },
    {
      "epoch": 27.314742284192388,
      "grad_norm": 5.285245418548584,
      "learning_rate": 2.7237714763173013e-05,
      "loss": 1.7296,
      "step": 348700
    },
    {
      "epoch": 27.322575591414694,
      "grad_norm": 4.653053283691406,
      "learning_rate": 2.7231187007154425e-05,
      "loss": 1.6899,
      "step": 348800
    },
    {
      "epoch": 27.330408898637003,
      "grad_norm": 4.92892599105835,
      "learning_rate": 2.722465925113583e-05,
      "loss": 1.6013,
      "step": 348900
    },
    {
      "epoch": 27.338242205859313,
      "grad_norm": 4.078545570373535,
      "learning_rate": 2.7218131495117237e-05,
      "loss": 1.6819,
      "step": 349000
    },
    {
      "epoch": 27.346075513081622,
      "grad_norm": 4.17213773727417,
      "learning_rate": 2.721160373909865e-05,
      "loss": 1.6505,
      "step": 349100
    },
    {
      "epoch": 27.353908820303932,
      "grad_norm": 4.7079033851623535,
      "learning_rate": 2.720507598308006e-05,
      "loss": 1.6544,
      "step": 349200
    },
    {
      "epoch": 27.36174212752624,
      "grad_norm": 6.9977216720581055,
      "learning_rate": 2.7198548227061464e-05,
      "loss": 1.6744,
      "step": 349300
    },
    {
      "epoch": 27.36957543474855,
      "grad_norm": 5.448028564453125,
      "learning_rate": 2.7192020471042877e-05,
      "loss": 1.686,
      "step": 349400
    },
    {
      "epoch": 27.37740874197086,
      "grad_norm": 5.269866943359375,
      "learning_rate": 2.7185492715024286e-05,
      "loss": 1.7598,
      "step": 349500
    },
    {
      "epoch": 27.38524204919317,
      "grad_norm": 4.347259521484375,
      "learning_rate": 2.7178964959005692e-05,
      "loss": 1.7043,
      "step": 349600
    },
    {
      "epoch": 27.39307535641548,
      "grad_norm": 5.855381965637207,
      "learning_rate": 2.71724372029871e-05,
      "loss": 1.6604,
      "step": 349700
    },
    {
      "epoch": 27.40090866363779,
      "grad_norm": 6.022572040557861,
      "learning_rate": 2.7165909446968514e-05,
      "loss": 1.6731,
      "step": 349800
    },
    {
      "epoch": 27.408741970860095,
      "grad_norm": 8.469236373901367,
      "learning_rate": 2.715938169094992e-05,
      "loss": 1.658,
      "step": 349900
    },
    {
      "epoch": 27.416575278082405,
      "grad_norm": 4.91775369644165,
      "learning_rate": 2.715285393493133e-05,
      "loss": 1.6401,
      "step": 350000
    },
    {
      "epoch": 27.424408585304715,
      "grad_norm": 3.7308571338653564,
      "learning_rate": 2.714632617891274e-05,
      "loss": 1.7117,
      "step": 350100
    },
    {
      "epoch": 27.432241892527024,
      "grad_norm": 6.446173191070557,
      "learning_rate": 2.7139798422894147e-05,
      "loss": 1.6648,
      "step": 350200
    },
    {
      "epoch": 27.440075199749334,
      "grad_norm": 5.128594875335693,
      "learning_rate": 2.7133270666875556e-05,
      "loss": 1.6897,
      "step": 350300
    },
    {
      "epoch": 27.447908506971643,
      "grad_norm": 4.754719257354736,
      "learning_rate": 2.712674291085697e-05,
      "loss": 1.5661,
      "step": 350400
    },
    {
      "epoch": 27.455741814193953,
      "grad_norm": 5.115499019622803,
      "learning_rate": 2.7120215154838374e-05,
      "loss": 1.6804,
      "step": 350500
    },
    {
      "epoch": 27.463575121416262,
      "grad_norm": 5.2170209884643555,
      "learning_rate": 2.711368739881978e-05,
      "loss": 1.7547,
      "step": 350600
    },
    {
      "epoch": 27.471408428638572,
      "grad_norm": 5.1109113693237305,
      "learning_rate": 2.7107159642801193e-05,
      "loss": 1.6554,
      "step": 350700
    },
    {
      "epoch": 27.47924173586088,
      "grad_norm": 4.320956707000732,
      "learning_rate": 2.7100631886782602e-05,
      "loss": 1.6667,
      "step": 350800
    },
    {
      "epoch": 27.48707504308319,
      "grad_norm": 7.345644474029541,
      "learning_rate": 2.7094104130764007e-05,
      "loss": 1.7424,
      "step": 350900
    },
    {
      "epoch": 27.4949083503055,
      "grad_norm": 4.876730918884277,
      "learning_rate": 2.708757637474542e-05,
      "loss": 1.6534,
      "step": 351000
    },
    {
      "epoch": 27.502741657527807,
      "grad_norm": 6.435165882110596,
      "learning_rate": 2.708104861872683e-05,
      "loss": 1.8049,
      "step": 351100
    },
    {
      "epoch": 27.510574964750116,
      "grad_norm": 6.283327102661133,
      "learning_rate": 2.7074520862708235e-05,
      "loss": 1.7709,
      "step": 351200
    },
    {
      "epoch": 27.518408271972426,
      "grad_norm": 5.107170104980469,
      "learning_rate": 2.7067993106689644e-05,
      "loss": 1.686,
      "step": 351300
    },
    {
      "epoch": 27.526241579194735,
      "grad_norm": 7.259622097015381,
      "learning_rate": 2.7061465350671057e-05,
      "loss": 1.7328,
      "step": 351400
    },
    {
      "epoch": 27.534074886417045,
      "grad_norm": 4.807400703430176,
      "learning_rate": 2.7054937594652462e-05,
      "loss": 1.6003,
      "step": 351500
    },
    {
      "epoch": 27.541908193639355,
      "grad_norm": 4.983867168426514,
      "learning_rate": 2.704840983863387e-05,
      "loss": 1.6397,
      "step": 351600
    },
    {
      "epoch": 27.549741500861664,
      "grad_norm": 4.943868160247803,
      "learning_rate": 2.7041882082615284e-05,
      "loss": 1.6174,
      "step": 351700
    },
    {
      "epoch": 27.557574808083974,
      "grad_norm": 5.0320539474487305,
      "learning_rate": 2.703535432659669e-05,
      "loss": 1.6777,
      "step": 351800
    },
    {
      "epoch": 27.565408115306283,
      "grad_norm": 4.831357479095459,
      "learning_rate": 2.70288265705781e-05,
      "loss": 1.6093,
      "step": 351900
    },
    {
      "epoch": 27.573241422528593,
      "grad_norm": 7.205153465270996,
      "learning_rate": 2.702229881455951e-05,
      "loss": 1.6645,
      "step": 352000
    },
    {
      "epoch": 27.581074729750902,
      "grad_norm": 5.636819362640381,
      "learning_rate": 2.7015771058540917e-05,
      "loss": 1.7188,
      "step": 352100
    },
    {
      "epoch": 27.58890803697321,
      "grad_norm": 5.575590133666992,
      "learning_rate": 2.7009243302522323e-05,
      "loss": 1.6562,
      "step": 352200
    },
    {
      "epoch": 27.596741344195518,
      "grad_norm": 3.92069673538208,
      "learning_rate": 2.7002715546503736e-05,
      "loss": 1.7269,
      "step": 352300
    },
    {
      "epoch": 27.604574651417828,
      "grad_norm": 5.823049068450928,
      "learning_rate": 2.6996187790485145e-05,
      "loss": 1.6669,
      "step": 352400
    },
    {
      "epoch": 27.612407958640137,
      "grad_norm": 5.813989639282227,
      "learning_rate": 2.698966003446655e-05,
      "loss": 1.69,
      "step": 352500
    },
    {
      "epoch": 27.620241265862447,
      "grad_norm": 5.322345733642578,
      "learning_rate": 2.6983132278447963e-05,
      "loss": 1.7514,
      "step": 352600
    },
    {
      "epoch": 27.628074573084756,
      "grad_norm": 3.6085498332977295,
      "learning_rate": 2.6976604522429372e-05,
      "loss": 1.6651,
      "step": 352700
    },
    {
      "epoch": 27.635907880307066,
      "grad_norm": 6.9404802322387695,
      "learning_rate": 2.6970076766410778e-05,
      "loss": 1.7733,
      "step": 352800
    },
    {
      "epoch": 27.643741187529375,
      "grad_norm": 4.944556713104248,
      "learning_rate": 2.6963549010392187e-05,
      "loss": 1.6305,
      "step": 352900
    },
    {
      "epoch": 27.651574494751685,
      "grad_norm": 4.915667533874512,
      "learning_rate": 2.69570212543736e-05,
      "loss": 1.6829,
      "step": 353000
    },
    {
      "epoch": 27.659407801973995,
      "grad_norm": 9.729304313659668,
      "learning_rate": 2.6950493498355006e-05,
      "loss": 1.7582,
      "step": 353100
    },
    {
      "epoch": 27.667241109196304,
      "grad_norm": 6.626019477844238,
      "learning_rate": 2.6943965742336415e-05,
      "loss": 1.795,
      "step": 353200
    },
    {
      "epoch": 27.67507441641861,
      "grad_norm": 6.558303356170654,
      "learning_rate": 2.6937437986317827e-05,
      "loss": 1.71,
      "step": 353300
    },
    {
      "epoch": 27.68290772364092,
      "grad_norm": 7.320735931396484,
      "learning_rate": 2.6930910230299233e-05,
      "loss": 1.7239,
      "step": 353400
    },
    {
      "epoch": 27.69074103086323,
      "grad_norm": 8.114997863769531,
      "learning_rate": 2.6924382474280642e-05,
      "loss": 1.7504,
      "step": 353500
    },
    {
      "epoch": 27.69857433808554,
      "grad_norm": 5.3506178855896,
      "learning_rate": 2.6917854718262055e-05,
      "loss": 1.6287,
      "step": 353600
    },
    {
      "epoch": 27.70640764530785,
      "grad_norm": 5.931430816650391,
      "learning_rate": 2.691132696224346e-05,
      "loss": 1.6174,
      "step": 353700
    },
    {
      "epoch": 27.714240952530158,
      "grad_norm": 6.832377910614014,
      "learning_rate": 2.6904799206224866e-05,
      "loss": 1.8324,
      "step": 353800
    },
    {
      "epoch": 27.722074259752468,
      "grad_norm": 7.334481716156006,
      "learning_rate": 2.689827145020628e-05,
      "loss": 1.7037,
      "step": 353900
    },
    {
      "epoch": 27.729907566974777,
      "grad_norm": 5.65428352355957,
      "learning_rate": 2.6891743694187688e-05,
      "loss": 1.6917,
      "step": 354000
    },
    {
      "epoch": 27.737740874197087,
      "grad_norm": 7.621372699737549,
      "learning_rate": 2.6885215938169094e-05,
      "loss": 1.7321,
      "step": 354100
    },
    {
      "epoch": 27.745574181419396,
      "grad_norm": 6.131124973297119,
      "learning_rate": 2.6878688182150506e-05,
      "loss": 1.7094,
      "step": 354200
    },
    {
      "epoch": 27.753407488641706,
      "grad_norm": 5.000133037567139,
      "learning_rate": 2.6872160426131915e-05,
      "loss": 1.7755,
      "step": 354300
    },
    {
      "epoch": 27.761240795864015,
      "grad_norm": 5.4158501625061035,
      "learning_rate": 2.686563267011332e-05,
      "loss": 1.731,
      "step": 354400
    },
    {
      "epoch": 27.76907410308632,
      "grad_norm": 4.369931697845459,
      "learning_rate": 2.6859104914094734e-05,
      "loss": 1.7775,
      "step": 354500
    },
    {
      "epoch": 27.77690741030863,
      "grad_norm": 5.320858478546143,
      "learning_rate": 2.6852577158076143e-05,
      "loss": 1.5339,
      "step": 354600
    },
    {
      "epoch": 27.78474071753094,
      "grad_norm": 6.515786170959473,
      "learning_rate": 2.684604940205755e-05,
      "loss": 1.769,
      "step": 354700
    },
    {
      "epoch": 27.79257402475325,
      "grad_norm": 5.150339603424072,
      "learning_rate": 2.6839521646038958e-05,
      "loss": 1.7004,
      "step": 354800
    },
    {
      "epoch": 27.80040733197556,
      "grad_norm": 3.775177240371704,
      "learning_rate": 2.683299389002037e-05,
      "loss": 1.692,
      "step": 354900
    },
    {
      "epoch": 27.80824063919787,
      "grad_norm": 4.969542980194092,
      "learning_rate": 2.6826466134001776e-05,
      "loss": 1.6946,
      "step": 355000
    },
    {
      "epoch": 27.81607394642018,
      "grad_norm": 6.374051570892334,
      "learning_rate": 2.6819938377983185e-05,
      "loss": 1.7763,
      "step": 355100
    },
    {
      "epoch": 27.82390725364249,
      "grad_norm": 6.423318386077881,
      "learning_rate": 2.6813410621964598e-05,
      "loss": 1.6655,
      "step": 355200
    },
    {
      "epoch": 27.831740560864798,
      "grad_norm": 3.9193267822265625,
      "learning_rate": 2.6806882865946004e-05,
      "loss": 1.7157,
      "step": 355300
    },
    {
      "epoch": 27.839573868087108,
      "grad_norm": 4.672954082489014,
      "learning_rate": 2.680035510992741e-05,
      "loss": 1.7517,
      "step": 355400
    },
    {
      "epoch": 27.847407175309417,
      "grad_norm": 5.286624431610107,
      "learning_rate": 2.6793827353908822e-05,
      "loss": 1.729,
      "step": 355500
    },
    {
      "epoch": 27.855240482531723,
      "grad_norm": 7.126539707183838,
      "learning_rate": 2.678729959789023e-05,
      "loss": 1.6803,
      "step": 355600
    },
    {
      "epoch": 27.863073789754033,
      "grad_norm": 6.713520050048828,
      "learning_rate": 2.6780771841871637e-05,
      "loss": 1.5918,
      "step": 355700
    },
    {
      "epoch": 27.870907096976342,
      "grad_norm": 5.95102596282959,
      "learning_rate": 2.677424408585305e-05,
      "loss": 1.6961,
      "step": 355800
    },
    {
      "epoch": 27.878740404198652,
      "grad_norm": 7.8874192237854,
      "learning_rate": 2.676771632983446e-05,
      "loss": 1.7491,
      "step": 355900
    },
    {
      "epoch": 27.88657371142096,
      "grad_norm": 4.541885852813721,
      "learning_rate": 2.6761188573815864e-05,
      "loss": 1.7061,
      "step": 356000
    },
    {
      "epoch": 27.89440701864327,
      "grad_norm": 5.530547142028809,
      "learning_rate": 2.6754660817797277e-05,
      "loss": 1.7444,
      "step": 356100
    },
    {
      "epoch": 27.90224032586558,
      "grad_norm": 4.634436130523682,
      "learning_rate": 2.6748133061778686e-05,
      "loss": 1.76,
      "step": 356200
    },
    {
      "epoch": 27.91007363308789,
      "grad_norm": 5.591368675231934,
      "learning_rate": 2.6741605305760092e-05,
      "loss": 1.7976,
      "step": 356300
    },
    {
      "epoch": 27.9179069403102,
      "grad_norm": 3.6085023880004883,
      "learning_rate": 2.67350775497415e-05,
      "loss": 1.7412,
      "step": 356400
    },
    {
      "epoch": 27.92574024753251,
      "grad_norm": 4.850705623626709,
      "learning_rate": 2.6728549793722913e-05,
      "loss": 1.7098,
      "step": 356500
    },
    {
      "epoch": 27.93357355475482,
      "grad_norm": 6.251829147338867,
      "learning_rate": 2.672202203770432e-05,
      "loss": 1.6695,
      "step": 356600
    },
    {
      "epoch": 27.94140686197713,
      "grad_norm": 5.3005900382995605,
      "learning_rate": 2.671549428168573e-05,
      "loss": 1.6117,
      "step": 356700
    },
    {
      "epoch": 27.949240169199435,
      "grad_norm": 6.148691177368164,
      "learning_rate": 2.670896652566714e-05,
      "loss": 1.6512,
      "step": 356800
    },
    {
      "epoch": 27.957073476421744,
      "grad_norm": 6.291402339935303,
      "learning_rate": 2.6702438769648547e-05,
      "loss": 1.7378,
      "step": 356900
    },
    {
      "epoch": 27.964906783644054,
      "grad_norm": 7.472916126251221,
      "learning_rate": 2.6695911013629952e-05,
      "loss": 1.7948,
      "step": 357000
    },
    {
      "epoch": 27.972740090866363,
      "grad_norm": 6.49450159072876,
      "learning_rate": 2.6689383257611365e-05,
      "loss": 1.7703,
      "step": 357100
    },
    {
      "epoch": 27.980573398088673,
      "grad_norm": 6.448363304138184,
      "learning_rate": 2.6682855501592774e-05,
      "loss": 1.728,
      "step": 357200
    },
    {
      "epoch": 27.988406705310982,
      "grad_norm": 5.325800895690918,
      "learning_rate": 2.667632774557418e-05,
      "loss": 1.6997,
      "step": 357300
    },
    {
      "epoch": 27.996240012533292,
      "grad_norm": 5.829065799713135,
      "learning_rate": 2.6669799989555592e-05,
      "loss": 1.5837,
      "step": 357400
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.7834792137145996,
      "eval_runtime": 3.0108,
      "eval_samples_per_second": 223.199,
      "eval_steps_per_second": 223.199,
      "step": 357448
    },
    {
      "epoch": 28.0,
      "eval_loss": 1.4587836265563965,
      "eval_runtime": 37.4366,
      "eval_samples_per_second": 341.003,
      "eval_steps_per_second": 341.003,
      "step": 357448
    },
    {
      "epoch": 28.0040733197556,
      "grad_norm": 5.486781597137451,
      "learning_rate": 2.6663272233537e-05,
      "loss": 1.7249,
      "step": 357500
    },
    {
      "epoch": 28.01190662697791,
      "grad_norm": 6.062199115753174,
      "learning_rate": 2.6656744477518407e-05,
      "loss": 1.7045,
      "step": 357600
    },
    {
      "epoch": 28.01973993420022,
      "grad_norm": 5.337403774261475,
      "learning_rate": 2.665021672149982e-05,
      "loss": 1.597,
      "step": 357700
    },
    {
      "epoch": 28.02757324142253,
      "grad_norm": 5.906771183013916,
      "learning_rate": 2.664368896548123e-05,
      "loss": 1.6527,
      "step": 357800
    },
    {
      "epoch": 28.035406548644836,
      "grad_norm": 8.390392303466797,
      "learning_rate": 2.6637161209462635e-05,
      "loss": 1.6667,
      "step": 357900
    },
    {
      "epoch": 28.043239855867146,
      "grad_norm": 5.615345478057861,
      "learning_rate": 2.6630633453444044e-05,
      "loss": 1.6727,
      "step": 358000
    },
    {
      "epoch": 28.051073163089455,
      "grad_norm": 7.000498294830322,
      "learning_rate": 2.6624105697425457e-05,
      "loss": 1.6903,
      "step": 358100
    },
    {
      "epoch": 28.058906470311765,
      "grad_norm": 6.244174003601074,
      "learning_rate": 2.6617577941406862e-05,
      "loss": 1.7094,
      "step": 358200
    },
    {
      "epoch": 28.066739777534075,
      "grad_norm": 5.120382785797119,
      "learning_rate": 2.661105018538827e-05,
      "loss": 1.7304,
      "step": 358300
    },
    {
      "epoch": 28.074573084756384,
      "grad_norm": 5.487682819366455,
      "learning_rate": 2.6604522429369684e-05,
      "loss": 1.7105,
      "step": 358400
    },
    {
      "epoch": 28.082406391978694,
      "grad_norm": 2.0456371307373047,
      "learning_rate": 2.659799467335109e-05,
      "loss": 1.6336,
      "step": 358500
    },
    {
      "epoch": 28.090239699201003,
      "grad_norm": 6.461060523986816,
      "learning_rate": 2.6591466917332496e-05,
      "loss": 1.6671,
      "step": 358600
    },
    {
      "epoch": 28.098073006423313,
      "grad_norm": 6.244451522827148,
      "learning_rate": 2.6584939161313908e-05,
      "loss": 1.6313,
      "step": 358700
    },
    {
      "epoch": 28.105906313645622,
      "grad_norm": 6.692719459533691,
      "learning_rate": 2.6578411405295317e-05,
      "loss": 1.6739,
      "step": 358800
    },
    {
      "epoch": 28.113739620867932,
      "grad_norm": 5.092113018035889,
      "learning_rate": 2.6571883649276723e-05,
      "loss": 1.6958,
      "step": 358900
    },
    {
      "epoch": 28.121572928090238,
      "grad_norm": 6.0093889236450195,
      "learning_rate": 2.6565355893258136e-05,
      "loss": 1.6861,
      "step": 359000
    },
    {
      "epoch": 28.129406235312548,
      "grad_norm": 5.332129955291748,
      "learning_rate": 2.6558828137239545e-05,
      "loss": 1.6582,
      "step": 359100
    },
    {
      "epoch": 28.137239542534857,
      "grad_norm": 5.22300386428833,
      "learning_rate": 2.655230038122095e-05,
      "loss": 1.6288,
      "step": 359200
    },
    {
      "epoch": 28.145072849757167,
      "grad_norm": 4.864418029785156,
      "learning_rate": 2.6545772625202363e-05,
      "loss": 1.6686,
      "step": 359300
    },
    {
      "epoch": 28.152906156979476,
      "grad_norm": 6.554720878601074,
      "learning_rate": 2.6539244869183772e-05,
      "loss": 1.6135,
      "step": 359400
    },
    {
      "epoch": 28.160739464201786,
      "grad_norm": 12.004694938659668,
      "learning_rate": 2.6532717113165178e-05,
      "loss": 1.7196,
      "step": 359500
    },
    {
      "epoch": 28.168572771424095,
      "grad_norm": 5.042219161987305,
      "learning_rate": 2.6526189357146587e-05,
      "loss": 1.6602,
      "step": 359600
    },
    {
      "epoch": 28.176406078646405,
      "grad_norm": 8.675390243530273,
      "learning_rate": 2.6519661601128e-05,
      "loss": 1.7073,
      "step": 359700
    },
    {
      "epoch": 28.184239385868715,
      "grad_norm": 4.524547100067139,
      "learning_rate": 2.6513133845109405e-05,
      "loss": 1.7389,
      "step": 359800
    },
    {
      "epoch": 28.192072693091024,
      "grad_norm": 6.083263397216797,
      "learning_rate": 2.6506606089090815e-05,
      "loss": 1.6031,
      "step": 359900
    },
    {
      "epoch": 28.199906000313334,
      "grad_norm": 4.092382907867432,
      "learning_rate": 2.6500078333072227e-05,
      "loss": 1.715,
      "step": 360000
    },
    {
      "epoch": 28.207739307535643,
      "grad_norm": 6.495607376098633,
      "learning_rate": 2.6493550577053633e-05,
      "loss": 1.6932,
      "step": 360100
    },
    {
      "epoch": 28.21557261475795,
      "grad_norm": 6.538754463195801,
      "learning_rate": 2.648702282103504e-05,
      "loss": 1.724,
      "step": 360200
    },
    {
      "epoch": 28.22340592198026,
      "grad_norm": 2.8199634552001953,
      "learning_rate": 2.648049506501645e-05,
      "loss": 1.688,
      "step": 360300
    },
    {
      "epoch": 28.23123922920257,
      "grad_norm": 5.328061580657959,
      "learning_rate": 2.647396730899786e-05,
      "loss": 1.6102,
      "step": 360400
    },
    {
      "epoch": 28.239072536424878,
      "grad_norm": 3.509141683578491,
      "learning_rate": 2.6467439552979266e-05,
      "loss": 1.6652,
      "step": 360500
    },
    {
      "epoch": 28.246905843647188,
      "grad_norm": 5.117928981781006,
      "learning_rate": 2.646091179696068e-05,
      "loss": 1.6523,
      "step": 360600
    },
    {
      "epoch": 28.254739150869497,
      "grad_norm": 4.931900501251221,
      "learning_rate": 2.6454384040942088e-05,
      "loss": 1.6271,
      "step": 360700
    },
    {
      "epoch": 28.262572458091807,
      "grad_norm": 5.558025360107422,
      "learning_rate": 2.6447856284923494e-05,
      "loss": 1.6144,
      "step": 360800
    },
    {
      "epoch": 28.270405765314116,
      "grad_norm": 6.995012283325195,
      "learning_rate": 2.6441328528904906e-05,
      "loss": 1.6751,
      "step": 360900
    },
    {
      "epoch": 28.278239072536426,
      "grad_norm": 5.25898551940918,
      "learning_rate": 2.6434800772886315e-05,
      "loss": 1.7705,
      "step": 361000
    },
    {
      "epoch": 28.286072379758735,
      "grad_norm": 3.836331605911255,
      "learning_rate": 2.642827301686772e-05,
      "loss": 1.6783,
      "step": 361100
    },
    {
      "epoch": 28.293905686981045,
      "grad_norm": 5.751520156860352,
      "learning_rate": 2.6421745260849134e-05,
      "loss": 1.7345,
      "step": 361200
    },
    {
      "epoch": 28.30173899420335,
      "grad_norm": 4.386177062988281,
      "learning_rate": 2.6415217504830543e-05,
      "loss": 1.6762,
      "step": 361300
    },
    {
      "epoch": 28.30957230142566,
      "grad_norm": 5.184260368347168,
      "learning_rate": 2.640868974881195e-05,
      "loss": 1.6262,
      "step": 361400
    },
    {
      "epoch": 28.31740560864797,
      "grad_norm": 7.059036731719971,
      "learning_rate": 2.6402161992793358e-05,
      "loss": 1.7006,
      "step": 361500
    },
    {
      "epoch": 28.32523891587028,
      "grad_norm": 7.77564001083374,
      "learning_rate": 2.639563423677477e-05,
      "loss": 1.6299,
      "step": 361600
    },
    {
      "epoch": 28.33307222309259,
      "grad_norm": 5.476596832275391,
      "learning_rate": 2.6389106480756176e-05,
      "loss": 1.7734,
      "step": 361700
    },
    {
      "epoch": 28.3409055303149,
      "grad_norm": 4.774062633514404,
      "learning_rate": 2.6382578724737582e-05,
      "loss": 1.7029,
      "step": 361800
    },
    {
      "epoch": 28.34873883753721,
      "grad_norm": 5.52388858795166,
      "learning_rate": 2.6376050968718994e-05,
      "loss": 1.6502,
      "step": 361900
    },
    {
      "epoch": 28.356572144759518,
      "grad_norm": 5.617105960845947,
      "learning_rate": 2.6369523212700404e-05,
      "loss": 1.7301,
      "step": 362000
    },
    {
      "epoch": 28.364405451981828,
      "grad_norm": 3.2985198497772217,
      "learning_rate": 2.636299545668181e-05,
      "loss": 1.6287,
      "step": 362100
    },
    {
      "epoch": 28.372238759204137,
      "grad_norm": 6.08480978012085,
      "learning_rate": 2.6356467700663222e-05,
      "loss": 1.5929,
      "step": 362200
    },
    {
      "epoch": 28.380072066426447,
      "grad_norm": 5.684905529022217,
      "learning_rate": 2.634993994464463e-05,
      "loss": 1.7041,
      "step": 362300
    },
    {
      "epoch": 28.387905373648753,
      "grad_norm": 6.776456356048584,
      "learning_rate": 2.6343412188626037e-05,
      "loss": 1.7976,
      "step": 362400
    },
    {
      "epoch": 28.395738680871062,
      "grad_norm": 4.9228515625,
      "learning_rate": 2.633688443260745e-05,
      "loss": 1.68,
      "step": 362500
    },
    {
      "epoch": 28.403571988093372,
      "grad_norm": 4.32758903503418,
      "learning_rate": 2.633035667658886e-05,
      "loss": 1.6956,
      "step": 362600
    },
    {
      "epoch": 28.41140529531568,
      "grad_norm": 6.372461795806885,
      "learning_rate": 2.6323828920570264e-05,
      "loss": 1.7829,
      "step": 362700
    },
    {
      "epoch": 28.41923860253799,
      "grad_norm": 5.396684646606445,
      "learning_rate": 2.6317301164551677e-05,
      "loss": 1.624,
      "step": 362800
    },
    {
      "epoch": 28.4270719097603,
      "grad_norm": 5.427470684051514,
      "learning_rate": 2.6310773408533086e-05,
      "loss": 1.5995,
      "step": 362900
    },
    {
      "epoch": 28.43490521698261,
      "grad_norm": 5.432698726654053,
      "learning_rate": 2.6304245652514492e-05,
      "loss": 1.6087,
      "step": 363000
    },
    {
      "epoch": 28.44273852420492,
      "grad_norm": 4.869606971740723,
      "learning_rate": 2.62977178964959e-05,
      "loss": 1.6723,
      "step": 363100
    },
    {
      "epoch": 28.45057183142723,
      "grad_norm": 7.078782081604004,
      "learning_rate": 2.6291190140477313e-05,
      "loss": 1.6808,
      "step": 363200
    },
    {
      "epoch": 28.45840513864954,
      "grad_norm": 6.35724401473999,
      "learning_rate": 2.628466238445872e-05,
      "loss": 1.6459,
      "step": 363300
    },
    {
      "epoch": 28.46623844587185,
      "grad_norm": 5.63402795791626,
      "learning_rate": 2.6278134628440125e-05,
      "loss": 1.7822,
      "step": 363400
    },
    {
      "epoch": 28.474071753094158,
      "grad_norm": 5.1392107009887695,
      "learning_rate": 2.6271606872421537e-05,
      "loss": 1.7265,
      "step": 363500
    },
    {
      "epoch": 28.481905060316464,
      "grad_norm": 3.861835241317749,
      "learning_rate": 2.6265079116402947e-05,
      "loss": 1.5481,
      "step": 363600
    },
    {
      "epoch": 28.489738367538774,
      "grad_norm": 5.743135452270508,
      "learning_rate": 2.6258551360384352e-05,
      "loss": 1.6719,
      "step": 363700
    },
    {
      "epoch": 28.497571674761083,
      "grad_norm": 7.171961784362793,
      "learning_rate": 2.6252023604365765e-05,
      "loss": 1.726,
      "step": 363800
    },
    {
      "epoch": 28.505404981983393,
      "grad_norm": 5.215254783630371,
      "learning_rate": 2.6245495848347174e-05,
      "loss": 1.7389,
      "step": 363900
    },
    {
      "epoch": 28.513238289205702,
      "grad_norm": 6.1023406982421875,
      "learning_rate": 2.623896809232858e-05,
      "loss": 1.7797,
      "step": 364000
    },
    {
      "epoch": 28.521071596428012,
      "grad_norm": 5.73948335647583,
      "learning_rate": 2.6232440336309992e-05,
      "loss": 1.6437,
      "step": 364100
    },
    {
      "epoch": 28.52890490365032,
      "grad_norm": 5.077038764953613,
      "learning_rate": 2.62259125802914e-05,
      "loss": 1.777,
      "step": 364200
    },
    {
      "epoch": 28.53673821087263,
      "grad_norm": 4.494650363922119,
      "learning_rate": 2.6219384824272807e-05,
      "loss": 1.6742,
      "step": 364300
    },
    {
      "epoch": 28.54457151809494,
      "grad_norm": 5.274543762207031,
      "learning_rate": 2.621285706825422e-05,
      "loss": 1.6903,
      "step": 364400
    },
    {
      "epoch": 28.55240482531725,
      "grad_norm": 8.390119552612305,
      "learning_rate": 2.620632931223563e-05,
      "loss": 1.6774,
      "step": 364500
    },
    {
      "epoch": 28.56023813253956,
      "grad_norm": 5.834065914154053,
      "learning_rate": 2.6199801556217035e-05,
      "loss": 1.7215,
      "step": 364600
    },
    {
      "epoch": 28.568071439761866,
      "grad_norm": 6.845667839050293,
      "learning_rate": 2.6193273800198444e-05,
      "loss": 1.7239,
      "step": 364700
    },
    {
      "epoch": 28.575904746984175,
      "grad_norm": 5.681276321411133,
      "learning_rate": 2.6186746044179857e-05,
      "loss": 1.6401,
      "step": 364800
    },
    {
      "epoch": 28.583738054206485,
      "grad_norm": 5.868860244750977,
      "learning_rate": 2.6180218288161262e-05,
      "loss": 1.6863,
      "step": 364900
    },
    {
      "epoch": 28.591571361428795,
      "grad_norm": 5.1723222732543945,
      "learning_rate": 2.6173690532142668e-05,
      "loss": 1.7739,
      "step": 365000
    },
    {
      "epoch": 28.599404668651104,
      "grad_norm": 5.459832191467285,
      "learning_rate": 2.616716277612408e-05,
      "loss": 1.7188,
      "step": 365100
    },
    {
      "epoch": 28.607237975873414,
      "grad_norm": 5.673518657684326,
      "learning_rate": 2.616063502010549e-05,
      "loss": 1.6899,
      "step": 365200
    },
    {
      "epoch": 28.615071283095723,
      "grad_norm": 4.674747467041016,
      "learning_rate": 2.6154107264086896e-05,
      "loss": 1.8216,
      "step": 365300
    },
    {
      "epoch": 28.622904590318033,
      "grad_norm": 4.22120475769043,
      "learning_rate": 2.6147579508068308e-05,
      "loss": 1.7445,
      "step": 365400
    },
    {
      "epoch": 28.630737897540342,
      "grad_norm": 6.858336448669434,
      "learning_rate": 2.6141051752049717e-05,
      "loss": 1.6545,
      "step": 365500
    },
    {
      "epoch": 28.638571204762652,
      "grad_norm": 4.8888702392578125,
      "learning_rate": 2.6134523996031123e-05,
      "loss": 1.5768,
      "step": 365600
    },
    {
      "epoch": 28.64640451198496,
      "grad_norm": 4.703309059143066,
      "learning_rate": 2.6127996240012536e-05,
      "loss": 1.7497,
      "step": 365700
    },
    {
      "epoch": 28.654237819207268,
      "grad_norm": 4.244599342346191,
      "learning_rate": 2.6121468483993945e-05,
      "loss": 1.6818,
      "step": 365800
    },
    {
      "epoch": 28.662071126429577,
      "grad_norm": 7.778921127319336,
      "learning_rate": 2.611494072797535e-05,
      "loss": 1.7943,
      "step": 365900
    },
    {
      "epoch": 28.669904433651887,
      "grad_norm": 6.767559051513672,
      "learning_rate": 2.6108412971956763e-05,
      "loss": 1.723,
      "step": 366000
    },
    {
      "epoch": 28.677737740874196,
      "grad_norm": 5.33215856552124,
      "learning_rate": 2.6101885215938172e-05,
      "loss": 1.7234,
      "step": 366100
    },
    {
      "epoch": 28.685571048096506,
      "grad_norm": 5.209953308105469,
      "learning_rate": 2.6095357459919578e-05,
      "loss": 1.6946,
      "step": 366200
    },
    {
      "epoch": 28.693404355318815,
      "grad_norm": 6.154477119445801,
      "learning_rate": 2.608882970390099e-05,
      "loss": 1.7457,
      "step": 366300
    },
    {
      "epoch": 28.701237662541125,
      "grad_norm": 5.537909507751465,
      "learning_rate": 2.60823019478824e-05,
      "loss": 1.6633,
      "step": 366400
    },
    {
      "epoch": 28.709070969763435,
      "grad_norm": 4.057497024536133,
      "learning_rate": 2.6075774191863805e-05,
      "loss": 1.6437,
      "step": 366500
    },
    {
      "epoch": 28.716904276985744,
      "grad_norm": 5.7715229988098145,
      "learning_rate": 2.606924643584521e-05,
      "loss": 1.6318,
      "step": 366600
    },
    {
      "epoch": 28.724737584208054,
      "grad_norm": 6.514410972595215,
      "learning_rate": 2.6062718679826624e-05,
      "loss": 1.6922,
      "step": 366700
    },
    {
      "epoch": 28.732570891430363,
      "grad_norm": 6.013428688049316,
      "learning_rate": 2.6056190923808033e-05,
      "loss": 1.6639,
      "step": 366800
    },
    {
      "epoch": 28.740404198652673,
      "grad_norm": 5.757922172546387,
      "learning_rate": 2.604966316778944e-05,
      "loss": 1.7564,
      "step": 366900
    },
    {
      "epoch": 28.74823750587498,
      "grad_norm": 5.415288925170898,
      "learning_rate": 2.604313541177085e-05,
      "loss": 1.6441,
      "step": 367000
    },
    {
      "epoch": 28.75607081309729,
      "grad_norm": 6.884953498840332,
      "learning_rate": 2.603660765575226e-05,
      "loss": 1.6908,
      "step": 367100
    },
    {
      "epoch": 28.763904120319598,
      "grad_norm": 7.709224700927734,
      "learning_rate": 2.6030079899733666e-05,
      "loss": 1.6602,
      "step": 367200
    },
    {
      "epoch": 28.771737427541908,
      "grad_norm": 5.22423791885376,
      "learning_rate": 2.602355214371508e-05,
      "loss": 1.6747,
      "step": 367300
    },
    {
      "epoch": 28.779570734764217,
      "grad_norm": 4.121147155761719,
      "learning_rate": 2.6017024387696488e-05,
      "loss": 1.7785,
      "step": 367400
    },
    {
      "epoch": 28.787404041986527,
      "grad_norm": 4.735245704650879,
      "learning_rate": 2.6010496631677894e-05,
      "loss": 1.7217,
      "step": 367500
    },
    {
      "epoch": 28.795237349208836,
      "grad_norm": 4.869054317474365,
      "learning_rate": 2.6003968875659306e-05,
      "loss": 1.6899,
      "step": 367600
    },
    {
      "epoch": 28.803070656431146,
      "grad_norm": 6.86794900894165,
      "learning_rate": 2.5997441119640715e-05,
      "loss": 1.7489,
      "step": 367700
    },
    {
      "epoch": 28.810903963653455,
      "grad_norm": 4.5364813804626465,
      "learning_rate": 2.599091336362212e-05,
      "loss": 1.7857,
      "step": 367800
    },
    {
      "epoch": 28.818737270875765,
      "grad_norm": 7.075077056884766,
      "learning_rate": 2.5984385607603534e-05,
      "loss": 1.7048,
      "step": 367900
    },
    {
      "epoch": 28.826570578098075,
      "grad_norm": 5.903591632843018,
      "learning_rate": 2.5977857851584943e-05,
      "loss": 1.7807,
      "step": 368000
    },
    {
      "epoch": 28.83440388532038,
      "grad_norm": 5.719820976257324,
      "learning_rate": 2.597133009556635e-05,
      "loss": 1.7272,
      "step": 368100
    },
    {
      "epoch": 28.84223719254269,
      "grad_norm": 6.06866979598999,
      "learning_rate": 2.5964802339547754e-05,
      "loss": 1.7342,
      "step": 368200
    },
    {
      "epoch": 28.850070499765,
      "grad_norm": 4.572632789611816,
      "learning_rate": 2.5958274583529167e-05,
      "loss": 1.6541,
      "step": 368300
    },
    {
      "epoch": 28.85790380698731,
      "grad_norm": 7.481992244720459,
      "learning_rate": 2.5951746827510576e-05,
      "loss": 1.7015,
      "step": 368400
    },
    {
      "epoch": 28.86573711420962,
      "grad_norm": 5.6272735595703125,
      "learning_rate": 2.5945219071491982e-05,
      "loss": 1.6705,
      "step": 368500
    },
    {
      "epoch": 28.87357042143193,
      "grad_norm": 6.107649803161621,
      "learning_rate": 2.5938691315473394e-05,
      "loss": 1.7733,
      "step": 368600
    },
    {
      "epoch": 28.881403728654238,
      "grad_norm": 4.511669158935547,
      "learning_rate": 2.5932163559454803e-05,
      "loss": 1.6508,
      "step": 368700
    },
    {
      "epoch": 28.889237035876548,
      "grad_norm": 4.72556734085083,
      "learning_rate": 2.592563580343621e-05,
      "loss": 1.7664,
      "step": 368800
    },
    {
      "epoch": 28.897070343098857,
      "grad_norm": 6.7794599533081055,
      "learning_rate": 2.5919108047417622e-05,
      "loss": 1.7019,
      "step": 368900
    },
    {
      "epoch": 28.904903650321167,
      "grad_norm": 6.529344081878662,
      "learning_rate": 2.591258029139903e-05,
      "loss": 1.7305,
      "step": 369000
    },
    {
      "epoch": 28.912736957543476,
      "grad_norm": 5.77842903137207,
      "learning_rate": 2.5906052535380437e-05,
      "loss": 1.7746,
      "step": 369100
    },
    {
      "epoch": 28.920570264765786,
      "grad_norm": 4.927175045013428,
      "learning_rate": 2.589952477936185e-05,
      "loss": 1.7466,
      "step": 369200
    },
    {
      "epoch": 28.928403571988092,
      "grad_norm": 5.683104991912842,
      "learning_rate": 2.589299702334326e-05,
      "loss": 1.7581,
      "step": 369300
    },
    {
      "epoch": 28.9362368792104,
      "grad_norm": 6.157924175262451,
      "learning_rate": 2.5886469267324664e-05,
      "loss": 1.8288,
      "step": 369400
    },
    {
      "epoch": 28.94407018643271,
      "grad_norm": 6.437180995941162,
      "learning_rate": 2.5879941511306077e-05,
      "loss": 1.7115,
      "step": 369500
    },
    {
      "epoch": 28.95190349365502,
      "grad_norm": 4.716246128082275,
      "learning_rate": 2.5873413755287486e-05,
      "loss": 1.6664,
      "step": 369600
    },
    {
      "epoch": 28.95973680087733,
      "grad_norm": 5.327229022979736,
      "learning_rate": 2.586688599926889e-05,
      "loss": 1.6727,
      "step": 369700
    },
    {
      "epoch": 28.96757010809964,
      "grad_norm": 4.613111972808838,
      "learning_rate": 2.5860358243250297e-05,
      "loss": 1.7274,
      "step": 369800
    },
    {
      "epoch": 28.97540341532195,
      "grad_norm": 3.6670103073120117,
      "learning_rate": 2.585383048723171e-05,
      "loss": 1.7688,
      "step": 369900
    },
    {
      "epoch": 28.98323672254426,
      "grad_norm": 6.0836663246154785,
      "learning_rate": 2.584730273121312e-05,
      "loss": 1.6608,
      "step": 370000
    },
    {
      "epoch": 28.99107002976657,
      "grad_norm": 6.96077299118042,
      "learning_rate": 2.5840774975194525e-05,
      "loss": 1.6794,
      "step": 370100
    },
    {
      "epoch": 28.998903336988878,
      "grad_norm": 4.301037788391113,
      "learning_rate": 2.5834247219175937e-05,
      "loss": 1.6647,
      "step": 370200
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.7793686389923096,
      "eval_runtime": 1.4714,
      "eval_samples_per_second": 456.693,
      "eval_steps_per_second": 456.693,
      "step": 370214
    },
    {
      "epoch": 29.0,
      "eval_loss": 1.4529783725738525,
      "eval_runtime": 27.2277,
      "eval_samples_per_second": 468.861,
      "eval_steps_per_second": 468.861,
      "step": 370214
    },
    {
      "epoch": 29.006736644211188,
      "grad_norm": 6.124927520751953,
      "learning_rate": 2.5827719463157347e-05,
      "loss": 1.6091,
      "step": 370300
    },
    {
      "epoch": 29.014569951433494,
      "grad_norm": 5.375725746154785,
      "learning_rate": 2.5821191707138752e-05,
      "loss": 1.647,
      "step": 370400
    },
    {
      "epoch": 29.022403258655803,
      "grad_norm": 4.484030246734619,
      "learning_rate": 2.5814663951120165e-05,
      "loss": 1.7287,
      "step": 370500
    },
    {
      "epoch": 29.030236565878113,
      "grad_norm": 5.88552713394165,
      "learning_rate": 2.5808136195101574e-05,
      "loss": 1.6102,
      "step": 370600
    },
    {
      "epoch": 29.038069873100422,
      "grad_norm": 5.201335430145264,
      "learning_rate": 2.580160843908298e-05,
      "loss": 1.6526,
      "step": 370700
    },
    {
      "epoch": 29.045903180322732,
      "grad_norm": 5.089762210845947,
      "learning_rate": 2.5795080683064392e-05,
      "loss": 1.7267,
      "step": 370800
    },
    {
      "epoch": 29.05373648754504,
      "grad_norm": 6.757511138916016,
      "learning_rate": 2.57885529270458e-05,
      "loss": 1.7922,
      "step": 370900
    },
    {
      "epoch": 29.06156979476735,
      "grad_norm": 7.184313774108887,
      "learning_rate": 2.5782025171027207e-05,
      "loss": 1.5935,
      "step": 371000
    },
    {
      "epoch": 29.06940310198966,
      "grad_norm": 4.3246917724609375,
      "learning_rate": 2.577549741500862e-05,
      "loss": 1.6479,
      "step": 371100
    },
    {
      "epoch": 29.07723640921197,
      "grad_norm": 5.177892208099365,
      "learning_rate": 2.576896965899003e-05,
      "loss": 1.6413,
      "step": 371200
    },
    {
      "epoch": 29.08506971643428,
      "grad_norm": 7.288870811462402,
      "learning_rate": 2.5762441902971435e-05,
      "loss": 1.7018,
      "step": 371300
    },
    {
      "epoch": 29.09290302365659,
      "grad_norm": 4.623996734619141,
      "learning_rate": 2.575591414695284e-05,
      "loss": 1.6853,
      "step": 371400
    },
    {
      "epoch": 29.100736330878895,
      "grad_norm": 5.461727142333984,
      "learning_rate": 2.5749386390934253e-05,
      "loss": 1.7686,
      "step": 371500
    },
    {
      "epoch": 29.108569638101205,
      "grad_norm": 6.592688083648682,
      "learning_rate": 2.5742858634915662e-05,
      "loss": 1.637,
      "step": 371600
    },
    {
      "epoch": 29.116402945323514,
      "grad_norm": 4.644654750823975,
      "learning_rate": 2.5736330878897068e-05,
      "loss": 1.7144,
      "step": 371700
    },
    {
      "epoch": 29.124236252545824,
      "grad_norm": 4.136919975280762,
      "learning_rate": 2.572980312287848e-05,
      "loss": 1.6921,
      "step": 371800
    },
    {
      "epoch": 29.132069559768134,
      "grad_norm": 5.392033576965332,
      "learning_rate": 2.572327536685989e-05,
      "loss": 1.7393,
      "step": 371900
    },
    {
      "epoch": 29.139902866990443,
      "grad_norm": 5.334928512573242,
      "learning_rate": 2.5716747610841295e-05,
      "loss": 1.7033,
      "step": 372000
    },
    {
      "epoch": 29.147736174212753,
      "grad_norm": 5.341820240020752,
      "learning_rate": 2.5710219854822708e-05,
      "loss": 1.682,
      "step": 372100
    },
    {
      "epoch": 29.155569481435062,
      "grad_norm": 6.059112071990967,
      "learning_rate": 2.5703692098804117e-05,
      "loss": 1.7064,
      "step": 372200
    },
    {
      "epoch": 29.163402788657372,
      "grad_norm": 5.452824115753174,
      "learning_rate": 2.5697164342785523e-05,
      "loss": 1.6816,
      "step": 372300
    },
    {
      "epoch": 29.17123609587968,
      "grad_norm": 4.638633728027344,
      "learning_rate": 2.5690636586766935e-05,
      "loss": 1.7728,
      "step": 372400
    },
    {
      "epoch": 29.17906940310199,
      "grad_norm": 5.700345516204834,
      "learning_rate": 2.5684108830748345e-05,
      "loss": 1.6586,
      "step": 372500
    },
    {
      "epoch": 29.1869027103243,
      "grad_norm": 6.339023113250732,
      "learning_rate": 2.567758107472975e-05,
      "loss": 1.6837,
      "step": 372600
    },
    {
      "epoch": 29.194736017546607,
      "grad_norm": 3.2011561393737793,
      "learning_rate": 2.5671053318711163e-05,
      "loss": 1.7067,
      "step": 372700
    },
    {
      "epoch": 29.202569324768916,
      "grad_norm": 4.511817455291748,
      "learning_rate": 2.5664525562692572e-05,
      "loss": 1.6576,
      "step": 372800
    },
    {
      "epoch": 29.210402631991226,
      "grad_norm": 5.255807399749756,
      "learning_rate": 2.5657997806673978e-05,
      "loss": 1.7414,
      "step": 372900
    },
    {
      "epoch": 29.218235939213535,
      "grad_norm": 8.8230562210083,
      "learning_rate": 2.565147005065539e-05,
      "loss": 1.7408,
      "step": 373000
    },
    {
      "epoch": 29.226069246435845,
      "grad_norm": 7.772135257720947,
      "learning_rate": 2.5644942294636796e-05,
      "loss": 1.6473,
      "step": 373100
    },
    {
      "epoch": 29.233902553658154,
      "grad_norm": 5.703039169311523,
      "learning_rate": 2.5638414538618205e-05,
      "loss": 1.699,
      "step": 373200
    },
    {
      "epoch": 29.241735860880464,
      "grad_norm": 6.511643409729004,
      "learning_rate": 2.563188678259961e-05,
      "loss": 1.6622,
      "step": 373300
    },
    {
      "epoch": 29.249569168102774,
      "grad_norm": 5.782864570617676,
      "learning_rate": 2.5625359026581024e-05,
      "loss": 1.6218,
      "step": 373400
    },
    {
      "epoch": 29.257402475325083,
      "grad_norm": 7.013703346252441,
      "learning_rate": 2.5618831270562433e-05,
      "loss": 1.7798,
      "step": 373500
    },
    {
      "epoch": 29.265235782547393,
      "grad_norm": 5.590566158294678,
      "learning_rate": 2.561230351454384e-05,
      "loss": 1.6602,
      "step": 373600
    },
    {
      "epoch": 29.273069089769702,
      "grad_norm": 7.1688923835754395,
      "learning_rate": 2.560577575852525e-05,
      "loss": 1.7159,
      "step": 373700
    },
    {
      "epoch": 29.28090239699201,
      "grad_norm": 4.887495040893555,
      "learning_rate": 2.559924800250666e-05,
      "loss": 1.6363,
      "step": 373800
    },
    {
      "epoch": 29.288735704214318,
      "grad_norm": 4.652416229248047,
      "learning_rate": 2.5592720246488066e-05,
      "loss": 1.6358,
      "step": 373900
    },
    {
      "epoch": 29.296569011436628,
      "grad_norm": 7.345946311950684,
      "learning_rate": 2.558619249046948e-05,
      "loss": 1.6316,
      "step": 374000
    },
    {
      "epoch": 29.304402318658937,
      "grad_norm": 6.058568477630615,
      "learning_rate": 2.5579664734450888e-05,
      "loss": 1.6208,
      "step": 374100
    },
    {
      "epoch": 29.312235625881247,
      "grad_norm": 5.8202128410339355,
      "learning_rate": 2.5573136978432294e-05,
      "loss": 1.7677,
      "step": 374200
    },
    {
      "epoch": 29.320068933103556,
      "grad_norm": 5.745121002197266,
      "learning_rate": 2.5566609222413706e-05,
      "loss": 1.6505,
      "step": 374300
    },
    {
      "epoch": 29.327902240325866,
      "grad_norm": 4.6609086990356445,
      "learning_rate": 2.5560081466395115e-05,
      "loss": 1.7226,
      "step": 374400
    },
    {
      "epoch": 29.335735547548175,
      "grad_norm": 5.306327819824219,
      "learning_rate": 2.555355371037652e-05,
      "loss": 1.7212,
      "step": 374500
    },
    {
      "epoch": 29.343568854770485,
      "grad_norm": 5.8936896324157715,
      "learning_rate": 2.5547025954357934e-05,
      "loss": 1.6263,
      "step": 374600
    },
    {
      "epoch": 29.351402161992795,
      "grad_norm": 6.488405227661133,
      "learning_rate": 2.554049819833934e-05,
      "loss": 1.6055,
      "step": 374700
    },
    {
      "epoch": 29.359235469215104,
      "grad_norm": 5.830362319946289,
      "learning_rate": 2.553397044232075e-05,
      "loss": 1.6878,
      "step": 374800
    },
    {
      "epoch": 29.367068776437414,
      "grad_norm": 7.02388334274292,
      "learning_rate": 2.5527442686302154e-05,
      "loss": 1.7038,
      "step": 374900
    },
    {
      "epoch": 29.37490208365972,
      "grad_norm": 5.376426696777344,
      "learning_rate": 2.5520914930283567e-05,
      "loss": 1.6356,
      "step": 375000
    },
    {
      "epoch": 29.38273539088203,
      "grad_norm": 7.558687210083008,
      "learning_rate": 2.5514387174264976e-05,
      "loss": 1.7829,
      "step": 375100
    },
    {
      "epoch": 29.39056869810434,
      "grad_norm": 4.346045017242432,
      "learning_rate": 2.5507859418246382e-05,
      "loss": 1.7212,
      "step": 375200
    },
    {
      "epoch": 29.39840200532665,
      "grad_norm": 4.780757427215576,
      "learning_rate": 2.5501331662227794e-05,
      "loss": 1.7593,
      "step": 375300
    },
    {
      "epoch": 29.406235312548958,
      "grad_norm": 4.948519229888916,
      "learning_rate": 2.5494803906209203e-05,
      "loss": 1.7517,
      "step": 375400
    },
    {
      "epoch": 29.414068619771268,
      "grad_norm": 4.494518756866455,
      "learning_rate": 2.548827615019061e-05,
      "loss": 1.7272,
      "step": 375500
    },
    {
      "epoch": 29.421901926993577,
      "grad_norm": 6.978155612945557,
      "learning_rate": 2.5481748394172022e-05,
      "loss": 1.6692,
      "step": 375600
    },
    {
      "epoch": 29.429735234215887,
      "grad_norm": 5.6708197593688965,
      "learning_rate": 2.547522063815343e-05,
      "loss": 1.6953,
      "step": 375700
    },
    {
      "epoch": 29.437568541438196,
      "grad_norm": 6.136294841766357,
      "learning_rate": 2.5468692882134837e-05,
      "loss": 1.639,
      "step": 375800
    },
    {
      "epoch": 29.445401848660506,
      "grad_norm": 4.653401851654053,
      "learning_rate": 2.546216512611625e-05,
      "loss": 1.6854,
      "step": 375900
    },
    {
      "epoch": 29.453235155882815,
      "grad_norm": 5.145391464233398,
      "learning_rate": 2.545563737009766e-05,
      "loss": 1.7495,
      "step": 376000
    },
    {
      "epoch": 29.46106846310512,
      "grad_norm": 4.745116710662842,
      "learning_rate": 2.5449109614079064e-05,
      "loss": 1.6489,
      "step": 376100
    },
    {
      "epoch": 29.46890177032743,
      "grad_norm": 6.383509159088135,
      "learning_rate": 2.5442581858060477e-05,
      "loss": 1.6271,
      "step": 376200
    },
    {
      "epoch": 29.47673507754974,
      "grad_norm": 4.706444263458252,
      "learning_rate": 2.5436054102041882e-05,
      "loss": 1.7349,
      "step": 376300
    },
    {
      "epoch": 29.48456838477205,
      "grad_norm": 6.301786422729492,
      "learning_rate": 2.542952634602329e-05,
      "loss": 1.6569,
      "step": 376400
    },
    {
      "epoch": 29.49240169199436,
      "grad_norm": 5.516454696655273,
      "learning_rate": 2.5422998590004697e-05,
      "loss": 1.6235,
      "step": 376500
    },
    {
      "epoch": 29.50023499921667,
      "grad_norm": 5.5239338874816895,
      "learning_rate": 2.541647083398611e-05,
      "loss": 1.6652,
      "step": 376600
    },
    {
      "epoch": 29.50806830643898,
      "grad_norm": 6.112838268280029,
      "learning_rate": 2.540994307796752e-05,
      "loss": 1.7585,
      "step": 376700
    },
    {
      "epoch": 29.51590161366129,
      "grad_norm": 6.375651836395264,
      "learning_rate": 2.5403415321948925e-05,
      "loss": 1.6345,
      "step": 376800
    },
    {
      "epoch": 29.523734920883598,
      "grad_norm": 6.408563613891602,
      "learning_rate": 2.5396887565930337e-05,
      "loss": 1.6787,
      "step": 376900
    },
    {
      "epoch": 29.531568228105908,
      "grad_norm": 7.702313423156738,
      "learning_rate": 2.5390359809911747e-05,
      "loss": 1.6715,
      "step": 377000
    },
    {
      "epoch": 29.539401535328217,
      "grad_norm": 6.576248645782471,
      "learning_rate": 2.5383832053893152e-05,
      "loss": 1.7249,
      "step": 377100
    },
    {
      "epoch": 29.547234842550523,
      "grad_norm": 5.234611511230469,
      "learning_rate": 2.5377304297874565e-05,
      "loss": 1.7614,
      "step": 377200
    },
    {
      "epoch": 29.555068149772833,
      "grad_norm": 3.909492015838623,
      "learning_rate": 2.5370776541855974e-05,
      "loss": 1.6638,
      "step": 377300
    },
    {
      "epoch": 29.562901456995142,
      "grad_norm": 4.587380409240723,
      "learning_rate": 2.536424878583738e-05,
      "loss": 1.6887,
      "step": 377400
    },
    {
      "epoch": 29.570734764217452,
      "grad_norm": 5.273612022399902,
      "learning_rate": 2.5357721029818792e-05,
      "loss": 1.7253,
      "step": 377500
    },
    {
      "epoch": 29.57856807143976,
      "grad_norm": 5.52647590637207,
      "learning_rate": 2.53511932738002e-05,
      "loss": 1.6533,
      "step": 377600
    },
    {
      "epoch": 29.58640137866207,
      "grad_norm": 4.502780437469482,
      "learning_rate": 2.5344665517781607e-05,
      "loss": 1.6577,
      "step": 377700
    },
    {
      "epoch": 29.59423468588438,
      "grad_norm": 3.708059787750244,
      "learning_rate": 2.533813776176302e-05,
      "loss": 1.7216,
      "step": 377800
    },
    {
      "epoch": 29.60206799310669,
      "grad_norm": 6.457362174987793,
      "learning_rate": 2.5331610005744426e-05,
      "loss": 1.6412,
      "step": 377900
    },
    {
      "epoch": 29.609901300329,
      "grad_norm": 6.285666465759277,
      "learning_rate": 2.5325082249725835e-05,
      "loss": 1.7348,
      "step": 378000
    },
    {
      "epoch": 29.61773460755131,
      "grad_norm": 5.376612186431885,
      "learning_rate": 2.531855449370724e-05,
      "loss": 1.7794,
      "step": 378100
    },
    {
      "epoch": 29.62556791477362,
      "grad_norm": 5.541882038116455,
      "learning_rate": 2.5312026737688653e-05,
      "loss": 1.7301,
      "step": 378200
    },
    {
      "epoch": 29.633401221995925,
      "grad_norm": 5.43241548538208,
      "learning_rate": 2.5305498981670062e-05,
      "loss": 1.7424,
      "step": 378300
    },
    {
      "epoch": 29.641234529218234,
      "grad_norm": 7.056483268737793,
      "learning_rate": 2.5298971225651468e-05,
      "loss": 1.6446,
      "step": 378400
    },
    {
      "epoch": 29.649067836440544,
      "grad_norm": 4.309573173522949,
      "learning_rate": 2.529244346963288e-05,
      "loss": 1.7428,
      "step": 378500
    },
    {
      "epoch": 29.656901143662854,
      "grad_norm": 4.723440647125244,
      "learning_rate": 2.528591571361429e-05,
      "loss": 1.6681,
      "step": 378600
    },
    {
      "epoch": 29.664734450885163,
      "grad_norm": 4.867049694061279,
      "learning_rate": 2.5279387957595695e-05,
      "loss": 1.7068,
      "step": 378700
    },
    {
      "epoch": 29.672567758107473,
      "grad_norm": 5.225713729858398,
      "learning_rate": 2.5272860201577108e-05,
      "loss": 1.6798,
      "step": 378800
    },
    {
      "epoch": 29.680401065329782,
      "grad_norm": 7.123312950134277,
      "learning_rate": 2.5266332445558517e-05,
      "loss": 1.6931,
      "step": 378900
    },
    {
      "epoch": 29.688234372552092,
      "grad_norm": 5.527884483337402,
      "learning_rate": 2.5259804689539923e-05,
      "loss": 1.712,
      "step": 379000
    },
    {
      "epoch": 29.6960676797744,
      "grad_norm": 5.384900093078613,
      "learning_rate": 2.5253276933521335e-05,
      "loss": 1.707,
      "step": 379100
    },
    {
      "epoch": 29.70390098699671,
      "grad_norm": 6.428394794464111,
      "learning_rate": 2.5246749177502745e-05,
      "loss": 1.5881,
      "step": 379200
    },
    {
      "epoch": 29.71173429421902,
      "grad_norm": 6.596502780914307,
      "learning_rate": 2.524022142148415e-05,
      "loss": 1.6079,
      "step": 379300
    },
    {
      "epoch": 29.71956760144133,
      "grad_norm": 4.619134426116943,
      "learning_rate": 2.5233693665465563e-05,
      "loss": 1.7364,
      "step": 379400
    },
    {
      "epoch": 29.727400908663636,
      "grad_norm": 5.051605224609375,
      "learning_rate": 2.522716590944697e-05,
      "loss": 1.6745,
      "step": 379500
    },
    {
      "epoch": 29.735234215885946,
      "grad_norm": 5.593976020812988,
      "learning_rate": 2.5220638153428378e-05,
      "loss": 1.6799,
      "step": 379600
    },
    {
      "epoch": 29.743067523108255,
      "grad_norm": 5.268374443054199,
      "learning_rate": 2.521411039740979e-05,
      "loss": 1.5459,
      "step": 379700
    },
    {
      "epoch": 29.750900830330565,
      "grad_norm": 5.370874881744385,
      "learning_rate": 2.5207582641391196e-05,
      "loss": 1.6996,
      "step": 379800
    },
    {
      "epoch": 29.758734137552874,
      "grad_norm": 6.166988849639893,
      "learning_rate": 2.5201054885372605e-05,
      "loss": 1.6642,
      "step": 379900
    },
    {
      "epoch": 29.766567444775184,
      "grad_norm": 5.626997470855713,
      "learning_rate": 2.519452712935401e-05,
      "loss": 1.6629,
      "step": 380000
    },
    {
      "epoch": 29.774400751997494,
      "grad_norm": 4.69712495803833,
      "learning_rate": 2.5187999373335424e-05,
      "loss": 1.6672,
      "step": 380100
    },
    {
      "epoch": 29.782234059219803,
      "grad_norm": 4.732374668121338,
      "learning_rate": 2.5181471617316833e-05,
      "loss": 1.6829,
      "step": 380200
    },
    {
      "epoch": 29.790067366442113,
      "grad_norm": 4.705809593200684,
      "learning_rate": 2.517494386129824e-05,
      "loss": 1.703,
      "step": 380300
    },
    {
      "epoch": 29.797900673664422,
      "grad_norm": 4.868627548217773,
      "learning_rate": 2.516841610527965e-05,
      "loss": 1.6639,
      "step": 380400
    },
    {
      "epoch": 29.805733980886732,
      "grad_norm": 5.84834623336792,
      "learning_rate": 2.516188834926106e-05,
      "loss": 1.6251,
      "step": 380500
    },
    {
      "epoch": 29.81356728810904,
      "grad_norm": 4.670013427734375,
      "learning_rate": 2.5155360593242466e-05,
      "loss": 1.7032,
      "step": 380600
    },
    {
      "epoch": 29.821400595331347,
      "grad_norm": 4.223379135131836,
      "learning_rate": 2.514883283722388e-05,
      "loss": 1.7399,
      "step": 380700
    },
    {
      "epoch": 29.829233902553657,
      "grad_norm": 3.9830257892608643,
      "learning_rate": 2.5142305081205288e-05,
      "loss": 1.7411,
      "step": 380800
    },
    {
      "epoch": 29.837067209775967,
      "grad_norm": 4.997430324554443,
      "learning_rate": 2.5135777325186693e-05,
      "loss": 1.743,
      "step": 380900
    },
    {
      "epoch": 29.844900516998276,
      "grad_norm": 6.688953399658203,
      "learning_rate": 2.5129249569168106e-05,
      "loss": 1.7448,
      "step": 381000
    },
    {
      "epoch": 29.852733824220586,
      "grad_norm": 4.783907890319824,
      "learning_rate": 2.5122721813149512e-05,
      "loss": 1.688,
      "step": 381100
    },
    {
      "epoch": 29.860567131442895,
      "grad_norm": 5.946957588195801,
      "learning_rate": 2.511619405713092e-05,
      "loss": 1.7586,
      "step": 381200
    },
    {
      "epoch": 29.868400438665205,
      "grad_norm": 7.059560298919678,
      "learning_rate": 2.5109666301112333e-05,
      "loss": 1.6249,
      "step": 381300
    },
    {
      "epoch": 29.876233745887514,
      "grad_norm": 5.567315578460693,
      "learning_rate": 2.510313854509374e-05,
      "loss": 1.7762,
      "step": 381400
    },
    {
      "epoch": 29.884067053109824,
      "grad_norm": 5.396101951599121,
      "learning_rate": 2.509661078907515e-05,
      "loss": 1.6098,
      "step": 381500
    },
    {
      "epoch": 29.891900360332134,
      "grad_norm": 6.1680474281311035,
      "learning_rate": 2.5090083033056554e-05,
      "loss": 1.6911,
      "step": 381600
    },
    {
      "epoch": 29.899733667554443,
      "grad_norm": 4.5797343254089355,
      "learning_rate": 2.5083555277037967e-05,
      "loss": 1.6865,
      "step": 381700
    },
    {
      "epoch": 29.90756697477675,
      "grad_norm": 4.647359848022461,
      "learning_rate": 2.5077027521019376e-05,
      "loss": 1.7106,
      "step": 381800
    },
    {
      "epoch": 29.91540028199906,
      "grad_norm": 4.061176776885986,
      "learning_rate": 2.507049976500078e-05,
      "loss": 1.6795,
      "step": 381900
    },
    {
      "epoch": 29.92323358922137,
      "grad_norm": 4.941357135772705,
      "learning_rate": 2.5063972008982194e-05,
      "loss": 1.6905,
      "step": 382000
    },
    {
      "epoch": 29.931066896443678,
      "grad_norm": 5.462188243865967,
      "learning_rate": 2.5057444252963603e-05,
      "loss": 1.7137,
      "step": 382100
    },
    {
      "epoch": 29.938900203665987,
      "grad_norm": 7.590524673461914,
      "learning_rate": 2.505091649694501e-05,
      "loss": 1.6889,
      "step": 382200
    },
    {
      "epoch": 29.946733510888297,
      "grad_norm": 4.442791938781738,
      "learning_rate": 2.504438874092642e-05,
      "loss": 1.6802,
      "step": 382300
    },
    {
      "epoch": 29.954566818110607,
      "grad_norm": 4.524953842163086,
      "learning_rate": 2.503786098490783e-05,
      "loss": 1.6361,
      "step": 382400
    },
    {
      "epoch": 29.962400125332916,
      "grad_norm": 4.368067741394043,
      "learning_rate": 2.5031333228889237e-05,
      "loss": 1.798,
      "step": 382500
    },
    {
      "epoch": 29.970233432555226,
      "grad_norm": 6.706622123718262,
      "learning_rate": 2.502480547287065e-05,
      "loss": 1.7484,
      "step": 382600
    },
    {
      "epoch": 29.978066739777535,
      "grad_norm": 3.6974527835845947,
      "learning_rate": 2.5018277716852055e-05,
      "loss": 1.7443,
      "step": 382700
    },
    {
      "epoch": 29.985900046999845,
      "grad_norm": 4.332879066467285,
      "learning_rate": 2.5011749960833464e-05,
      "loss": 1.7434,
      "step": 382800
    },
    {
      "epoch": 29.99373335422215,
      "grad_norm": 4.823998928070068,
      "learning_rate": 2.5005222204814877e-05,
      "loss": 1.7119,
      "step": 382900
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.7724727392196655,
      "eval_runtime": 1.4134,
      "eval_samples_per_second": 475.454,
      "eval_steps_per_second": 475.454,
      "step": 382980
    },
    {
      "epoch": 30.0,
      "eval_loss": 1.4499377012252808,
      "eval_runtime": 26.8104,
      "eval_samples_per_second": 476.159,
      "eval_steps_per_second": 476.159,
      "step": 382980
    }
  ],
  "logging_steps": 100,
  "max_steps": 765960,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 236991210393600.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
